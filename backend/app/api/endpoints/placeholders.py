"""
ç»Ÿä¸€çš„åŸºäºAgentåŸºç¡€è®¾æ–½çš„å ä½ç¬¦API
å……åˆ†åˆ©ç”¨ç°æœ‰çš„Agentç³»ç»Ÿã€DomainæœåŠ¡å’ŒåŸºç¡€è®¾æ–½å±‚èƒ½åŠ›
"""

import logging
from typing import Any, Dict, List
from datetime import datetime

from fastapi import APIRouter, Depends, HTTPException, Query
from fastapi.responses import StreamingResponse
from sqlalchemy.orm import Session
import json
import asyncio

from app import crud
from app.api.deps import get_current_user, get_db
from app.models.user import User
from app.schemas.base import APIResponse
from app.schemas.template_placeholder import (
    TemplatePlaceholder,
    TemplatePlaceholderCreate,
    TemplatePlaceholderUpdate
)
from app.schemas.frontend_adapters import (
    adapt_placeholder_for_frontend, adapt_error_for_frontend,
    adapt_analysis_progress_for_frontend
)
from app.utils.error_validation import (
    ParameterValidator, ValidationResult, ErrorResponseBuilder
)
from app.middleware.error_handling import APIErrorHandler, create_error_response

# æ ¸å¿ƒï¼šä½¿ç”¨ç°æœ‰çš„AgentåŸºç¡€è®¾æ–½
from app.services.infrastructure.agents.facade import AgentFacade
from app.services.infrastructure.agents.types import (
    AgentInput,
    PlaceholderSpec,
    SchemaInfo,
    TaskContext,
    AgentConstraints,
)

# Domainå±‚ä¸šåŠ¡æœåŠ¡
from app.services.domain.placeholder.services.placeholder_analysis_domain_service import (
    PlaceholderAnalysisDomainService
)

# Applicationå±‚æœåŠ¡åè°ƒ
from app.services.application.placeholder.placeholder_service import PlaceholderApplicationService

from app.core.container import container

logger = logging.getLogger(__name__)
router = APIRouter()

class PlaceholderOrchestrationService:
    """
    å ä½ç¬¦ç¼–æ’æœåŠ¡
    åè°ƒDomainå±‚ä¸šåŠ¡é€»è¾‘å’ŒInfrastructureå±‚Agentç³»ç»Ÿ
    """

    def __init__(self):
        # ä½¿ç”¨ç°æœ‰çš„å®Œæ•´Agentç³»ç»Ÿ
        self.agent_facade = AgentFacade(container)

        # Domainå±‚ä¸šåŠ¡æœåŠ¡
        self.domain_service = PlaceholderAnalysisDomainService()

        # Applicationå±‚æœåŠ¡
        self.app_service = PlaceholderApplicationService()

        # Schemaç¼“å­˜ - é¿å…é‡å¤è·å–
        self._schema_cache = {}
        self._cache_ttl = 300  # 5åˆ†é’Ÿç¼“å­˜

        logger.info("ğŸš€ å ä½ç¬¦ç¼–æ’æœåŠ¡åˆå§‹åŒ–ï¼ŒåŸºäºå®Œæ•´AgentåŸºç¡€è®¾æ–½")

    async def analyze_placeholder_with_full_pipeline(
        self,
        placeholder_name: str,
        placeholder_text: str,
        template_id: str,
        data_source_id: str = None,
        template_context: Dict[str, Any] = None,
        user_id: str = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨å®Œæ•´çš„Agent Pipelineè¿›è¡Œå ä½ç¬¦åˆ†æ

        PipelineåŒ…æ‹¬ï¼š
        1. Domainå±‚ä¸šåŠ¡éœ€æ±‚åˆ†æ
        2. SchemaæŸ¥è¯¢ (ä½¿ç”¨SchemaListColumnsTool)
        3. æ™ºèƒ½SQLç”Ÿæˆ (ä½¿ç”¨SQLDraftTool + è¯­ä¹‰è¯†åˆ«)
        4. SQLéªŒè¯ä¼˜åŒ– (ä½¿ç”¨SQLValidateTool + SQLRefineTool)
        5. å¯é€‰SQLæ‰§è¡Œæµ‹è¯• (ä½¿ç”¨SQLExecuteTool)
        6. ç»“æœæŒä¹…åŒ–å’Œç¼“å­˜
        """

        try:
            logger.info(f"ğŸ” å¯åŠ¨å®Œæ•´Agent Pipelineåˆ†æ: {placeholder_name}")

            # ==========================================
            # å‰ç½®æ£€æŸ¥: å‘¨æœŸæ€§å ä½ç¬¦ç‰¹æ®Šå¤„ç†
            # ==========================================
            if self._is_period_placeholder(placeholder_text):
                logger.info(f"ğŸ• æ£€æµ‹åˆ°å‘¨æœŸæ€§å ä½ç¬¦ï¼Œä½¿ç”¨ä¸“é—¨å¤„ç†é€»è¾‘: {placeholder_name}")
                period_result = await self._handle_period_placeholder(
                    placeholder_name=placeholder_name,
                    placeholder_text=placeholder_text,
                    template_id=template_id,
                    template_context=template_context,
                    **kwargs
                )

                # åŒ…è£…å‘¨æœŸæ€§å ä½ç¬¦ç»“æœä¸ºAPIResponseæ ¼å¼
                logger.info(f"ğŸ”§ period_resultç±»å‹: {type(period_result)}, å†…å®¹é¢„è§ˆ: {str(period_result)[:200]}")
                if period_result.get("status") == "success":
                    try:
                        # æ„å»ºä¸æ™®é€šå ä½ç¬¦ç›¸åŒçš„å‰ç«¯é€‚é…ç»“æœ
                        placeholder_dict = {
                            "name": placeholder_name,
                            "text": placeholder_text,
                            "kind": "period",
                            "priority": "normal",
                            "confidence_score": period_result.get("confidence_score", 1.0)
                        }

                        logger.info(f"ğŸ”§ è°ƒç”¨ adapt_placeholder_for_frontendï¼Œè¾“å…¥: {placeholder_dict}")
                        adapted_placeholder = adapt_placeholder_for_frontend(placeholder_dict)
                        logger.info(f"ğŸ”§ adapt_placeholder_for_frontend è¿”å›æˆåŠŸ")

                        logger.info(f"ğŸ”§ è°ƒç”¨ adapt_analysis_progress_for_frontend")
                        progress_info = adapt_analysis_progress_for_frontend(
                            current_step=1,
                            total_steps=1,
                            step_name="å‘¨æœŸè®¡ç®—å®Œæˆ",
                            status="completed",
                            progress_percent=100.0
                        )
                        logger.info(f"ğŸ”§ adapt_analysis_progress_for_frontend è¿”å›æˆåŠŸ")

                    except Exception as adapt_error:
                        logger.error(f"ğŸ”§ å‰ç«¯é€‚é…å‡½æ•°è°ƒç”¨å¤±è´¥: {adapt_error}")
                        import traceback
                        logger.error(f"ğŸ”§ é€‚é…é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
                        raise adapt_error  # ä¸ä½¿ç”¨ç®€åŒ–ç»“æ„ï¼Œç›´æ¥æŠ›å‡ºé”™è¯¯

                    try:
                        # æ„å»ºæµ‹è¯•ç»“æœï¼Œç”¨äºå‰ç«¯æµ‹è¯•ç»“æœç»„ä»¶æ˜¾ç¤º
                        test_result_for_frontend = {
                            "success": True,
                            "result_type": "period_value",
                            "computed_value": period_result.get("analysis_result", {}).get("computed_value"),
                            "period_info": {
                                "start_date": period_result.get("analysis_result", {}).get("period_meta", {}).get("start_date"),
                                "end_date": period_result.get("analysis_result", {}).get("period_meta", {}).get("end_date"),
                                "period_type": period_result.get("analysis_result", {}).get("period_meta", {}).get("period"),
                                "display_value": period_result.get("analysis_result", {}).get("computed_value")
                            },
                            "message": f"å‘¨æœŸè®¡ç®—å®Œæˆï¼Œæ—¶é—´æ®µï¼š{period_result.get('analysis_result', {}).get('computed_value', '')}",
                            "execution_time_ms": 10
                        }

                        # æ„å»ºåŸå§‹ç»“æœ
                        raw_result = {
                            "placeholder": adapted_placeholder.dict() if hasattr(adapted_placeholder, 'dict') else adapted_placeholder,
                            "progress": progress_info.dict() if hasattr(progress_info, 'dict') else progress_info,
                            "analysis_result": period_result.get("analysis_result"),
                            "generated_sql": period_result.get("generated_sql"),
                            "test_result": test_result_for_frontend,  # æ·»åŠ æµ‹è¯•ç»“æœç”¨äºå‰ç«¯æ˜¾ç¤º
                            "business_validation": {},  # å‘¨æœŸæ€§å ä½ç¬¦ä¸éœ€è¦ä¸šåŠ¡éªŒè¯
                            "analyzed_at": period_result.get("analyzed_at")
                        }

                        # é€’å½’åºåˆ—åŒ–æ‰€æœ‰datetimeå¯¹è±¡
                        frontend_result = self._serialize_datetime_objects(raw_result)
                        logger.info(f"ğŸ”§ æ„å»º frontend_result æˆåŠŸ")
                    except Exception as result_error:
                        logger.error(f"ğŸ”§ æ„å»º frontend_result å¤±è´¥: {result_error}")
                        import traceback
                        logger.error(f"ğŸ”§ ç»“æœæ„å»ºé”™è¯¯å †æ ˆ: {traceback.format_exc()}")
                        raise result_error  # ä¸ä½¿ç”¨ç®€åŒ–ç»“æ„ï¼Œç›´æ¥æŠ›å‡ºé”™è¯¯

                    try:
                        logger.info(f"ğŸ”§ å³å°†è¿”å›å­—å…¸æ ¼å¼ç»“æœ")
                        # è¿”å›å­—å…¸æ ¼å¼ï¼Œè€Œä¸æ˜¯APIResponseå¯¹è±¡
                        # æ„å»ºæµ‹è¯•ç»“æœï¼ŒåŒ…å«è®¡ç®—å‡ºçš„å‘¨æœŸå€¼
                        test_result = {
                            "success": True,
                            "result_type": "period_value",
                            "computed_value": period_result.get("analysis_result", {}).get("computed_value"),
                            "period_info": {
                                "start_date": period_result.get("analysis_result", {}).get("period_meta", {}).get("start_date"),
                                "end_date": period_result.get("analysis_result", {}).get("period_meta", {}).get("end_date"),
                                "period_type": period_result.get("analysis_result", {}).get("period_meta", {}).get("period"),
                                "display_value": period_result.get("analysis_result", {}).get("computed_value")
                            },
                            "message": f"å‘¨æœŸè®¡ç®—å®Œæˆï¼Œæ—¶é—´æ®µï¼š{period_result.get('analysis_result', {}).get('computed_value', '')}"
                        }

                        dict_result = {
                            "status": "success",
                            "placeholder_name": placeholder_name,
                            "generated_sql": period_result.get("generated_sql", {}),
                            "analysis_result": period_result.get("analysis_result", {}),
                            "test_result": test_result,  # æ·»åŠ æµ‹è¯•ç»“æœç”¨äºå‰ç«¯æ˜¾ç¤º
                            "confidence_score": period_result.get("confidence_score", 1.0),
                            "analyzed_at": period_result.get("analyzed_at"),
                            "context_used": {
                                "template_context": bool(template_context),
                                "period_calculation": True,
                                "pipeline_type": "period_handler"
                            },
                            "frontend_data": frontend_result  # ä¿ç•™å‰ç«¯éœ€è¦çš„æ•°æ®
                        }
                        logger.info(f"ğŸ”§ å­—å…¸æ ¼å¼ç»“æœåˆ›å»ºæˆåŠŸ")
                        return dict_result
                    except Exception as api_error:
                        logger.error(f"ğŸ”§ ç»“æœåˆ›å»ºå¤±è´¥: {api_error}")
                        import traceback
                        logger.error(f"ğŸ”§ ç»“æœåˆ›å»ºé”™è¯¯å †æ ˆ: {traceback.format_exc()}")
                        raise api_error
                else:
                    # é”™è¯¯æƒ…å†µï¼Œè¿”å›å­—å…¸æ ¼å¼
                    error_result = {
                        "status": "error",
                        "placeholder_name": placeholder_name,
                        "error": period_result.get("error", "å‘¨æœŸæ€§å ä½ç¬¦å¤„ç†å¤±è´¥"),
                        "generated_sql": {"sql": "", placeholder_name: ""},
                        "analysis_result": {
                            "description": "å‘¨æœŸæ€§å ä½ç¬¦å¤„ç†å¤±è´¥",
                            "analysis_type": "period_placeholder_error",
                            "suggestions": ["æ£€æŸ¥æ¨¡æ¿ä¸Šä¸‹æ–‡", "éªŒè¯æ—¶é—´å‚æ•°"]
                        },
                        "confidence_score": 0.0,
                        "analyzed_at": period_result.get("analyzed_at"),
                        "context_used": {
                            "template_context": bool(template_context),
                            "period_calculation": False,
                            "pipeline_type": "period_handler_error"
                        }
                    }
                    return error_result

            # ==========================================
            # ç¬¬1æ­¥: Domainå±‚ä¸šåŠ¡éœ€æ±‚åˆ†æ
            # ==========================================
            # è§„èŒƒåŒ– template_contextï¼š
            # - è‹¥ä¸ºå­—ç¬¦ä¸²ï¼ˆæ•´ä»½æ¨¡æ¿å†…å®¹ï¼‰ï¼Œæå–â€œåŒ…å«å ä½ç¬¦çš„æ®µè½â€ä½œä¸º snippetï¼Œä¾›ä¸‹æ¸¸å‚è€ƒ
            # - è‹¥éå­—å…¸ï¼Œä»ä¿ç•™ snippetï¼Œä½†ç”¨äºè°ƒåº¦/æ—¶é—´æ¨æ–­çš„ template_context å­—æ®µè½¬ä¸ºç©ºå­—å…¸ï¼Œé¿å… .get æŠ¥é”™
            template_context_snippet = None
            if isinstance(template_context, str):
                try:
                    template_context_snippet = self._extract_placeholder_snippet(
                        template_context or "",
                        placeholder_text or "",
                        placeholder_name or ""
                    )
                except Exception as e:
                    logger.warning(f"æå–æ¨¡æ¿æ®µè½å¤±è´¥: {e}")
                logger.warning(f"template_context ä¸ºå­—ç¬¦ä¸²ï¼Œå·²æå–æ®µè½ä½œä¸º snippet")
                template_context = {}
            elif template_context and not isinstance(template_context, dict):
                logger.warning(f"template_context éå­—å…¸ç±»å‹({type(template_context)}), å·²è‡ªåŠ¨è½¬æ¢ä¸ºç©ºå­—å…¸ä»¥ç¡®ä¿å®‰å…¨")
                template_context = {}

            business_context = {
                "template_id": template_id,
                "data_source_id": data_source_id,
                "template_context": template_context or {},
                "execution_context": kwargs.get("execution_context", {}),
                "time_column": kwargs.get("time_column"),
                "data_range": kwargs.get("data_range", "day")
            }

            business_requirements = await self.domain_service.analyze_placeholder_business_requirements(
                placeholder_text=placeholder_text,
                business_context=business_context,
                user_id=user_id
            )

            logger.info(f"âœ… ä¸šåŠ¡éœ€æ±‚åˆ†æå®Œæˆ: {business_requirements.get('business_type')}")

            # ==========================================
            # ç¬¬2æ­¥: æ„å»ºAgentè¾“å…¥ï¼Œåˆ©ç”¨ç°æœ‰å·¥å…·é“¾
            # ==========================================

            # ç¡®å®šè¯­ä¹‰ç±»å‹ï¼ˆç”¨äºSQLDraftToolçš„æ™ºèƒ½ç”Ÿæˆï¼‰
            semantic_type = self._map_business_to_semantic_type(business_requirements)

            # æ„å»ºSchemaä¿¡æ¯ï¼ˆé€šè¿‡DataSourceContextè·å–çœŸå®è¡¨ç»“æ„ï¼‰
            schema_info = await self._get_schema_from_data_source_context(user_id, data_source_id)
            logger.info(f"ğŸ” [AgentInputæ„å»º] Schemaä¿¡æ¯è·å–å®Œæˆ: è¡¨æ•°é‡={len(schema_info.tables) if schema_info else 0}")
            logger.debug(f"ğŸ” [AgentInputæ„å»º] è¡¨åè¯¦æƒ…: {schema_info.tables if schema_info else []}")  # æ”¹ä¸ºdebugçº§åˆ«

            # æ„å»ºä»»åŠ¡ä¸Šä¸‹æ–‡ - ä¸ºplaceholder APIæä¾›é»˜è®¤è°ƒåº¦ä¿¡æ¯
            from datetime import datetime, timedelta
            from app.utils.time_context import TimeContextManager

            # ä¸ºplaceholder APIæä¾›é»˜è®¤çš„cronè¡¨è¾¾å¼å’Œæ—¶é—´çª—å£
            default_cron = "0 9 * * *"  # æ¯å¤©ä¸Šåˆ9ç‚¹
            current_time = datetime.now()

            # å¦‚æœæ²¡æœ‰æä¾›ä»»åŠ¡è°ƒåº¦ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤å€¼
            task_schedule = kwargs.get("task_schedule")
            if not task_schedule or not task_schedule.get("cron_expression"):
                # æ„å»ºé»˜è®¤è°ƒåº¦ä¿¡æ¯
                time_manager = TimeContextManager()
                time_context = time_manager.build_task_time_context(default_cron, current_time)

                task_schedule = {
                    "cron_expression": default_cron,
                    "timezone": kwargs.get("timezone", "Asia/Shanghai"),
                    "execution_time": current_time.isoformat(),
                    "start_date": time_context.get("data_start_time"),
                    "end_date": time_context.get("data_end_time"),
                }

                logger.info(f"ğŸ•’ ä¸ºplaceholder APIç”Ÿæˆé»˜è®¤è°ƒåº¦: {default_cron}, "
                           f"æ—¶é—´çª—å£: {task_schedule['start_date']} ~ {task_schedule['end_date']}")

            task_context = TaskContext(
                timezone=task_schedule.get("timezone", "Asia/Shanghai"),
                window={
                    "data_source_id": data_source_id,
                    "time_column": kwargs.get("time_column"),
                    "data_range": kwargs.get("data_range", "day"),
                    "task_schedule": task_schedule,
                    "start_date": task_schedule.get("start_date"),
                    "end_date": task_schedule.get("end_date"),
                    "cron_expression": task_schedule.get("cron_expression"),
                    "execution_time": task_schedule.get("execution_time")
                }
            )

            # åŠ è½½æ¨¡æ¿å†…å®¹ä»¥æä¾›æ›´å®Œæ•´çš„ä¸Šä¸‹æ–‡
            template_content = ""
            try:
                from app.db.session import get_db_session
                from app import crud

                with get_db_session() as db:
                    template_obj = crud.template.get(db, id=template_id)
                    if template_obj:
                        template_content = template_obj.content or ""
                        logger.info(f"âœ… åŠ è½½æ¨¡æ¿å†…å®¹: {len(template_content)} å­—ç¬¦")
                    else:
                        logger.warning(f"âš ï¸ æœªæ‰¾åˆ°æ¨¡æ¿: {template_id}")
            except Exception as e:
                logger.warning(f"âš ï¸ åŠ è½½æ¨¡æ¿å†…å®¹å¤±è´¥: {e}")

            # æ„å»ºAgentè¾“å…¥ - åŒ…å«å®Œæ•´ä¸Šä¸‹æ–‡ä¿¡æ¯
            agent_input = AgentInput(
                user_prompt=f"åˆ†æå ä½ç¬¦'{placeholder_name}': {placeholder_text}",
                placeholder=PlaceholderSpec(
                    id=placeholder_name,
                    description=placeholder_text,
                    type=semantic_type,
                    granularity=business_requirements.get("time_sensitivity", "daily")
                ),
                schema=schema_info,
                context=task_context,
                constraints=AgentConstraints(
                    sql_only=True,
                    output_kind="sql",
                    max_attempts=3,
                    policy_row_limit=kwargs.get("row_limit", 1000)
                ),
                template_id=template_id,
                data_source={
                    "data_source_id": data_source_id,
                    "semantic_type": semantic_type,  # ä¼ ç»™SQLDraftTool
                    "business_requirements": business_requirements,
                    "tables": schema_info.tables if schema_info else [],  # ä¼ é€’è¡¨ä¿¡æ¯
                    "available_tables": schema_info.tables if schema_info else [],  # ä¼ é€’å¯ç”¨è¡¨ä¿¡æ¯
                },
                task_driven_context={
                    "template_context": template_context or {},
                    "template_context_snippet": template_context_snippet,
                    "template_content": template_content,  # æ·»åŠ æ¨¡æ¿å†…å®¹
                    "business_context": business_context,
                    "requirements": kwargs.get("requirements", ""),
                    "top_n": business_requirements.get("top_n"),  # ç”¨äºrankingç±»å‹

                    # ğŸ“‹ é‡è¦ï¼šå ä½ç¬¦ä¸Šä¸‹æ–‡æ®µè½ï¼ˆä¸ºæ¨¡å‹æä¾›ç²¾ç¡®çš„æ–‡æœ¬ä¸Šä¸‹æ–‡ï¼‰
                    "placeholder_context_snippet": template_context_snippet,
                    "surrounding_text": template_context_snippet,  # ä¸ºæ¨¡å‹æä¾›å‘¨å›´æ–‡æœ¬ä¿¡æ¯
                    "context_extraction_success": bool(template_context_snippet),

                    # â° é‡è¦ï¼šæ—¶é—´è°ƒåº¦å’Œç»Ÿè®¡èŒƒå›´ä¿¡æ¯
                    "cron_expression": task_schedule.get("cron_expression", "0 9 * * *"),
                    "time_range": {
                        "start_date": task_schedule.get("start_date"),
                        "end_date": task_schedule.get("end_date"),
                        "time_column": kwargs.get("time_column"),
                        "timezone": task_schedule.get("timezone", "Asia/Shanghai")
                    },
                    "scheduling_info": {
                        "execution_time": task_schedule.get("execution_time"),
                        "schedule_type": self._infer_schedule_type(task_schedule.get("cron_expression", "0 9 * * *")),
                        "previous_period_desc": self._describe_previous_period(task_schedule.get("cron_expression", "0 9 * * *"))
                    },

                    # ğŸ” Schemaä¿¡æ¯ä¼ é€’ï¼ˆç¡®ä¿æ¨¡å‹èƒ½çœ‹åˆ°è¡¨ç»“æ„ï¼‰
                    "schema_context": {
                        "available_tables": schema_info.tables if schema_info else [],
                        "table_count": len(schema_info.tables) if schema_info else 0,
                        "schema_source": "DataSourceContextBuilder"
                    },

                    "placeholder_contexts": [  # æ·»åŠ å ä½ç¬¦ä¸Šä¸‹æ–‡æ•°ç»„
                        {
                            "placeholder_name": placeholder_name,
                            "placeholder_text": placeholder_text,
                            "semantic_type": semantic_type,
                            "surrounding_context": template_context_snippet,  # å ä½ç¬¦å‘¨å›´çš„æ–‡æœ¬
                            "parsed_params": {
                                "top_n": business_requirements.get("top_n"),
                                "time_sensitivity": business_requirements.get("time_sensitivity")
                            }
                        }
                    ]
                },
                user_id=user_id
            )

            # ==========================================
            # ç¬¬3æ­¥: æ‰§è¡ŒAgent Pipeline - ä½¿ç”¨ä»»åŠ¡éªŒè¯æ™ºèƒ½æ¨¡å¼
            # ==========================================
            logger.info(f"ğŸ¤– æ‰§è¡ŒAgent Pipelineï¼Œè¯­ä¹‰ç±»å‹: {semantic_type}")

            # ğŸ¯ ä½¿ç”¨ä»»åŠ¡éªŒè¯æ™ºèƒ½æ¨¡å¼ - ç»Ÿä¸€çš„SQLéªŒè¯å’Œç”Ÿæˆç³»ç»Ÿ
            logger.info(f"ğŸ¯ ä½¿ç”¨ä»»åŠ¡éªŒè¯æ™ºèƒ½æ¨¡å¼ - è‡ªåŠ¨SQLå¥åº·æ£€æŸ¥ä¸æ™ºèƒ½å›é€€")

            agent_result = await self.agent_facade.execute_task_validation(agent_input)

            # ğŸ”§ æ·»åŠ è°ƒè¯•ä¿¡æ¯
            logger.info(f"ğŸ”§ [Debug] Agentæ‰§è¡Œç»“æœ: success={agent_result.success}")
            logger.info(f"ğŸ”§ [Debug] Agent result type: {type(agent_result.result)}")
            logger.info(f"ğŸ”§ [Debug] Agent metadata type: {type(agent_result.metadata)}")
            if agent_result.result:
                logger.info(f"ğŸ”§ [Debug] Agent resultå†…å®¹(å‰100å­—ç¬¦): {str(agent_result.result)[:100]}")
            if isinstance(agent_result.metadata, dict):
                logger.info(f"ğŸ”§ [Debug] Agent metadata keys: {list(agent_result.metadata.keys())}")

            if not agent_result.success:
                logger.error(f"âŒ Agent Pipelineæ‰§è¡Œå¤±è´¥: {agent_result.metadata}")

                # ğŸ”„ æ–°å¢ï¼šæ™ºèƒ½æ¢å¤æœºåˆ¶
                recovery_result = await self._attempt_pipeline_recovery(
                    agent_input, agent_result, placeholder_name, semantic_type
                )

                # å®‰å…¨åœ°æ£€æŸ¥æ¢å¤ç»“æœ
                if isinstance(recovery_result, dict) and recovery_result.get("recovered"):
                    logger.info(f"âœ… Agent Pipelineå·²æ¢å¤: {recovery_result['method']}")
                    # ä½¿ç”¨æ¢å¤åçš„ç»“æœç»§ç»­å¤„ç†
                    agent_result = recovery_result["result"]
                else:
                    # å°è¯•éƒ¨åˆ†æˆåŠŸè¿”å›ï¼šè‹¥å­˜åœ¨å€™é€‰SQLï¼Œç›´æ¥è¿”å›ç»™å‰ç«¯æ˜¾ç¤ºï¼Œæµ‹è¯•ç»“æœæ˜¾ç¤ºéªŒè¯/æ‰§è¡ŒçŠ¶æ€
                    try:
                        meta = agent_result.metadata if isinstance(agent_result.metadata, dict) else {}
                        candidate_sql = agent_result.result or meta.get('final_sql') or meta.get('partial_result') or meta.get('current_sql')
                        if candidate_sql and isinstance(candidate_sql, str) and candidate_sql.strip():
                            # æ„å»ºå¢å¼ºçš„æµ‹è¯•ç»“æœï¼Œç¡®ä¿å‰ç«¯èƒ½æ­£ç¡®æ˜¾ç¤ºå¤±è´¥ä¿¡æ¯
                            existing_test_result = meta.get('test_result', {})
                            error_message = meta.get('error', 'éªŒè¯/æ‰§è¡Œæœªå®Œæˆ')

                            test_result = {
                                "executed": existing_test_result.get("executed", False),
                                "success": existing_test_result.get("executed", False) and existing_test_result.get("rows") is not None,
                                "rows": existing_test_result.get("rows", []),
                                "columns": existing_test_result.get("columns", []),
                                "row_count": existing_test_result.get("row_count", 0),
                                "data": existing_test_result.get("rows", []),  # å‰ç«¯æœŸæœ›çš„å­—æ®µå
                                "message": existing_test_result.get("message", error_message),
                                "error": error_message if not existing_test_result.get("executed", False) else existing_test_result.get("error"),
                                "execution_time_ms": existing_test_result.get("execution_time_ms", 0)
                            }

                            # æ„é€ éƒ¨åˆ†æˆåŠŸç»“æœï¼ˆstatus=partialï¼‰
                            partial = {
                                "status": "partial",
                                "placeholder_name": placeholder_name,
                                "generated_sql": {
                                    placeholder_name: candidate_sql,
                                    "sql": candidate_sql,
                                },
                                "test_result": test_result,
                                "analysis_result": {
                                    "description": "éƒ¨åˆ†å®Œæˆï¼šå·²ç”ŸæˆSQLï¼ŒéªŒè¯/æ‰§è¡Œä¿¡æ¯å¦‚ä¸‹",
                                    "analysis_type": "partial_agent_pipeline",
                                    "semantic_type": semantic_type,
                                    "analysis_summary": meta.get("analysis_summary", "å·²ç”ŸæˆSQLï¼Œç­‰å¾…ç”¨æˆ·ç¡®è®¤æˆ–åç»­æ‰§è¡Œ")
                                },
                                "confidence_score": meta.get("quality_score", 0.7),
                                "analyzed_at": datetime.now().isoformat()
                            }

                            # é€‚é…å‰ç«¯æ ¼å¼ï¼ˆä¸æˆåŠŸè·¯å¾„ä¸€è‡´ï¼‰
                            placeholder_dict = {
                                "text": placeholder_text,
                                "kind": partial.get("analysis_result", {}).get("semantic_type", "statistical"),
                                "confidence": partial.get("confidence_score", 0.7),
                                "needs_reanalysis": False
                            }
                            try:
                                adapted_placeholder = adapt_placeholder_for_frontend(placeholder_dict)
                            except Exception:
                                adapted_placeholder = type("_Shim", (), {"dict": lambda self=None: {
                                    "text": placeholder_text,
                                    "kind": placeholder_dict.get("kind", "statistical"),
                                    "display_name": placeholder_dict.get("kind", "statistical"),
                                    "description": "",
                                    "status": "completed",
                                    "confidence": placeholder_dict.get("confidence", 0.7),
                                    "needs_reanalysis": False,
                                    "badge_color": "default",
                                    "icon": None,
                                    "tooltip": None
                                }})()

                            progress_info = adapt_analysis_progress_for_frontend(
                                current_step=3,
                                total_steps=4,
                                step_name="å·²ç”ŸæˆSQLï¼Œå¾…æ‰§è¡Œ",
                                status="running",
                                progress_percent=75.0
                            )

                            frontend_result = {
                                "placeholder": adapted_placeholder.dict(),
                                "progress": progress_info.dict(),
                                "analysis_result": partial.get("analysis_result"),
                                "generated_sql": partial.get("generated_sql"),
                                "test_result": partial.get("test_result"),
                                "analyzed_at": partial.get("analyzed_at")
                            }

                            frontend_result = _orchestration_service._serialize_datetime_objects(frontend_result)
                            return APIResponse(
                                success=True,
                                data=frontend_result,
                                message=f"å·²ç”ŸæˆSQLï¼ˆéƒ¨åˆ†å®Œæˆï¼‰: {placeholder_name}"
                            )
                    except Exception as e:
                        logger.warning(f"æ„å»ºéƒ¨åˆ†æˆåŠŸç»“æœå¤±è´¥: {e}")

                    # æ¢å¤å¤±è´¥ä¸”æ— å€™é€‰SQLï¼šè¿”å›å¢å¼ºçš„é”™è¯¯ä¿¡æ¯
                    return self._create_enhanced_error_result(
                        placeholder_name,
                        agent_result.metadata if isinstance(agent_result.metadata, dict) else {},
                        recovery_result.get("recovery_attempts", []) if isinstance(recovery_result, dict) else []
                    )

            # ==========================================
            # ç¬¬4æ­¥: ç»“æœå¤„ç†å’Œå¢å¼º
            # ==========================================

            # ä»Agentç»“æœä¸­æå–SQLå’Œå…ƒæ•°æ®
            generated_sql = agent_result.result
            agent_metadata = agent_result.metadata if isinstance(agent_result.metadata, dict) else {}

            # ğŸ”§ æ·»åŠ æˆåŠŸè·¯å¾„è°ƒè¯•ä¿¡æ¯
            logger.info(f"ğŸ”§ [Debug] è¿›å…¥æˆåŠŸå¤„ç†åˆ†æ”¯")
            logger.info(f"ğŸ”§ [Debug] æå–çš„SQL: {generated_sql}")
            logger.info(f"ğŸ”§ [Debug] agent_metadata keys: {list(agent_metadata.keys()) if agent_metadata else 'empty'}")

            # ğŸ” è°ƒè¯•ï¼šæŸ¥çœ‹execution_summaryå’Œobservationsçš„å†…å®¹
            if "execution_summary" in agent_metadata:
                logger.info(f"ğŸ” [Debug] execution_summary: {agent_metadata['execution_summary']}")
            if "observations" in agent_metadata:
                logger.info(f"ğŸ” [Debug] observations: {agent_metadata['observations']}")

            # æå–æµ‹è¯•ç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
            # ğŸ”‘ å…³é”®ä¿®å¤ï¼šä»execution_summaryæˆ–observationsä¸­æå–SQLæ‰§è¡Œç»“æœ
            test_result = agent_metadata.get("test_result")

            # ç­–ç•¥1: ä»execution_summaryæå–
            if not test_result and "execution_summary" in agent_metadata:
                exec_summary = agent_metadata.get("execution_summary", "")
                logger.info(f"ğŸ” [Debug] å°è¯•ä»execution_summaryæå–ï¼Œå†…å®¹: {exec_summary}")
                # æ£€æŸ¥æ˜¯å¦åŒ…å«æˆåŠŸæ‰§è¡Œçš„å…³é”®è¯
                if isinstance(exec_summary, str) and ("æˆåŠŸ" in exec_summary or "è¿”å›" in exec_summary or "rows" in exec_summary.lower()):
                    test_result = {
                        "executed": True,
                        "success": True,
                        "message": exec_summary,
                        "source": "execution_summary"
                    }
                    logger.info(f"âœ… [Debug] ä»execution_summaryæå–åˆ°æµ‹è¯•ç»“æœ")

            # ç­–ç•¥2: ä»observationsæå–ï¼ˆobservationsæ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼‰
            if not test_result and "observations" in agent_metadata:
                observations = agent_metadata.get("observations", [])
                logger.info(f"ğŸ” [Debug] æ£€æŸ¥observationsï¼Œç±»å‹: {type(observations)}, æ•°é‡: {len(observations) if isinstance(observations, list) else 'N/A'}")

                # observationsæ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼ŒæŸ¥æ‰¾åŒ…å«"sql.execute"æˆ–"æ‰§è¡ŒSQL"çš„è®°å½•
                for idx, obs in enumerate(observations):
                    obs_str = str(obs)
                    if "sql.execute" in obs_str or "æ‰§è¡ŒSQL" in obs_str or "MySQLæŸ¥è¯¢æ‰§è¡ŒæˆåŠŸ" in obs_str:
                        # åˆ¤æ–­æ˜¯å¦æˆåŠŸ
                        is_success = "æˆåŠŸ" in obs_str or "è¿”å›" in obs_str
                        test_result = {
                            "executed": True,
                            "success": is_success,
                            "message": obs_str,
                            "source": f"observations[{idx}]"
                        }
                        logger.info(f"âœ… [Debug] ä»observations[{idx}]æå–åˆ°æµ‹è¯•ç»“æœ: {obs_str[:100]}")
                        break

            # ç­–ç•¥3: å¦‚æœPTAVæˆåŠŸä½†æ²¡æœ‰æ˜ç¡®çš„test_resultï¼Œæ¨æ–­ä¸ºå·²æ‰§è¡ŒæˆåŠŸ
            if not test_result and agent_result.success and generated_sql:
                # AgentæˆåŠŸè¿”å›äº†SQLï¼Œä¸”æ˜¯PTAVæ¨¡å¼ï¼ˆæœ‰observationsï¼‰ï¼Œæ¨æ–­å·²æ‰§è¡Œ
                if agent_metadata.get("observations"):
                    test_result = {
                        "executed": True,
                        "success": True,
                        "message": "Agent PipelineæˆåŠŸç”Ÿæˆå¹¶éªŒè¯SQL",
                        "source": "inferred_from_success"
                    }
                    logger.info(f"âœ… [Debug] ä»AgentæˆåŠŸçŠ¶æ€æ¨æ–­æµ‹è¯•ç»“æœ")

            # æœ€åçš„é»˜è®¤å€¼
            if not test_result:
                test_result = {
                    "executed": False,
                    "success": False,
                    "message": "æœªæ‰¾åˆ°SQLæ‰§è¡Œç»“æœ"
                }
                logger.warning(f"âš ï¸ [Debug] æœªèƒ½ä»agent_metadataä¸­æå–æµ‹è¯•ç»“æœ")

            # Domainå±‚ä¸šåŠ¡è§„åˆ™éªŒè¯
            validation_result = self.domain_service.validate_placeholder_business_rules(
                placeholder_text=placeholder_text,
                template_context=template_context or {},
                data_source_context={"data_source_id": data_source_id}
            )

            # æ„å»ºå®Œæ•´ç»“æœ
            result = {
                "status": "success",
                "placeholder_name": placeholder_name,
                "generated_sql": {
                    placeholder_name: generated_sql,
                    "sql": generated_sql,
                },
                "test_result": test_result,  # æ–°å¢ï¼šåŒ…å«æ‰§è¡Œç»“æœ
                "analysis_result": {
                    "description": "åŸºäºå®Œæ•´AgentåŸºç¡€è®¾æ–½çš„æ™ºèƒ½åˆ†æ",
                    "analysis_type": "full_agent_pipeline",
                    "semantic_type": semantic_type,
                    "business_requirements": business_requirements,
                    "analysis_summary": agent_metadata.get("analysis_summary", "Agent Pipelineåˆ†æå®Œæˆ"),
                    "suggestions": agent_metadata.get("suggestions", []),
                    "execution_stats": {
                        "tools_used": agent_metadata.get("tools_used", []),
                        "execution_time_ms": agent_metadata.get("execution_time_ms", 0),
                        "agent_facade_used": True,
                        "domain_service_used": True,
                        "steps_executed": agent_metadata.get("steps_executed", [])
                    }
                },
                "business_validation": validation_result,
                "confidence_score": self._calculate_confidence_score(agent_result, business_requirements),
                "analyzed_at": datetime.now().isoformat(),
                "context_used": {
                    "template_context": bool(template_context),
                    "data_source_info": bool(data_source_id),
                    "business_analysis": True,
                    "agent_pipeline": True,
                    "tools_chain": agent_metadata.get("tools_used", [])
                }
            }

            logger.info(f"âœ… å®Œæ•´Agent Pipelineåˆ†ææˆåŠŸ: {placeholder_name}")
            logger.info(f"ğŸ”§ [Debug] æœ€ç»ˆè¿”å›ç»“æœkeys: {list(result.keys())}")
            logger.info(f"ğŸ”§ [Debug] generated_sqlç»“æ„: {type(result.get('generated_sql'))}")
            return result

        except Exception as e:
            logger.error(f"âŒ Agent Pipelineåˆ†æå¼‚å¸¸: {e}")
            return self._create_error_result(placeholder_name, str(e))

    def _map_business_to_semantic_type(self, business_requirements: Dict[str, Any]) -> str:
        """å°†ä¸šåŠ¡éœ€æ±‚æ˜ å°„åˆ°Agentå·¥å…·çš„è¯­ä¹‰ç±»å‹"""
        business_type = business_requirements.get("business_type", "").lower()
        semantic_intent = business_requirements.get("semantic_intent", "").lower()

        # æ˜ å°„åˆ°SQLDraftToolæ”¯æŒçš„è¯­ä¹‰ç±»å‹
        if "ranking" in business_type or "top" in semantic_intent or "æ’è¡Œ" in semantic_intent:
            return "ranking"
        elif "compare" in business_type or "å¯¹æ¯”" in semantic_intent or "æ¯”è¾ƒ" in semantic_intent:
            return "compare"
        elif "period" in business_type or "å‘¨æœŸ" in semantic_intent or "æ—¶é—´" in semantic_intent:
            return "period"
        elif "chart" in business_type or "å›¾è¡¨" in semantic_intent:
            return "chart"
        else:
            return "stat"  # é»˜è®¤ç»Ÿè®¡ç±»å‹

    def _calculate_confidence_score(self, agent_result, business_requirements: Dict[str, Any]) -> float:
        """è®¡ç®—ç½®ä¿¡åº¦åˆ†æ•°"""
        base_score = 0.8

        # Agentæ‰§è¡ŒæˆåŠŸåŠ åˆ†
        if agent_result.success:
            base_score += 0.1

        # ä¸šåŠ¡éœ€æ±‚æ˜ç¡®åº¦åŠ åˆ†
        if business_requirements.get("priority") == "high":
            base_score += 0.05

        # è¯­ä¹‰è¯†åˆ«å‡†ç¡®æ€§åŠ åˆ†
        if business_requirements.get("semantic_intent"):
            base_score += 0.05

        return min(base_score, 1.0)

    def _infer_schedule_type(self, cron_expression: str) -> str:
        """æ ¹æ®cronè¡¨è¾¾å¼æ¨æ–­è°ƒåº¦ç±»å‹"""
        try:
            parts = cron_expression.split()
            if len(parts) < 5:
                return "daily"

            minute, hour, dom, month, dow = parts[:5]

            # æŒ‡å®šäº†æ˜ŸæœŸï¼ˆå¦‚ 0 9 * * 1ï¼‰ï¼Œè§†ä¸ºæ¯å‘¨
            if dow not in ('*', '?'):
                return "weekly"
            # æŒ‡å®šäº†æ—¥æœŸï¼ˆå¦‚ 0 9 1 * *ï¼‰ï¼Œè§†ä¸ºæ¯æœˆ
            if dom not in ('*', '?'):
                return "monthly"
            # æŒ‡å®šäº†æœˆä»½ï¼ˆå¦‚ 0 0 1 1 *ï¼‰ï¼Œè§†ä¸ºæ¯å¹´
            if month not in ('*', '?'):
                return "yearly"
            return "daily"
        except Exception:
            return "daily"

    def _describe_previous_period(self, cron_expression: str) -> str:
        """æè¿°å‰ä¸€ä¸ªç»Ÿè®¡å‘¨æœŸ"""
        schedule_type = self._infer_schedule_type(cron_expression)

        descriptions = {
            "daily": "ç»Ÿè®¡æ˜¨å¤©çš„æ•°æ® (å‰ä¸€å¤©)",
            "weekly": "ç»Ÿè®¡ä¸Šå‘¨çš„æ•°æ® (ä¸Šå‘¨ä¸€è‡³ä¸Šå‘¨æ—¥)",
            "monthly": "ç»Ÿè®¡ä¸Šä¸ªæœˆçš„æ•°æ® (ä¸Šæœˆ1æ—¥è‡³ä¸Šæœˆæœ€åä¸€å¤©)",
            "yearly": "ç»Ÿè®¡å»å¹´çš„æ•°æ® (å»å¹´1æœˆ1æ—¥è‡³12æœˆ31æ—¥)"
        }

        return descriptions.get(schedule_type, "ç»Ÿè®¡å‰ä¸€ä¸ªå‘¨æœŸçš„æ•°æ®")

    async def _get_schema_from_data_source_context(self, user_id: str, data_source_id: str = None) -> SchemaInfo:
        """é€šè¿‡DataSourceContextè·å–Schemaä¿¡æ¯ï¼Œå¸¦ç¼“å­˜æœºåˆ¶"""
        logger.info(f"ğŸ” [Schemaè·å–] å¼€å§‹è·å–Schema: user_id={user_id}, data_source_id={data_source_id}")

        if not data_source_id:
            logger.warning("ğŸ” [Schemaè·å–] æ²¡æœ‰æ•°æ®æºIDï¼Œè¿”å›ç©ºSchemaè®©SchemaListColumnsToolè‡ªåŠ¨å¤„ç†")
            return SchemaInfo(tables=[], columns={})

        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"{user_id}:{data_source_id}"
        from datetime import datetime
        current_time = datetime.now()

        if cache_key in self._schema_cache:
            cached_data, cache_time = self._schema_cache[cache_key]
            if (current_time - cache_time).total_seconds() < self._cache_ttl:
                logger.info(f"ğŸ” [Schemaç¼“å­˜] ä½¿ç”¨ç¼“å­˜çš„Schemaä¿¡æ¯: è¡¨æ•°é‡={len(cached_data.tables)}")
                return cached_data

        try:
            from app.services.application.context.data_source_context_server import DataSourceContextBuilder
            logger.info(f"ğŸ” [Schemaè·å–] å¼€å§‹è°ƒç”¨DataSourceContextBuilder")

            # ä½¿ç”¨ç°æœ‰çš„DataSourceContextBuilder
            data_source_builder = DataSourceContextBuilder()
            context_result = await data_source_builder.build_data_source_context(
                user_id=user_id,
                data_source_id=data_source_id,
                force_refresh=False,
                names_only=True
            )

            logger.info(f"ğŸ” [Schemaè·å–] DataSourceContextBuilderç»“æœ: success={context_result and context_result.get('success')}")
            if context_result:
                logger.info(f"ğŸ” [Schemaè·å–] ä¸Šä¸‹æ–‡ç»“æœé”®: {list(context_result.keys())}")
                logger.info(f"ğŸ” [Schemaè·å–] tablesæ•°æ®: {context_result.get('tables', [])[:2]}...")  # åªæ‰“å°å‰2ä¸ª

            if context_result and context_result.get("success"):
                # ä»…æå–è¡¨åï¼Œåˆ—ä¿¡æ¯ç•™å¾… schema.get_columns è·å–ï¼ˆä¸¤æ­¥Schemaï¼‰
                tables_payload = context_result.get("tables", [])
                logger.info(f"ğŸ” [Schemaè·å–] è·å–åˆ° {len(tables_payload)} ä¸ªè¡¨ï¼ˆä»…è¿”å›è¡¨åï¼Œåˆ—ä¿¡æ¯å»¶åè·å–ï¼‰")

                tables = []
                for table_info in tables_payload:
                    table_name = table_info.get("table_name", "")
                    if table_name:
                        tables.append(table_name)

                final_schema = SchemaInfo(tables=tables, columns={})
                logger.info(f"ğŸ” [Schemaè·å–] æœ€ç»ˆæ„å»ºSchema: è¡¨={len(tables)}, åˆ—ä¿¡æ¯å»¶å")
                logger.debug(f"ğŸ” [Schemaè·å–] è¡¨ååˆ—è¡¨: {tables}")  # æ”¹ä¸ºdebugçº§åˆ«

                # ç¼“å­˜ç»“æœ
                self._schema_cache[cache_key] = (final_schema, current_time)
                logger.debug(f"ğŸ” [Schemaç¼“å­˜] å·²ç¼“å­˜Schemaä¿¡æ¯: {cache_key}")

                return final_schema
            else:
                logger.warning(f"ğŸ” [Schemaè·å–] DataSourceContextæ„å»ºå¤±è´¥æˆ–ä¸æˆåŠŸ")

        except Exception as e:
            logger.error(f"ğŸ” [Schemaè·å–] é€šè¿‡DataSourceContextè·å–Schemaå¤±è´¥: {e}")
            import traceback
            logger.error(f"ğŸ” [Schemaè·å–] é”™è¯¯å †æ ˆ: {traceback.format_exc()}")

        # å›é€€ï¼šè¿”å›ç©ºSchemaè®©SchemaListColumnsToolè‡ªåŠ¨å¤„ç†
        logger.warning("ğŸ” [Schemaè·å–] å›é€€åˆ°ç©ºSchemaï¼Œè®©SchemaListColumnsToolè‡ªåŠ¨å¤„ç†")
        return SchemaInfo(tables=[], columns={})

    def _serialize_datetime_objects(self, obj):
        """é€’å½’åºåˆ—åŒ–datetimeå¯¹è±¡ä¸ºISOæ ¼å¼å­—ç¬¦ä¸²"""
        from datetime import datetime, date

        if isinstance(obj, (datetime, date)):
            return obj.isoformat()
        elif isinstance(obj, dict):
            return {k: self._serialize_datetime_objects(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._serialize_datetime_objects(item) for item in obj]
        else:
            return obj

    def _is_period_placeholder(self, placeholder_text: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºå‘¨æœŸæ€§å ä½ç¬¦"""
        text_lower = placeholder_text.lower()
        period_keywords = ["å‘¨æœŸ", "æ—¥æœŸ", "æ—¶é—´", "period", "date", "ç»Ÿè®¡å‘¨æœŸ", "æŠ¥å‘Šå‘¨æœŸ", "æ•°æ®å‘¨æœŸ"]
        return any(keyword in text_lower for keyword in period_keywords)

    async def _handle_period_placeholder(
        self,
        placeholder_name: str,
        placeholder_text: str,
        template_id: str,
        template_context: Dict[str, Any] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """å¤„ç†å‘¨æœŸæ€§å ä½ç¬¦"""
        try:
            # å¯¼å…¥å‘¨æœŸå¤„ç†å™¨
            from app.services.domain.placeholder.core.handlers.period_handler import PeriodHandler
            from app.utils.time_context import TimeContextManager

            logger.info(f"ğŸ• å¼€å§‹å¤„ç†å‘¨æœŸæ€§å ä½ç¬¦: {placeholder_text}")
            logger.info(f"ğŸ”§ è¾“å…¥å‚æ•° - placeholder_name: {placeholder_name}, template_id: {template_id}")
            logger.info(f"ğŸ”§ template_contextç±»å‹: {type(template_context)}, å†…å®¹: {template_context}")
            logger.info(f"ğŸ”§ kwargså†…å®¹: {kwargs}")

            # æ„å»ºæ—¶é—´ä¸Šä¸‹æ–‡
            time_ctx = {}
            logger.info(f"ğŸ”§ å¼€å§‹æ„å»ºæ—¶é—´ä¸Šä¸‹æ–‡")

            # å¦‚æœæœ‰æ¨¡æ¿ä¸Šä¸‹æ–‡ï¼Œå°è¯•ä»ä¸­æå–æ—¶é—´ä¿¡æ¯
            if template_context:
                logger.info(f"ğŸ”§ å¤„ç†template_contextï¼Œç±»å‹: {type(template_context)}")

                # ç¡®ä¿template_contextæ˜¯å­—å…¸
                if not isinstance(template_context, dict):
                    logger.error(f"ğŸ”§ template_contextä¸æ˜¯å­—å…¸ç±»å‹: {type(template_context)}")
                    template_context = {}

                # ç¡®ä¿scheduleæ˜¯å­—å…¸æ ¼å¼
                schedule = template_context.get("schedule") if isinstance(template_context, dict) else None
                logger.info(f"ğŸ”§ scheduleç±»å‹: {type(schedule)}, å†…å®¹: {schedule}")

                if isinstance(schedule, dict):
                    time_ctx["schedule"] = schedule
                    time_ctx["cron_expression"] = schedule.get("cron_expression")
                elif isinstance(schedule, str):
                    # å¦‚æœscheduleæ˜¯å­—ç¬¦ä¸²ï¼Œå‡è®¾å®ƒæ˜¯cronè¡¨è¾¾å¼
                    time_ctx["cron_expression"] = schedule
                    time_ctx["schedule"] = {"cron_expression": schedule}
                else:
                    time_ctx["schedule"] = {}

                # å…¶ä»–æ—¶é—´å‚æ•°
                if isinstance(template_context, dict):
                    if template_context.get("execution_time"):
                        time_ctx["execution_time"] = template_context["execution_time"]
                    if template_context.get("start_date"):
                        time_ctx["start_date"] = template_context["start_date"]
                    if template_context.get("end_date"):
                        time_ctx["end_date"] = template_context["end_date"]

            # ä»kwargsä¸­è·å–é¢å¤–çš„æ—¶é—´å‚æ•°
            data_range = kwargs.get("data_range", "day")
            time_ctx.update({
                "data_range": data_range,
                "time_column": kwargs.get("time_column")
            })

            # å¦‚æœæ²¡æœ‰å…·ä½“çš„æ—¶é—´ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤çš„å½“å‰æ—¶é—´å¤„ç†
            if not time_ctx.get("execution_time"):
                from datetime import datetime
                time_ctx["execution_time"] = datetime.now().isoformat()

            # åŸºäºdata_rangeç”Ÿæˆé»˜è®¤çš„cronè¡¨è¾¾å¼ï¼ˆå¦‚æœæ²¡æœ‰æä¾›ï¼‰
            if not time_ctx.get("cron_expression"):
                if data_range == "day":
                    time_ctx["cron_expression"] = "0 9 * * *"  # æ¯å¤©9ç‚¹
                elif data_range == "week":
                    time_ctx["cron_expression"] = "0 9 * * 1"  # æ¯å‘¨ä¸€9ç‚¹
                elif data_range == "month":
                    time_ctx["cron_expression"] = "0 9 1 * *"  # æ¯æœˆ1æ—¥9ç‚¹

                # æ›´æ–°scheduleå­—å…¸
                if "schedule" not in time_ctx:
                    time_ctx["schedule"] = {}
                time_ctx["schedule"]["cron_expression"] = time_ctx.get("cron_expression")

            # ä½¿ç”¨å‘¨æœŸå¤„ç†å™¨è®¡ç®—ç»“æœ
            logger.info(f"ğŸ”§ è°ƒç”¨PeriodHandlerï¼Œtime_ctx: {time_ctx}")

            period_handler = PeriodHandler()
            computed_result = await period_handler.compute(placeholder_text, time_ctx)

            logger.info(f"âœ… å‘¨æœŸæ€§å ä½ç¬¦å¤„ç†å®Œæˆ: {computed_result}")

            # æ„å»ºè¿”å›ç»“æœ
            result = {
                "status": "success",
                "placeholder_name": placeholder_name,
                "generated_sql": {
                    placeholder_name: "",  # å‘¨æœŸæ€§å ä½ç¬¦ä¸éœ€è¦SQL
                    "sql": "",
                },
                "analysis_result": {
                    "description": "å‘¨æœŸæ€§å ä½ç¬¦ç›´æ¥è®¡ç®—",
                    "analysis_type": "period_placeholder",
                    "semantic_type": "period",
                    "computed_value": computed_result.get("value", ""),
                    "period_meta": computed_result.get("meta", {}),
                    "analysis_summary": f"å‘¨æœŸæ€§å ä½ç¬¦ '{placeholder_text}' å·²è®¡ç®—å®Œæˆ",
                    "execution_stats": {
                        "tools_used": ["period_handler"],
                        "execution_time_ms": 10,
                        "agent_facade_used": False,
                        "period_handler_used": True
                    }
                },
                "confidence_score": 1.0,  # å‘¨æœŸè®¡ç®—æœ‰å¾ˆé«˜çš„å‡†ç¡®æ€§
                "analyzed_at": datetime.now().isoformat(),
                "context_used": {
                    "template_context": bool(template_context),
                    "period_calculation": True,
                    "time_context": time_ctx
                }
            }

            return result

        except Exception as e:
            import traceback
            logger.error(f"âŒ å‘¨æœŸæ€§å ä½ç¬¦å¤„ç†å¤±è´¥: {e}")
            logger.error(f"ğŸ”§ å®Œæ•´å¼‚å¸¸ä¿¡æ¯: {traceback.format_exc()}")
            return self._create_error_result(placeholder_name, f"å‘¨æœŸæ€§å ä½ç¬¦å¤„ç†å¤±è´¥: {str(e)}")

    async def _attempt_pipeline_recovery(
        self,
        agent_input,
        failed_result,
        placeholder_name: str,
        semantic_type: str
    ) -> Dict[str, Any]:
        """æ™ºèƒ½Pipelineæ¢å¤æœºåˆ¶"""
        recovery_attempts = []
        logger.info(f"ğŸ”„ å¼€å§‹Agent Pipelineæ¢å¤ï¼Œè¯­ä¹‰ç±»å‹: {semantic_type}")

        # å°è¯•1: æ£€æŸ¥æ˜¯å¦æ˜¯SQLéªŒè¯é—®é¢˜
        if self._is_sql_validation_failure(failed_result):
            recovery_attempts.append("sql_validation_bypass")
            logger.info("ğŸ”„ å°è¯•SQLéªŒè¯å®¹é”™æ¢å¤")

            try:
                # ä¿®æ”¹çº¦æŸæ¡ä»¶ï¼Œå…è®¸æ›´å®½æ¾çš„éªŒè¯
                relaxed_input = agent_input
                if hasattr(relaxed_input, 'constraints'):
                    # å¯ç”¨éªŒè¯å®¹é”™æ¨¡å¼
                    relaxed_input.constraints.validation_mode = "tolerant"
                    relaxed_input.constraints.bypass_minor_errors = True

                # é‡æ–°æ‰§è¡Œ - ä½¿ç”¨ä»»åŠ¡éªŒè¯æ™ºèƒ½æ¨¡å¼
                recovery_result = await self.agent_facade.execute_task_validation(relaxed_input)
                if recovery_result.success:
                    return {
                        "recovered": True,
                        "method": "sql_validation_bypass",
                        "result": recovery_result,
                        "recovery_attempts": recovery_attempts
                    }

            except Exception as e:
                logger.warning(f"ğŸ”„ SQLéªŒè¯å®¹é”™æ¢å¤å¤±è´¥: {e}")

        # å°è¯•2: ç®€åŒ–è¯­ä¹‰ç±»å‹
        if semantic_type != "stat":
            recovery_attempts.append("semantic_simplification")
            logger.info("ğŸ”„ å°è¯•ç®€åŒ–è¯­ä¹‰ç±»å‹æ¢å¤")

            try:
                # ç®€åŒ–ä¸ºåŸºç¡€ç»Ÿè®¡ç±»å‹
                simplified_input = agent_input
                if hasattr(simplified_input, 'placeholder'):
                    simplified_input.placeholder.type = "stat"

                recovery_result = await self.agent_facade.execute_task_validation(simplified_input)
                if recovery_result.success:
                    return {
                        "recovered": True,
                        "method": "semantic_simplification",
                        "result": recovery_result,
                        "recovery_attempts": recovery_attempts
                    }

            except Exception as e:
                logger.warning(f"ğŸ”„ è¯­ä¹‰ç®€åŒ–æ¢å¤å¤±è´¥: {e}")

        # å°è¯•3: ä»»åŠ¡éªŒè¯æ™ºèƒ½æ¨¡å¼å·²ç»åŒ…å«äº†æ™ºèƒ½å›é€€ï¼Œä¸éœ€è¦é¢å¤–çš„æ¢å¤æœºåˆ¶
        recovery_attempts.append("task_validation_intelligent_built_in")
        logger.info("ğŸ”„ ä»»åŠ¡éªŒè¯æ™ºèƒ½æ¨¡å¼å·²å†…ç½®å›é€€æœºåˆ¶ï¼Œè·³è¿‡é¢å¤–æ¢å¤")

        # æ—¢ç„¶ä½¿ç”¨äº†execute_task_validationï¼Œå®ƒå·²ç»åŒ…å«äº†PTAVå›é€€æœºåˆ¶
        # å¦‚æœè¿˜å¤±è´¥ï¼Œè¯´æ˜æ˜¯æ›´æ ¹æœ¬æ€§çš„é—®é¢˜ï¼Œåº”è¯¥ç›´æ¥è¿”å›é”™è¯¯
        logger.warning("ğŸ”„ ä»»åŠ¡éªŒè¯æ™ºèƒ½æ¨¡å¼å¤±è´¥ï¼Œå¯èƒ½æ˜¯é…ç½®æˆ–è¿æ¥é—®é¢˜")

        return {
            "recovered": False,
            "recovery_attempts": recovery_attempts,
            "reason": "execute_task_validationå·²å†…ç½®æ™ºèƒ½å›é€€ï¼Œæ— éœ€é¢å¤–æ¢å¤æœºåˆ¶"
        }

    def _is_sql_validation_failure(self, failed_result) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯SQLéªŒè¯å¤±è´¥"""
        if not hasattr(failed_result, 'metadata') or not failed_result.metadata:
            return False

        error_info = failed_result.metadata
        # å®‰å…¨åœ°å¤„ç†APIResponseå¯¹è±¡
        if isinstance(error_info, dict):
            error_message = error_info.get("error", "")
            reasoning = error_info.get("reasoning", "")
        elif hasattr(error_info, '__dict__'):
            # å¦‚æœæ˜¯APIResponseç­‰å¯¹è±¡ï¼Œå°è¯•è®¿é—®å±æ€§
            error_message = getattr(error_info, 'error', "") or str(error_info)
            reasoning = getattr(error_info, 'reasoning', "")
        else:
            # å›é€€åˆ°å­—ç¬¦ä¸²è¡¨ç¤º
            error_message = str(error_info)
            reasoning = ""

        sql_validation_keywords = [
            "sql.validate", "è¯­æ³•é”™è¯¯", "æ‹¬å·", "DATE_SUB", "DATE_ADD",
            "SQLè¯­å¥", "è¯­æ³•æ­£ç¡®æ€§", "éªŒè¯å¤±è´¥"
        ]

        return any(keyword in str(error_message) + str(reasoning)
                  for keyword in sql_validation_keywords)

    async def _generate_basic_sql_fallback(
        self,
        placeholder_name: str,
        user_prompt: str,
        schema_info,
        user_id: str
    ) -> Dict[str, Any]:
        """åŸºç¡€SQLç”Ÿæˆå›é€€æœºåˆ¶ - å·²ç¦ç”¨ï¼Œéµå¾ªå•ä¸€èŒè´£åŸåˆ™"""
        logger.error("ğŸ”„ [æ¢å¤æœºåˆ¶] SQLDraftToolå·²åˆ é™¤ï¼Œæ¢å¤æœºåˆ¶ä¸å†æ”¯æŒSQLç”Ÿæˆ")
        return {
            "success": False,
            "error": "Agentè®¡åˆ’ç”Ÿæˆå¤±è´¥ï¼Œä¸”æ¢å¤æœºåˆ¶å·²ç¦ç”¨ã€‚è¯·æ£€æŸ¥Agenté…ç½®å’Œè®¡åˆ’ç”Ÿæˆé€»è¾‘ã€‚"
        }

    def _create_error_result(self, placeholder_name: str, error_message: str) -> Dict[str, Any]:
        """åˆ›å»ºé”™è¯¯ç»“æœ"""
        return {
            "status": "error",
            "placeholder_name": placeholder_name,
            "generated_sql": {"sql": "", placeholder_name: ""},
            "error": error_message,
            "confidence_score": 0.0,
            "analyzed_at": datetime.now().isoformat(),
            "analysis_result": {
                "description": "Agent Pipelineæ‰§è¡Œå¤±è´¥",
                "analysis_type": "error_fallback",
                "suggestions": ["è¯·æ£€æŸ¥è¾“å…¥å‚æ•°", "éªŒè¯æ•°æ®æºè¿æ¥"]
            }
        }

    def _create_enhanced_error_result(
        self,
        placeholder_name: str,
        error_metadata: Dict[str, Any],
        recovery_attempts: List[str] = None
    ) -> Dict[str, Any]:
        """åˆ›å»ºå¢å¼ºçš„é”™è¯¯ç»“æœï¼ŒåŒ…å«æ¢å¤å°è¯•ä¿¡æ¯"""
        # ç¡®ä¿ error_metadata æ˜¯å­—å…¸ç±»å‹
        if not isinstance(error_metadata, dict):
            error_metadata = {"error": str(error_metadata), "reasoning": ""}

        error_message = error_metadata.get("error", "Agentæ‰§è¡Œå¤±è´¥")
        reasoning = error_metadata.get("reasoning", "")

        # æ ¹æ®é”™è¯¯ç±»å‹æä¾›æ›´å…·ä½“çš„å»ºè®®
        suggestions = ["è¯·æ£€æŸ¥è¾“å…¥å‚æ•°", "éªŒè¯æ•°æ®æºè¿æ¥"]

        if "sql.validate" in str(error_metadata):
            suggestions.extend([
                "SQLè¯­æ³•å¯èƒ½å­˜åœ¨é—®é¢˜ï¼Œå»ºè®®ç®€åŒ–æŸ¥è¯¢æ¡ä»¶",
                "æ£€æŸ¥å ä½ç¬¦æè¿°æ˜¯å¦å‡†ç¡®",
                "è€ƒè™‘ä½¿ç”¨åŸºç¡€ç»Ÿè®¡ç±»å‹è€Œéå¤æ‚è¯­ä¹‰"
            ])

        if "DATE_SUB" in str(error_metadata) or "æ‹¬å·" in str(error_metadata):
            suggestions.extend([
                "SQLä¸­çš„æ—¥æœŸå‡½æ•°è¯­æ³•éœ€è¦æ£€æŸ¥",
                "æ‹¬å·åŒ¹é…å¯èƒ½å­˜åœ¨é—®é¢˜"
            ])

        return {
            "status": "error",
            "placeholder_name": placeholder_name,
            "generated_sql": {"sql": "", placeholder_name: ""},
            "error": error_message,
            "error_details": {
                "reasoning": reasoning,
                "recovery_attempts": recovery_attempts or [],
                "error_type": self._classify_error_type(error_metadata)
            },
            "confidence_score": 0.0,
            "analyzed_at": datetime.now().isoformat(),
            "analysis_result": {
                "description": f"Agent Pipelineæ‰§è¡Œå¤±è´¥: {reasoning[:100]}..." if len(reasoning) > 100 else reasoning,
                "analysis_type": "enhanced_error_fallback",
                "suggestions": suggestions,
                "recovery_info": {
                    "attempts_made": len(recovery_attempts) if recovery_attempts else 0,
                    "recoverable": self._is_recoverable_error(error_metadata),
                    "next_actions": ["å°è¯•ç®€åŒ–å ä½ç¬¦æè¿°", "æ£€æŸ¥æ•°æ®æºé…ç½®", "è”ç³»æŠ€æœ¯æ”¯æŒ"]
                }
            }
        }

    def _classify_error_type(self, error_metadata: Dict[str, Any]) -> str:
        """åˆ†ç±»é”™è¯¯ç±»å‹"""
        error_str = str(error_metadata).lower()

        if "sql" in error_str and "validate" in error_str:
            return "sql_validation_error"
        elif "schema" in error_str:
            return "schema_error"
        elif "connection" in error_str or "database" in error_str:
            return "connection_error"
        elif "timeout" in error_str:
            return "timeout_error"
        else:
            return "general_error"

    def _extract_placeholder_snippet(self, template_text: str, placeholder_text: str, placeholder_name: str) -> str:
        """ä»æ•´ä»½æ¨¡æ¿æ–‡æœ¬ä¸­æå–â€œåŒ…å«å ä½ç¬¦çš„æ®µè½/é‚»è¿‘è¡Œâ€ã€‚

        è§„åˆ™ï¼š
        - ä»¥æ¢è¡Œåˆ†æ®µï¼Œæ‰¾åˆ°åŒ…å« placeholder_text æˆ– placeholder_name çš„è¡Œ
        - å‘ä¸Š/å‘ä¸‹æ‰©å±•åˆ°æœ€è¿‘çš„ç©ºè¡Œï¼ˆæ®µè½è¾¹ç•Œï¼‰
        - å¦‚æœªå‘½ä¸­ï¼Œåˆ™è¿”å›å‰500å­—ç¬¦çš„é¢„è§ˆ
        """
        try:
            if not template_text:
                return ""
            lines = template_text.splitlines()
            keys = [k for k in [placeholder_text, placeholder_name, placeholder_name and f"{{{{{placeholder_name}}}}}"] if k]
            hit_idx = -1
            for i, ln in enumerate(lines):
                for k in keys:
                    if k and k in ln:
                        hit_idx = i
                        break
                if hit_idx >= 0:
                    break
            if hit_idx < 0:
                # æœªå‘½ä¸­ï¼Œè¿”å›å‰500å­—ç¬¦
                return (template_text[:500] + ("â€¦" if len(template_text) > 500 else ""))

            # æ‰©å±•åˆ°æ®µè½è¾¹ç•Œï¼ˆç©ºè¡Œï¼‰
            start = hit_idx
            while start > 0 and lines[start].strip() != "" and lines[start-1].strip() != "":
                start -= 1
            end = hit_idx
            while end + 1 < len(lines) and lines[end].strip() != "" and lines[end+1].strip() != "":
                end += 1
            snippet_lines = lines[start:end+1]
            # å†å‘ä¸¤ä¾§è¡¥å……ä¸€è¡Œä¸Šä¸‹æ–‡
            if start > 0:
                snippet_lines.insert(0, lines[start-1])
            if end + 1 < len(lines):
                snippet_lines.append(lines[end+1])
            snippet = "\n".join(snippet_lines).strip()
            return snippet
        except Exception:
            return (template_text[:500] + ("â€¦" if len(template_text) > 500 else ""))

    def _is_recoverable_error(self, error_metadata: Dict[str, Any]) -> bool:
        """åˆ¤æ–­é”™è¯¯æ˜¯å¦å¯æ¢å¤"""
        error_str = str(error_metadata).lower()

        # SQLéªŒè¯é”™è¯¯é€šå¸¸å¯æ¢å¤
        if "sql" in error_str and "validate" in error_str:
            return True

        # è¶…æ—¶é”™è¯¯å¯æ¢å¤
        if "timeout" in error_str:
            return True

        # è¿æ¥é”™è¯¯ä¸€èˆ¬ä¸å¯æ¢å¤
        if "connection" in error_str:
            return False

        return True

# å…¨å±€æœåŠ¡å®ä¾‹
_orchestration_service = PlaceholderOrchestrationService()

# ================================================================================
# APIè·¯ç”±å®šä¹‰ - å……åˆ†åˆ©ç”¨AgentåŸºç¡€è®¾æ–½
# ================================================================================

@router.get("/", response_model=APIResponse[List[TemplatePlaceholder]])
async def get_placeholders(
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
    skip: int = Query(0, ge=0),
    limit: int = Query(100, ge=1, le=1000),
    template_id: str = Query(None, description="æŒ‰æ¨¡æ¿IDè¿‡æ»¤"),
) -> APIResponse[List[TemplatePlaceholder]]:
    """è·å–å ä½ç¬¦åˆ—è¡¨"""
    try:
        logger.info(f"è·å–å ä½ç¬¦åˆ—è¡¨: template_id={template_id}")
        if template_id:
            placeholders = crud.template_placeholder.get_by_template(
                db=db, template_id=template_id
            )
        else:
            placeholders = crud.template_placeholder.get_multi(
                db=db, skip=skip, limit=limit
            )

        # ç›´æ¥è¿”å›TemplatePlaceholderæ ¼å¼ï¼Œä¸ä½¿ç”¨å‰ç«¯é€‚é…å™¨
        template_placeholders = []
        for p in placeholders:
            # ç¡®ä¿æ‰€æœ‰å¿…éœ€å­—æ®µéƒ½å­˜åœ¨ï¼ˆåŒ…æ‹¬agent_configç”¨äºè¿”å›test_resultï¼‰
            template_placeholder = TemplatePlaceholder(
                id=p.id,
                template_id=p.template_id,
                placeholder_name=p.placeholder_name,
                placeholder_text=p.placeholder_text or p.placeholder_name,
                placeholder_type=p.placeholder_type or "statistical",
                content_type=p.content_type or "text",
                agent_analyzed=p.agent_analyzed or False,
                target_database=p.target_database,
                target_table=p.target_table,
                required_fields=p.required_fields,
                generated_sql=p.generated_sql,
                sql_validated=p.sql_validated or False,
                execution_order=p.execution_order or 1,
                cache_ttl_hours=p.cache_ttl_hours or 24,
                is_required=p.is_required if p.is_required is not None else True,
                is_active=p.is_active if p.is_active is not None else True,
                agent_workflow_id=p.agent_workflow_id,
                agent_config=p.agent_config or {},  # ğŸ”‘ åŒ…å«test_resultç­‰ä¿¡æ¯
                description=p.description,
                confidence_score=p.confidence_score or 0.0,
                content_hash=p.content_hash,
                original_type=p.original_type,
                extracted_description=p.extracted_description,
                parsing_metadata=p.parsing_metadata,
                created_at=p.created_at,
                updated_at=p.updated_at,
                analyzed_at=p.analyzed_at
            )
            template_placeholders.append(template_placeholder)

        return APIResponse(
            success=True,
            data=template_placeholders,
            message="è·å–å ä½ç¬¦åˆ—è¡¨æˆåŠŸ"
        )
    except Exception as e:
        logger.error(f"è·å–å ä½ç¬¦åˆ—è¡¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail="è·å–å ä½ç¬¦åˆ—è¡¨å¤±è´¥")

@router.get("/{placeholder_id}", response_model=APIResponse[TemplatePlaceholder])
async def get_placeholder(
    placeholder_id: str,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
) -> APIResponse[TemplatePlaceholder]:
    """è·å–å•ä¸ªå ä½ç¬¦è¯¦æƒ…"""
    try:
        placeholder = crud.template_placeholder.get(db=db, id=placeholder_id)
        if not placeholder:
            raise HTTPException(status_code=404, detail="å ä½ç¬¦ä¸å­˜åœ¨")

        return APIResponse(
            success=True,
            data=TemplatePlaceholder.from_orm(placeholder),
            message="è·å–å ä½ç¬¦è¯¦æƒ…æˆåŠŸ"
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–å ä½ç¬¦è¯¦æƒ…å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail="è·å–å ä½ç¬¦è¯¦æƒ…å¤±è´¥")

@router.post("/", response_model=APIResponse[TemplatePlaceholder])
async def create_placeholder(
    placeholder_in: TemplatePlaceholderCreate,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
) -> APIResponse[TemplatePlaceholder]:
    """åˆ›å»ºæ–°å ä½ç¬¦"""
    try:
        placeholder = crud.template_placeholder.create(
            db=db, obj_in=placeholder_in
        )
        return APIResponse(
            success=True,
            data=TemplatePlaceholder.from_orm(placeholder),
            message="åˆ›å»ºå ä½ç¬¦æˆåŠŸ"
        )
    except Exception as e:
        logger.error(f"åˆ›å»ºå ä½ç¬¦å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail="åˆ›å»ºå ä½ç¬¦å¤±è´¥")

@router.put("/{placeholder_id}", response_model=APIResponse[TemplatePlaceholder])
async def update_placeholder(
    placeholder_id: str,
    placeholder_in: TemplatePlaceholderUpdate,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
) -> APIResponse[TemplatePlaceholder]:
    """æ›´æ–°å ä½ç¬¦"""
    try:
        placeholder = crud.template_placeholder.get(db=db, id=placeholder_id)
        if not placeholder:
            raise HTTPException(status_code=404, detail="å ä½ç¬¦ä¸å­˜åœ¨")

        placeholder = crud.template_placeholder.update(
            db=db, db_obj=placeholder, obj_in=placeholder_in
        )
        return APIResponse(
            success=True,
            data=TemplatePlaceholder.from_orm(placeholder),
            message="æ›´æ–°å ä½ç¬¦æˆåŠŸ"
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ›´æ–°å ä½ç¬¦å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail="æ›´æ–°å ä½ç¬¦å¤±è´¥")

@router.delete("/{placeholder_id}", response_model=APIResponse[bool])
async def delete_placeholder(
    placeholder_id: str,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
) -> APIResponse[bool]:
    """åˆ é™¤å ä½ç¬¦"""
    try:
        placeholder = crud.template_placeholder.get(db=db, id=placeholder_id)
        if not placeholder:
            raise HTTPException(status_code=404, detail="å ä½ç¬¦ä¸å­˜åœ¨")

        crud.template_placeholder.remove(db=db, id=placeholder_id)
        return APIResponse(
            success=True,
            data=True,
            message="åˆ é™¤å ä½ç¬¦æˆåŠŸ"
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ é™¤å ä½ç¬¦å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail="åˆ é™¤å ä½ç¬¦å¤±è´¥")

# ================================================================================
# æ ¸å¿ƒåŠŸèƒ½ï¼šåŸºäºå®Œæ•´AgentåŸºç¡€è®¾æ–½çš„å ä½ç¬¦åˆ†æ
# ================================================================================

@router.post("/analyze", response_model=APIResponse[Dict[str, Any]])
async def analyze_placeholder_with_agent_pipeline(
    request: Dict[str, Any],
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
) -> APIResponse[Dict[str, Any]]:
    """
    ä½¿ç”¨å®Œæ•´AgentåŸºç¡€è®¾æ–½è¿›è¡Œå ä½ç¬¦åˆ†æ

    åŒ…æ‹¬å®Œæ•´çš„å·¥å…·é“¾ï¼š
    - SchemaListColumnsTool: è‡ªåŠ¨æŸ¥è¯¢æ•°æ®åº“ç»“æ„
    - SQLDraftTool: æ™ºèƒ½SQLç”Ÿæˆï¼ˆæ”¯æŒè¯­ä¹‰ç±»å‹ï¼‰
    - SQLValidateTool: SQLéªŒè¯
    - SQLRefineTool: SQLä¼˜åŒ–
    - å¯é€‰SQLExecuteTool: SQLæ‰§è¡Œæµ‹è¯•

    æ”¯æŒçš„å‚æ•°:
    - placeholder_name: å ä½ç¬¦åç§°
    - placeholder_text: å ä½ç¬¦æ–‡æœ¬
    - template_id: æ¨¡æ¿ID
    - data_source_id: æ•°æ®æºID (å¯é€‰)
    - template_context: æ¨¡æ¿ä¸Šä¸‹æ–‡ (å¯é€‰)
    - time_column: æ—¶é—´åˆ—å (å¯é€‰ï¼Œå°†è‡ªåŠ¨æ£€æµ‹)
    - data_range: æ•°æ®èŒƒå›´ (é»˜è®¤: day)
    - requirements: é¢å¤–éœ€æ±‚ (å¯é€‰)
    - execute_sql: æ˜¯å¦æ‰§è¡ŒSQLæµ‹è¯• (é»˜è®¤: false)
    - row_limit: è¡Œæ•°é™åˆ¶ (é»˜è®¤: 1000)
    """
    try:
        # ä½¿ç”¨ç»Ÿä¸€éªŒè¯å™¨éªŒè¯å‚æ•°
        validation_results = []

        # éªŒè¯å¿…éœ€å­—æ®µ
        required_validation = ParameterValidator.validate_required_fields(
            request, ["placeholder_name", "placeholder_text", "template_id"]
        )
        validation_results.append(required_validation)

        # éªŒè¯template_idæ ¼å¼
        if request.get("template_id"):
            template_id_validation = ParameterValidator.validate_uuid(
                request.get("template_id"), "template_id"
            )
            validation_results.append(template_id_validation)

        # éªŒè¯placeholder_nameé•¿åº¦
        if request.get("placeholder_name"):
            name_validation = ParameterValidator.validate_string_length(
                request.get("placeholder_name"), "placeholder_name", 1, 100
            )
            validation_results.append(name_validation)

        # æ£€æŸ¥éªŒè¯ç»“æœ
        error_response = ErrorResponseBuilder.build_validation_error_response(validation_results)
        if error_response:
            raise HTTPException(
                status_code=400,
                detail=error_response.user_friendly_message
            )

        placeholder_name = request.get("placeholder_name")
        placeholder_text = request.get("placeholder_text")
        template_id = request.get("template_id")

        logger.info(f"ğŸš€ å¯åŠ¨Agent Pipelineåˆ†æ: {placeholder_name}")

        # ä½¿ç”¨å®Œæ•´çš„AgentåŸºç¡€è®¾æ–½è¿›è¡Œåˆ†æ
        # å…¼å®¹ï¼šå¦‚æœå‰ç«¯æ˜¾å¼ä¼ å…¥ sql æˆ– current_sqlï¼Œä¸”è¯·æ±‚ execute_sql=trueï¼Œåˆ™å°†å…¶æ˜ å°„åˆ° data_source.sql_to_test
        # è¿™æ ·Agentä¼šè‡ªåŠ¨è¿›å…¥ SQL éªŒè¯/æ‰§è¡Œè·¯å¾„ï¼Œé¿å… missing_current_sql
        forwarded_kwargs = {k: v for k, v in request.items() if k not in [
            'placeholder_name', 'placeholder_text', 'template_id',
            'data_source_id', 'template_context'
        ]}

        # ç»Ÿä¸€ SQL å­—æ®µæ”¶é›†
        incoming_sql = request.get("sql") or request.get("current_sql")
        if incoming_sql:
            # å°† SQL é€ä¼ åˆ°ä¸Šä¸‹æ–‡ï¼Œä¾› Facade/Orchestrator æå–
            forwarded_kwargs["current_sql"] = incoming_sql

        result = await _orchestration_service.analyze_placeholder_with_full_pipeline(
            placeholder_name=placeholder_name,
            placeholder_text=placeholder_text,
            template_id=template_id,
            data_source_id=request.get("data_source_id"),
            template_context=request.get("template_context"),
            user_id=str(current_user.id),
            **forwarded_kwargs
        )

        # ç»Ÿä¸€ç»“æœä¸ºå­—å…¸ï¼Œé˜²æ­¢ä¸Šæ¸¸è¯¯è¿”å›Pydanticå¯¹è±¡
        if not isinstance(result, dict):
            try:
                if hasattr(result, 'dict') and callable(result.dict):
                    result = result.dict()
                elif hasattr(result, 'model_dump') and callable(result.model_dump):
                    result = result.model_dump()
                else:
                    result = {"status": "error", "error": "invalid_result_type", "raw": str(result)}
            except Exception:
                result = {"status": "error", "error": "invalid_result_type"}

        # åºåˆ—åŒ–ç»“æœä¸­çš„datetimeå¯¹è±¡ï¼Œé˜²æ­¢JSONåºåˆ—åŒ–é”™è¯¯
        result = _orchestration_service._serialize_datetime_objects(result)

        # è‡ªåŠ¨ä¿å­˜åˆ†æç»“æœåˆ°æ•°æ®åº“
        # ç­–ç•¥ï¼šåªè¦ç”Ÿæˆäº†SQLï¼ˆæ— è®ºæ˜¯å¦éªŒè¯é€šè¿‡ï¼‰ï¼Œéƒ½ä¿å­˜SQLå’ŒéªŒè¯ç»“æœ
        # è¿™æ ·å‰ç«¯åˆ·æ–°åå¯ä»¥çœ‹åˆ°SQLå’Œæµ‹è¯•çŠ¶æ€ï¼Œagentå¯ä»¥æ ¹æ®æµ‹è¯•ç»“æœå†³å®šæ˜¯å¦ä¿®æ­£
        should_persist = False
        if isinstance(result.get("generated_sql"), dict):
            should_persist = bool(result.get("generated_sql", {}).get("sql"))
        elif isinstance(result.get("generated_sql"), str):
            should_persist = bool(result.get("generated_sql", "").strip())

        logger.info(f"ğŸ” [Debug] ä¿å­˜æ£€æŸ¥ - should_persist={should_persist}, result.status={result.get('status')}, has_sql={bool(result.get('generated_sql'))}")

        saved_placeholder_obj = None
        if should_persist:  # åªè¦æœ‰SQLå°±ä¿å­˜ï¼ˆåŒ…æ‹¬éªŒè¯å¤±è´¥çš„SQLï¼‰
            try:
                saved_placeholder_obj = await _save_placeholder_result(
                    db=db,
                    template_id=template_id,
                    placeholder_name=placeholder_name,
                    placeholder_text=placeholder_text,
                    result=result
                )
                # å°†æ•°æ®åº“IDæ·»åŠ åˆ°ç»“æœä¸­
                if saved_placeholder_obj:
                    result["placeholder_id"] = saved_placeholder_obj.id
                    result["placeholder_db_saved"] = True
                    logger.info(f"âœ… å ä½ç¬¦å·²ä¿å­˜åˆ°æ•°æ®åº“: {placeholder_name} (ID: {saved_placeholder_obj.id})")
            except Exception as save_error:
                logger.warning(f"ä¿å­˜å ä½ç¬¦ç»“æœå¤±è´¥: {save_error}")
                result["placeholder_db_saved"] = False
                # ä¸å½±å“ä¸»æµç¨‹

        # é€‚é…å‰ç«¯æ ¼å¼ï¼ˆå…¼å®¹å‘¨æœŸæ€§å ä½ç¬¦ï¼šå­˜åœ¨ frontend_data ä¹Ÿè§†ä¸ºæˆåŠŸï¼‰
        status_ok = (result.get("status") == "success") or bool(result.get("frontend_data"))
        if status_ok:
                # æ£€æŸ¥æ˜¯å¦æœ‰é¢„æ„å»ºçš„å‰ç«¯æ•°æ®ï¼ˆæ¥è‡ªå‘¨æœŸæ€§å ä½ç¬¦ï¼‰
                if result.get("frontend_data"):
                    # ä½¿ç”¨é¢„æ„å»ºçš„å‰ç«¯æ•°æ®
                    frontend_result = result.get("frontend_data")
                else:
                    # æ„å»ºå ä½ç¬¦æ˜¾ç¤ºä¿¡æ¯ï¼ˆå¸¸è§„Agent Pipelineç»“æœï¼‰
                    placeholder_dict = {
                        "text": placeholder_text,
                        "kind": result.get("analysis_result", {}).get("semantic_type", "statistical"),
                        "confidence": result.get("confidence_score", 0.8),
                        "needs_reanalysis": False
                    }
                    try:
                        adapted_placeholder = adapt_placeholder_for_frontend(placeholder_dict)
                    except Exception:
                        # å›é€€ï¼šæ„é€ æœ€ç®€å ä½ç¬¦ä»¥é¿å…å‰ç«¯æ¸²æŸ“å¤±è´¥
                        adapted_placeholder = type("_Shim", (), {"dict": lambda self=None: {
                            "text": placeholder_text,
                            "kind": placeholder_dict.get("kind", "statistical"),
                            "display_name": placeholder_dict.get("kind", "statistical"),
                            "description": "",
                            "status": "completed",
                            "confidence": placeholder_dict.get("confidence", 0.8),
                            "needs_reanalysis": False,
                            "badge_color": "default",
                            "icon": None,
                            "tooltip": None
                        }})()

                    # æ„å»ºåˆ†æè¿›åº¦ä¿¡æ¯
                    progress_info = adapt_analysis_progress_for_frontend(
                        current_step=4,
                        total_steps=4,
                        step_name="åˆ†æå®Œæˆ",
                        status="completed",
                        progress_percent=100.0
                    )

                    # æ•´åˆç»“æœï¼ˆåŒ…å«test_resultç”¨äºå‰ç«¯éªŒè¯æ˜¾ç¤ºå’Œagentä¿®æ­£å†³ç­–ï¼‰
                    frontend_result = {
                        "placeholder": adapted_placeholder.dict(),
                        "progress": progress_info.dict(),
                        "analysis_result": result.get("analysis_result"),
                        "generated_sql": result.get("generated_sql"),
                        "test_result": result.get("test_result"),  # ğŸ”‘ å…³é”®ï¼šåŒ…å«æµ‹è¯•ç»“æœ
                        "business_validation": result.get("business_validation"),
                        "analyzed_at": result.get("analyzed_at")
                    }

                # å¯¹æ•´ä¸ªfrontend_resultå†æ¬¡åºåˆ—åŒ–datetimeå¯¹è±¡
                frontend_result = _orchestration_service._serialize_datetime_objects(frontend_result)

                # ğŸ”§ è°ƒè¯•æœ€ç»ˆè¿”å›
                logger.info(f"ğŸ”§ [Debug] å³å°†è¿”å›APIResponseï¼Œfrontend_result keys: {list(frontend_result.keys())}")
                logger.info(f"ğŸ”§ [Debug] frontend_result.generated_sql type: {type(frontend_result.get('generated_sql'))}")

                return APIResponse(
                    success=True,
                    data=frontend_result,
                    message=f"Agent Pipelineåˆ†ææˆåŠŸ: {placeholder_name}"
                )
        else:
            # é”™è¯¯æƒ…å†µä½¿ç”¨é”™è¯¯é€‚é…å™¨
            # å…ˆåºåˆ—åŒ–resultä¸­çš„datetimeå¯¹è±¡
            serialized_result = _orchestration_service._serialize_datetime_objects(result)

            error_info = adapt_error_for_frontend(
                error_message=serialized_result.get("error", "åˆ†æå¤±è´¥"),
                error_type="analysis",
                error_code="placeholder_analysis_failed",
                details=serialized_result
            )

            return APIResponse(
                success=False,
                data=error_info.dict(),
                message=f"Agent Pipelineåˆ†æå¤±è´¥: {placeholder_name}"
            )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Agent Pipelineåˆ†æå¤±è´¥: {e}")
        # åˆ›å»ºé”™è¯¯é€‚é…ä¿¡æ¯ï¼ˆå¸¦å®¹é”™ï¼‰
        try:
            error_info = adapt_error_for_frontend(
                error_message=str(e),
                error_type="agent_service",
                error_code="agent_service_unavailable",
                details={
                    "agent_context": {
                        "placeholder_name": placeholder_name,
                        "template_id": template_id,
                        "user_id": str(current_user.id)
                    },
                    "error_type": type(e).__name__
                }
            )
            data_payload = error_info.dict()
            user_msg = error_info.user_friendly_message
        except Exception as adapt_exc:
            logger.error(f"é”™è¯¯é€‚é…å¤±è´¥: {adapt_exc}")
            data_payload = {
                "error_code": "agent_service_unavailable",
                "error_message": str(e),
                "user_friendly_message": "AIåˆ†ææš‚ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•",
                "error_type": "agent_service",
                "severity": "error",
            }
            user_msg = data_payload["user_friendly_message"]

        return APIResponse(
            success=False,
            data=data_payload,
            message=user_msg
        )


@router.post("/batch-analyze", response_model=APIResponse[Dict[str, Any]])
async def batch_analyze_with_agent_pipeline(
    request: Dict[str, Any],
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
) -> APIResponse[Dict[str, Any]]:
    """æ‰¹é‡ä½¿ç”¨Agent Pipelineåˆ†ææ¨¡æ¿ä¸­çš„æ‰€æœ‰å ä½ç¬¦"""
    try:
        template_id = request.get("template_id")
        data_source_id = request.get("data_source_id")

        if not template_id:
            raise HTTPException(status_code=400, detail="ç¼ºå°‘template_idå‚æ•°")

        logger.info(f"ğŸ”„ æ‰¹é‡Agent Pipelineåˆ†æ: template_id={template_id}")

        # è·å–æ¨¡æ¿ä¸­çš„æ‰€æœ‰å ä½ç¬¦
        placeholders = crud.template_placeholder.get_by_template(
            db=db, template_id=template_id
        )

        results = []
        success_count = 0

        for placeholder in placeholders:
            try:
                result = await _orchestration_service.analyze_placeholder_with_full_pipeline(
                    placeholder_name=placeholder.placeholder_name,
                    placeholder_text=placeholder.placeholder_text,
                    template_id=template_id,
                    data_source_id=data_source_id,
                    user_id=str(current_user.id)
                )
                # ç»Ÿä¸€å­—å…¸åŒ–
                if not isinstance(result, dict):
                    try:
                        if hasattr(result, 'dict') and callable(result.dict):
                            result = result.dict()
                        elif hasattr(result, 'model_dump') and callable(result.model_dump):
                            result = result.model_dump()
                        else:
                            result = {"status": "error", "error": "invalid_result_type", "raw": str(result)}
                    except Exception:
                        result = {"status": "error", "error": "invalid_result_type"}
                # åºåˆ—åŒ–ç»“æœä¸­çš„datetimeå¯¹è±¡
                result = _orchestration_service._serialize_datetime_objects(result)
                results.append(result)
                if result.get("status") == "success":
                    success_count += 1
            except Exception as e:
                logger.error(f"æ‰¹é‡åˆ†æå•ä¸ªå ä½ç¬¦å¤±è´¥: {placeholder.placeholder_name}, {e}")
                results.append({
                    "status": "error",
                    "placeholder_name": placeholder.placeholder_name,
                    "error": str(e)
                })

        # åºåˆ—åŒ–æ‰€æœ‰ç»“æœä¸­çš„datetimeå¯¹è±¡
        batch_data = {
            "template_id": template_id,
            "total_placeholders": len(placeholders),
            "success_count": success_count,
            "results": results,
            "analyzed_at": datetime.now().isoformat()
        }
        batch_data = _orchestration_service._serialize_datetime_objects(batch_data)

        return APIResponse(
            success=success_count > 0,
            data=batch_data,
            message=f"æ‰¹é‡Agent Pipelineåˆ†æå®Œæˆ: {success_count}/{len(placeholders)} æˆåŠŸ"
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ‰¹é‡Agent Pipelineåˆ†æå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡åˆ†æå¤±è´¥: {str(e)}")

# ================================================================================
# SQLéªŒè¯æœåŠ¡ - ç‹¬ç«‹åŠŸèƒ½
# ================================================================================

@router.post("/validate-sql", response_model=APIResponse[Dict[str, Any]])
async def validate_placeholder_sql(
    request: Dict[str, Any],
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
) -> APIResponse[Dict[str, Any]]:
    """éªŒè¯å­˜å‚¨çš„å ä½ç¬¦SQLå¹¶è¿”å›çœŸå®æ•°æ® - ç‹¬ç«‹åŠŸèƒ½"""
    try:
        from app.crud.crud_data_source import crud_data_source
        from app.services.data.validation.sql_validation_service import sql_validation_service

        sql_template = request.get("sql_template")
        data_source_id = request.get("data_source_id")
        placeholder_name = request.get("placeholder_name", "SQLéªŒè¯")
        execution_mode = request.get("execution_mode", "test")
        fixed_date = request.get("fixed_date")
        days_offset = request.get("days_offset", -1)

        if not sql_template:
            raise HTTPException(status_code=400, detail="ç¼ºå°‘sql_templateå‚æ•°")
        if not data_source_id:
            raise HTTPException(status_code=400, detail="ç¼ºå°‘data_source_idå‚æ•°")

        # éªŒè¯æ•°æ®æºæƒé™
        ds = crud_data_source.get_user_data_source(db, data_source_id=data_source_id, user_id=current_user.id)
        if not ds:
            raise HTTPException(status_code=404, detail="æ•°æ®æºä¸å­˜åœ¨æˆ–æ— æƒé™è®¿é—®")

        logger.info(f"ğŸ” å ä½ç¬¦SQLéªŒè¯è¯·æ±‚: {placeholder_name}")

        # æ‰§è¡ŒéªŒè¯
        result = await sql_validation_service.validate_and_execute_placeholder_sql(
            sql_template=sql_template,
            data_source_id=str(data_source_id),
            placeholder_name=placeholder_name,
            execution_mode=execution_mode,
            fixed_date=fixed_date,
            days_offset=days_offset
        )

        # ä¼˜åŒ–è¿”å›ç»“æ„ï¼šå°†æŸ¥è¯¢ç»“æœæåˆ°é¡¶å±‚ï¼Œæ–¹ä¾¿å‰ç«¯è®¿é—®
        if result.get("success"):
            execution_result = result.get("execution_result", {})
            enhanced_result = {
                **result,
                # ğŸ”‘ å°†æŸ¥è¯¢æ•°æ®æåˆ°é¡¶å±‚ï¼Œæ–¹ä¾¿å‰ç«¯ç›´æ¥è®¿é—®
                "rows": execution_result.get("rows", []),
                "row_count": execution_result.get("row_count", 0),
                "primary_value": execution_result.get("primary_value"),
                "columns": execution_result.get("metadata", {}).get("columns", []),
            }

            return APIResponse(
                success=True,
                data=enhanced_result,
                message=f"âœ… SQLéªŒè¯æˆåŠŸï¼Œè¿”å› {enhanced_result['row_count']} è¡Œæ•°æ®"
            )
        else:
            return APIResponse(
                success=False,
                data=result,
                message=f"âŒ SQLéªŒè¯å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}"
            )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ å ä½ç¬¦SQLéªŒè¯å¼‚å¸¸: {e}")
        raise HTTPException(status_code=500, detail=f"éªŒè¯è¿‡ç¨‹å¼‚å¸¸: {str(e)}")


@router.post("/batch-validate-sql", response_model=APIResponse[Dict[str, Any]])
async def batch_validate_placeholder_sqls(
    request: Dict[str, Any],
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
) -> APIResponse[Dict[str, Any]]:
    """æ‰¹é‡éªŒè¯å¤šä¸ªå ä½ç¬¦SQL - ç‹¬ç«‹åŠŸèƒ½"""
    try:
        from app.crud.crud_data_source import crud_data_source
        from app.services.data.validation.sql_validation_service import sql_validation_service

        sql_templates = request.get("sql_templates", {})
        data_source_id = request.get("data_source_id")
        execution_mode = request.get("execution_mode", "test")
        fixed_date = request.get("fixed_date")

        if not sql_templates:
            raise HTTPException(status_code=400, detail="ç¼ºå°‘sql_templateså‚æ•°")
        if not data_source_id:
            raise HTTPException(status_code=400, detail="ç¼ºå°‘data_source_idå‚æ•°")

        # éªŒè¯æ•°æ®æºæƒé™
        ds = crud_data_source.get_user_data_source(db, data_source_id=data_source_id, user_id=current_user.id)
        if not ds:
            raise HTTPException(status_code=404, detail="æ•°æ®æºä¸å­˜åœ¨æˆ–æ— æƒé™è®¿é—®")

        logger.info(f"ğŸ” æ‰¹é‡å ä½ç¬¦SQLéªŒè¯è¯·æ±‚: {len(sql_templates)} ä¸ª")

        # æ‰§è¡Œæ‰¹é‡éªŒè¯
        result = await sql_validation_service.batch_validate_placeholder_sqls(
            sql_templates=sql_templates,
            data_source_id=str(data_source_id),
            execution_mode=execution_mode,
            fixed_date=fixed_date
        )

        return APIResponse(
            success=result.get("success", False),
            data=result,
            message=f"æ‰¹é‡éªŒè¯å®Œæˆ: {result.get('summary', {}).get('successful_count', 0)} æˆåŠŸ"
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ æ‰¹é‡å ä½ç¬¦SQLéªŒè¯å¼‚å¸¸: {e}")
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡éªŒè¯è¿‡ç¨‹å¼‚å¸¸: {str(e)}")


# ================================================================================
# å…¼å®¹æ€§æ¥å£ - æ˜ å°„åˆ°Agent Pipeline
# ================================================================================

@router.post("/test-sql", response_model=APIResponse[Dict[str, Any]])
async def test_sql_with_agent(
    request: Dict[str, Any],
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
) -> APIResponse[Dict[str, Any]]:
    """ç›´æ¥è¿æ¥æ•°æ®æºæ‰§è¡ŒSQLå¹¶è¿”å›ç»“æœï¼ˆä¸ä½¿ç”¨Agentï¼‰ã€‚"""
    try:
        from app.crud.crud_data_source import crud_data_source
        from app.services.data.query.query_executor_service import query_executor_service

        sql = request.get("sql")
        data_source_id = request.get("data_source_id")
        placeholder_name = request.get("placeholder_name", "SQLæµ‹è¯•")

        if not sql:
            raise HTTPException(status_code=400, detail="ç¼ºå°‘SQLå‚æ•°")

        if not data_source_id:
            raise HTTPException(status_code=400, detail="ç¼ºå°‘æ•°æ®æºID")

        # é‰´æƒï¼šç¡®è®¤æ•°æ®æºå±äºå½“å‰ç”¨æˆ·ä¸”å¯ç”¨
        ds = crud_data_source.get_user_data_source(db, data_source_id=data_source_id, user_id=current_user.id)
        if not ds:
            raise HTTPException(status_code=404, detail="æ•°æ®æºä¸å­˜åœ¨æˆ–æ— æƒé™è®¿é—®")

        logger.info(f"ğŸ§ª ç›´æ¥SQLæµ‹è¯•ï¼ˆéAgentï¼‰: {placeholder_name}")

        # æ‰§è¡ŒæŸ¥è¯¢ï¼ˆQueryExecutorService å·²åŒ…å«SQLå®‰å…¨æ ¡éªŒï¼Œä»…å…è®¸SELECTï¼‰
        result = await query_executor_service.execute_query(sql, {"data_source_id": str(data_source_id)})

        success = bool(result.get("success"))
        meta = result.get("metadata", {}) or {}
        data = result.get("data", []) or []

        # ç»Ÿä¸€å‰ç«¯æœŸæœ›çš„ test_result ç»“æ„
        test_result = {
            "success": success,
            "message": meta.get("message") or ("æŸ¥è¯¢æˆåŠŸ" if success else meta.get("error") or result.get("error") or "æŸ¥è¯¢å¤±è´¥"),
            "data": data,  # è¿”å›è®°å½•åˆ—è¡¨ï¼ˆlist[dict]ï¼‰
            "row_count": meta.get("row_count", len(data)),
            "execution_time_ms": int((meta.get("execution_time") or result.get("execution_time") or 0) * 1000),
            "columns": meta.get("columns", []),
        }

        # åºåˆ—åŒ–è¿”å›ç»“æ„
        response_payload = _orchestration_service._serialize_datetime_objects({
            "placeholder_name": placeholder_name,
            "sql": sql,
            "test_result": test_result,
            "tested_at": datetime.now().isoformat(),
            "agent_executed": False
        })

        return APIResponse(
            success=success,
            data=response_payload,
            message="SQLæ‰§è¡ŒæˆåŠŸ" if success else (meta.get("error") or result.get("error") or "SQLæ‰§è¡Œå¤±è´¥")
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ç›´æ¥SQLæµ‹è¯•å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æµ‹è¯•å¤±è´¥: {str(e)}")

# ================================================================================
# è¾…åŠ©å‡½æ•°
# ================================================================================

@router.put("/{placeholder_id}/sql", response_model=APIResponse[TemplatePlaceholder])
async def update_placeholder_sql(
    placeholder_id: str,
    request: Dict[str, Any],
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
) -> APIResponse[TemplatePlaceholder]:
    """
    æ›´æ–°å ä½ç¬¦SQLé…ç½®

    æ”¯æŒå­—æ®µï¼š
    - generated_sql: SQLè¯­å¥
    - execution_order: æ‰§è¡Œé¡ºåº
    - cache_ttl_hours: ç¼“å­˜TTL (å°æ—¶)
    - is_active: æ˜¯å¦å¯ç”¨
    - placeholder_type: ç±»å‹ (ç»Ÿè®¡/æ’å/å¯¹æ¯”ç­‰)
    - description: æè¿°
    """
    try:
        # éªŒè¯placeholderå­˜åœ¨
        placeholder = crud.template_placeholder.get(db=db, id=placeholder_id)
        if not placeholder:
            raise HTTPException(status_code=404, detail="å ä½ç¬¦ä¸å­˜åœ¨")

        # éªŒè¯æƒé™ï¼ˆå¯é€‰ï¼šæ£€æŸ¥æ¨¡æ¿æ‰€æœ‰è€…ï¼‰

        # æ„å»ºæ›´æ–°æ•°æ®
        update_data = {}

        # SQLç›¸å…³å­—æ®µ
        if "generated_sql" in request:
            update_data["generated_sql"] = request["generated_sql"]
            update_data["sql_validated"] = False  # SQLæ”¹å˜åéœ€è¦é‡æ–°éªŒè¯

        # é…ç½®å­—æ®µ
        if "execution_order" in request:
            update_data["execution_order"] = int(request["execution_order"])

        if "cache_ttl_hours" in request:
            ttl = int(request["cache_ttl_hours"])
            if 1 <= ttl <= 24*30:  # 1å°æ—¶åˆ°30å¤©
                update_data["cache_ttl_hours"] = ttl

        if "is_active" in request:
            update_data["is_active"] = bool(request["is_active"])

        if "placeholder_type" in request:
            valid_types = ["ç»Ÿè®¡", "æ’å", "å¯¹æ¯”", "è¶‹åŠ¿", "å›¾è¡¨", "è‡ªå®šä¹‰"]
            if request["placeholder_type"] in valid_types:
                update_data["placeholder_type"] = request["placeholder_type"]

        if "description" in request:
            update_data["description"] = request["description"]

        # æ‰§è¡Œæ›´æ–°
        placeholder_update = TemplatePlaceholderUpdate(**update_data)
        updated_placeholder = crud.template_placeholder.update(
            db=db, db_obj=placeholder, obj_in=placeholder_update
        )
        db.commit()

        logger.info(f"âœ… å ä½ç¬¦SQLé…ç½®æ›´æ–°æˆåŠŸ: {placeholder_id}")

        return APIResponse(
            success=True,
            data=updated_placeholder,
            message="å ä½ç¬¦é…ç½®æ›´æ–°æˆåŠŸ"
        )

    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        logger.error(f"âŒ æ›´æ–°å ä½ç¬¦SQLé…ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ›´æ–°å¤±è´¥: {str(e)}")


@router.get("/template/{template_id}/list", response_model=APIResponse[List[TemplatePlaceholder]])
async def get_template_placeholders(
    template_id: str,
    include_inactive: bool = False,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
) -> APIResponse[List[TemplatePlaceholder]]:
    """è·å–æ¨¡æ¿çš„æ‰€æœ‰å ä½ç¬¦ï¼ˆç”¨äºç¼–è¾‘ç•Œé¢ï¼‰"""
    try:
        placeholders = crud.template_placeholder.get_by_template(
            db=db,
            template_id=template_id,
            include_inactive=include_inactive
        )

        return APIResponse(
            success=True,
            data=placeholders,
            message=f"è·å–åˆ° {len(placeholders)} ä¸ªå ä½ç¬¦"
        )

    except Exception as e:
        logger.error(f"âŒ è·å–æ¨¡æ¿å ä½ç¬¦å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–å¤±è´¥: {str(e)}")


async def _save_placeholder_result(
    db: Session,
    template_id: str,
    placeholder_name: str,
    placeholder_text: str,
    result: Dict[str, Any]
):
    """ä¿å­˜Agent Pipelineåˆ†æç»“æœåˆ°æ•°æ®åº“"""
    try:
        logger.info(f"ğŸ” [Debug] ä¿å­˜å ä½ç¬¦å¼€å§‹: name='{placeholder_name}', template_id='{template_id}'")

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        existing = crud.template_placeholder.get_by_template_and_name(
            db=db, template_id=template_id, name=placeholder_name
        )

        if existing:
            logger.info(f"ğŸ” [Debug] æ‰¾åˆ°ç°æœ‰è®°å½•: id={existing.id}, placeholder_name='{existing.placeholder_name}'")
            if existing.generated_sql:
                logger.info(f"ğŸ” [Debug] ç°æœ‰SQL: {existing.generated_sql[:100]}...")
            else:
                logger.info(f"ğŸ” [Debug] ç°æœ‰SQL: None")
        else:
            logger.warning(f"âš ï¸ [Debug] æœªæ‰¾åˆ°ç°æœ‰è®°å½•ï¼Œå°†åˆ›å»ºæ–°è®°å½•")

        generated_sql = result.get("generated_sql", {})
        if isinstance(generated_sql, dict):
            sql_content = generated_sql.get("sql", "")
        elif isinstance(generated_sql, str):
            sql_content = generated_sql.strip()
        else:
            sql_content = ""

        logger.info(f"ğŸ” [Debug] æå–çš„SQLå†…å®¹é•¿åº¦: {len(sql_content)} å­—ç¬¦")
        if sql_content:
            logger.info(f"ğŸ” [Debug] SQLé¢„è§ˆ: {sql_content[:100]}...")

        analysis_result = result.get("analysis_result", {})
        semantic_type = analysis_result.get("semantic_type", "stat")

        # æå–æµ‹è¯•ç»“æœçŠ¶æ€
        test_result = result.get("test_result", {})
        sql_validated = test_result.get("executed", False) and test_result.get("success", False)

        logger.info(f"ğŸ” [Debug] æµ‹è¯•ç»“æœçŠ¶æ€ - executed={test_result.get('executed')}, success={test_result.get('success')}, sql_validated={sql_validated}")

        # æ„å»ºè¦ä¿å­˜çš„æ•°æ®ï¼ˆåŒ…æ‹¬SQLéªŒè¯çŠ¶æ€å’Œæµ‹è¯•ç»“æœï¼‰
        placeholder_data = {
            "placeholder_name": placeholder_name,
            "placeholder_text": placeholder_text,
            "placeholder_type": "variable",
            "content_type": "text",
            "generated_sql": sql_content,
            "sql_validated": sql_validated,  # ğŸ”‘ ä¿å­˜éªŒè¯çŠ¶æ€
            "confidence_score": result.get("confidence_score", 0.8),
            "agent_analyzed": True,
            "is_active": True,
            "execution_order": 1,
            "cache_ttl_hours": 24,
            "description": f"Agent Pipelineåˆ†æ({semantic_type}): {placeholder_name}",
            # ğŸ”‘ å°†test_resultä¿å­˜åˆ°agent_configä¸­ï¼Œä¾›å‰ç«¯æŸ¥è¯¢ä½¿ç”¨
            "agent_config": {
                "last_test_result": test_result,
                "last_analysis_result": analysis_result,
                "semantic_type": semantic_type
            }
        }

        saved_placeholder = None
        if existing:
            # æ›´æ–°ç°æœ‰å ä½ç¬¦
            logger.info(f"ğŸ” [Debug] å‡†å¤‡æ›´æ–°ç°æœ‰è®°å½• id={existing.id}")
            placeholder_update = TemplatePlaceholderUpdate(**{
                k: v for k, v in placeholder_data.items()
                if k not in ["id", "template_id", "created_at", "updated_at"]
            })
            saved_placeholder = crud.template_placeholder.update(
                db=db, db_obj=existing, obj_in=placeholder_update
            )
            logger.info(f"ğŸ” [Debug] æ›´æ–°æˆåŠŸ: id={saved_placeholder.id}")
        else:
            # åˆ›å»ºæ–°å ä½ç¬¦
            logger.info(f"ğŸ” [Debug] å‡†å¤‡åˆ›å»ºæ–°è®°å½•")
            placeholder_create = TemplatePlaceholderCreate(
                template_id=template_id,
                **{k: v for k, v in placeholder_data.items()
                   if k not in ["id", "template_id", "created_at", "updated_at"]}
            )
            saved_placeholder = crud.template_placeholder.create(
                db=db, obj_in=placeholder_create
            )
            logger.info(f"ğŸ” [Debug] åˆ›å»ºæˆåŠŸ: id={saved_placeholder.id}")

        logger.info(f"ğŸ” [Debug] å‡†å¤‡æäº¤æ•°æ®åº“äº‹åŠ¡...")
        db.commit()
        logger.info(f"âœ… [Debug] æ•°æ®åº“äº‹åŠ¡æäº¤æˆåŠŸ")
        logger.info(f"âœ… ä¿å­˜Agent Pipelineç»“æœæˆåŠŸ: {placeholder_name}")
        return saved_placeholder

    except Exception as e:
        db.rollback()
        logger.error(f"âŒ ä¿å­˜Agent Pipelineç»“æœå¤±è´¥: {e}")
        logger.exception(e)  # æ‰“å°å®Œæ•´å †æ ˆè·Ÿè¸ª
        raise


# ================================================================================
# SSE æµå¼åˆ†æAPI - è§£å†³å‰ç«¯è¶…æ—¶é—®é¢˜
# ================================================================================

async def generate_analysis_progress(
    placeholder_name: str,
    placeholder_text: str,
    template_id: str,
    data_source_id: str = None,
    template_context: Dict[str, Any] = None,
    user_id: str = None,
    **kwargs
):
    """
    ç”Ÿæˆåˆ†æè¿›åº¦çš„SSEæµ
    åœ¨åˆ†æè¿‡ç¨‹ä¸­å‘é€é˜¶æ®µä¿¡æ¯ï¼Œé¿å…å‰ç«¯è¶…æ—¶
    """
    try:
        # å‘é€å¼€å§‹ä¿¡å·
        yield f"data: {json.dumps({'stage': 'started', 'message': f'å¼€å§‹åˆ†æå ä½ç¬¦: {placeholder_name}', 'progress': 0})}\n\n"
        await asyncio.sleep(0.1)

        # å‘é€Schemaåˆ†æé˜¶æ®µ
        yield f"data: {json.dumps({'stage': 'schema_analysis', 'message': 'æ­£åœ¨è·å–æ•°æ®åº“Schemaä¿¡æ¯...', 'progress': 20})}\n\n"
        await asyncio.sleep(0.5)

        # å‘é€SQLç”Ÿæˆé˜¶æ®µ
        yield f"data: {json.dumps({'stage': 'sql_generation', 'message': 'æ­£åœ¨ç”ŸæˆSQLæŸ¥è¯¢è¯­å¥...', 'progress': 40})}\n\n"
        await asyncio.sleep(0.5)

        # è°ƒç”¨å®é™…çš„åˆ†æé€»è¾‘
        result = await _orchestration_service.analyze_placeholder_with_full_pipeline(
            placeholder_name=placeholder_name,
            placeholder_text=placeholder_text,
            template_id=template_id,
            data_source_id=data_source_id,
            template_context=template_context,
            user_id=user_id,
            **kwargs
        )

        # å‘é€SQLéªŒè¯é˜¶æ®µ
        yield f"data: {json.dumps({'stage': 'sql_validation', 'message': 'æ­£åœ¨éªŒè¯SQLè¯­æ³•å’Œé€»è¾‘...', 'progress': 60})}\n\n"
        await asyncio.sleep(0.5)

        # å‘é€æ‰§è¡Œæµ‹è¯•é˜¶æ®µï¼ˆå¦‚æœæœ‰ï¼‰
        if result.get("test_result"):
            yield f"data: {json.dumps({'stage': 'sql_execution', 'message': 'æ­£åœ¨æ‰§è¡ŒSQLæµ‹è¯•...', 'progress': 80})}\n\n"
            await asyncio.sleep(0.5)

        # å‘é€å®Œæˆä¿¡å·
        yield f"data: {json.dumps({'stage': 'completed', 'message': 'åˆ†æå®Œæˆ', 'progress': 100, 'result': result})}\n\n"

    except Exception as e:
        # å‘é€é”™è¯¯ä¿¡å·
        yield f"data: {json.dumps({'stage': 'error', 'message': f'åˆ†æå¤±è´¥: {str(e)}', 'progress': -1, 'error': str(e)})}\n\n"


@router.post("/analyze-stream")
async def analyze_placeholder_with_stream(
    request: Dict[str, Any],
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
):
    """
    æµå¼å ä½ç¬¦åˆ†æAPI - é€šè¿‡SSEè¿”å›è¿›åº¦ä¿¡æ¯

    è§£å†³å‰ç«¯30ç§’è¶…æ—¶é—®é¢˜ï¼Œå®æ—¶æ˜¾ç¤ºåˆ†æè¿›åº¦
    """
    try:
        # å‚æ•°éªŒè¯
        placeholder_name = request.get("placeholder_name")
        placeholder_text = request.get("placeholder_text")
        template_id = request.get("template_id")

        if not all([placeholder_name, placeholder_text, template_id]):
            raise HTTPException(
                status_code=400,
                detail="ç¼ºå°‘å¿…éœ€å‚æ•°: placeholder_name, placeholder_text, template_id"
            )

        # å‡†å¤‡åˆ†æå‚æ•°
        kwargs = {k: v for k, v in request.items() if k not in [
            'placeholder_name', 'placeholder_text', 'template_id',
            'data_source_id', 'template_context'
        ]}

        # è¿”å›SSEæµ
        return StreamingResponse(
            generate_analysis_progress(
                placeholder_name=placeholder_name,
                placeholder_text=placeholder_text,
                template_id=template_id,
                data_source_id=request.get("data_source_id"),
                template_context=request.get("template_context"),
                user_id=str(current_user.id),
                **kwargs
            ),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Headers": "*",
            }
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æµå¼åˆ†æå¯åŠ¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å¯åŠ¨æµå¼åˆ†æå¤±è´¥: {str(e)}")
