"""
å ä½ç¬¦å¤„ç†å™¨ - æ ¸å¿ƒæ•°æ®æŸ¥è¯¢æ„å»ºå¼•æ“

ä¸“é—¨å¤„ç†å ä½ç¬¦ä¸­çš„æç¤ºè¯ï¼Œæ„å»ºå‡†ç¡®çš„æ•°æ®æŸ¥è¯¢ï¼Œå¹¶è·å–æ­£ç¡®çš„æ•°æ®ã€‚
è¿™æ˜¯å®ç°ä»å ä½ç¬¦åˆ°æ•°æ®çš„æ ¸å¿ƒå¤„ç†æµç¨‹ã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. æ™ºèƒ½è§£æå ä½ç¬¦è¯­æ³•å’Œæç¤ºè¯
2. æ„å»ºç²¾ç¡®çš„æ•°æ®åº“æŸ¥è¯¢
3. æ‰§è¡ŒæŸ¥è¯¢å¹¶è·å–æ•°æ®
4. éªŒè¯æ•°æ®å‡†ç¡®æ€§å’Œå®Œæ•´æ€§
"""

import re
import json
import asyncio
from typing import Dict, Any, List, Optional, Tuple, Union
from datetime import datetime, timedelta
from dataclasses import dataclass, field

from ..enhanced.enhanced_data_query_agent import EnhancedDataQueryAgent, SemanticQueryRequest
from ..tools.data_processing_tools import DataValidationTool, DataTransformationTool
from ..base import AgentResult


@dataclass
class PlaceholderContext:
    """å ä½ç¬¦ä¸Šä¸‹æ–‡"""
    original_placeholder: str
    parsed_content: Dict[str, Any]
    data_source: str
    table_name: Optional[str] = None
    schema_info: Dict[str, Any] = field(default_factory=dict)
    user_context: Dict[str, Any] = field(default_factory=dict)


@dataclass
class QuerySpec:
    """æŸ¥è¯¢è§„æ ¼è¯´æ˜"""
    select_fields: List[str]
    from_table: str
    where_conditions: List[Dict[str, Any]]
    group_by_fields: List[str]
    order_by_fields: List[Dict[str, str]]  # [{"field": "date", "direction": "desc"}]
    aggregations: List[Dict[str, str]]     # [{"function": "sum", "field": "amount", "alias": "total"}]
    calculations: List[Dict[str, Any]]     # éœ€è¦è®¡ç®—çš„æŒ‡æ ‡
    limit: Optional[int] = None


@dataclass
class DataResult:
    """æ•°æ®ç»“æœ"""
    success: bool
    data: List[Dict[str, Any]]
    row_count: int
    columns: List[str]
    query_info: Dict[str, Any]
    data_quality: Dict[str, Any]
    processing_log: List[str]
    error_message: Optional[str] = None


class PlaceholderParser:
    """å ä½ç¬¦è§£æå™¨"""
    
    def __init__(self):
        # å®šä¹‰å ä½ç¬¦çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        self.placeholder_pattern = r'\{\{([^}]+)\}\}'
        
        # å®šä¹‰å…³é”®è¯æ˜ å°„
        self.time_keywords = {
            "æœ€è¿‘3ä¸ªæœˆ": {"type": "relative", "value": 3, "unit": "month"},
            "æœ€è¿‘6ä¸ªæœˆ": {"type": "relative", "value": 6, "unit": "month"},
            "æœ€è¿‘1å¹´": {"type": "relative", "value": 1, "unit": "year"},
            "æœ¬æœˆ": {"type": "current", "unit": "month"},
            "æœ¬å­£åº¦": {"type": "current", "unit": "quarter"},
            "æœ¬å¹´": {"type": "current", "unit": "year"},
            "ä¸Šä¸ªæœˆ": {"type": "previous", "unit": "month"},
            "å»å¹´åŒæœŸ": {"type": "year_ago", "unit": "same_period"}
        }
        
        self.metric_keywords = {
            "é”€å”®é¢": {"field": "sales_amount", "type": "numeric", "aggregation": "sum"},
            "é”€å”®é‡": {"field": "sales_quantity", "type": "numeric", "aggregation": "sum"},
            "è®¢å•æ•°": {"field": "order_count", "type": "numeric", "aggregation": "count"},
            "å®¢æˆ·æ•°": {"field": "customer_count", "type": "numeric", "aggregation": "count_distinct"},
            "å¹³å‡å®¢å•ä»·": {"field": "avg_order_value", "type": "calculated", "formula": "sales_amount / order_count"},
            "åˆ©æ¶¦": {"field": "profit", "type": "numeric", "aggregation": "sum"},
            "åˆ©æ¶¦ç‡": {"field": "profit_rate", "type": "calculated", "formula": "profit / sales_amount * 100"}
        }
        
        self.dimension_keywords = {
            "åœ°åŒº": {"field": "region", "type": "category"},
            "äº§å“": {"field": "product_name", "type": "category"}, 
            "äº§å“ç±»åˆ«": {"field": "product_category", "type": "category"},
            "æ¸ é“": {"field": "channel", "type": "category"},
            "å®¢æˆ·ç±»å‹": {"field": "customer_type", "type": "category"},
            "æœˆä»½": {"field": "month", "type": "date_part"},
            "å­£åº¦": {"field": "quarter", "type": "date_part"},
            "å¹´ä»½": {"field": "year", "type": "date_part"}
        }
        
        self.calculation_keywords = {
            "åŒæ¯”å¢é•¿ç‡": {"type": "yoy_growth", "requires": ["current_period", "same_period_last_year"]},
            "ç¯æ¯”å¢é•¿ç‡": {"type": "mom_growth", "requires": ["current_period", "previous_period"]},
            "ç´¯è®¡å€¼": {"type": "cumulative", "requires": ["running_total"]},
            "å¹³å‡å€¼": {"type": "average", "function": "avg"},
            "æœ€å¤§å€¼": {"type": "maximum", "function": "max"},
            "æœ€å°å€¼": {"type": "minimum", "function": "min"},
            "å æ¯”": {"type": "percentage", "requires": ["part", "total"]}
        }
    
    async def parse_placeholder(self, placeholder_text: str) -> PlaceholderContext:
        """è§£æå ä½ç¬¦å†…å®¹"""
        try:
            # æå–å ä½ç¬¦å†…å®¹
            matches = re.findall(self.placeholder_pattern, placeholder_text)
            if not matches:
                raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆçš„å ä½ç¬¦æ ¼å¼ {{...}}")
            
            placeholder_content = matches[0]
            
            # åˆ†æå ä½ç¬¦ç»“æ„
            parsed_content = {
                "analysis_type": self._extract_analysis_type(placeholder_content),
                "time_range": self._extract_time_range(placeholder_content),
                "metrics": self._extract_metrics(placeholder_content),
                "dimensions": self._extract_dimensions(placeholder_content),
                "calculations": self._extract_calculations(placeholder_content),
                "filters": self._extract_filters(placeholder_content),
                "data_source": self._identify_data_source(placeholder_content)
            }
            
            # åˆ›å»ºä¸Šä¸‹æ–‡
            context = PlaceholderContext(
                original_placeholder=placeholder_text,
                parsed_content=parsed_content,
                data_source=parsed_content["data_source"]
            )
            
            print(f"ğŸ“ å ä½ç¬¦è§£æå®Œæˆ:")
            print(f"   åŸå§‹å†…å®¹: {placeholder_text}")
            print(f"   åˆ†æç±»å‹: {parsed_content['analysis_type']}")
            print(f"   æ—¶é—´èŒƒå›´: {parsed_content['time_range']}")
            print(f"   æŒ‡æ ‡: {parsed_content['metrics']}")
            print(f"   ç»´åº¦: {parsed_content['dimensions']}")
            print(f"   è®¡ç®—: {parsed_content['calculations']}")
            
            return context
            
        except Exception as e:
            print(f"âŒ å ä½ç¬¦è§£æå¤±è´¥: {e}")
            raise
    
    def _extract_analysis_type(self, content: str) -> str:
        """æå–åˆ†æç±»å‹"""
        if "é”€å”®" in content:
            return "sales_analysis"
        elif "ç”¨æˆ·" in content or "å®¢æˆ·" in content:
            return "customer_analysis"
        elif "è´¢åŠ¡" in content or "æ”¶å…¥" in content or "åˆ©æ¶¦" in content:
            return "financial_analysis"
        elif "è®¢å•" in content:
            return "order_analysis"
        elif "äº§å“" in content:
            return "product_analysis"
        else:
            return "general_analysis"
    
    def _extract_time_range(self, content: str) -> Optional[Dict[str, Any]]:
        """æå–æ—¶é—´èŒƒå›´"""
        for keyword, time_info in self.time_keywords.items():
            if keyword in content:
                return {
                    "keyword": keyword,
                    **time_info
                }
        return None
    
    def _extract_metrics(self, content: str) -> List[Dict[str, Any]]:
        """æå–æŒ‡æ ‡"""
        metrics = []
        for keyword, metric_info in self.metric_keywords.items():
            if keyword in content:
                metrics.append({
                    "keyword": keyword,
                    **metric_info
                })
        return metrics
    
    def _extract_dimensions(self, content: str) -> List[Dict[str, Any]]:
        """æå–ç»´åº¦"""
        dimensions = []
        
        # æŸ¥æ‰¾æ˜ç¡®çš„åˆ†ç»„ç»´åº¦
        group_patterns = [
            r"æŒ‰(\w+)åˆ†ç»„", r"æŒ‰(\w+)ç»Ÿè®¡", r"åˆ†(\w+)ç»Ÿè®¡", r"(\w+)ç»´åº¦"
        ]
        
        for pattern in group_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                if match in self.dimension_keywords:
                    dim_info = self.dimension_keywords[match]
                    dimensions.append({
                        "keyword": match,
                        **dim_info
                    })
        
        # æŸ¥æ‰¾éšå«çš„ç»´åº¦
        for keyword, dim_info in self.dimension_keywords.items():
            if keyword in content and not any(d["keyword"] == keyword for d in dimensions):
                dimensions.append({
                    "keyword": keyword,
                    **dim_info
                })
        
        return dimensions
    
    def _extract_calculations(self, content: str) -> List[Dict[str, Any]]:
        """æå–è®¡ç®—è¦æ±‚"""
        calculations = []
        for keyword, calc_info in self.calculation_keywords.items():
            if keyword in content:
                calculations.append({
                    "keyword": keyword,
                    **calc_info
                })
        return calculations
    
    def _extract_filters(self, content: str) -> List[Dict[str, Any]]:
        """æå–è¿‡æ»¤æ¡ä»¶"""
        filters = []
        
        # æå–å…·ä½“çš„è¿‡æ»¤æ¡ä»¶
        # ä¾‹å¦‚: "å¤§äº1000", "ç­‰äºåŒ—äº¬", "åŒ…å«VIP"
        filter_patterns = [
            (r"å¤§äº(\d+)", {"operator": ">", "type": "numeric"}),
            (r"å°äº(\d+)", {"operator": "<", "type": "numeric"}),
            (r"ç­‰äº(\w+)", {"operator": "=", "type": "exact"}),
            (r"åŒ…å«(\w+)", {"operator": "like", "type": "text"}),
            (r"ä¸åŒ…å«(\w+)", {"operator": "not_like", "type": "text"})
        ]
        
        for pattern, filter_info in filter_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                filters.append({
                    "value": match,
                    **filter_info
                })
        
        return filters
    
    def _identify_data_source(self, content: str) -> str:
        """è¯†åˆ«æ•°æ®æº"""
        # æ ¹æ®åˆ†æç±»å‹æ¨æ–­æ•°æ®æº
        if "é”€å”®" in content:
            return "sales_database"
        elif "ç”¨æˆ·" in content or "å®¢æˆ·" in content:
            return "user_database"
        elif "è®¢å•" in content:
            return "order_database"
        elif "è´¢åŠ¡" in content:
            return "financial_database"
        else:
            return "main_database"


class QueryBuilder:
    """æŸ¥è¯¢æ„å»ºå™¨"""
    
    def __init__(self):
        # è¡¨åæ˜ å°„
        self.table_mapping = {
            "sales_analysis": "sales_data",
            "customer_analysis": "customer_data", 
            "financial_analysis": "financial_data",
            "order_analysis": "order_data",
            "product_analysis": "product_data"
        }
        
        # å­—æ®µæ˜ å°„
        self.field_mapping = {
            "sales_amount": "amount",
            "sales_quantity": "quantity",
            "order_count": "order_id",
            "customer_count": "customer_id",
            "region": "region_name",
            "product_name": "product_name",
            "product_category": "category"
        }
    
    async def build_query_spec(self, context: PlaceholderContext) -> QuerySpec:
        """æ„å»ºæŸ¥è¯¢è§„æ ¼"""
        try:
            parsed = context.parsed_content
            
            # ç¡®å®šä¸»è¡¨
            table_name = self.table_mapping.get(parsed["analysis_type"], "main_table")
            
            # æ„å»ºSELECTå­—æ®µ
            select_fields = []
            aggregations = []
            
            # æ·»åŠ ç»´åº¦å­—æ®µ
            for dim in parsed["dimensions"]:
                field_name = self.field_mapping.get(dim["field"], dim["field"])
                select_fields.append(field_name)
            
            # æ·»åŠ æŒ‡æ ‡å­—æ®µï¼ˆéœ€è¦èšåˆï¼‰
            for metric in parsed["metrics"]:
                field_name = self.field_mapping.get(metric["field"], metric["field"])
                if metric["type"] == "numeric":
                    agg_function = metric["aggregation"]
                    aggregations.append({
                        "function": agg_function,
                        "field": field_name,
                        "alias": f"{agg_function}_{field_name}"
                    })
            
            # æ„å»ºWHEREæ¡ä»¶
            where_conditions = []
            
            # æ·»åŠ æ—¶é—´æ¡ä»¶
            if parsed["time_range"]:
                time_condition = self._build_time_condition(parsed["time_range"])
                if time_condition:
                    where_conditions.append(time_condition)
            
            # æ·»åŠ è¿‡æ»¤æ¡ä»¶
            for filter_item in parsed["filters"]:
                where_conditions.append({
                    "field": "value_field",  # éœ€è¦æ ¹æ®ä¸Šä¸‹æ–‡ç¡®å®š
                    "operator": filter_item["operator"],
                    "value": filter_item["value"]
                })
            
            # æ„å»ºGROUP BY
            group_by_fields = [
                self.field_mapping.get(dim["field"], dim["field"]) 
                for dim in parsed["dimensions"]
            ]
            
            # æ„å»ºORDER BY
            order_by_fields = []
            if group_by_fields:
                order_by_fields.append({
                    "field": group_by_fields[0],
                    "direction": "asc"
                })
            
            # æ„å»ºè®¡ç®—å­—æ®µ
            calculations = []
            for calc in parsed["calculations"]:
                calc_spec = self._build_calculation_spec(calc)
                if calc_spec:
                    calculations.append(calc_spec)
            
            query_spec = QuerySpec(
                select_fields=select_fields,
                from_table=table_name,
                where_conditions=where_conditions,
                group_by_fields=group_by_fields,
                order_by_fields=order_by_fields,
                aggregations=aggregations,
                calculations=calculations
            )
            
            print(f"ğŸ”§ æŸ¥è¯¢è§„æ ¼æ„å»ºå®Œæˆ:")
            print(f"   è¡¨å: {table_name}")
            print(f"   é€‰æ‹©å­—æ®µ: {select_fields}")
            print(f"   èšåˆå‡½æ•°: {[f\"{a['function']}({a['field']})\" for a in aggregations]}")
            print(f"   åˆ†ç»„å­—æ®µ: {group_by_fields}")
            print(f"   æ¡ä»¶æ•°é‡: {len(where_conditions)}")
            
            return query_spec
            
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢è§„æ ¼æ„å»ºå¤±è´¥: {e}")
            raise
    
    def _build_time_condition(self, time_range: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """æ„å»ºæ—¶é—´æ¡ä»¶"""
        try:
            if time_range["type"] == "relative":
                # è®¡ç®—ç›¸å¯¹æ—¶é—´
                if time_range["unit"] == "month":
                    start_date = datetime.now() - timedelta(days=time_range["value"] * 30)
                elif time_range["unit"] == "year":
                    start_date = datetime.now() - timedelta(days=time_range["value"] * 365)
                else:
                    return None
                
                return {
                    "field": "date",
                    "operator": ">=",
                    "value": start_date.strftime("%Y-%m-%d")
                }
            
            elif time_range["type"] == "current":
                # å½“å‰å‘¨æœŸ
                now = datetime.now()
                if time_range["unit"] == "month":
                    start_date = now.replace(day=1)
                elif time_range["unit"] == "quarter":
                    quarter = (now.month - 1) // 3 + 1
                    start_month = (quarter - 1) * 3 + 1
                    start_date = now.replace(month=start_month, day=1)
                else:
                    return None
                
                return {
                    "field": "date", 
                    "operator": ">=",
                    "value": start_date.strftime("%Y-%m-%d")
                }
            
            return None
            
        except Exception:
            return None
    
    def _build_calculation_spec(self, calculation: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """æ„å»ºè®¡ç®—è§„æ ¼"""
        calc_type = calculation["type"]
        
        if calc_type == "yoy_growth":
            return {
                "type": "year_over_year_growth",
                "formula": "(current_value - previous_year_value) / previous_year_value * 100",
                "requires_historical_data": True
            }
        elif calc_type == "mom_growth":
            return {
                "type": "month_over_month_growth", 
                "formula": "(current_value - previous_month_value) / previous_month_value * 100",
                "requires_historical_data": True
            }
        elif calc_type == "average":
            return {
                "type": "average",
                "function": calculation["function"]
            }
        elif calc_type == "percentage":
            return {
                "type": "percentage",
                "formula": "value / total * 100"
            }
        
        return None


class DataQueryExecutor:
    """æ•°æ®æŸ¥è¯¢æ‰§è¡Œå™¨"""
    
    def __init__(self):
        self.data_agent = EnhancedDataQueryAgent()
        self.validation_tool = DataValidationTool()
        self.transform_tool = DataTransformationTool()
    
    async def execute_query(
        self, 
        context: PlaceholderContext, 
        query_spec: QuerySpec
    ) -> DataResult:
        """æ‰§è¡Œæ•°æ®æŸ¥è¯¢"""
        try:
            processing_log = []
            processing_log.append(f"å¼€å§‹æ‰§è¡ŒæŸ¥è¯¢: {context.original_placeholder}")
            
            # 1. æ„å»ºè¯­ä¹‰æŸ¥è¯¢è¯·æ±‚
            semantic_request = self._build_semantic_request(context, query_spec)
            processing_log.append(f"æ„å»ºè¯­ä¹‰æŸ¥è¯¢è¯·æ±‚å®Œæˆ")
            
            # 2. æ‰§è¡ŒæŸ¥è¯¢
            query_result = await self.data_agent.execute_semantic_query(semantic_request)
            processing_log.append(f"æ•°æ®åº“æŸ¥è¯¢æ‰§è¡Œå®Œæˆ")
            
            if not query_result.success:
                return DataResult(
                    success=False,
                    data=[],
                    row_count=0,
                    columns=[],
                    query_info={},
                    data_quality={},
                    processing_log=processing_log,
                    error_message=query_result.error_message
                )
            
            # 3. æå–æ•°æ®
            raw_data = query_result.data.results if hasattr(query_result.data, 'results') else []
            processing_log.append(f"è·å–åŸå§‹æ•°æ®: {len(raw_data)} æ¡è®°å½•")
            
            # 4. æ•°æ®éªŒè¯
            validation_result = await self._validate_data(raw_data)
            processing_log.append(f"æ•°æ®éªŒè¯å®Œæˆ: è´¨é‡åˆ†æ•° {validation_result.get('quality_score', 0):.2f}")
            
            # 5. æ•°æ®å¤„ç†å’Œè½¬æ¢
            processed_data = await self._process_data(raw_data, query_spec)
            processing_log.append(f"æ•°æ®å¤„ç†å®Œæˆ: {len(processed_data)} æ¡è®°å½•")
            
            # 6. åº”ç”¨è®¡ç®—
            final_data = await self._apply_calculations(processed_data, query_spec.calculations)
            processing_log.append(f"è®¡ç®—åº”ç”¨å®Œæˆ")
            
            # 7. æ„å»ºç»“æœ
            columns = list(final_data[0].keys()) if final_data else []
            
            result = DataResult(
                success=True,
                data=final_data,
                row_count=len(final_data),
                columns=columns,
                query_info={
                    "original_placeholder": context.original_placeholder,
                    "table_name": query_spec.from_table,
                    "execution_time": query_result.metadata.get('execution_time', 0) if hasattr(query_result, 'metadata') else 0
                },
                data_quality=validation_result,
                processing_log=processing_log
            )
            
            print(f"âœ… æŸ¥è¯¢æ‰§è¡ŒæˆåŠŸ:")
            print(f"   æ•°æ®è¡Œæ•°: {result.row_count}")
            print(f"   æ•°æ®åˆ—æ•°: {len(result.columns)}")
            print(f"   æ•°æ®è´¨é‡: {validation_result.get('quality_score', 0):.2f}/1.0")
            
            return result
            
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {e}")
            return DataResult(
                success=False,
                data=[],
                row_count=0,
                columns=[],
                query_info={},
                data_quality={},
                processing_log=processing_log,
                error_message=str(e)
            )
    
    def _build_semantic_request(
        self, 
        context: PlaceholderContext, 
        query_spec: QuerySpec
    ) -> SemanticQueryRequest:
        """æ„å»ºè¯­ä¹‰æŸ¥è¯¢è¯·æ±‚"""
        
        # æ„å»ºè‡ªç„¶è¯­è¨€æŸ¥è¯¢æè¿°
        query_parts = []
        
        # æ·»åŠ æŸ¥è¯¢ç›®æ ‡
        if query_spec.aggregations:
            metrics = [f"{agg['function']}({agg['field']})" for agg in query_spec.aggregations]
            query_parts.append(f"è®¡ç®— {', '.join(metrics)}")
        
        # æ·»åŠ æ•°æ®æ¥æº
        query_parts.append(f"ä» {query_spec.from_table} è¡¨")
        
        # æ·»åŠ åˆ†ç»„
        if query_spec.group_by_fields:
            query_parts.append(f"æŒ‰ {', '.join(query_spec.group_by_fields)} åˆ†ç»„")
        
        # æ·»åŠ æ¡ä»¶
        if query_spec.where_conditions:
            conditions = [f"{cond.get('field', '')} {cond.get('operator', '')} {cond.get('value', '')}" 
                         for cond in query_spec.where_conditions]
            query_parts.append(f"æ¡ä»¶: {', '.join(conditions)}")
        
        natural_query = "ï¼Œ".join(query_parts)
        
        return SemanticQueryRequest(
            query=natural_query,
            data_source=context.data_source,
            natural_language=True,
            semantic_enhancement=True,
            intent_analysis=True,
            query_optimization=True,
            context={
                "table_name": query_spec.from_table,
                "select_fields": query_spec.select_fields,
                "aggregations": query_spec.aggregations,
                "group_by": query_spec.group_by_fields,
                "where_conditions": query_spec.where_conditions
            }
        )
    
    async def _validate_data(self, data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """éªŒè¯æ•°æ®è´¨é‡"""
        try:
            validation_result = await self.validation_tool.execute(
                data,
                context={"validation_rules": ["not_empty", "type_consistency", "completeness"]}
            )
            
            if validation_result.success:
                return validation_result.data
            else:
                return {"quality_score": 0.5, "issues": [validation_result.error_message]}
                
        except Exception:
            return {"quality_score": 0.5, "issues": ["éªŒè¯è¿‡ç¨‹å¼‚å¸¸"]}
    
    async def _process_data(
        self, 
        data: List[Dict[str, Any]], 
        query_spec: QuerySpec
    ) -> List[Dict[str, Any]]:
        """å¤„ç†æ•°æ®"""
        try:
            # æ•°æ®æ¸…ç†å’Œè½¬æ¢
            transform_result = await self.transform_tool.execute(
                data,
                context={
                    "operations": ["clean_nulls", "standardize_formats"],
                    "target_schema": {
                        field: {"type": "auto_detect"} for field in query_spec.select_fields
                    }
                }
            )
            
            if transform_result.success:
                return transform_result.data.get('transformed_data', data)
            else:
                return data
                
        except Exception:
            return data
    
    async def _apply_calculations(
        self, 
        data: List[Dict[str, Any]], 
        calculations: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """åº”ç”¨è®¡ç®—"""
        try:
            if not calculations:
                return data
            
            # è¿™é‡Œå®ç°å…·ä½“çš„è®¡ç®—é€»è¾‘
            # ä¾‹å¦‚åŒæ¯”å¢é•¿ç‡ã€ç¯æ¯”å¢é•¿ç‡ç­‰
            
            # ç¤ºä¾‹ï¼šç®€å•çš„ç™¾åˆ†æ¯”è®¡ç®—
            for calc in calculations:
                if calc["type"] == "percentage":
                    total = sum(float(row.get("amount", 0)) for row in data)
                    for row in data:
                        if total > 0:
                            row["percentage"] = round(float(row.get("amount", 0)) / total * 100, 2)
                        else:
                            row["percentage"] = 0
            
            return data
            
        except Exception:
            return data


class PlaceholderProcessor:
    """å ä½ç¬¦å¤„ç†å™¨ - ä¸»æ§åˆ¶å™¨"""
    
    def __init__(self):
        self.parser = PlaceholderParser()
        self.query_builder = QueryBuilder()
        self.query_executor = DataQueryExecutor()
    
    async def process_placeholder(self, placeholder_text: str) -> DataResult:
        """å¤„ç†å ä½ç¬¦çš„å®Œæ•´æµç¨‹"""
        try:
            print(f"ğŸ¯ å¼€å§‹å¤„ç†å ä½ç¬¦: {placeholder_text}")
            print("=" * 50)
            
            # æ­¥éª¤1: è§£æå ä½ç¬¦
            print("ğŸ“ æ­¥éª¤1: è§£æå ä½ç¬¦å†…å®¹...")
            context = await self.parser.parse_placeholder(placeholder_text)
            
            # æ­¥éª¤2: æ„å»ºæŸ¥è¯¢è§„æ ¼
            print("\nğŸ”§ æ­¥éª¤2: æ„å»ºæŸ¥è¯¢è§„æ ¼...")
            query_spec = await self.query_builder.build_query_spec(context)
            
            # æ­¥éª¤3: æ‰§è¡Œæ•°æ®æŸ¥è¯¢
            print("\nğŸš€ æ­¥éª¤3: æ‰§è¡Œæ•°æ®æŸ¥è¯¢...")
            result = await self.query_executor.execute_query(context, query_spec)
            
            # æ­¥éª¤4: ç»“æœæ€»ç»“
            print(f"\nâœ… å¤„ç†å®Œæˆ!")
            if result.success:
                print(f"   ğŸ“Š æˆåŠŸè·å– {result.row_count} æ¡æ•°æ®è®°å½•")
                print(f"   ğŸ“‹ åŒ…å«å­—æ®µ: {', '.join(result.columns)}")
                print(f"   ğŸ¯ æ•°æ®è´¨é‡åˆ†æ•°: {result.data_quality.get('quality_score', 0):.2f}")
                
                # æ˜¾ç¤ºæ•°æ®æ ·ä¾‹
                if result.data:
                    print(f"   ğŸ“„ æ•°æ®æ ·ä¾‹:")
                    for i, row in enumerate(result.data[:2], 1):
                        print(f"      è¡Œ{i}: {row}")
            else:
                print(f"   âŒ å¤„ç†å¤±è´¥: {result.error_message}")
            
            return result
            
        except Exception as e:
            print(f"âŒ å ä½ç¬¦å¤„ç†å¼‚å¸¸: {e}")
            return DataResult(
                success=False,
                data=[],
                row_count=0,
                columns=[],
                query_info={"error": str(e)},
                data_quality={},
                processing_log=[f"å¤„ç†å¼‚å¸¸: {e}"],
                error_message=str(e)
            )
    
    async def process_multiple_placeholders(
        self, 
        placeholder_list: List[str]
    ) -> List[DataResult]:
        """æ‰¹é‡å¤„ç†å¤šä¸ªå ä½ç¬¦"""
        results = []
        
        print(f"ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç† {len(placeholder_list)} ä¸ªå ä½ç¬¦")
        print("=" * 60)
        
        for i, placeholder in enumerate(placeholder_list, 1):
            print(f"\nã€å ä½ç¬¦ {i}/{len(placeholder_list)}ã€‘")
            result = await self.process_placeholder(placeholder)
            results.append(result)
            
            # æ˜¾ç¤ºå¤„ç†è¿›åº¦
            success_count = sum(1 for r in results if r.success)
            print(f"   å½“å‰è¿›åº¦: {i}/{len(placeholder_list)} (æˆåŠŸ: {success_count})")
        
        print(f"\nğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆ!")
        print(f"   æ€»è®¡: {len(results)} ä¸ªå ä½ç¬¦")
        print(f"   æˆåŠŸ: {sum(1 for r in results if r.success)} ä¸ª") 
        print(f"   å¤±è´¥: {sum(1 for r in results if not r.success)} ä¸ª")
        
        return results


async def demo_placeholder_processing():
    """æ¼”ç¤ºå ä½ç¬¦å¤„ç†åŠŸèƒ½"""
    processor = PlaceholderProcessor()
    
    # æµ‹è¯•å ä½ç¬¦ç¤ºä¾‹
    test_placeholders = [
        "{{é”€å”®æ•°æ®åˆ†æ:æŸ¥è¯¢æœ€è¿‘3ä¸ªæœˆçš„é”€å”®é¢,æŒ‰åœ°åŒºåˆ†ç»„,åŒ…å«åŒæ¯”å¢é•¿ç‡}}",
        "{{å®¢æˆ·åˆ†æ:ç»Ÿè®¡æœ¬å¹´åº¦å®¢æˆ·æ•°,æŒ‰å®¢æˆ·ç±»å‹åˆ†ç»„,è®¡ç®—å¹³å‡å®¢å•ä»·}}",
        "{{äº§å“åˆ†æ:è·å–æœ€è¿‘6ä¸ªæœˆäº§å“é”€å”®é‡,æŒ‰äº§å“ç±»åˆ«åˆ†ç»„,åŒ…å«å æ¯”}}",
        "{{è®¢å•åˆ†æ:æŸ¥è¯¢æœ¬å­£åº¦è®¢å•æ•°æ®,æŒ‰æœˆä»½åˆ†ç»„,è®¡ç®—å®Œæˆç‡}}"
    ]
    
    # å•ä¸ªå ä½ç¬¦å¤„ç†æ¼”ç¤º
    print("ğŸ¯ å•ä¸ªå ä½ç¬¦å¤„ç†æ¼”ç¤º:")
    result = await processor.process_placeholder(test_placeholders[0])
    
    # æ‰¹é‡å¤„ç†æ¼”ç¤º
    print(f"\n" + "="*60)
    print("ğŸš€ æ‰¹é‡å ä½ç¬¦å¤„ç†æ¼”ç¤º:")
    results = await processor.process_multiple_placeholders(test_placeholders)
    
    return results


if __name__ == "__main__":
    asyncio.run(demo_placeholder_processing())