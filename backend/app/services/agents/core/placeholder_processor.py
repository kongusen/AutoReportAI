"""
Âç†‰ΩçÁ¨¶Â§ÑÁêÜÂô® - Ê†∏ÂøÉÊï∞ÊçÆÊü•ËØ¢ÊûÑÂª∫ÂºïÊìé

‰∏ìÈó®Â§ÑÁêÜÂç†‰ΩçÁ¨¶‰∏≠ÁöÑÊèêÁ§∫ËØçÔºåÊûÑÂª∫ÂáÜÁ°ÆÁöÑÊï∞ÊçÆÊü•ËØ¢ÔºåÂπ∂Ëé∑ÂèñÊ≠£Á°ÆÁöÑÊï∞ÊçÆ„ÄÇ
ËøôÊòØÂÆûÁé∞‰ªéÂç†‰ΩçÁ¨¶Âà∞Êï∞ÊçÆÁöÑÊ†∏ÂøÉÂ§ÑÁêÜÊµÅÁ®ã„ÄÇ

Ê†∏ÂøÉÂäüËÉΩÔºö
1. Êô∫ËÉΩËß£ÊûêÂç†‰ΩçÁ¨¶ËØ≠Ê≥ïÂíåÊèêÁ§∫ËØç
2. ÊûÑÂª∫Á≤æÁ°ÆÁöÑÊï∞ÊçÆÂ∫ìÊü•ËØ¢
3. ÊâßË°åÊü•ËØ¢Âπ∂Ëé∑ÂèñÊï∞ÊçÆ
4. È™åËØÅÊï∞ÊçÆÂáÜÁ°ÆÊÄßÂíåÂÆåÊï¥ÊÄß
"""

import re
import json
import asyncio
from typing import Dict, Any, List, Optional, Tuple, Union
from datetime import datetime, timedelta
from dataclasses import dataclass, field

from ..enhanced.enhanced_data_query_agent import EnhancedDataQueryAgent, SemanticQueryRequest
from ..tools.data_processing_tools import DataValidationTool, DataTransformationTool
from ..base import AgentResult


@dataclass
class PlaceholderContext:
    """Âç†‰ΩçÁ¨¶‰∏ä‰∏ãÊñá"""
    original_placeholder: str
    parsed_content: Dict[str, Any]
    data_source: str
    table_name: Optional[str] = None
    schema_info: Dict[str, Any] = field(default_factory=dict)
    user_context: Dict[str, Any] = field(default_factory=dict)


@dataclass
class QuerySpec:
    """Êü•ËØ¢ËßÑÊ†ºËØ¥Êòé"""
    select_fields: List[str]
    from_table: str
    where_conditions: List[Dict[str, Any]]
    group_by_fields: List[str]
    order_by_fields: List[Dict[str, str]]  # [{"field": "date", "direction": "desc"}]
    aggregations: List[Dict[str, str]]     # [{"function": "sum", "field": "amount", "alias": "total"}]
    calculations: List[Dict[str, Any]]     # ÈúÄË¶ÅËÆ°ÁÆóÁöÑÊåáÊ†á
    limit: Optional[int] = None


@dataclass
class DataResult:
    """Êï∞ÊçÆÁªìÊûú"""
    success: bool
    data: List[Dict[str, Any]]
    row_count: int
    columns: List[str]
    query_info: Dict[str, Any]
    data_quality: Dict[str, Any]
    processing_log: List[str]
    error_message: Optional[str] = None


class PlaceholderParser:
    """Âç†‰ΩçÁ¨¶Ëß£ÊûêÂô®"""
    
    def __init__(self):
        # ÂÆö‰πâÂç†‰ΩçÁ¨¶ÁöÑÊ≠£ÂàôË°®ËææÂºèÊ®°Âºè
        self.placeholder_pattern = r'\{\{([^}]+)\}\}'
        
        # ÂÆö‰πâÂÖ≥ÈîÆËØçÊò†Â∞Ñ
        self.time_keywords = {
            "ÊúÄËøë3‰∏™Êúà": {"type": "relative", "value": 3, "unit": "month"},
            "ÊúÄËøë6‰∏™Êúà": {"type": "relative", "value": 6, "unit": "month"},
            "ÊúÄËøë1Âπ¥": {"type": "relative", "value": 1, "unit": "year"},
            "Êú¨Êúà": {"type": "current", "unit": "month"},
            "Êú¨Â≠£Â∫¶": {"type": "current", "unit": "quarter"},
            "Êú¨Âπ¥": {"type": "current", "unit": "year"},
            "‰∏ä‰∏™Êúà": {"type": "previous", "unit": "month"},
            "ÂéªÂπ¥ÂêåÊúü": {"type": "year_ago", "unit": "same_period"}
        }
        
        self.metric_keywords = {
            "ÈîÄÂîÆÈ¢ù": {"field": "sales_amount", "type": "numeric", "aggregation": "sum"},
            "ÈîÄÂîÆÈáè": {"field": "sales_quantity", "type": "numeric", "aggregation": "sum"},
            "ËÆ¢ÂçïÊï∞": {"field": "order_count", "type": "numeric", "aggregation": "count"},
            "ÂÆ¢Êà∑Êï∞": {"field": "customer_count", "type": "numeric", "aggregation": "count_distinct"},
            "Âπ≥ÂùáÂÆ¢Âçï‰ª∑": {"field": "avg_order_value", "type": "calculated", "formula": "sales_amount / order_count"},
            "Âà©Ê∂¶": {"field": "profit", "type": "numeric", "aggregation": "sum"},
            "Âà©Ê∂¶Áéá": {"field": "profit_rate", "type": "calculated", "formula": "profit / sales_amount * 100"}
        }
        
        self.dimension_keywords = {
            "Âú∞Âå∫": {"field": "region", "type": "category"},
            "‰∫ßÂìÅ": {"field": "product_name", "type": "category"}, 
            "‰∫ßÂìÅÁ±ªÂà´": {"field": "product_category", "type": "category"},
            "Ê∏†ÈÅì": {"field": "channel", "type": "category"},
            "ÂÆ¢Êà∑Á±ªÂûã": {"field": "customer_type", "type": "category"},
            "Êúà‰ªΩ": {"field": "month", "type": "date_part"},
            "Â≠£Â∫¶": {"field": "quarter", "type": "date_part"},
            "Âπ¥‰ªΩ": {"field": "year", "type": "date_part"}
        }
        
        self.calculation_keywords = {
            "ÂêåÊØîÂ¢ûÈïøÁéá": {"type": "yoy_growth", "requires": ["current_period", "same_period_last_year"]},
            "ÁéØÊØîÂ¢ûÈïøÁéá": {"type": "mom_growth", "requires": ["current_period", "previous_period"]},
            "Á¥ØËÆ°ÂÄº": {"type": "cumulative", "requires": ["running_total"]},
            "Âπ≥ÂùáÂÄº": {"type": "average", "function": "avg"},
            "ÊúÄÂ§ßÂÄº": {"type": "maximum", "function": "max"},
            "ÊúÄÂ∞èÂÄº": {"type": "minimum", "function": "min"},
            "Âç†ÊØî": {"type": "percentage", "requires": ["part", "total"]}
        }
    
    async def parse_placeholder(self, placeholder_text: str) -> PlaceholderContext:
        """Ëß£ÊûêÂç†‰ΩçÁ¨¶ÂÜÖÂÆπ"""
        try:
            # ÊèêÂèñÂç†‰ΩçÁ¨¶ÂÜÖÂÆπ
            matches = re.findall(self.placeholder_pattern, placeholder_text)
            if not matches:
                raise ValueError("Êú™ÊâæÂà∞ÊúâÊïàÁöÑÂç†‰ΩçÁ¨¶Ê†ºÂºè {{...}}")
            
            placeholder_content = matches[0]
            
            # ÂàÜÊûêÂç†‰ΩçÁ¨¶ÁªìÊûÑ
            parsed_content = {
                "analysis_type": self._extract_analysis_type(placeholder_content),
                "time_range": self._extract_time_range(placeholder_content),
                "metrics": self._extract_metrics(placeholder_content),
                "dimensions": self._extract_dimensions(placeholder_content),
                "calculations": self._extract_calculations(placeholder_content),
                "filters": self._extract_filters(placeholder_content),
                "data_source": self._identify_data_source(placeholder_content)
            }
            
            # ÂàõÂª∫‰∏ä‰∏ãÊñá
            context = PlaceholderContext(
                original_placeholder=placeholder_text,
                parsed_content=parsed_content,
                data_source=parsed_content["data_source"]
            )
            
            print(f"üìù Âç†‰ΩçÁ¨¶Ëß£ÊûêÂÆåÊàê:")
            print(f"   ÂéüÂßãÂÜÖÂÆπ: {placeholder_text}")
            print(f"   ÂàÜÊûêÁ±ªÂûã: {parsed_content['analysis_type']}")
            print(f"   Êó∂Èó¥ËåÉÂõ¥: {parsed_content['time_range']}")
            print(f"   ÊåáÊ†á: {parsed_content['metrics']}")
            print(f"   Áª¥Â∫¶: {parsed_content['dimensions']}")
            print(f"   ËÆ°ÁÆó: {parsed_content['calculations']}")
            
            return context
            
        except Exception as e:
            print(f"‚ùå Âç†‰ΩçÁ¨¶Ëß£ÊûêÂ§±Ë¥•: {e}")
            raise
    
    def _extract_analysis_type(self, content: str) -> str:
        """ÊèêÂèñÂàÜÊûêÁ±ªÂûã"""
        if "ÈîÄÂîÆ" in content:
            return "sales_analysis"
        elif "Áî®Êà∑" in content or "ÂÆ¢Êà∑" in content:
            return "customer_analysis"
        elif "Ë¥¢Âä°" in content or "Êî∂ÂÖ•" in content or "Âà©Ê∂¶" in content:
            return "financial_analysis"
        elif "ËÆ¢Âçï" in content:
            return "order_analysis"
        elif "‰∫ßÂìÅ" in content:
            return "product_analysis"
        else:
            return "general_analysis"
    
    def _extract_time_range(self, content: str) -> Optional[Dict[str, Any]]:
        """ÊèêÂèñÊó∂Èó¥ËåÉÂõ¥"""
        for keyword, time_info in self.time_keywords.items():
            if keyword in content:
                return {
                    "keyword": keyword,
                    **time_info
                }
        return None
    
    def _extract_metrics(self, content: str) -> List[Dict[str, Any]]:
        """ÊèêÂèñÊåáÊ†á"""
        metrics = []
        for keyword, metric_info in self.metric_keywords.items():
            if keyword in content:
                metrics.append({
                    "keyword": keyword,
                    **metric_info
                })
        return metrics
    
    def _extract_dimensions(self, content: str) -> List[Dict[str, Any]]:
        """ÊèêÂèñÁª¥Â∫¶"""
        dimensions = []
        
        # Êü•ÊâæÊòéÁ°ÆÁöÑÂàÜÁªÑÁª¥Â∫¶
        group_patterns = [
            r"Êåâ(\w+)ÂàÜÁªÑ", r"Êåâ(\w+)ÁªüËÆ°", r"ÂàÜ(\w+)ÁªüËÆ°", r"(\w+)Áª¥Â∫¶"
        ]
        
        for pattern in group_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                if match in self.dimension_keywords:
                    dim_info = self.dimension_keywords[match]
                    dimensions.append({
                        "keyword": match,
                        **dim_info
                    })
        
        # Êü•ÊâæÈöêÂê´ÁöÑÁª¥Â∫¶
        for keyword, dim_info in self.dimension_keywords.items():
            if keyword in content and not any(d["keyword"] == keyword for d in dimensions):
                dimensions.append({
                    "keyword": keyword,
                    **dim_info
                })
        
        return dimensions
    
    def _extract_calculations(self, content: str) -> List[Dict[str, Any]]:
        """ÊèêÂèñËÆ°ÁÆóË¶ÅÊ±Ç"""
        calculations = []
        for keyword, calc_info in self.calculation_keywords.items():
            if keyword in content:
                calculations.append({
                    "keyword": keyword,
                    **calc_info
                })
        return calculations
    
    def _extract_filters(self, content: str) -> List[Dict[str, Any]]:
        """ÊèêÂèñËøáÊª§Êù°‰ª∂"""
        filters = []
        
        # ÊèêÂèñÂÖ∑‰ΩìÁöÑËøáÊª§Êù°‰ª∂
        # ‰æãÂ¶Ç: "Â§ß‰∫é1000", "Á≠â‰∫éÂåó‰∫¨", "ÂåÖÂê´VIP"
        filter_patterns = [
            (r"Â§ß‰∫é(\d+)", {"operator": ">", "type": "numeric"}),
            (r"Â∞è‰∫é(\d+)", {"operator": "<", "type": "numeric"}),
            (r"Á≠â‰∫é(\w+)", {"operator": "=", "type": "exact"}),
            (r"ÂåÖÂê´(\w+)", {"operator": "like", "type": "text"}),
            (r"‰∏çÂåÖÂê´(\w+)", {"operator": "not_like", "type": "text"})
        ]
        
        for pattern, filter_info in filter_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                filters.append({
                    "value": match,
                    **filter_info
                })
        
        return filters
    
    def _identify_data_source(self, content: str) -> str:
        """ËØÜÂà´Êï∞ÊçÆÊ∫ê"""
        # Ê†πÊçÆÂàÜÊûêÁ±ªÂûãÊé®Êñ≠Êï∞ÊçÆÊ∫ê
        if "ÈîÄÂîÆ" in content:
            return "sales_database"
        elif "Áî®Êà∑" in content or "ÂÆ¢Êà∑" in content:
            return "user_database"
        elif "ËÆ¢Âçï" in content:
            return "order_database"
        elif "Ë¥¢Âä°" in content:
            return "financial_database"
        else:
            return "main_database"


class QueryBuilder:
    """Êü•ËØ¢ÊûÑÂª∫Âô®"""
    
    def __init__(self):
        # Ë°®ÂêçÊò†Â∞Ñ
        self.table_mapping = {
            "sales_analysis": "sales_data",
            "customer_analysis": "customer_data", 
            "financial_analysis": "financial_data",
            "order_analysis": "order_data",
            "product_analysis": "product_data"
        }
        
        # Â≠óÊÆµÊò†Â∞Ñ
        self.field_mapping = {
            "sales_amount": "amount",
            "sales_quantity": "quantity",
            "order_count": "order_id",
            "customer_count": "customer_id",
            "region": "region_name",
            "product_name": "product_name",
            "product_category": "category"
        }
    
    async def build_query_spec(self, context: PlaceholderContext) -> QuerySpec:
        """ÊûÑÂª∫Êü•ËØ¢ËßÑÊ†º"""
        try:
            parsed = context.parsed_content
            
            # Á°ÆÂÆö‰∏ªË°®
            table_name = self.table_mapping.get(parsed["analysis_type"], "main_table")
            
            # ÊûÑÂª∫SELECTÂ≠óÊÆµ
            select_fields = []
            aggregations = []
            
            # Ê∑ªÂä†Áª¥Â∫¶Â≠óÊÆµ
            for dim in parsed["dimensions"]:
                field_name = self.field_mapping.get(dim["field"], dim["field"])
                select_fields.append(field_name)
            
            # Ê∑ªÂä†ÊåáÊ†áÂ≠óÊÆµÔºàÈúÄË¶ÅËÅöÂêàÔºâ
            for metric in parsed["metrics"]:
                field_name = self.field_mapping.get(metric["field"], metric["field"])
                if metric["type"] == "numeric":
                    agg_function = metric["aggregation"]
                    aggregations.append({
                        "function": agg_function,
                        "field": field_name,
                        "alias": f"{agg_function}_{field_name}"
                    })
            
            # ÊûÑÂª∫WHEREÊù°‰ª∂
            where_conditions = []
            
            # Ê∑ªÂä†Êó∂Èó¥Êù°‰ª∂
            if parsed["time_range"]:
                time_condition = self._build_time_condition(parsed["time_range"])
                if time_condition:
                    where_conditions.append(time_condition)
            
            # Ê∑ªÂä†ËøáÊª§Êù°‰ª∂
            for filter_item in parsed["filters"]:
                where_conditions.append({
                    "field": "value_field",  # ÈúÄË¶ÅÊ†πÊçÆ‰∏ä‰∏ãÊñáÁ°ÆÂÆö
                    "operator": filter_item["operator"],
                    "value": filter_item["value"]
                })
            
            # ÊûÑÂª∫GROUP BY
            group_by_fields = [
                self.field_mapping.get(dim["field"], dim["field"]) 
                for dim in parsed["dimensions"]
            ]
            
            # ÊûÑÂª∫ORDER BY
            order_by_fields = []
            if group_by_fields:
                order_by_fields.append({
                    "field": group_by_fields[0],
                    "direction": "asc"
                })
            
            # ÊûÑÂª∫ËÆ°ÁÆóÂ≠óÊÆµ
            calculations = []
            for calc in parsed["calculations"]:
                calc_spec = self._build_calculation_spec(calc)
                if calc_spec:
                    calculations.append(calc_spec)
            
            query_spec = QuerySpec(
                select_fields=select_fields,
                from_table=table_name,
                where_conditions=where_conditions,
                group_by_fields=group_by_fields,
                order_by_fields=order_by_fields,
                aggregations=aggregations,
                calculations=calculations
            )
            
            print(f"üîß Êü•ËØ¢ËßÑÊ†ºÊûÑÂª∫ÂÆåÊàê:")
            print(f"   Ë°®Âêç: {table_name}")
            print(f"   ÈÄâÊã©Â≠óÊÆµ: {select_fields}")
            print(f"   ËÅöÂêàÂáΩÊï∞: {[f\"{a['function']}({a['field']})\" for a in aggregations]}")
            print(f"   ÂàÜÁªÑÂ≠óÊÆµ: {group_by_fields}")
            print(f"   Êù°‰ª∂Êï∞Èáè: {len(where_conditions)}")
            
            return query_spec
            
        except Exception as e:
            print(f"‚ùå Êü•ËØ¢ËßÑÊ†ºÊûÑÂª∫Â§±Ë¥•: {e}")
            raise
    
    def _build_time_condition(self, time_range: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """ÊûÑÂª∫Êó∂Èó¥Êù°‰ª∂"""
        try:
            if time_range["type"] == "relative":
                # ËÆ°ÁÆóÁõ∏ÂØπÊó∂Èó¥
                if time_range["unit"] == "month":
                    start_date = datetime.now() - timedelta(days=time_range["value"] * 30)
                elif time_range["unit"] == "year":
                    start_date = datetime.now() - timedelta(days=time_range["value"] * 365)
                else:
                    return None
                
                return {
                    "field": "date",
                    "operator": ">=",
                    "value": start_date.strftime("%Y-%m-%d")
                }
            
            elif time_range["type"] == "current":
                # ÂΩìÂâçÂë®Êúü
                now = datetime.now()
                if time_range["unit"] == "month":
                    start_date = now.replace(day=1)
                elif time_range["unit"] == "quarter":
                    quarter = (now.month - 1) // 3 + 1
                    start_month = (quarter - 1) * 3 + 1
                    start_date = now.replace(month=start_month, day=1)
                else:
                    return None
                
                return {
                    "field": "date", 
                    "operator": ">=",
                    "value": start_date.strftime("%Y-%m-%d")
                }
            
            return None
            
        except Exception:
            return None
    
    def _build_calculation_spec(self, calculation: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """ÊûÑÂª∫ËÆ°ÁÆóËßÑÊ†º"""
        calc_type = calculation["type"]
        
        if calc_type == "yoy_growth":
            return {
                "type": "year_over_year_growth",
                "formula": "(current_value - previous_year_value) / previous_year_value * 100",
                "requires_historical_data": True
            }
        elif calc_type == "mom_growth":
            return {
                "type": "month_over_month_growth", 
                "formula": "(current_value - previous_month_value) / previous_month_value * 100",
                "requires_historical_data": True
            }
        elif calc_type == "average":
            return {
                "type": "average",
                "function": calculation["function"]
            }
        elif calc_type == "percentage":
            return {
                "type": "percentage",
                "formula": "value / total * 100"
            }
        
        return None


class DataQueryExecutor:
    """Êï∞ÊçÆÊü•ËØ¢ÊâßË°åÂô®"""
    
    def __init__(self):
        self.data_agent = EnhancedDataQueryAgent()
        self.validation_tool = DataValidationTool()
        self.transform_tool = DataTransformationTool()
    
    async def execute_query(
        self, 
        context: PlaceholderContext, 
        query_spec: QuerySpec
    ) -> DataResult:
        """ÊâßË°åÊï∞ÊçÆÊü•ËØ¢"""
        try:
            processing_log = []
            processing_log.append(f"ÂºÄÂßãÊâßË°åÊü•ËØ¢: {context.original_placeholder}")
            
            # 1. ÊûÑÂª∫ËØ≠‰πâÊü•ËØ¢ËØ∑Ê±Ç
            semantic_request = self._build_semantic_request(context, query_spec)
            processing_log.append(f"ÊûÑÂª∫ËØ≠‰πâÊü•ËØ¢ËØ∑Ê±ÇÂÆåÊàê")
            
            # 2. ÊâßË°åÊü•ËØ¢
            query_result = await self.data_agent.execute_semantic_query(semantic_request)
            processing_log.append(f"Êï∞ÊçÆÂ∫ìÊü•ËØ¢ÊâßË°åÂÆåÊàê")
            
            if not query_result.success:
                return DataResult(
                    success=False,
                    data=[],
                    row_count=0,
                    columns=[],
                    query_info={},
                    data_quality={},
                    processing_log=processing_log,
                    error_message=query_result.error_message
                )
            
            # 3. ÊèêÂèñÊï∞ÊçÆ
            raw_data = query_result.data.results if hasattr(query_result.data, 'results') else []
            processing_log.append(f"Ëé∑ÂèñÂéüÂßãÊï∞ÊçÆ: {len(raw_data)} Êù°ËÆ∞ÂΩï")
            
            # 4. Êï∞ÊçÆÈ™åËØÅ
            validation_result = await self._validate_data(raw_data)
            processing_log.append(f"Êï∞ÊçÆÈ™åËØÅÂÆåÊàê: Ë¥®ÈáèÂàÜÊï∞ {validation_result.get('quality_score', 0):.2f}")
            
            # 5. Êï∞ÊçÆÂ§ÑÁêÜÂíåËΩ¨Êç¢
            processed_data = await self._process_data(raw_data, query_spec)
            processing_log.append(f"Êï∞ÊçÆÂ§ÑÁêÜÂÆåÊàê: {len(processed_data)} Êù°ËÆ∞ÂΩï")
            
            # 6. Â∫îÁî®ËÆ°ÁÆó
            final_data = await self._apply_calculations(processed_data, query_spec.calculations)
            processing_log.append(f"ËÆ°ÁÆóÂ∫îÁî®ÂÆåÊàê")
            
            # 7. ÊûÑÂª∫ÁªìÊûú
            columns = list(final_data[0].keys()) if final_data else []
            
            result = DataResult(
                success=True,
                data=final_data,
                row_count=len(final_data),
                columns=columns,
                query_info={
                    "original_placeholder": context.original_placeholder,
                    "table_name": query_spec.from_table,
                    "execution_time": query_result.metadata.get('execution_time', 0) if hasattr(query_result, 'metadata') else 0
                },
                data_quality=validation_result,
                processing_log=processing_log
            )
            
            print(f"‚úÖ Êü•ËØ¢ÊâßË°åÊàêÂäü:")
            print(f"   Êï∞ÊçÆË°åÊï∞: {result.row_count}")
            print(f"   Êï∞ÊçÆÂàóÊï∞: {len(result.columns)}")
            print(f"   Êï∞ÊçÆË¥®Èáè: {validation_result.get('quality_score', 0):.2f}/1.0")
            
            return result
            
        except Exception as e:
            print(f"‚ùå Êü•ËØ¢ÊâßË°åÂ§±Ë¥•: {e}")
            return DataResult(
                success=False,
                data=[],
                row_count=0,
                columns=[],
                query_info={},
                data_quality={},
                processing_log=processing_log,
                error_message=str(e)
            )
    
    def _build_semantic_request(
        self, 
        context: PlaceholderContext, 
        query_spec: QuerySpec
    ) -> SemanticQueryRequest:
        """ÊûÑÂª∫ËØ≠‰πâÊü•ËØ¢ËØ∑Ê±Ç"""
        
        # ÊûÑÂª∫Ëá™ÁÑ∂ËØ≠Ë®ÄÊü•ËØ¢ÊèèËø∞
        query_parts = []
        
        # Ê∑ªÂä†Êü•ËØ¢ÁõÆÊ†á
        if query_spec.aggregations:
            metrics = [f"{agg['function']}({agg['field']})" for agg in query_spec.aggregations]
            query_parts.append(f"ËÆ°ÁÆó {', '.join(metrics)}")
        
        # Ê∑ªÂä†Êï∞ÊçÆÊù•Ê∫ê
        query_parts.append(f"‰ªé {query_spec.from_table} Ë°®")
        
        # Ê∑ªÂä†ÂàÜÁªÑ
        if query_spec.group_by_fields:
            query_parts.append(f"Êåâ {', '.join(query_spec.group_by_fields)} ÂàÜÁªÑ")
        
        # Ê∑ªÂä†Êù°‰ª∂
        if query_spec.where_conditions:
            conditions = [f"{cond.get('field', '')} {cond.get('operator', '')} {cond.get('value', '')}" 
                         for cond in query_spec.where_conditions]
            query_parts.append(f"Êù°‰ª∂: {', '.join(conditions)}")
        
        natural_query = "Ôºå".join(query_parts)
        
        return SemanticQueryRequest(
            query=natural_query,
            data_source=context.data_source,
            natural_language=True,
            semantic_enhancement=True,
            intent_analysis=True,
            query_optimization=True,
            context={
                "table_name": query_spec.from_table,
                "select_fields": query_spec.select_fields,
                "aggregations": query_spec.aggregations,
                "group_by": query_spec.group_by_fields,
                "where_conditions": query_spec.where_conditions
            }
        )
    
    async def _validate_data(self, data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """È™åËØÅÊï∞ÊçÆË¥®Èáè"""
        try:
            validation_result = await self.validation_tool.execute(
                data,
                context={"validation_rules": ["not_empty", "type_consistency", "completeness"]}
            )
            
            if validation_result.success:
                return validation_result.data
            else:
                return {"quality_score": 0.5, "issues": [validation_result.error_message]}
                
        except Exception:
            return {"quality_score": 0.5, "issues": ["È™åËØÅËøáÁ®ãÂºÇÂ∏∏"]}
    
    async def _process_data(
        self, 
        data: List[Dict[str, Any]], 
        query_spec: QuerySpec
    ) -> List[Dict[str, Any]]:
        """Â§ÑÁêÜÊï∞ÊçÆ"""
        try:
            # Êï∞ÊçÆÊ∏ÖÁêÜÂíåËΩ¨Êç¢
            transform_result = await self.transform_tool.execute(
                data,
                context={
                    "operations": ["clean_nulls", "standardize_formats"],
                    "target_schema": {
                        field: {"type": "auto_detect"} for field in query_spec.select_fields
                    }
                }
            )
            
            if transform_result.success:
                return transform_result.data.get('transformed_data', data)
            else:
                return data
                
        except Exception:
            return data
    
    async def _apply_calculations(
        self, 
        data: List[Dict[str, Any]], 
        calculations: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """Â∫îÁî®ËÆ°ÁÆó"""
        try:
            if not calculations:
                return data
            
            # ËøôÈáåÂÆûÁé∞ÂÖ∑‰ΩìÁöÑËÆ°ÁÆóÈÄªËæë
            # ‰æãÂ¶ÇÂêåÊØîÂ¢ûÈïøÁéá„ÄÅÁéØÊØîÂ¢ûÈïøÁéáÁ≠â
            
            # Á§∫‰æãÔºöÁÆÄÂçïÁöÑÁôæÂàÜÊØîËÆ°ÁÆó
            for calc in calculations:
                if calc["type"] == "percentage":
                    total = sum(float(row.get("amount", 0)) for row in data)
                    for row in data:
                        if total > 0:
                            row["percentage"] = round(float(row.get("amount", 0)) / total * 100, 2)
                        else:
                            row["percentage"] = 0
            
            return data
            
        except Exception:
            return data


class PlaceholderProcessor:
    """Âç†‰ΩçÁ¨¶Â§ÑÁêÜÂô® - ‰∏ªÊéßÂà∂Âô®"""
    
    def __init__(self):
        self.parser = PlaceholderParser()
        self.query_builder = QueryBuilder()
        self.query_executor = DataQueryExecutor()
    
    async def process_placeholder(self, placeholder_text: str) -> DataResult:
        """Â§ÑÁêÜÂç†‰ΩçÁ¨¶ÁöÑÂÆåÊï¥ÊµÅÁ®ã"""
        try:
            print(f"üéØ ÂºÄÂßãÂ§ÑÁêÜÂç†‰ΩçÁ¨¶: {placeholder_text}")
            print("=" * 50)
            
            # Ê≠•È™§1: Ëß£ÊûêÂç†‰ΩçÁ¨¶
            print("üìù Ê≠•È™§1: Ëß£ÊûêÂç†‰ΩçÁ¨¶ÂÜÖÂÆπ...")
            context = await self.parser.parse_placeholder(placeholder_text)
            
            # Ê≠•È™§2: ÊûÑÂª∫Êü•ËØ¢ËßÑÊ†º
            print("\nüîß Ê≠•È™§2: ÊûÑÂª∫Êü•ËØ¢ËßÑÊ†º...")
            query_spec = await self.query_builder.build_query_spec(context)
            
            # Ê≠•È™§3: ÊâßË°åÊï∞ÊçÆÊü•ËØ¢
            print("\nüöÄ Ê≠•È™§3: ÊâßË°åÊï∞ÊçÆÊü•ËØ¢...")
            result = await self.query_executor.execute_query(context, query_spec)
            
            # Ê≠•È™§4: ÁªìÊûúÊÄªÁªì
            print(f"\n‚úÖ Â§ÑÁêÜÂÆåÊàê!")
            if result.success:
                print(f"   üìä ÊàêÂäüËé∑Âèñ {result.row_count} Êù°Êï∞ÊçÆËÆ∞ÂΩï")
                print(f"   üìã ÂåÖÂê´Â≠óÊÆµ: {', '.join(result.columns)}")
                print(f"   üéØ Êï∞ÊçÆË¥®ÈáèÂàÜÊï∞: {result.data_quality.get('quality_score', 0):.2f}")
                
                # ÊòæÁ§∫Êï∞ÊçÆÊ†∑‰æã
                if result.data:
                    print(f"   üìÑ Êï∞ÊçÆÊ†∑‰æã:")
                    for i, row in enumerate(result.data[:2], 1):
                        print(f"      Ë°å{i}: {row}")
            else:
                print(f"   ‚ùå Â§ÑÁêÜÂ§±Ë¥•: {result.error_message}")
            
            return result
            
        except Exception as e:
            print(f"‚ùå Âç†‰ΩçÁ¨¶Â§ÑÁêÜÂºÇÂ∏∏: {e}")
            return DataResult(
                success=False,
                data=[],
                row_count=0,
                columns=[],
                query_info={"error": str(e)},
                data_quality={},
                processing_log=[f"Â§ÑÁêÜÂºÇÂ∏∏: {e}"],
                error_message=str(e)
            )
    
    async def process_multiple_placeholders(
        self, 
        placeholder_list: List[str]
    ) -> List[DataResult]:
        """ÊâπÈáèÂ§ÑÁêÜÂ§ö‰∏™Âç†‰ΩçÁ¨¶"""
        results = []
        
        print(f"üöÄ ÂºÄÂßãÊâπÈáèÂ§ÑÁêÜ {len(placeholder_list)} ‰∏™Âç†‰ΩçÁ¨¶")
        print("=" * 60)
        
        for i, placeholder in enumerate(placeholder_list, 1):
            print(f"\n„ÄêÂç†‰ΩçÁ¨¶ {i}/{len(placeholder_list)}„Äë")
            result = await self.process_placeholder(placeholder)
            results.append(result)
            
            # ÊòæÁ§∫Â§ÑÁêÜËøõÂ∫¶
            success_count = sum(1 for r in results if r.success)
            print(f"   ÂΩìÂâçËøõÂ∫¶: {i}/{len(placeholder_list)} (ÊàêÂäü: {success_count})")
        
        print(f"\nüéâ ÊâπÈáèÂ§ÑÁêÜÂÆåÊàê!")
        print(f"   ÊÄªËÆ°: {len(results)} ‰∏™Âç†‰ΩçÁ¨¶")
        print(f"   ÊàêÂäü: {sum(1 for r in results if r.success)} ‰∏™") 
        print(f"   Â§±Ë¥•: {sum(1 for r in results if not r.success)} ‰∏™")
        
        return results


async def demo_placeholder_processing():
    """ÊºîÁ§∫Âç†‰ΩçÁ¨¶Â§ÑÁêÜÂäüËÉΩ"""
    processor = PlaceholderProcessor()
    
    # ÊµãËØïÂç†‰ΩçÁ¨¶Á§∫‰æã
    test_placeholders = [
        "{{ÈîÄÂîÆÊï∞ÊçÆÂàÜÊûê:Êü•ËØ¢ÊúÄËøë3‰∏™ÊúàÁöÑÈîÄÂîÆÈ¢ù,ÊåâÂú∞Âå∫ÂàÜÁªÑ,ÂåÖÂê´ÂêåÊØîÂ¢ûÈïøÁéá}}",
        "{{ÂÆ¢Êà∑ÂàÜÊûê:ÁªüËÆ°Êú¨Âπ¥Â∫¶ÂÆ¢Êà∑Êï∞,ÊåâÂÆ¢Êà∑Á±ªÂûãÂàÜÁªÑ,ËÆ°ÁÆóÂπ≥ÂùáÂÆ¢Âçï‰ª∑}}",
        "{{‰∫ßÂìÅÂàÜÊûê:Ëé∑ÂèñÊúÄËøë6‰∏™Êúà‰∫ßÂìÅÈîÄÂîÆÈáè,Êåâ‰∫ßÂìÅÁ±ªÂà´ÂàÜÁªÑ,ÂåÖÂê´Âç†ÊØî}}",
        "{{ËÆ¢ÂçïÂàÜÊûê:Êü•ËØ¢Êú¨Â≠£Â∫¶ËÆ¢ÂçïÊï∞ÊçÆ,ÊåâÊúà‰ªΩÂàÜÁªÑ,ËÆ°ÁÆóÂÆåÊàêÁéá}}"
    ]
    
    # Âçï‰∏™Âç†‰ΩçÁ¨¶Â§ÑÁêÜÊºîÁ§∫
    print("üéØ Âçï‰∏™Âç†‰ΩçÁ¨¶Â§ÑÁêÜÊºîÁ§∫:")
    result = await processor.process_placeholder(test_placeholders[0])
    
    # ÊâπÈáèÂ§ÑÁêÜÊºîÁ§∫
    print(f"\n" + "="*60)
    print("üöÄ ÊâπÈáèÂç†‰ΩçÁ¨¶Â§ÑÁêÜÊºîÁ§∫:")
    results = await processor.process_multiple_placeholders(test_placeholders)
    
    return results


if __name__ == "__main__":
    asyncio.run(demo_placeholder_processing())