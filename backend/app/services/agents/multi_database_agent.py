"""
Multi-Database Agent

å¤šæ•°æ®åº“æ™ºèƒ½ä»£ç†ï¼Œè´Ÿè´£åˆ†æå ä½ç¬¦éœ€æ±‚ã€é€‰æ‹©ç›®æ ‡è¡¨å’Œç”ŸæˆSQLæŸ¥è¯¢
"""

import asyncio
import json
import logging
import re
from typing import Dict, List, Any, Optional
from datetime import datetime

from sqlalchemy.orm import Session

from app.services.connectors.base_connector import BaseConnector
from app.services.connectors.doris_connector import DorisConnector, DorisConfig
from app.core.ai_service_factory import UserAIServiceFactory

logger = logging.getLogger(__name__)


class MultiDatabaseAgent:
    """å¤šæ•°æ®åº“æ™ºèƒ½ä»£ç†"""
    
    def __init__(self, db_session=None, user_id=None):
        """åˆå§‹åŒ–å¤šæ•°æ®åº“ä»£ç†"""
        self.db_session = db_session
        self.user_id = user_id
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–AIæœåŠ¡
        try:
            from app.services.agents.core.ai_service import UnifiedAIService
            if user_id and db_session:
                from app.core.ai_service_factory import UserAIServiceFactory
                factory = UserAIServiceFactory()
                self.ai_service = factory.get_user_ai_service(user_id)
                self.logger.info(f"ä½¿ç”¨ç”¨æˆ·ç‰¹å®šAIæœåŠ¡: {user_id}")
            else:
                self.ai_service = UnifiedAIService(db_session=db_session)
                self.logger.info("ä½¿ç”¨ç³»ç»Ÿé»˜è®¤AIæœåŠ¡")
        except Exception as e:
            self.logger.warning(f"AIæœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            try:
                self.ai_service = UnifiedAIService(db_session=db_session)
                self.logger.info("å›é€€åˆ°ç³»ç»Ÿé»˜è®¤AIæœåŠ¡")
            except Exception as e2:
                self.logger.error(f"ç³»ç»Ÿé»˜è®¤AIæœåŠ¡ä¹Ÿå¤±è´¥: {e2}")
                self.ai_service = None
    
    async def analyze_placeholder_requirements(
        self, 
        agent_input: Dict[str, Any],
        execution_context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        åˆ†æå ä½ç¬¦éœ€æ±‚
        
        Args:
            agent_input: åŒ…å«å ä½ç¬¦ä¿¡æ¯çš„å­—å…¸
            execution_context: å¯é€‰æ‰§è¡Œä¸Šä¸‹æ–‡
        
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        start_time = datetime.now()
        
        try:
            placeholder_name = agent_input.get("placeholder_name", "")
            placeholder_type = agent_input.get("placeholder_type", "")
            data_source = agent_input.get("data_source", {})
            
            self.logger.info(f"ğŸš€ å¼€å§‹Agentåˆ†æå ä½ç¬¦: {placeholder_name}")
            self.logger.info(f"ğŸ“Š å ä½ç¬¦ç±»å‹: {placeholder_type}, æ•°æ®æº: {data_source.get('id', 'unknown')}")
            if execution_context:
                self.logger.debug(f"æ‰§è¡Œä¸Šä¸‹æ–‡: {execution_context}")
            
            # 1. è·å–æ•°æ®æºç»“æ„ä¿¡æ¯
            self.logger.info("ğŸ” è·å–æ•°æ®æºç»“æ„ä¿¡æ¯...")
            enhanced_schema = await self._get_enhanced_schema(data_source)
            
            if not enhanced_schema:
                return {
                    "success": False,
                    "error": "æ— æ³•è·å–æ•°æ®æºç»“æ„ä¿¡æ¯"
                }
            
            # 2. æ‰§è¡ŒAIè¯­ä¹‰åˆ†æ
            self.logger.info("ğŸ§  ä½¿ç”¨ ai_agent åˆ†ææ¨¡å¼")
            if self.user_id:
                self.logger.info(f"ä½¿ç”¨ç”¨æˆ·ç‰¹å®šAIæœåŠ¡è¿›è¡Œåˆ†æ: {self.user_id}")
            else:
                self.logger.info("ä½¿ç”¨ç³»ç»Ÿé»˜è®¤AIæœåŠ¡è¿›è¡Œåˆ†æ")
            
            semantic_analysis = await self._perform_ai_agent_analysis(
                placeholder_name, placeholder_type, enhanced_schema, data_source
            )
            
            if not semantic_analysis.get("success"):
                return {
                    "success": False,
                    "error": f"AI Agentåˆ†æå¤±è´¥: {semantic_analysis.get('error', 'æœªçŸ¥é”™è¯¯')}"
                }
            
            # 3. æ‰§è¡Œæ™ºèƒ½ç›®æ ‡é€‰æ‹©
            self.logger.info("ğŸ¯ æ‰§è¡Œæ™ºèƒ½ç›®æ ‡é€‰æ‹©...")
            target_selection = await self._perform_intelligent_target_selection(
                semantic_analysis, enhanced_schema
            )
            
            if not target_selection.get("success"):
                return {
                    "success": False,
                    "error": f"æ™ºèƒ½ç›®æ ‡é€‰æ‹©å¤±è´¥: {target_selection.get('error', 'æœªçŸ¥é”™è¯¯')}"
                }
            
            # 4. ç”Ÿæˆæ™ºèƒ½SQL
            self.logger.info("âš™ï¸ ç”Ÿæˆæ™ºèƒ½SQL...")
            generated_sql = await self._generate_intelligent_sql(
                semantic_analysis, target_selection, enhanced_schema
            )
            
            if not generated_sql:
                return {
                    "success": False,
                    "error": "AI SQLç”Ÿæˆå¤±è´¥: AIæœåŠ¡æœªåˆå§‹åŒ–ï¼Œè¯·æä¾›æœ‰æ•ˆçš„æ•°æ®åº“ä¼šè¯"
                }
            
            # 5. æ‰§è¡ŒSQLè´¨é‡éªŒè¯å’Œæ”¹è¿›
            self.logger.info("ğŸ”§ æ‰§è¡ŒSQLè´¨é‡éªŒè¯å’Œæ”¹è¿›...")
            final_sql = await self._self_validate_and_improve_sql(
                generated_sql, data_source.get("id"), target_selection
            )
            
            # 6. æ„å»ºåˆ†æå…ƒæ•°æ®
            analysis_metadata = {
                "analysis_mode": "ai_agent",
                "intent": semantic_analysis.get("intent", "unknown"),
                "data_operation": semantic_analysis.get("data_operation", "unknown"),
                "relevant_tables_count": len(enhanced_schema.get("tables", [])),
                "analysis_duration_seconds": (datetime.now() - start_time).total_seconds(),
                "ai_service_used": "user_specific" if self.user_id else "system_default"
            }
            
            return {
                "success": True,
                "target_database": data_source.get("name", ""),
                "target_table": target_selection.get("table", ""),
                "required_fields": target_selection.get("fields", []),
                "generated_sql": final_sql,
                "confidence_score": semantic_analysis.get("confidence", 0.8),
                "reasoning": semantic_analysis.get("reasoning", []),
                "optimizations": semantic_analysis.get("optimizations", []),
                "estimated_time_ms": 100,
                "analysis_metadata": analysis_metadata
            }
            
        except Exception as e:
            self.logger.error(f"å ä½ç¬¦éœ€æ±‚åˆ†æå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    async def _get_enhanced_schema(self, data_source: Dict[str, Any]) -> Dict[str, Any]:
        """è·å–å¢å¼ºçš„æ•°æ®æºschemaä¿¡æ¯"""
        try:
            data_source_id = data_source.get("id")
            source_type = data_source.get("source_type")
            
            # åˆ›å»ºè¿æ¥å™¨
            connector = await self._create_connector(data_source)
            if not connector:
                return None
            
            try:
                # è¿æ¥æ•°æ®åº“
                await connector.connect()
                
                # è·å–æ•°æ®åº“å’Œè¡¨ä¿¡æ¯
                databases = await connector.get_databases()
                tables = await connector.get_tables()
                
                # è·å–è¡¨ç»“æ„è¯¦æƒ…
                table_details = {}
                for table in tables:
                    try:
                        columns = await connector.get_table_columns(table)
                        table_details[table] = columns
                    except Exception as e:
                        self.logger.warning(f"è·å–è¡¨ {table} ç»“æ„å¤±è´¥: {e}")
                        table_details[table] = []
                
                return {
                    "source_type": source_type,
                    "databases": databases,
                    "tables": tables,
                    "table_details": table_details,
                    "total_tables": len(tables),
                    "total_columns": sum(len(cols) for cols in table_details.values())
                }
                
            finally:
                await connector.disconnect()
                
        except Exception as e:
            self.logger.error(f"è·å–å¢å¼ºschemaå¤±è´¥: {e}")
            return None
    
    async def _create_connector(self, data_source: Dict[str, Any]) -> Optional[BaseConnector]:
        """åˆ›å»ºæ•°æ®åº“è¿æ¥å™¨"""
        try:
            source_type = data_source.get("source_type")
            
            if source_type == "doris":
                # ä»æ•°æ®åº“è·å–æ•°æ®æºè¯¦ç»†ä¿¡æ¯
                from app.models.data_source import DataSource
                ds = self.db_session.query(DataSource).filter(
                    DataSource.id == data_source.get("id")
                ).first()
                
                if not ds:
                    return None
                
                config = DorisConfig(
                    source_type=ds.source_type,
                    name=ds.name,
                    fe_hosts=ds.doris_fe_hosts if isinstance(ds.doris_fe_hosts, list) else [ds.doris_fe_hosts],
                    query_port=ds.doris_query_port or 9030,
                    http_port=ds.doris_http_port or 8030,
                    database=ds.doris_database or "default",
                    username=ds.doris_username or "root",
                    password=ds.doris_password or ""
                )
                
                return DorisConnector(config)
            else:
                self.logger.warning(f"ä¸æ”¯æŒçš„æ•°æ®æºç±»å‹: {source_type}")
                return None
                
        except Exception as e:
            self.logger.error(f"åˆ›å»ºè¿æ¥å™¨å¤±è´¥: {e}")
            return None
    
    async def _perform_ai_agent_analysis(
        self, 
        placeholder_name: str, 
        placeholder_type: str, 
        enhanced_schema: Dict, 
        data_source: Dict
    ) -> Dict[str, Any]:
        """æ‰§è¡ŒAIä»£ç†åˆ†æ"""
        try:
            if not self.ai_service:
                return {
                    "success": False,
                    "error": "AIæœåŠ¡æœªåˆå§‹åŒ–ï¼Œè¯·æä¾›æœ‰æ•ˆçš„æ•°æ®åº“ä¼šè¯"
                }
            
            # æ„å»ºåˆ†æä¸Šä¸‹æ–‡
            context = {
                "placeholder_name": placeholder_name,
                "placeholder_type": placeholder_type,
                "data_source": data_source,
                "schema_info": enhanced_schema
            }
            
            # æ„å»ºAIåˆ†ææç¤º
            prompt = self._build_ai_analysis_prompt(context, enhanced_schema)
            
            # æ‰§è¡ŒAIåˆ†æ
            response = await self.ai_service.analyze_with_context(
                context=str(context), prompt=prompt, task_type="placeholder_analysis"
            )
            
            self.logger.info(f"AIå“åº”å†…å®¹: {response[:200]}...")  # Debugging line
            
            if response:
                try:
                    ai_result = {"success": True, "data": json.loads(response)}
                    self.logger.info("AIå“åº”JSONè§£ææˆåŠŸ")
                except json.JSONDecodeError as e:
                    self.logger.warning(f"AIå“åº”JSONè§£æå¤±è´¥: {e}")
                    self.logger.warning(f"AIå“åº”åŸå§‹å†…å®¹: {response}")
                    
                    # å°è¯•ä»å“åº”ä¸­æå–JSON
                    json_start = response.find('{')
                    json_end = response.rfind('}') + 1
                    if json_start != -1 and json_end > json_start:
                        try:
                            json_str = response[json_start:json_end]
                            ai_result = {"success": True, "data": json.loads(json_str)}
                            self.logger.info("ä»å“åº”ä¸­æå–JSONæˆåŠŸ")
                        except json.JSONDecodeError:
                            self.logger.error("æå–çš„JSONä»ç„¶æ— æ•ˆ")
                            ai_result = {"success": True, "data": {"intent": "statistical", "data_operation": "count", "reasoning": [response]}}
                    else:
                        ai_result = {"success": True, "data": {"intent": "statistical", "data_operation": "count", "reasoning": [response]}}
            else:
                ai_result = {"success": True, "data": {"intent": "statistical", "data_operation": "count", "reasoning": ["AIåˆ†æå¤±è´¥"]}}
            
            if ai_result.get("success"):
                analysis_data = ai_result.get("data", {})
                return {
                    "success": True,
                    "intent": analysis_data.get("intent", "statistical"),
                    "data_operation": analysis_data.get("data_operation", "count"),
                    "business_domain": analysis_data.get("business_domain", ""),
                    "target_metrics": analysis_data.get("target_metrics", []),
                    "time_dimension": analysis_data.get("time_dimension"),
                    "grouping_dimensions": analysis_data.get("grouping_dimensions", []),
                    "filters": analysis_data.get("filters", []),
                    "aggregations": analysis_data.get("aggregations", []),
                    "reasoning": analysis_data.get("reasoning", []),
                    "confidence": analysis_data.get("confidence", 0.8),
                    "optimizations": analysis_data.get("optimizations", [])
                }
            else:
                return {
                    "success": False,
                    "error": "AIåˆ†æå¤±è´¥"
                }
                
        except Exception as e:
            self.logger.error(f"AIä»£ç†åˆ†æå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def _build_ai_analysis_prompt(self, context: Dict, enhanced_schema: Dict) -> str:
        """æ„å»ºAIåˆ†ææç¤º - ä¼˜åŒ–ç‰ˆæœ¬"""
        placeholder_name = context.get("placeholder_name", "")
        placeholder_type = context.get("placeholder_type", "")
        data_source = context.get("data_source", {})
        
        # æ„å»ºè¡¨ç»“æ„ä¿¡æ¯
        tables_info = []
        for table_name, columns in enhanced_schema.get("table_details", {}).items():
            table_info = f"è¡¨å: {table_name}\nå­—æ®µ: {', '.join([col.get('name', '') for col in columns])}"
            tables_info.append(table_info)
        
        tables_text = "\n\n".join(tables_info)
        
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆã€‚è¯·åˆ†æä»¥ä¸‹å ä½ç¬¦çš„ä¸šåŠ¡éœ€æ±‚ï¼Œå¹¶è¿”å›JSONæ ¼å¼çš„åˆ†æç»“æœã€‚

å ä½ç¬¦ä¿¡æ¯:
- åç§°: {placeholder_name}
- ç±»å‹: {placeholder_type}
- æ•°æ®æº: {data_source.get('name', 'unknown')}

æ•°æ®åº“ç»“æ„ä¿¡æ¯:
{tables_text}

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡æœ¬ï¼š
{{
    "intent": "statistical",
    "data_operation": "count",
    "business_domain": "travel_service",
    "target_metrics": ["å¯¼æ¸¸æ•°é‡"],
    "time_dimension": null,
    "grouping_dimensions": [],
    "filters": ["city_id = 'æ˜†æ˜'"],
    "aggregations": ["count"],
    "reasoning": ["æ ¹æ®å ä½ç¬¦åç§°ï¼Œç›®æ ‡æ˜¯ç»Ÿè®¡æ˜†æ˜æ³¨å†Œçš„å¯¼æ¸¸æ•°é‡"],
    "confidence": 0.9,
    "optimizations": ["è€ƒè™‘å»ºç«‹ç´¢å¼•åœ¨city_idå­—æ®µä¸Š"]
}}

é‡è¦è¦æ±‚ï¼š
1. åªè¿”å›JSONå¯¹è±¡ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šã€æ³¨é‡Šæˆ–å…¶ä»–æ–‡æœ¬
2. ç¡®ä¿JSONè¯­æ³•å®Œå…¨æ­£ç¡®
3. å­—æ®µåå¿…é¡»æ˜¯æ•°æ®åº“ä¸­å®é™…å­˜åœ¨çš„å­—æ®µå
4. èšåˆå‡½æ•°å¿…é¡»æ˜¯æ ‡å‡†çš„SQLèšåˆå‡½æ•°
è¯·ç›´æ¥è¿”å›JSONå¯¹è±¡ï¼Œä¸è¦æœ‰ä»»ä½•å‰ç¼€æˆ–åç¼€ã€‚
"""
        return prompt
    
    async def _perform_intelligent_target_selection(
        self, 
        semantic_analysis: Dict, 
        enhanced_schema: Dict
    ) -> Dict[str, Any]:
        """æ‰§è¡Œæ™ºèƒ½ç›®æ ‡é€‰æ‹©"""
        try:
            intent = semantic_analysis.get("intent", "statistical")
            business_domain = semantic_analysis.get("business_domain", "")
            target_metrics = semantic_analysis.get("target_metrics", [])
            
            # åŸºäºè¯­ä¹‰åˆ†æç­›é€‰ç›¸å…³è¡¨
            relevant_tables = []
            for table_name, columns in enhanced_schema.get("table_details", {}).items():
                relevance_score = self._calculate_table_relevance(
                    table_name, columns, intent, business_domain, target_metrics
                )
                if relevance_score > 0.3:  # ç›¸å…³æ€§é˜ˆå€¼
                    relevant_tables.append((table_name, relevance_score))
            
            # æŒ‰ç›¸å…³æ€§æ’åº
            relevant_tables.sort(key=lambda x: x[1], reverse=True)
            
            if relevant_tables:
                best_table = relevant_tables[0][0]
                self.logger.info(f"âœ… è·å–åˆ° {len(relevant_tables)} ä¸ªç›¸å…³è¡¨: {[t[0] for t in relevant_tables]}")
                
                # é€‰æ‹©æœ€ä½³è¡¨çš„å­—æ®µ
                table_columns = enhanced_schema.get("table_details", {}).get(best_table, [])
                selected_fields = self._select_relevant_fields(
                    table_columns, semantic_analysis
                )
                
                return {
                    "success": True,
                    "table": best_table,
                    "fields": selected_fields,
                    "field_mapping": {},
                    "relevance_score": relevant_tables[0][1],
                    "alternative_tables": [t[0] for t in relevant_tables[1:3]]
                }
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç›¸å…³è¡¨ï¼Œä½¿ç”¨é»˜è®¤è¡¨
                default_table = list(enhanced_schema.get("table_details", {}).keys())[0] if enhanced_schema.get("table_details") else "default_table"
                self.logger.warning(f"æœªæ‰¾åˆ°ç›¸å…³è¡¨ï¼Œä½¿ç”¨é»˜è®¤è¡¨: {default_table}")
                
                return {
                    "success": True,
                    "table": default_table,
                    "fields": [],
                    "field_mapping": {},
                    "relevance_score": 0.1,
                    "alternative_tables": []
                }
                
        except Exception as e:
            self.logger.error(f"æ™ºèƒ½ç›®æ ‡é€‰æ‹©å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def _calculate_table_relevance(
        self, 
        table_name: str, 
        columns: List[Dict], 
        intent: str, 
        business_domain: str, 
        target_metrics: List[str]
    ) -> float:
        """è®¡ç®—è¡¨ç›¸å…³æ€§åˆ†æ•°"""
        score = 0.0
        
        # åŸºäºè¡¨åçš„ç›¸å…³æ€§
        table_name_lower = table_name.lower()
        if business_domain.lower() in table_name_lower:
            score += 0.4
        if any(metric.lower() in table_name_lower for metric in target_metrics):
            score += 0.3
        
        # åŸºäºå­—æ®µåçš„ç›¸å…³æ€§
        column_names = [col.get("name", "").lower() for col in columns]
        for metric in target_metrics:
            if any(metric.lower() in col for col in column_names):
                score += 0.2
        
        return min(score, 1.0)
    
    def _select_relevant_fields(
        self, 
        columns: List[Dict], 
        semantic_analysis: Dict
    ) -> List[str]:
        """é€‰æ‹©ç›¸å…³å­—æ®µ"""
        relevant_fields = []
        target_metrics = semantic_analysis.get("target_metrics", [])
        
        for column in columns:
            column_name = column.get("name", "").lower()
            if any(metric.lower() in column_name for metric in target_metrics):
                relevant_fields.append(column.get("name", ""))
        
        return relevant_fields
    
    async def _generate_intelligent_sql(
        self, 
        semantic_analysis: Dict, 
        target_selection: Dict, 
        enhanced_schema: Dict
    ) -> str:
        """åŸºäºè¯­ä¹‰åˆ†æå’Œç›®æ ‡é€‰æ‹©ç”Ÿæˆæ™ºèƒ½SQL - ä¼˜åŒ–ç‰ˆæœ¬"""
        try:
            # é¦–å…ˆå°è¯•AIé©±åŠ¨çš„SQLç”Ÿæˆ
            ai_generated_sql = await self._generate_sql_with_ai(semantic_analysis, target_selection, enhanced_schema)
            if ai_generated_sql and self._validate_sql_syntax(ai_generated_sql):
                return ai_generated_sql
            
            # å¦‚æœAIç”Ÿæˆå¤±è´¥æˆ–SQLæ— æ•ˆï¼Œä½¿ç”¨æ¨¡æ¿åŒ–ç”Ÿæˆ
            table_name = self._sanitize_identifier(target_selection.get('table', 'default_table'))
            fields = target_selection.get('fields', [])
            field_mapping = target_selection.get('field_mapping', {})
            
            sql = self._generate_sql_by_template(semantic_analysis.get('intent', 'general'),
                                                semantic_analysis.get('data_operation', 'select'),
                                                table_name, fields, field_mapping)
            
            if not self._validate_sql_syntax(sql):
                self.logger.warning(f"ç”Ÿæˆçš„SQLè¯­æ³•æ— æ•ˆï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ: {sql}")
                sql = self._generate_fallback_sql(table_name)
            
            return sql
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆæ™ºèƒ½SQLå¤±è´¥: {e}")
            return self._generate_fallback_sql()
    
    async def _generate_sql_with_ai(
        self, 
        semantic_analysis: Dict, 
        target_selection: Dict, 
        enhanced_schema: Dict
    ) -> str:
        """ä½¿ç”¨AIç”ŸæˆSQL"""
        try:
            if not self.ai_service:
                return None
            
            prompt = self._build_sql_generation_prompt(semantic_analysis, target_selection, enhanced_schema)
            
            response = await self.ai_service.analyze_with_context(
                context="", prompt=prompt, task_type="sql_generation"
            )
            
            if response:
                sql = self._extract_sql_from_response(response)
                if sql and self._validate_sql_syntax(sql):
                    return sql
            
            return None
            
        except Exception as e:
            self.logger.error(f"AI SQLç”Ÿæˆå¤±è´¥: {e}")
            return None
    
    def _build_sql_generation_prompt(
        self, 
        semantic_analysis: Dict, 
        target_selection: Dict, 
        enhanced_schema: Dict
    ) -> str:
        """æ„å»ºSQLç”Ÿæˆæç¤º"""
        table_name = target_selection.get('table', '')
        fields = target_selection.get('fields', [])
        
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªSQLä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆæ ‡å‡†çš„SQLæŸ¥è¯¢è¯­å¥ï¼š

åˆ†æç»“æœ:
- æ„å›¾: {semantic_analysis.get('intent', '')}
- æ•°æ®æ“ä½œ: {semantic_analysis.get('data_operation', '')}
- ä¸šåŠ¡é¢†åŸŸ: {semantic_analysis.get('business_domain', '')}
- ç›®æ ‡æŒ‡æ ‡: {semantic_analysis.get('target_metrics', [])}
- è¿‡æ»¤æ¡ä»¶: {semantic_analysis.get('filters', [])}
- èšåˆå‡½æ•°: {semantic_analysis.get('aggregations', [])}

ç›®æ ‡è¡¨: {table_name}
ç›¸å…³å­—æ®µ: {fields}

è¯·ç”Ÿæˆä¸€ä¸ªæ ‡å‡†çš„SQLæŸ¥è¯¢è¯­å¥ï¼Œåªè¿”å›SQLè¯­å¥ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæˆ–å…¶ä»–æ–‡æœ¬ã€‚
ç¡®ä¿SQLè¯­æ³•å®Œå…¨æ­£ç¡®ã€‚
"""
        return prompt
    
    def _extract_sql_from_response(self, response: str) -> str:
        """ä»AIå“åº”ä¸­æå–SQL"""
        # æŸ¥æ‰¾SQLè¯­å¥
        sql_patterns = [
            r'SELECT\s+.*?FROM\s+.*?(?:WHERE\s+.*?)?(?:GROUP BY\s+.*?)?(?:ORDER BY\s+.*?)?(?:LIMIT\s+\d+)?;?',
            r'SELECT\s+.*?FROM\s+.*?(?:WHERE\s+.*?)?(?:GROUP BY\s+.*?)?(?:ORDER BY\s+.*?)?(?:LIMIT\s+\d+)?',
        ]
        
        for pattern in sql_patterns:
            match = re.search(pattern, response, re.IGNORECASE | re.DOTALL)
            if match:
                return match.group(0).strip()
        
        return None
    
    def _generate_sql_by_template(
        self, 
        intent: str, 
        operation: str, 
        table_name: str, 
        fields: List[str], 
        field_mapping: Dict
    ) -> str:
        """åŸºäºæ¨¡æ¿ç”ŸæˆSQL"""
        if operation == "count":
            return f"SELECT COUNT(*) as total_count FROM {table_name}"
        elif operation == "sum":
            if fields:
                field = fields[0]
                return f"SELECT SUM({field}) as total_sum FROM {table_name}"
            else:
                return f"SELECT COUNT(*) as total_count FROM {table_name}"
        else:
            return f"SELECT COUNT(*) as total_count FROM {table_name}"
    
    def _sanitize_identifier(self, identifier: str) -> str:
        """æ¸…ç†æ ‡è¯†ç¬¦"""
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œåªä¿ç•™å­—æ¯ã€æ•°å­—å’Œä¸‹åˆ’çº¿
        return re.sub(r'[^a-zA-Z0-9_]', '', identifier)
    
    def _is_valid_field_name(self, field_name: str) -> bool:
        """æ£€æŸ¥å­—æ®µåæ˜¯å¦æœ‰æ•ˆ"""
        if not field_name:
            return False
        # æ£€æŸ¥æ˜¯å¦åªåŒ…å«å­—æ¯ã€æ•°å­—å’Œä¸‹åˆ’çº¿
        return bool(re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*$', field_name))
    
    def _clean_field_names(self, fields: List[str]) -> List[str]:
        """æ¸…ç†å­—æ®µååˆ—è¡¨"""
        cleaned_fields = []
        for field in fields:
            if self._is_valid_field_name(field):
                cleaned_fields.append(field)
        return cleaned_fields
    
    def _validate_sql_syntax(self, sql: str) -> bool:
        """éªŒè¯SQLè¯­æ³•"""
        if not sql:
            return False
        
        sql_upper = sql.upper()
        
        # åŸºæœ¬è¯­æ³•æ£€æŸ¥
        if not sql_upper.startswith('SELECT'):
            return False
        
        if 'FROM' not in sql_upper:
            return False
        
        # æ£€æŸ¥FROMåé¢æ˜¯å¦æœ‰è¡¨å
        if not re.search(r'FROM\s+\w+', sql, re.IGNORECASE):
            return False
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„è¯­æ³•é”™è¯¯
        if 'SELECTSELECT' in sql_upper or 'FROMFROM' in sql_upper:
            return False
        
        return True
    
    def _fix_sql_syntax_errors(self, sql: str, table_name: str = "default_table") -> str:
        """ä¿®å¤SQLè¯­æ³•é”™è¯¯"""
        if not sql:
            return f"SELECT COUNT(*) as total_count FROM {table_name}"
        
        # ä¿®å¤å¸¸è§çš„AIç”Ÿæˆé”™è¯¯
        sql = re.sub(r'SELECT\s+([^F]+)FROM', r'SELECT \1 FROM', sql, flags=re.IGNORECASE)
        sql = re.sub(r'SELECT\s+([^F]+)T\s+([^F]+)FROM', r'SELECT \1 FROM', sql, flags=re.IGNORECASE)
        sql = re.sub(r'SELECT\s+([^F]+)OUNT\s*\(\s*\*\s*\)', r'SELECT COUNT(*)', sql, flags=re.IGNORECASE)
        
        # ç¡®ä¿åŸºæœ¬ç»“æ„æ­£ç¡®
        if not sql.upper().startswith('SELECT'):
            sql = f"SELECT {sql}"
        
        # æ£€æŸ¥æ˜¯å¦ç¼ºå°‘è¡¨å
        if 'FROM' in sql.upper() and not re.search(r'FROM\s+\w+', sql, re.IGNORECASE):
            # å¦‚æœFROMåé¢æ²¡æœ‰è¡¨åï¼Œæ·»åŠ è¡¨å
            sql = re.sub(r'FROM\s*$', f'FROM {table_name}', sql, flags=re.IGNORECASE)
        elif 'FROM' not in sql.upper():
            sql = f"{sql} FROM {table_name}"
        
        return sql
    
    def _generate_fallback_sql(self, table_name: str = "default_table") -> str:
        """ç”Ÿæˆå¤‡ç”¨SQL"""
        return f"SELECT COUNT(*) as total_count FROM {table_name}"
    
    async def _self_validate_and_improve_sql(
        self, 
        sql: str, 
        data_source_id: str, 
        target_selection: Dict
    ) -> str:
        """SQLè´¨é‡éªŒè¯å’Œè‡ªæˆ‘æ”¹è¿› - å¢å¼ºç‰ˆæœ¬"""
        try:
            # åˆå§‹è¯­æ³•æ£€æŸ¥
            table_name = target_selection.get('table', 'default_table')
            if not self._validate_sql_syntax(sql):
                sql = self._fix_sql_syntax_errors(sql, table_name)
                if not self._validate_sql_syntax(sql):
                    return self._generate_fallback_sql(table_name)
            
            # å­—æ®µåéªŒè¯
            table_name = target_selection.get('table', 'default_table')
            fields = target_selection.get('fields', [])
            
            # æ¸…ç†å­—æ®µå
            cleaned_fields = self._clean_field_names(fields)
            if cleaned_fields != fields:
                self.logger.info(f"å­—æ®µåå·²æ¸…ç†: {fields} -> {cleaned_fields}")
            
            # æœ€ç»ˆè¯­æ³•æ£€æŸ¥
            if not self._validate_sql_syntax(sql):
                return self._generate_fallback_sql(table_name)
            
            return sql
            
        except Exception as e:
            self.logger.error(f"SQLéªŒè¯å’Œæ”¹è¿›å¤±è´¥: {e}")
            return self._generate_fallback_sql(target_selection.get('table', 'default_table'))
