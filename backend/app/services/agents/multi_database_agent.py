"""
å¤šåº“å¤šè¡¨æ™ºèƒ½Agent - ä¸ºAgentæä¾›ç»Ÿä¸€çš„å¤šåº“å¤šè¡¨è®¿é—®èƒ½åŠ›
"""
import asyncio
import logging
import re
from datetime import datetime
from typing import Dict, List, Optional, Any
from dataclasses import dataclass

from app.services.data_discovery.metadata_discovery_service import MetadataDiscoveryService, DiscoveryResult
from app.services.data_discovery.intelligent_query_router import IntelligentQueryRouter, QueryPlan
from app.services.data_discovery.cross_database_executor import CrossDatabaseExecutor, ExecutionResult


@dataclass
class AgentQueryRequest:
    """AgentæŸ¥è¯¢è¯·æ±‚"""
    query: str  # è‡ªç„¶è¯­è¨€æŸ¥è¯¢
    data_source_id: str  # æ•°æ®æºID
    context: Optional[Dict[str, Any]] = None  # ä¸Šä¸‹æ–‡ä¿¡æ¯
    max_rows: Optional[int] = 1000  # æœ€å¤§è¿”å›è¡Œæ•°
    timeout: Optional[int] = 300  # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰


@dataclass
class AgentQueryResponse:
    """AgentæŸ¥è¯¢å“åº”"""
    success: bool
    data: Optional[Any] = None  # æŸ¥è¯¢ç»“æœæ•°æ®
    explanation: str = ""  # æŸ¥è¯¢è§£é‡Š
    sql_queries: List[str] = None  # æ‰§è¡Œçš„SQLæŸ¥è¯¢
    metadata: Dict[str, Any] = None  # å…ƒæ•°æ®ä¿¡æ¯
    performance_stats: Dict[str, Any] = None  # æ€§èƒ½ç»Ÿè®¡
    errors: List[str] = None  # é”™è¯¯ä¿¡æ¯
    
    def __post_init__(self):
        if self.sql_queries is None:
            self.sql_queries = []
        if self.metadata is None:
            self.metadata = {}
        if self.performance_stats is None:
            self.performance_stats = {}
        if self.errors is None:
            self.errors = []


class MultiDatabaseAgent:
    """å¤šåº“å¤šè¡¨æ™ºèƒ½Agent"""
    
    def __init__(self, db_session=None, user_id=None):
        self.logger = logging.getLogger(__name__)
        self.metadata_service = MetadataDiscoveryService()
        self.query_router = IntelligentQueryRouter()
        self.executor = CrossDatabaseExecutor()
        self.user_id = user_id
        
        # åˆå§‹åŒ–AIæœåŠ¡
        try:
            from app.services.agents.core.ai_service import UnifiedAIService
            # å¦‚æœæä¾›äº†ç”¨æˆ·IDï¼Œä½¿ç”¨ç”¨æˆ·ç‰¹å®šçš„AIæœåŠ¡
            if user_id and db_session:
                from app.core.ai_service_factory import UserAIServiceFactory
                factory = UserAIServiceFactory()
                self.ai_service = factory.get_user_ai_service(user_id)
                self.logger.info(f"ä½¿ç”¨ç”¨æˆ·ç‰¹å®šAIæœåŠ¡: {user_id}")
            else:
                # ä½¿ç”¨ç³»ç»Ÿé»˜è®¤AIæœåŠ¡
                self.ai_service = UnifiedAIService(db_session=db_session)
                self.logger.info("ä½¿ç”¨ç³»ç»Ÿé»˜è®¤AIæœåŠ¡")
        except Exception as e:
            self.logger.warning(f"AIæœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            # å°è¯•å›é€€åˆ°ç³»ç»Ÿé»˜è®¤æœåŠ¡
            try:
                self.ai_service = UnifiedAIService(db_session=db_session)
                self.logger.info("å›é€€åˆ°ç³»ç»Ÿé»˜è®¤AIæœåŠ¡")
            except Exception as e2:
                self.logger.error(f"ç³»ç»Ÿé»˜è®¤AIæœåŠ¡ä¹Ÿå¤±è´¥: {e2}")
                self.ai_service = None
        
        # ç¼“å­˜å·²å‘ç°çš„å…ƒæ•°æ®
        self._metadata_cache = {}
        self._cache_ttl = 3600  # 1å°æ—¶ç¼“å­˜
    
    async def query(self, request: AgentQueryRequest) -> AgentQueryResponse:
        """
        æ‰§è¡Œæ™ºèƒ½æŸ¥è¯¢
        
        è¿™æ˜¯Agentè®¿é—®å¤šåº“å¤šè¡¨æ•°æ®çš„ä¸»è¦å…¥å£ç‚¹
        """
        start_time = datetime.now()
        
        try:
            self.logger.info(f"Agent query: {request.query}")
            
            # 1. ç¡®ä¿å…ƒæ•°æ®å·²å‘ç°
            await self._ensure_metadata_discovered(request.data_source_id)
            
            # 2. æ™ºèƒ½æŸ¥è¯¢è·¯ç”±
            query_plan = await self.query_router.route_query(
                request.query,
                request.data_source_id,
                request.context
            )
            
            # 3. æ‰§è¡ŒæŸ¥è¯¢è®¡åˆ’
            execution_result = await self.executor.execute_query_plan(
                query_plan,
                request.data_source_id
            )
            
            # 4. æ„å»ºå“åº”
            response = await self._build_response(
                request,
                query_plan,
                execution_result,
                start_time
            )
            
            self.logger.info(f"Agent query completed: success={response.success}")
            return response
            
        except Exception as e:
            self.logger.error(f"Error in agent query: {e}")
            return AgentQueryResponse(
                success=False,
                explanation=f"æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {str(e)}",
                errors=[str(e)],
                performance_stats={
                    'total_time': (datetime.now() - start_time).total_seconds()
                }
            )
    
    async def discover_schema(self, data_source_id: str, force_refresh: bool = False) -> Dict[str, Any]:
        """
        å‘ç°æ•°æ®æºçš„æ•°æ®åº“æ¨¡å¼
        
        Args:
            data_source_id: æ•°æ®æºID
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°å…ƒæ•°æ®
            
        Returns:
            æ•°æ®åº“æ¨¡å¼ä¿¡æ¯
        """
        try:
            # æ£€æŸ¥ç¼“å­˜
            cache_key = f"schema_{data_source_id}"
            if not force_refresh and cache_key in self._metadata_cache:
                cached_data, cached_time = self._metadata_cache[cache_key]
                if (datetime.now() - cached_time).seconds < self._cache_ttl:
                    return cached_data
            
            # æ‰§è¡Œå…ƒæ•°æ®å‘ç°
            discovery_result = await self.metadata_service.discover_data_source_metadata(
                data_source_id,
                full_discovery=True
            )
            
            schema_info = {
                'data_source_id': data_source_id,
                'databases': discovery_result.databases_found,
                'tables': discovery_result.tables_found,
                'columns': discovery_result.columns_found,
                'relations': discovery_result.relations_found,
                'discovery_time': discovery_result.discovery_time,
                'last_updated': datetime.now().isoformat()
            }
            
            # æ›´æ–°ç¼“å­˜
            self._metadata_cache[cache_key] = (schema_info, datetime.now())
            
            return schema_info
            
        except Exception as e:
            self.logger.error(f"Error discovering schema: {e}")
            raise
    
    async def get_available_tables(self, data_source_id: str, business_domain: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        è·å–å¯ç”¨çš„è¡¨åˆ—è¡¨
        
        Args:
            data_source_id: æ•°æ®æºID
            business_domain: ä¸šåŠ¡åŸŸè¿‡æ»¤ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            è¡¨ä¿¡æ¯åˆ—è¡¨
        """
        try:
            from sqlalchemy.orm import Session
            from app.db.session import get_db_session
            from app.models.table_schema import Database, Table
            
            with get_db_session() as db:
                query = db.query(Table).join(Database).filter(
                    Database.data_source_id == data_source_id,
                    Table.is_active == True
                )
                
                if business_domain:
                    query = query.filter(Database.business_domain == business_domain)
                
                tables = query.all()
                
                table_list = []
                for table in tables:
                    table_info = {
                        'id': str(table.id),
                        'name': table.name,
                        'display_name': table.display_name,
                        'database': table.database.name,
                        'database_display_name': table.database.display_name,
                        'table_type': table.table_type.value if table.table_type else 'table',
                        'row_count': table.row_count,
                        'size_mb': table.size_mb,
                        'business_tags': table.business_tags,
                        'data_sensitivity': table.data_sensitivity,
                        'last_analyzed': table.last_analyzed.isoformat() if table.last_analyzed else None
                    }
                    table_list.append(table_info)
                
                return table_list
                
        except Exception as e:
            self.logger.error(f"Error getting available tables: {e}")
            raise
    
    async def get_table_schema(self, table_id: str) -> Dict[str, Any]:
        """
        è·å–è¡¨çš„è¯¦ç»†ç»“æ„ä¿¡æ¯
        
        Args:
            table_id: è¡¨ID
            
        Returns:
            è¡¨ç»“æ„ä¿¡æ¯
        """
        try:
            from app.db.session import get_db_session
            from app.models.table_schema import Table, TableColumn
            
            with get_db_session() as db:
                table = db.query(Table).filter(Table.id == table_id).first()
                if not table:
                    raise ValueError(f"Table {table_id} not found")
                
                columns = db.query(TableColumn).filter(
                    TableColumn.table_id == table_id
                ).order_by(TableColumn.ordinal_position).all()
                
                column_list = []
                for column in columns:
                    column_info = {
                        'name': column.name,
                        'display_name': column.display_name,
                        'data_type': column.data_type.value if column.data_type else 'unknown',
                        'raw_type': column.raw_type,
                        'is_nullable': column.is_nullable,
                        'is_primary_key': column.is_primary_key,
                        'is_foreign_key': column.is_foreign_key,
                        'default_value': column.default_value,
                        'comment': column.column_comment,
                        'business_meaning': column.business_meaning,
                        'ordinal_position': column.ordinal_position
                    }
                    column_list.append(column_info)
                
                schema_info = {
                    'table_id': str(table.id),
                    'name': table.name,
                    'display_name': table.display_name,
                    'database': table.database.name,
                    'table_type': table.table_type.value if table.table_type else 'table',
                    'columns': column_list,
                    'row_count': table.row_count,
                    'size_mb': table.size_mb,
                    'business_tags': table.business_tags,
                    'data_sensitivity': table.data_sensitivity
                }
                
                return schema_info
                
        except Exception as e:
            self.logger.error(f"Error getting table schema: {e}")
            raise
    
    async def suggest_queries(self, data_source_id: str, business_context: Optional[str] = None) -> List[str]:
        """
        åŸºäºæ•°æ®ç»“æ„å»ºè®®å¯èƒ½çš„æŸ¥è¯¢
        
        Args:
            data_source_id: æ•°æ®æºID
            business_context: ä¸šåŠ¡ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            å»ºè®®çš„æŸ¥è¯¢åˆ—è¡¨
        """
        try:
            tables = await self.get_available_tables(data_source_id)
            
            suggestions = []
            
            # åŸºäºè¡¨åå’Œä¸šåŠ¡æ ‡ç­¾ç”Ÿæˆå»ºè®®
            for table in tables:
                table_name = table['display_name'] or table['name']
                
                # åŸºç¡€æŸ¥è¯¢å»ºè®®
                suggestions.append(f"æŸ¥è¯¢{table_name}çš„æ‰€æœ‰æ•°æ®")
                suggestions.append(f"ç»Ÿè®¡{table_name}çš„æ•°é‡")
                
                # åŸºäºä¸šåŠ¡æ ‡ç­¾çš„å»ºè®®
                if table.get('business_tags'):
                    for tag in table['business_tags']:
                        suggestions.append(f"åˆ†æ{tag}ç›¸å…³çš„{table_name}æ•°æ®")
                
                # æ—¶é—´ç›¸å…³å»ºè®®
                suggestions.append(f"æŸ¥è¯¢{table_name}æœ€è¿‘30å¤©çš„æ•°æ®")
                suggestions.append(f"æŒ‰æœˆç»Ÿè®¡{table_name}çš„è¶‹åŠ¿")
            
            # è·¨è¡¨æŸ¥è¯¢å»ºè®®
            if len(tables) > 1:
                suggestions.append("æŸ¥è¯¢ä¸åŒè¡¨ä¹‹é—´çš„å…³è”å…³ç³»")
                suggestions.append("ç”Ÿæˆè·¨è¡¨çš„ç»¼åˆæŠ¥å‘Š")
            
            return suggestions[:10]  # è¿”å›å‰10ä¸ªå»ºè®®
            
        except Exception as e:
            self.logger.error(f"Error generating query suggestions: {e}")
            return []
    
    async def explain_query(self, query: str, data_source_id: str) -> Dict[str, Any]:
        """
        è§£é‡ŠæŸ¥è¯¢çš„æ‰§è¡Œè®¡åˆ’å’Œæ¶‰åŠçš„è¡¨
        
        Args:
            query: è‡ªç„¶è¯­è¨€æŸ¥è¯¢
            data_source_id: æ•°æ®æºID
            
        Returns:
            æŸ¥è¯¢è§£é‡Šä¿¡æ¯
        """
        try:
            # ç”ŸæˆæŸ¥è¯¢è®¡åˆ’
            query_plan = await self.query_router.route_query(query, data_source_id)
            
            explanation = {
                'original_query': query,
                'parsed_intent': query_plan.estimated_complexity,  # ç®€åŒ–å¤„ç†
                'involved_tables': [],
                'join_conditions': query_plan.join_conditions,
                'filters': query_plan.where_conditions,
                'aggregations': query_plan.group_by_columns,
                'complexity': query_plan.estimated_complexity,
                'cross_database': query_plan.cross_database,
                'execution_strategy': 'è·¨æ•°æ®åº“æŸ¥è¯¢' if query_plan.cross_database else 'å•æ•°æ®åº“æŸ¥è¯¢'
            }
            
            # æ·»åŠ æ¶‰åŠçš„è¡¨ä¿¡æ¯
            for table_candidate in query_plan.primary_tables + query_plan.join_tables:
                table_info = {
                    'name': table_candidate.table.name,
                    'display_name': table_candidate.table.display_name,
                    'database': table_candidate.table.database.name,
                    'relevance_score': table_candidate.relevance_score,
                    'role': 'ä¸»è¡¨' if table_candidate in query_plan.primary_tables else 'å…³è”è¡¨'
                }
                explanation['involved_tables'].append(table_info)
            
            return explanation
            
        except Exception as e:
            self.logger.error(f"Error explaining query: {e}")
            raise
    
    async def analyze_placeholder_requirements(self, agent_input: Dict[str, Any], execution_context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        åˆ†æå ä½ç¬¦éœ€æ±‚å¹¶ç”Ÿæˆå¯¹åº”çš„SQLæŸ¥è¯¢ - æ™ºèƒ½åˆ†ææ¶æ„
        ä½¿ç”¨AIé©±åŠ¨çš„åˆ†æå’Œæ™ºèƒ½è¡¨é€‰æ‹©æœºåˆ¶
        
        Args:
            agent_input: å ä½ç¬¦åˆ†æè¾“å…¥
            execution_context: æ‰§è¡Œæ—¶é—´ä¸Šä¸‹æ–‡
        
        Returns:
            åˆ†æç»“æœï¼ŒåŒ…å«é«˜è´¨é‡çš„SQLå’Œç›¸å…³ä¿¡æ¯
        """
        analysis_start_time = datetime.now()
        placeholder_name = agent_input.get('placeholder_name', '')
        placeholder_type = agent_input.get('placeholder_type', '')
        
        try:
            self.logger.info(f"ğŸš€ å¼€å§‹Agentåˆ†æå ä½ç¬¦: {placeholder_name}")
            
            # æå–å ä½ç¬¦ä¿¡æ¯
            data_source = agent_input.get('data_source', {})
            data_source_id = data_source.get('id', '')
            schema_info = agent_input.get('schema_info', {})
            
            # å‚æ•°éªŒè¯
            if not data_source_id:
                raise ValueError("æ•°æ®æºIDä¸èƒ½ä¸ºç©º")
            if not placeholder_name:
                raise ValueError("å ä½ç¬¦åç§°ä¸èƒ½ä¸ºç©º")
            
            self.logger.info(f"ğŸ“Š å ä½ç¬¦ç±»å‹: {placeholder_type}, æ•°æ®æº: {data_source_id}")
            
            # 1. è·å–å®æ—¶æ•°æ®æºç»“æ„ä¿¡æ¯ - åŸºäºAIè¯­ä¹‰ç­›é€‰
            self.logger.info("ğŸ” è·å–æ•°æ®æºç»“æ„ä¿¡æ¯...")
            enhanced_schema = await self._get_enhanced_schema_info(data_source_id, placeholder_name)
            
            if not enhanced_schema.get('table_schemas'):
                self.logger.warning("âš ï¸ æœªè·å–åˆ°è¡¨ç»“æ„ä¿¡æ¯ï¼Œä½¿ç”¨å›é€€æ–¹æ¡ˆ")
                return await self._create_fallback_solution(placeholder_name, data_source_id)
            
            relevant_tables = list(enhanced_schema.get('table_schemas', {}).keys())
            self.logger.info(f"âœ… è·å–åˆ° {len(relevant_tables)} ä¸ªç›¸å…³è¡¨: {relevant_tables}")
            
            # 2. æ ¹æ®å ä½ç¬¦ç±»å‹é€‰æ‹©åˆ†ææ–¹æ³•
            analysis_mode = "ai_agent" if placeholder_type in ["statistic", "statistical", "analysis", "chart", "table"] else "semantic"
            self.logger.info(f"ğŸ§  ä½¿ç”¨ {analysis_mode} åˆ†ææ¨¡å¼")
            
            if analysis_mode == "ai_agent":
                # ä½¿ç”¨AI Agentè¿›è¡Œæ·±åº¦åˆ†æ
                semantic_analysis = await self._perform_ai_agent_analysis(
                    placeholder_name, placeholder_type, enhanced_schema, data_source
                )
                self.logger.info(f"ğŸ¯ AIåˆ†æå®Œæˆ: {semantic_analysis.get('intent', 'unknown')}")
            else:
                # ä½¿ç”¨è¯­ä¹‰åˆ†æä½œä¸ºåå¤‡
                semantic_analysis = await self._analyze_placeholder_semantics(
                    placeholder_name, placeholder_type, enhanced_schema
                )
                self.logger.info(f"ğŸ“‹ è¯­ä¹‰åˆ†æå®Œæˆ: {semantic_analysis.get('intent', 'unknown')}")
            
            # 3. æ™ºèƒ½å­—æ®µé€‰æ‹©å’Œè¡¨é€‰æ‹©
            self.logger.info("ğŸ¯ æ‰§è¡Œæ™ºèƒ½ç›®æ ‡é€‰æ‹©...")
            target_selection = await self._intelligent_target_selection(
                semantic_analysis, enhanced_schema
            )
            self.logger.info(f"âœ… æ™ºèƒ½ç›®æ ‡é€‰æ‹©å®Œæˆ: è¡¨={target_selection.get('table', 'unknown')}, å­—æ®µæ•°={len(target_selection.get('fields', []))}")
            
            # 4. ç”Ÿæˆåˆå§‹SQLï¼ˆåŸºäºçœŸå®è¡¨ç»“æ„ï¼‰
            self.logger.info("âš™ï¸ ç”Ÿæˆæ™ºèƒ½SQL...")
            initial_sql = await self._generate_intelligent_sql(
                semantic_analysis, target_selection, enhanced_schema
            )
            self.logger.info(f"âœ… ç”Ÿæˆæ™ºèƒ½SQL: {initial_sql}")
            
            # 5. SQLè´¨é‡éªŒè¯å’Œè‡ªæˆ‘ä¿®æ­£
            self.logger.info("ğŸ”§ æ‰§è¡ŒSQLè´¨é‡éªŒè¯å’Œæ”¹è¿›...")
            validated_sql = await self._self_validate_and_improve_sql(
                initial_sql, data_source_id, target_selection
            )
            
            # 6. åº”ç”¨æ‰§è¡Œæ—¶é—´å‚æ•°æ›¿æ¢
            if execution_context and execution_context.get("sql_parameters"):
                validated_sql = self._apply_sql_parameter_substitution(validated_sql, execution_context["sql_parameters"])
                self.logger.info(f"Applied SQL parameter substitution with {len(execution_context['sql_parameters'])} parameters")
            
            # 7. è®¡ç®—å¢å¼ºç½®ä¿¡åº¦
            confidence_score = self._calculate_enhanced_confidence_score(
                semantic_analysis, target_selection, validated_sql
            )
            
            # 8. è®¡ç®—åˆ†æè€—æ—¶
            analysis_duration = (datetime.now() - analysis_start_time).total_seconds()
            
            # 9. æ„å»ºå¢å¼ºçš„è¿”å›ç»“æœ
            result = {
                "success": True,
                "target_database": target_selection.get('database', 'default'),
                "target_table": target_selection.get('table', 'default_table'),
                "required_fields": target_selection.get('fields', ['*']),
                "generated_sql": validated_sql,
                "confidence_score": confidence_score,
                "analysis_metadata": {
                    "placeholder_name": placeholder_name,
                    "placeholder_type": placeholder_type,
                    "analysis_mode": analysis_mode,
                    "intent": semantic_analysis.get('intent', ''),
                    "data_operation": semantic_analysis.get('data_operation', ''),
                    "business_domain": semantic_analysis.get('business_domain', ''),
                    "relevant_tables_count": len(relevant_tables),
                    "analysis_duration_seconds": round(analysis_duration, 3),
                    "ai_service_available": self.ai_service is not None
                },
                "reasoning": semantic_analysis.get('reasoning', ''),
                "suggested_optimizations": semantic_analysis.get('optimizations', []),
                "estimated_execution_time": target_selection.get('estimated_time', 1000),
                "schema_quality": enhanced_schema.get('quality_metrics', {}),
                "field_mapping": target_selection.get('field_mapping', {}),
                "quality_metrics": {
                    "table_relevance": len(relevant_tables),
                    "field_coverage": len(target_selection.get('fields', [])),
                    "sql_complexity": len(validated_sql.split()) if validated_sql else 0
                }
            }
            
            self.logger.info(f"âœ… Agentåˆ†æå®Œæˆ: {placeholder_name}")
            self.logger.info(f"ğŸ“Š åˆ†æç»“æœ: è¡¨={result['target_table']}, ç½®ä¿¡åº¦={confidence_score:.2f}, è€—æ—¶={analysis_duration:.3f}s")
            return result
            
        except Exception as e:
            analysis_duration = (datetime.now() - analysis_start_time).total_seconds()
            self.logger.error(f"âŒ Agentåˆ†æå¤±è´¥: {placeholder_name}, é”™è¯¯: {e}, è€—æ—¶: {analysis_duration:.3f}s")
            
            # åˆ›å»ºå›é€€è§£å†³æ–¹æ¡ˆ
            try:
                fallback_result = await self._create_fallback_solution(placeholder_name, data_source_id)
                fallback_result.update({
                    "analysis_metadata": {
                        "placeholder_name": placeholder_name,
                        "placeholder_type": placeholder_type,
                        "analysis_mode": "fallback",
                        "analysis_duration_seconds": round(analysis_duration, 3),
                        "error": str(e),
                        "ai_service_available": self.ai_service is not None
                    }
                })
                return fallback_result
            except Exception as fallback_error:
                self.logger.error(f"âŒ å›é€€æ–¹æ¡ˆä¹Ÿå¤±è´¥: {fallback_error}")
                return {
                    "success": False,
                    "error": str(e),
                    "fallback_error": str(fallback_error),
                    "generated_sql": "SELECT 1 as placeholder_value",
                    "target_table": "unknown",
                    "confidence_score": 0.0,
                    "analysis_metadata": {
                        "placeholder_name": placeholder_name,
                        "placeholder_type": placeholder_type,
                        "analysis_mode": "emergency_fallback",
                        "analysis_duration_seconds": round(analysis_duration, 3),
                        "error": str(e)
                    }
                }
    
    def _build_natural_query_from_placeholder(self, placeholder_name: str, placeholder_type: str, schema_info: Dict) -> str:
        """ä»å ä½ç¬¦ä¿¡æ¯æ„å»ºè‡ªç„¶è¯­è¨€æŸ¥è¯¢"""
        # è§£æå ä½ç¬¦åç§°ä¸­çš„è¯­ä¹‰ä¿¡æ¯
        name_parts = placeholder_name.lower().split(':')
        if len(name_parts) >= 2:
            category = name_parts[0]  # å¦‚ï¼š"ç»Ÿè®¡", "åŒºåŸŸ", "å‘¨æœŸ"
            metric = name_parts[1]    # å¦‚ï¼š"æ€»æ•°", "åœ°åŒºåç§°", "å¼€å§‹æ—¥æœŸ"
            
            # æ„å»ºæ›´æ™ºèƒ½çš„æŸ¥è¯¢
            if 'ç»Ÿè®¡' in category or 'count' in metric or 'æ•°é‡' in metric or 'ä»¶æ•°' in metric:
                return f"ç»Ÿè®¡ {metric} çš„æ€»æ•°"
            elif 'åŒºåŸŸ' in category or 'åœ°åŒº' in metric:
                return f"è·å– {metric} ä¿¡æ¯"
            elif 'å‘¨æœŸ' in category or 'æ—¥æœŸ' in metric or 'æ—¶é—´' in metric:
                return f"è·å– {metric} çš„æ—¶é—´ä¿¡æ¯"
            elif 'å æ¯”' in metric or 'ç™¾åˆ†æ¯”' in metric:
                return f"è®¡ç®— {metric} çš„æ¯”ä¾‹"
        
        # å›é€€åˆ°ç®€å•çš„æŸ¥è¯¢
        return f"æŸ¥è¯¢ä¸ {placeholder_name} ç›¸å…³çš„æ•°æ®"
    
    def _generate_sql_from_query_plan(self, query_plan, schema_info: Dict) -> str:
        """åŸºäºæŸ¥è¯¢è®¡åˆ’å’Œå¢å¼ºçš„schemaä¿¡æ¯ç”Ÿæˆæ›´ç²¾ç¡®çš„SQL"""
        if not query_plan.primary_tables:
            # ä½¿ç”¨schema_infoä¸­çš„è¡¨ä¿¡æ¯
            tables = schema_info.get('tables', [])
            if tables:
                best_table = self._select_best_table_from_schema(schema_info)
                return f"SELECT COUNT(*) as count FROM {best_table}"
            return "SELECT COUNT(*) as count FROM default_table"
        
        primary_table = query_plan.primary_tables[0].table
        table_name = primary_table.name
        
        # ä»å¢å¼ºçš„schemaä¿¡æ¯ä¸­è·å–è¡¨çš„è¯¦ç»†ç»“æ„
        table_schemas = schema_info.get('table_schemas', {})
        table_schema = table_schemas.get(table_name, {})
        
        # åŸºäºæŸ¥è¯¢è®¡åˆ’å’Œè¡¨ç»“æ„ç”Ÿæˆæ›´æ™ºèƒ½çš„SQL
        if query_plan.aggregate_functions:
            # åŒ…å«èšåˆå‡½æ•°
            agg_func = query_plan.aggregate_functions[0]
            if 'count' in agg_func.lower():
                sql = f"SELECT COUNT(*) as total_count FROM {table_name}"
            elif 'sum' in agg_func.lower():
                # å°è¯•æ‰¾åˆ°æ•°å€¼å­—æ®µè¿›è¡Œæ±‚å’Œ
                numeric_field = self._find_numeric_field(table_schema)
                if numeric_field:
                    sql = f"SELECT SUM({numeric_field}) as total_sum FROM {table_name}"
                else:
                    sql = f"SELECT COUNT(*) as total_count FROM {table_name}"
            elif 'avg' in agg_func.lower():
                # å°è¯•æ‰¾åˆ°æ•°å€¼å­—æ®µè¿›è¡Œå¹³å‡å€¼è®¡ç®—
                numeric_field = self._find_numeric_field(table_schema)
                if numeric_field:
                    sql = f"SELECT AVG({numeric_field}) as avg_value FROM {table_name}"
                else:
                    sql = f"SELECT COUNT(*) as total_count FROM {table_name}"
            else:
                sql = f"SELECT {agg_func} FROM {table_name}"
        else:
            # åŸºäºè¡¨ç»“æ„ç”Ÿæˆåˆé€‚çš„å­—æ®µåˆ—è¡¨
            important_fields = self._select_important_fields(table_schema)
            if important_fields:
                fields_str = ', '.join(important_fields[:5])  # é™åˆ¶å­—æ®µæ•°é‡
                sql = f"SELECT {fields_str} FROM {table_name}"
            else:
                sql = f"SELECT * FROM {table_name}"
        
        # æ·»åŠ WHEREæ¡ä»¶
        if query_plan.where_conditions:
            sql += f" WHERE {' AND '.join(query_plan.where_conditions)}"
        
        # æ·»åŠ æ™ºèƒ½LIMITï¼ˆåŸºäºè¡¨å¤§å°ï¼‰
        table_metadata = table_schema.get('metadata', {})
        estimated_rows = table_metadata.get('estimated_rows', 0)
        if estimated_rows > 10000:
            sql += " LIMIT 50"  # å¤§è¡¨é™åˆ¶æ›´å°‘è¡Œæ•°
        else:
            sql += " LIMIT 100"
        
        self.logger.info(f"Generated enhanced SQL: {sql}")
        return sql
    
    def _calculate_confidence_score(self, query_plan, placeholder_name: str, schema_info: Dict) -> float:
        """è®¡ç®—åˆ†æçš„ç½®ä¿¡åº¦åˆ†æ•°"""
        base_score = 0.5
        
        # å¦‚æœæ‰¾åˆ°äº†ç›¸å…³è¡¨ï¼Œæé«˜ç½®ä¿¡åº¦
        if query_plan.primary_tables:
            base_score += 0.3
        
        # å¦‚æœå ä½ç¬¦åç§°ä¸è¡¨å­—æ®µåŒ¹é…ï¼Œæé«˜ç½®ä¿¡åº¦
        if self._has_matching_fields(placeholder_name, schema_info):
            base_score += 0.2
        
        # é™åˆ¶åœ¨0-1èŒƒå›´å†…
        return min(1.0, base_score)
    
    def _extract_required_fields(self, query_plan) -> List[str]:
        """ä»æŸ¥è¯¢è®¡åˆ’ä¸­æå–éœ€è¦çš„å­—æ®µ"""
        if query_plan.select_columns:
            return query_plan.select_columns
        return ["*"]
    
    def _suggest_optimizations(self, query_plan) -> List[str]:
        """å»ºè®®æŸ¥è¯¢ä¼˜åŒ–"""
        optimizations = []
        
        if query_plan.estimated_complexity > 5:
            optimizations.append("è€ƒè™‘æ·»åŠ ç´¢å¼•ä»¥æé«˜æŸ¥è¯¢æ€§èƒ½")
        
        if query_plan.cross_database:
            optimizations.append("è·¨æ•°æ®åº“æŸ¥è¯¢ï¼Œè€ƒè™‘æ•°æ®ç¼“å­˜ç­–ç•¥")
        
        if len(query_plan.join_tables) > 3:
            optimizations.append("å¤šè¡¨JOINï¼Œå»ºè®®æ£€æŸ¥æŸ¥è¯¢è®¡åˆ’")
        
        return optimizations
    
    def _apply_sql_parameter_substitution(self, sql: str, sql_parameters: Dict[str, str]) -> str:
        """åº”ç”¨SQLå‚æ•°æ›¿æ¢ï¼Œæ”¯æŒæ—¶é—´ç›¸å…³çš„åŠ¨æ€å‚æ•°"""
        try:
            substituted_sql = sql
            
            # æŒ‰å‚æ•°é•¿åº¦æ’åºï¼Œç¡®ä¿è¾ƒé•¿çš„å‚æ•°å…ˆè¢«æ›¿æ¢ï¼ˆé¿å…éƒ¨åˆ†åŒ¹é…é—®é¢˜ï¼‰
            sorted_parameters = sorted(sql_parameters.items(), key=lambda x: len(x[0]), reverse=True)
            
            for parameter, value in sorted_parameters:
                if parameter in substituted_sql:
                    substituted_sql = substituted_sql.replace(parameter, value)
                    self.logger.debug(f"Replaced {parameter} with {value}")
            
            # è®°å½•æ›¿æ¢åçš„SQLç”¨äºè°ƒè¯•
            if substituted_sql != sql:
                self.logger.info(f"Original SQL: {sql}")
                self.logger.info(f"Substituted SQL: {substituted_sql}")
            
            return substituted_sql
            
        except Exception as e:
            self.logger.error(f"Error applying SQL parameter substitution: {e}")
            # å‡ºé”™æ—¶è¿”å›åŸSQL
            return sql
    
    def _has_matching_fields(self, placeholder_name: str, schema_info: Dict) -> bool:
        """æ£€æŸ¥å ä½ç¬¦åç§°æ˜¯å¦ä¸æ•°æ®åº“å­—æ®µåŒ¹é…"""
        placeholder_lower = placeholder_name.lower()
        
        # æ£€æŸ¥schema_infoä¸­çš„å®é™…å­—æ®µ
        table_schemas = schema_info.get('table_schemas', {})
        for table_name, table_schema in table_schemas.items():
            columns = table_schema.get('columns', [])
            for column in columns:
                column_name = column.get('name', '').lower()
                if any(keyword in placeholder_lower for keyword in [column_name]):
                    return True
        
        # å›é€€åˆ°å…³é”®è¯åŒ¹é…
        common_keywords = ['count', 'sum', 'total', 'date', 'time', 'name', 'id']
        return any(keyword in placeholder_lower for keyword in common_keywords)
    
    def _select_best_table_from_schema(self, schema_info: Dict) -> str:
        """ä»schemaä¿¡æ¯ä¸­é€‰æ‹©æœ€ä½³è¡¨"""
        tables = schema_info.get('tables', [])
        table_schemas = schema_info.get('table_schemas', {})
        
        if not tables:
            return "default_table"
        
        # åŸºäºè¡¨çš„ä¸šåŠ¡ç›¸å…³æ€§é€‰æ‹©
        best_table = tables[0]
        best_score = 0.0
        
        for table_name in tables:
            table_schema = table_schemas.get(table_name, {})
            metadata = table_schema.get('metadata', {})
            relevance = metadata.get('business_relevance', 0.0)
            
            if relevance > best_score:
                best_score = relevance
                best_table = table_name
        
        return best_table
    
    def _find_numeric_field(self, table_schema: Dict) -> str:
        """åœ¨è¡¨ç»“æ„ä¸­æŸ¥æ‰¾æ•°å€¼å­—æ®µ"""
        columns = table_schema.get('columns', [])
        
        # ä¼˜å…ˆæŸ¥æ‰¾æ˜æ˜¾çš„æ•°å€¼å­—æ®µ
        for column in columns:
            column_name = column.get('name', '').lower()
            column_type = column.get('type', '').lower()
            
            # åŸºäºå­—æ®µååˆ¤æ–­
            if any(keyword in column_name for keyword in ['amount', 'price', 'cost', 'value', 'count', 'num']):
                return column.get('name', '')
            
            # åŸºäºå­—æ®µç±»å‹åˆ¤æ–­
            if any(type_keyword in column_type for type_keyword in ['int', 'float', 'decimal', 'number', 'bigint']):
                return column.get('name', '')
        
        return None
    
    def _select_important_fields(self, table_schema: Dict) -> List[str]:
        """é€‰æ‹©è¡¨ä¸­é‡è¦çš„å­—æ®µ"""
        columns = table_schema.get('columns', [])
        important_fields = []
        
        # ä¼˜å…ˆçº§å­—æ®µå…³é”®è¯
        priority_keywords = ['id', 'name', 'title', 'status', 'date', 'time', 'created', 'updated']
        
        # å…ˆæ·»åŠ ä¼˜å…ˆçº§å­—æ®µ
        for column in columns:
            column_name = column.get('name', '')
            if any(keyword in column_name.lower() for keyword in priority_keywords):
                important_fields.append(column_name)
        
        # å¦‚æœé‡è¦å­—æ®µä¸å¤Ÿï¼Œæ·»åŠ å…¶ä»–å­—æ®µ
        if len(important_fields) < 3:
            for column in columns:
                column_name = column.get('name', '')
                if column_name not in important_fields:
                    important_fields.append(column_name)
                    if len(important_fields) >= 5:
                        break
        
        return important_fields
    
    async def _get_enhanced_schema_info(self, data_source_id: str, placeholder_name: str = "") -> Dict[str, Any]:
        """è·å–å¢å¼ºçš„æ•°æ®æºç»“æ„ä¿¡æ¯ - å®Œå…¨ä½¿ç”¨æ–°API"""
        try:
            from app.models.data_source import DataSource
            from app.services.connectors.connector_factory import create_connector
            from app.db.session import get_db_session
            
            with get_db_session() as db:
                data_source = db.query(DataSource).filter(DataSource.id == data_source_id).first()
                if not data_source:
                    raise ValueError(f"æ•°æ®æº {data_source_id} ä¸å­˜åœ¨")
                
                connector = create_connector(data_source)
                await connector.connect()
                
                try:
                    # ä½¿ç”¨æ–°APIè·å–å®Œæ•´ä¿¡æ¯
                    databases = await connector.get_databases()
                    tables = await connector.get_tables()
                    
                    # åŸºäºLLMè¿›è¡Œæ™ºèƒ½è¡¨é€‰æ‹©
                    relevant_tables = await self._ai_select_relevant_tables(tables, placeholder_name)
                    self.logger.info(f"åŸºäºAIè¯­ä¹‰åˆ†æç­›é€‰åˆ° {len(relevant_tables)} ä¸ªç›¸å…³è¡¨: {relevant_tables}")
                    
                    # è·å–ç›¸å…³è¡¨çš„è¯¦ç»†ç»“æ„
                    table_schemas = {}
                    for table_name in relevant_tables:
                        try:
                            schema_info = await connector.get_table_schema(table_name)
                            table_schemas[table_name] = schema_info
                            
                            # å¢å¼ºå­—æ®µåˆ†æ
                            columns = schema_info.get('columns', [])
                            schema_info['enhanced_metadata'] = {
                                'business_fields': self._identify_business_fields(columns),
                                'key_fields': self._identify_key_fields(columns),
                                'numeric_fields': self._identify_numeric_fields(columns),
                                'date_fields': self._identify_date_fields(columns),
                                'text_fields': self._identify_text_fields(columns)
                            }
                            
                        except Exception as e:
                            self.logger.warning(f"è·å–è¡¨ {table_name} ç»“æ„å¤±è´¥: {e}")
                    
                    enhanced_schema = {
                        'data_source_id': data_source_id,
                        'data_source_name': data_source.name,
                        'databases': databases,
                        'tables': tables,
                        'table_schemas': table_schemas,
                        'quality_metrics': {
                            'total_tables': len(tables),
                            'analyzed_tables': len(table_schemas),
                            'coverage_rate': (len(table_schemas) / len(tables)) * 100 if tables else 0,
                            'total_fields': sum(len(schema.get('columns', [])) for schema in table_schemas.values())
                        },
                        'retrieved_at': datetime.now().isoformat(),
                        'source': 'enhanced_connector_api'
                    }
                    
                    self.logger.info(f"è·å–å¢å¼ºschemaå®Œæˆ: {len(tables)}ä¸ªè¡¨, {enhanced_schema['quality_metrics']['total_fields']}ä¸ªå­—æ®µ")
                    return enhanced_schema
                    
                finally:
                    await connector.disconnect()
                    
        except Exception as e:
            self.logger.error(f"è·å–å¢å¼ºschemaä¿¡æ¯å¤±è´¥: {e}")
            return {
                'data_source_id': data_source_id,
                'error': str(e),
                'databases': [],
                'tables': [],
                'table_schemas': {},
                'quality_metrics': {'total_tables': 0, 'analyzed_tables': 0, 'coverage_rate': 0}
            }
    
    async def _analyze_placeholder_semantics(self, placeholder_name: str, placeholder_type: str, enhanced_schema: Dict) -> Dict[str, Any]:
        """æ™ºèƒ½åˆ†æå ä½ç¬¦è¯­ä¹‰"""
        try:
            analysis = {
                'placeholder_name': placeholder_name,
                'placeholder_type': placeholder_type,
                'intent': None,
                'business_domain': None,
                'data_operation': None,
                'temporal_scope': None,
                'aggregation_type': None,
                'reasoning': []
            }
            
            name_lower = placeholder_name.lower()
            
            # æ„å›¾åˆ†æ
            if any(kw in name_lower for kw in ['ç»Ÿè®¡', 'count', 'æ•°é‡', 'ä»¶æ•°', 'æ€»è®¡']):
                analysis['intent'] = 'statistical'
                analysis['data_operation'] = 'count'
                analysis['aggregation_type'] = 'COUNT'
                analysis['reasoning'].append('è¯†åˆ«ä¸ºç»Ÿè®¡ç±»å ä½ç¬¦')
            elif any(kw in name_lower for kw in ['åŒºåŸŸ', 'åœ°åŒº', 'åœ°ç‚¹', 'region', 'area']):
                analysis['intent'] = 'dimensional'
                analysis['data_operation'] = 'group_by'
                analysis['business_domain'] = 'geographic'
                analysis['reasoning'].append('è¯†åˆ«ä¸ºåœ°ç†ç»´åº¦å ä½ç¬¦')
            elif any(kw in name_lower for kw in ['å‘¨æœŸ', 'æ—¶é—´', 'æ—¥æœŸ', 'date', 'time', 'period']):
                analysis['intent'] = 'temporal'
                analysis['data_operation'] = 'filter'
                analysis['temporal_scope'] = 'date_range'
                analysis['reasoning'].append('è¯†åˆ«ä¸ºæ—¶é—´å‘¨æœŸå ä½ç¬¦')
            elif any(kw in name_lower for kw in ['é‡‘é¢', 'è´¹ç”¨', 'ä»·æ ¼', 'amount', 'price', 'cost']):
                analysis['intent'] = 'financial'
                analysis['data_operation'] = 'sum'
                analysis['aggregation_type'] = 'SUM'
                analysis['reasoning'].append('è¯†åˆ«ä¸ºé‡‘é¢ç±»å ä½ç¬¦')
            else:
                analysis['intent'] = 'general'
                analysis['data_operation'] = 'select'
                analysis['reasoning'].append('è¯†åˆ«ä¸ºä¸€èˆ¬æŸ¥è¯¢å ä½ç¬¦')
            
            # ä¸šåŠ¡é¢†åŸŸåˆ†æ
            if any(kw in name_lower for kw in ['æŠ•è¯‰', 'complaint', 'ä¸¾æŠ¥', 'åé¦ˆ']):
                analysis['business_domain'] = 'complaint_management'
                analysis['reasoning'].append('è¯†åˆ«ä¸ºæŠ•è¯‰ç®¡ç†ä¸šåŠ¡åŸŸ')
            elif any(kw in name_lower for kw in ['æ—…è¡Œ', 'æ—…æ¸¸', 'travel', 'tour']):
                analysis['business_domain'] = 'travel_business'
                analysis['reasoning'].append('è¯†åˆ«ä¸ºæ—…æ¸¸ä¸šåŠ¡åŸŸ')
            elif any(kw in name_lower for kw in ['ç”¨æˆ·', 'å®¢æˆ·', 'user', 'customer']):
                analysis['business_domain'] = 'customer_management'
                analysis['reasoning'].append('è¯†åˆ«ä¸ºå®¢æˆ·ç®¡ç†ä¸šåŠ¡åŸŸ')
            
            # ä¼˜åŒ–å»ºè®®
            analysis['optimizations'] = self._generate_semantic_optimizations(analysis)
            
            self.logger.info(f"å ä½ç¬¦è¯­ä¹‰åˆ†æå®Œæˆ: {placeholder_name} -> {analysis['intent']}")
            return analysis
            
        except Exception as e:
            self.logger.error(f"å ä½ç¬¦è¯­ä¹‰åˆ†æå¤±è´¥: {e}")
            return {
                'placeholder_name': placeholder_name,
                'intent': 'general',
                'data_operation': 'select',
                'reasoning': [f'åˆ†æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {str(e)}']
            }
    
    async def _intelligent_target_selection(self, semantic_analysis: Dict, enhanced_schema: Dict) -> Dict[str, Any]:
        """æ™ºèƒ½ç›®æ ‡è¡¨å’Œå­—æ®µé€‰æ‹©"""
        try:
            table_schemas = enhanced_schema.get('table_schemas', {})
            
            # è¡¨é€‰æ‹©è¯„åˆ†
            table_scores = {}
            for table_name, schema in table_schemas.items():
                score = 0.0
                
                # åŸºäºä¸šåŠ¡åŸŸåŒ¹é…
                business_domain = semantic_analysis.get('business_domain', '')
                if business_domain == 'complaint_management':
                    if any(kw in table_name.lower() for kw in ['complaint', 'report', 'feedback']):
                        score += 3.0
                elif business_domain == 'travel_business':
                    if any(kw in table_name.lower() for kw in ['travel', 'tour', 'booking']):
                        score += 3.0
                elif business_domain == 'customer_management':
                    if any(kw in table_name.lower() for kw in ['user', 'customer', 'member']):
                        score += 3.0
                
                # åŸºäºå­—æ®µä¸°å¯Œåº¦
                columns = schema.get('columns', [])
                enhanced_metadata = schema.get('enhanced_metadata', {})
                
                score += len(columns) * 0.1  # å­—æ®µæ•°é‡åŠ åˆ†
                score += len(enhanced_metadata.get('business_fields', [])) * 0.5  # ä¸šåŠ¡å­—æ®µåŠ åˆ†
                score += len(enhanced_metadata.get('key_fields', [])) * 0.3  # å…³é”®å­—æ®µåŠ åˆ†
                
                table_scores[table_name] = score
            
            # é€‰æ‹©æœ€ä½³è¡¨
            if table_scores:
                best_table = max(table_scores, key=table_scores.get)
                best_schema = table_schemas[best_table]
            else:
                # å›é€€åˆ°ç¬¬ä¸€ä¸ªå¯ç”¨è¡¨
                tables = enhanced_schema.get('tables', [])
                best_table = tables[0] if tables else 'default_table'
                best_schema = table_schemas.get(best_table, {})
            
            # æ™ºèƒ½å­—æ®µé€‰æ‹©
            selected_fields = self._select_optimal_fields(semantic_analysis, best_schema)
            
            # æ„å»ºå­—æ®µæ˜ å°„
            field_mapping = self._create_field_mapping(semantic_analysis, best_schema)
            
            result = {
                'database': enhanced_schema.get('databases', ['default'])[0] if enhanced_schema.get('databases') else 'default',
                'table': best_table,
                'fields': selected_fields,
                'field_mapping': field_mapping,
                'table_score': table_scores.get(best_table, 0.0),
                'estimated_time': self._estimate_query_time(best_schema, selected_fields),
                'selection_reasoning': f'é€‰æ‹©è¡¨ {best_table}ï¼Œè¯„åˆ†: {table_scores.get(best_table, 0.0):.2f}'
            }
            
            self.logger.info(f"æ™ºèƒ½ç›®æ ‡é€‰æ‹©å®Œæˆ: è¡¨={best_table}, å­—æ®µæ•°={len(selected_fields)}")
            return result
            
        except Exception as e:
            self.logger.error(f"æ™ºèƒ½ç›®æ ‡é€‰æ‹©å¤±è´¥: {e}")
            return {
                'database': 'default',
                'table': 'default_table',
                'fields': ['*'],
                'field_mapping': {},
                'selection_reasoning': f'é€‰æ‹©å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {str(e)}'
            }
    
    async def _generate_intelligent_sql(self, semantic_analysis: Dict, target_selection: Dict, enhanced_schema: Dict) -> str:
        """åŸºäºè¯­ä¹‰åˆ†æå’Œç›®æ ‡é€‰æ‹©ç”Ÿæˆæ™ºèƒ½SQL - ä¼˜åŒ–ç‰ˆæœ¬"""
        try:
            table_name = target_selection.get('table', 'default_table')
            fields = target_selection.get('fields', [])
            field_mapping = target_selection.get('field_mapping', {})
            
            # éªŒè¯è¡¨å
            if not table_name or not isinstance(table_name, str):
                self.logger.warning(f"æ— æ•ˆçš„è¡¨å: {table_name}")
                return self._generate_fallback_sql()
            
            # æ¸…ç†è¡¨åï¼Œç¡®ä¿å®‰å…¨
            table_name = self._sanitize_identifier(table_name)
            
            # å°è¯•ä½¿ç”¨AIç”ŸæˆSQL
            ai_generated_sql = await self._generate_sql_with_ai(semantic_analysis, target_selection, enhanced_schema)
            
            if ai_generated_sql and self._validate_sql_syntax(ai_generated_sql):
                self.logger.info(f"AIç”ŸæˆSQLæˆåŠŸ: {ai_generated_sql}")
                return ai_generated_sql
            
            # å¦‚æœAIç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ¿åŒ–SQLç”Ÿæˆ
            self.logger.info("AIç”ŸæˆSQLå¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ¿åŒ–SQLç”Ÿæˆ")
            sql = self._generate_sql_by_template(semantic_analysis.get('intent', 'general'), 
                                               semantic_analysis.get('data_operation', 'select'), 
                                               table_name, fields, field_mapping)
            
            # éªŒè¯ç”Ÿæˆçš„SQL
            if not self._validate_sql_syntax(sql):
                self.logger.warning(f"ç”Ÿæˆçš„SQLè¯­æ³•æ— æ•ˆï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ: {sql}")
                sql = self._generate_fallback_sql(table_name)
            
            self.logger.info(f"ç”Ÿæˆæ™ºèƒ½SQL: {sql}")
            return sql
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆæ™ºèƒ½SQLå¤±è´¥: {e}")
            return self._generate_fallback_sql()
    
    async def _generate_sql_with_ai(self, semantic_analysis: Dict, target_selection: Dict, enhanced_schema: Dict) -> str:
        """ä½¿ç”¨AIç”ŸæˆSQL"""
        try:
            if not self.ai_service:
                self.logger.warning("AIæœåŠ¡ä¸å¯ç”¨ï¼Œè·³è¿‡AI SQLç”Ÿæˆ")
                return None
            
            # æ„å»ºSQLç”Ÿæˆæç¤º
            sql_prompt = self._build_sql_generation_prompt(semantic_analysis, target_selection, enhanced_schema)
            
            # æ„å»ºä¸Šä¸‹æ–‡
            context = {
                "semantic_analysis": semantic_analysis,
                "target_selection": target_selection,
                "table_schema": enhanced_schema.get('table_schemas', {}).get(target_selection.get('table', ''), {})
            }
            
            # è°ƒç”¨AIæœåŠ¡ç”ŸæˆSQL
            response = await self.ai_service.analyze_with_context(
                context=str(context),
                prompt=sql_prompt,
                task_type="sql_generation",
                use_cache=True,
                use_rate_limiter=True
            )
            
            if response:
                # æ¸…ç†å“åº”ï¼Œæå–SQLè¯­å¥
                sql = self._extract_sql_from_response(response)
                if sql and self._validate_sql_syntax(sql):
                    return sql
            
            return None
            
        except Exception as e:
            self.logger.error(f"AI SQLç”Ÿæˆå¤±è´¥: {e}")
            return None
    
    def _extract_sql_from_response(self, response: str) -> str:
        """ä»AIå“åº”ä¸­æå–SQLè¯­å¥"""
        if not response or not isinstance(response, str):
            return None
        
        # æ¸…ç†å“åº”æ–‡æœ¬
        response = response.strip()
        
        # æŸ¥æ‰¾SQLè¯­å¥çš„å¼€å§‹å’Œç»“æŸ
        sql_keywords = ['SELECT', 'select']
        for keyword in sql_keywords:
            if keyword in response:
                start_idx = response.find(keyword)
                # æ‰¾åˆ°SQLè¯­å¥çš„ç»“æŸï¼ˆé€šå¸¸æ˜¯åˆ†å·æˆ–æ¢è¡Œï¼‰
                end_idx = response.find(';', start_idx)
                if end_idx == -1:
                    # å¦‚æœæ²¡æœ‰åˆ†å·ï¼Œæ‰¾åˆ°ç¬¬ä¸€ä¸ªæ¢è¡Œç¬¦
                    end_idx = response.find('\n', start_idx)
                    if end_idx == -1:
                        end_idx = len(response)
                
                sql = response[start_idx:end_idx].strip()
                
                # éªŒè¯SQLè¯­æ³•
                if self._validate_sql_syntax(sql):
                    return sql
        
        return None
    
    def _generate_sql_by_template(self, intent: str, data_operation: str, table_name: str, fields: List[str], field_mapping: Dict) -> str:
        """ä½¿ç”¨æ¨¡æ¿ç”ŸæˆSQLï¼Œé¿å…å­—ç¬¦ä¸²æ‹¼æ¥é”™è¯¯"""
        
        # SQLæ¨¡æ¿å®šä¹‰
        sql_templates = {
            'statistical_count': "SELECT COUNT(*) as total_count FROM {table}",
            'statistical_sum': "SELECT SUM({field}) as total_sum FROM {table}",
            'statistical_avg': "SELECT AVG({field}) as average_value FROM {table}",
            'dimensional_group': "SELECT {group_field}, COUNT(*) as count FROM {table} GROUP BY {group_field}",
            'temporal_date': "SELECT DATE({date_field}) as date, COUNT(*) as count FROM {table} GROUP BY DATE({date_field})",
            'general_select': "SELECT {fields} FROM {table} LIMIT 100",
            'fallback_count': "SELECT COUNT(*) as total_count FROM {table}"
        }
        
        try:
            if intent == 'statistical':
                if data_operation == 'count':
                    return sql_templates['statistical_count'].format(table=table_name)
                elif data_operation == 'sum':
                    amount_field = field_mapping.get('amount_field')
                    if amount_field and self._is_valid_field_name(amount_field):
                        return sql_templates['statistical_sum'].format(field=amount_field, table=table_name)
                    else:
                        return sql_templates['statistical_count'].format(table=table_name)
                elif data_operation == 'avg':
                    numeric_field = field_mapping.get('numeric_field')
                    if numeric_field and self._is_valid_field_name(numeric_field):
                        return sql_templates['statistical_avg'].format(field=numeric_field, table=table_name)
                    else:
                        return sql_templates['statistical_count'].format(table=table_name)
                else:
                    return sql_templates['statistical_count'].format(table=table_name)
                    
            elif intent == 'dimensional':
                group_field = field_mapping.get('group_field')
                if group_field and self._is_valid_field_name(group_field):
                    return sql_templates['dimensional_group'].format(group_field=group_field, table=table_name)
                else:
                    return sql_templates['general_select'].format(fields='*', table=table_name)
                    
            elif intent == 'temporal':
                date_field = field_mapping.get('date_field')
                if date_field and self._is_valid_field_name(date_field):
                    return sql_templates['temporal_date'].format(date_field=date_field, table=table_name)
                else:
                    return sql_templates['general_select'].format(fields='*', table=table_name)
                    
            else:
                # ä¸€èˆ¬æŸ¥è¯¢
                if fields and len(fields) > 0:
                    cleaned_fields = self._clean_field_names(fields[:5])
                    if cleaned_fields:
                        fields_str = ', '.join(cleaned_fields)
                        return sql_templates['general_select'].format(fields=fields_str, table=table_name)
                    else:
                        return sql_templates['fallback_count'].format(table=table_name)
                else:
                    return sql_templates['fallback_count'].format(table=table_name)
                    
        except Exception as e:
            self.logger.error(f"SQLæ¨¡æ¿ç”Ÿæˆå¤±è´¥: {e}")
            return sql_templates['fallback_count'].format(table=table_name)
    
    def _sanitize_identifier(self, identifier: str) -> str:
        """æ¸…ç†å’ŒéªŒè¯SQLæ ‡è¯†ç¬¦"""
        if not identifier or not isinstance(identifier, str):
            return 'default_table'
        
        # ç§»é™¤å±é™©å­—ç¬¦ï¼Œåªä¿ç•™å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿
        cleaned = ''.join(c for c in identifier if c.isalnum() or c == '_')
        
        # ç¡®ä¿ä¸ä»¥æ•°å­—å¼€å¤´
        if cleaned and cleaned[0].isdigit():
            cleaned = 't_' + cleaned
            
        # ç¡®ä¿ä¸ä¸ºç©º
        if not cleaned:
            cleaned = 'default_table'
            
        return cleaned
    
    def _is_valid_field_name(self, field_name: str) -> bool:
        """éªŒè¯å­—æ®µåæ˜¯å¦æœ‰æ•ˆ"""
        if not field_name or not isinstance(field_name, str):
            return False
        
        # æ£€æŸ¥æ˜¯å¦åªåŒ…å«æœ‰æ•ˆå­—ç¬¦
        if not all(c.isalnum() or c in ['_', '.'] for c in field_name):
            return False
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºSQLå…³é”®å­—
        sql_keywords = {'select', 'from', 'where', 'group', 'by', 'order', 'limit', 'count', 'sum', 'avg', 'max', 'min'}
        if field_name.lower() in sql_keywords:
            return False
            
        return True
    
    def _clean_field_names(self, fields: List[str]) -> List[str]:
        """æ¸…ç†å­—æ®µååˆ—è¡¨"""
        cleaned_fields = []
        for field in fields:
            if field and isinstance(field, str):
                cleaned_field = self._sanitize_identifier(field)
                if cleaned_field and self._is_valid_field_name(cleaned_field):
                    cleaned_fields.append(cleaned_field)
        return cleaned_fields
    
    def _validate_sql_syntax(self, sql: str) -> bool:
        """åŸºæœ¬çš„SQLè¯­æ³•éªŒè¯"""
        if not sql or not isinstance(sql, str):
            return False
        
        sql_upper = sql.upper()
        
        # æ£€æŸ¥åŸºæœ¬ç»“æ„
        if not sql_upper.startswith('SELECT'):
            return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«FROM
        if 'FROM' not in sql_upper:
            return False
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„è¯­æ³•é”™è¯¯
        error_patterns = [
            'SELECTSELECT', 'FROMFROM', 'WHEREWHERE', 'GROUPGROUP', 'ORDERORDER',
            'SELECT*FROM', 'SELECTFROM', 'SELECT,FROM'
        ]
        
        for pattern in error_patterns:
            if pattern in sql_upper:
                return False
        
        return True
    
    def _generate_fallback_sql(self, table_name: str = 'default_table') -> str:
        """ç”Ÿæˆå¤‡ç”¨SQL"""
        safe_table = self._sanitize_identifier(table_name)
        return f"SELECT COUNT(*) as total_count FROM {safe_table}"
    
    async def _self_validate_and_improve_sql(self, sql: str, data_source_id: str, target_selection: Dict) -> str:
        """SQLè´¨é‡éªŒè¯å’Œè‡ªæˆ‘æ”¹è¿› - å¢å¼ºç‰ˆæœ¬"""
        try:
            # ç¬¬0è½®ï¼šåŸºæœ¬è¯­æ³•æ£€æŸ¥
            if not self._validate_sql_syntax(sql):
                self.logger.warning(f"SQLè¯­æ³•éªŒè¯å¤±è´¥ï¼Œå°è¯•ä¿®å¤: {sql}")
                sql = self._fix_sql_syntax_errors(sql)
                if not self._validate_sql_syntax(sql):
                    self.logger.error(f"SQLè¯­æ³•ä¿®å¤å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨SQL")
                    return self._generate_fallback_sql(target_selection.get('table', 'default_table'))
            
            from app.models.data_source import DataSource
            from app.services.connectors.connector_factory import create_connector
            from app.db.session import get_db_session
            
            with get_db_session() as db:
                data_source = db.query(DataSource).filter(DataSource.id == data_source_id).first()
                if not data_source:
                    return sql
                
                connector = create_connector(data_source)
                await connector.connect()
                
                try:
                    # ç¬¬1è½®ï¼šè¡¨å­˜åœ¨æ€§éªŒè¯
                    tables = await connector.get_tables()
                    sql = self._validate_and_fix_table_names(sql, tables)
                    
                    # ç¬¬2è½®ï¼šå­—æ®µå­˜åœ¨æ€§éªŒè¯
                    table_name = target_selection.get('table')
                    if table_name and table_name in tables:
                        schema_info = await connector.get_table_schema(table_name)
                        sql = self._validate_and_fix_field_names(sql, schema_info, target_selection)
                    
                    # ç¬¬3è½®ï¼šæ€§èƒ½ä¼˜åŒ–
                    sql = self._optimize_sql_performance(sql, target_selection)
                    
                    # ç¬¬4è½®ï¼šæœ€ç»ˆè¯­æ³•éªŒè¯
                    if not self._validate_sql_syntax(sql):
                        self.logger.warning(f"æœ€ç»ˆSQLè¯­æ³•éªŒè¯å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨SQL: {sql}")
                        return self._generate_fallback_sql(table_name)
                    
                    self.logger.info(f"SQLéªŒè¯å’Œæ”¹è¿›å®Œæˆ: {sql}")
                    return sql
                    
                finally:
                    await connector.disconnect()
                    
        except Exception as e:
            self.logger.error(f"SQLéªŒè¯å’Œæ”¹è¿›å¤±è´¥: {e}")
            return self._generate_fallback_sql(target_selection.get('table', 'default_table'))
    
    def _fix_sql_syntax_errors(self, sql: str) -> str:
        """ä¿®å¤å¸¸è§çš„SQLè¯­æ³•é”™è¯¯"""
        if not sql or not isinstance(sql, str):
            return self._generate_fallback_sql()
        
        # ä¿®å¤å¸¸è§çš„è¯­æ³•é”™è¯¯æ¨¡å¼
        fixes = [
            # ä¿®å¤é‡å¤çš„SELECT
            (r'SELECT\s+SELECT', 'SELECT'),
            (r'SELECT\s+FROM', 'SELECT * FROM'),
            (r'SELECT\s*,', 'SELECT *,'),
            (r'SELECT\s*FROM', 'SELECT * FROM'),
            
            # ä¿®å¤å­—æ®µåé”™è¯¯
            (r'SELEid', 'SELECT id'),
            (r's_idT', 'SELECT'),
            (r's_idOUNT', 'COUNT'),
            
            # ä¿®å¤è¡¨åé”™è¯¯
            (r'FROM\s+FROM', 'FROM'),
            (r'FROM\s*,', 'FROM'),
            
            # ä¿®å¤WHEREå­å¥é”™è¯¯
            (r'WHERE\s+WHERE', 'WHERE'),
            (r'WHERE\s*,', 'WHERE'),
            
            # ä¿®å¤GROUP BYé”™è¯¯
            (r'GROUP\s+GROUP', 'GROUP'),
            (r'GROUP\s*,', 'GROUP'),
            
            # ä¿®å¤ORDER BYé”™è¯¯
            (r'ORDER\s+ORDER', 'ORDER'),
            (r'ORDER\s*,', 'ORDER'),
        ]
        
        fixed_sql = sql
        for pattern, replacement in fixes:
            fixed_sql = re.sub(pattern, replacement, fixed_sql, flags=re.IGNORECASE)
        
        # å¦‚æœä¿®å¤åä»ç„¶æœ‰é—®é¢˜ï¼Œä½¿ç”¨å¤‡ç”¨SQL
        if not self._validate_sql_syntax(fixed_sql):
            self.logger.warning(f"SQLè¯­æ³•ä¿®å¤åä»æ— æ•ˆï¼Œä½¿ç”¨å¤‡ç”¨SQL: {fixed_sql}")
            return self._generate_fallback_sql()
        
        return fixed_sql
    
    # æ–°å¢çš„è¾…åŠ©æ–¹æ³•
    def _identify_business_fields(self, columns: List[Dict]) -> List[str]:
        """è¯†åˆ«ä¸šåŠ¡ç›¸å…³å­—æ®µ"""
        business_fields = []
        for col in columns:
            name = col.get('name', '').lower()
            if any(kw in name for kw in ['name', 'title', 'desc', 'type', 'status', 'category', 'region', 'area']):
                business_fields.append(col.get('name', ''))
        return business_fields
    
    def _identify_key_fields(self, columns: List[Dict]) -> List[str]:
        """è¯†åˆ«å…³é”®å­—æ®µï¼ˆä¸»é”®ã€å¤–é”®ç­‰ï¼‰"""
        key_fields = []
        for col in columns:
            name = col.get('name', '').lower()
            if col.get('key') == 'PRI' or 'id' in name or col.get('key') == 'UNI':
                key_fields.append(col.get('name', ''))
        return key_fields
    
    def _identify_numeric_fields(self, columns: List[Dict]) -> List[str]:
        """è¯†åˆ«æ•°å€¼å­—æ®µ"""
        numeric_fields = []
        for col in columns:
            col_type = col.get('type', '').lower()
            name = col.get('name', '').lower()
            if any(t in col_type for t in ['int', 'float', 'decimal', 'double', 'numeric']) or \
               any(kw in name for kw in ['amount', 'price', 'cost', 'value', 'count', 'num']):
                numeric_fields.append(col.get('name', ''))
        return numeric_fields
    
    def _identify_date_fields(self, columns: List[Dict]) -> List[str]:
        """è¯†åˆ«æ—¥æœŸå­—æ®µ"""
        date_fields = []
        for col in columns:
            col_type = col.get('type', '').lower()
            name = col.get('name', '').lower()
            if any(t in col_type for t in ['date', 'time', 'timestamp']) or \
               any(kw in name for kw in ['date', 'time', 'created', 'updated', 'modified']):
                date_fields.append(col.get('name', ''))
        return date_fields
    
    async def _ai_select_relevant_tables(self, all_tables: List[str], placeholder_name: str) -> List[str]:
        """åŸºäºAIè¯­ä¹‰åˆ†ææ™ºèƒ½é€‰æ‹©ç›¸å…³è¡¨"""
        if not placeholder_name or not self.ai_service:
            # å›é€€åˆ°è§„åˆ™åŒ¹é…
            return self._select_relevant_tables(all_tables, placeholder_name)
        
        try:
            # æ„å»ºåˆ†æä¸Šä¸‹æ–‡
            context = f"""
æ•°æ®åº“ä¸­æœ‰ä»¥ä¸‹è¡¨ï¼š
{chr(10).join([f"- {table}" for table in all_tables])}

ç”¨æˆ·éœ€æ±‚å ä½ç¬¦: {placeholder_name}
"""
            
            # æ„å»ºåˆ†ææç¤º
            prompt = """
ä½œä¸ºæ•°æ®åˆ†æä¸“å®¶ï¼Œè¯·åˆ†æç”¨æˆ·éœ€æ±‚å ä½ç¬¦ä¸æ•°æ®è¡¨çš„ç›¸å…³æ€§ã€‚

åˆ†æè¦æ±‚ï¼š
1. ç†è§£å ä½ç¬¦çš„ä¸šåŠ¡å«ä¹‰å’Œæ•°æ®éœ€æ±‚
2. åˆ†ææ¯ä¸ªè¡¨åå¯èƒ½å¯¹åº”çš„ä¸šåŠ¡åŠŸèƒ½
3. é€‰æ‹©æœ€ç›¸å…³çš„1-3ä¸ªè¡¨æ¥æ»¡è¶³è¯¥éœ€æ±‚
4. ä¼˜å…ˆé€‰æ‹©æ ¸å¿ƒä¸šåŠ¡è¡¨ï¼Œé¿å…è¾…åŠ©è¡¨

è¯·è¿”å›JSONæ ¼å¼çš„åˆ†æç»“æœï¼š
{
    "selected_tables": ["è¡¨å1", "è¡¨å2", "è¡¨å3"],
    "reasoning": {
        "è¡¨å1": "é€‰æ‹©ç†ç”±",
        "è¡¨å2": "é€‰æ‹©ç†ç”±"
    },
    "confidence": 0.9
}
"""
            
            # è°ƒç”¨AIæœåŠ¡
            response = await self.ai_service.analyze_with_context(
                context=context,
                prompt=prompt,
                task_type="intelligent_table_selection",
                use_cache=True,
                use_rate_limiter=True
            )
            
            # è§£æAIå“åº”
            if response:
                import json
                try:
                    result = json.loads(response)
                    selected_tables = result.get("selected_tables", [])
                    reasoning = result.get("reasoning", {})
                    confidence = result.get("confidence", 0.0)
                    
                    # éªŒè¯é€‰æ‹©çš„è¡¨æ˜¯å¦å­˜åœ¨
                    valid_tables = [table for table in selected_tables if table in all_tables]
                    
                    self.logger.info(f"AIè¡¨é€‰æ‹©ç»“æœ: {valid_tables}, ç½®ä¿¡åº¦: {confidence}")
                    for table, reason in reasoning.items():
                        if table in valid_tables:
                            self.logger.info(f"  {table}: {reason}")
                    
                    return valid_tables[:3] if valid_tables else all_tables[:3]
                    
                except json.JSONDecodeError as e:
                    self.logger.warning(f"AIå“åº”JSONè§£æå¤±è´¥: {e}")
                    
        except Exception as e:
            self.logger.error(f"AIè¡¨é€‰æ‹©å¤±è´¥: {e}")
            
        # å›é€€åˆ°è§„åˆ™åŒ¹é…
        return self._select_relevant_tables(all_tables, placeholder_name)
    
    def _select_relevant_tables(self, all_tables: List[str], placeholder_name: str) -> List[str]:
        """åŸºäºå ä½ç¬¦åç§°æ™ºèƒ½é€‰æ‹©ç›¸å…³è¡¨"""
        if not placeholder_name:
            # å¦‚æœæ²¡æœ‰å ä½ç¬¦åç§°ï¼Œè¿”å›å‰5ä¸ªè¡¨ä½œä¸ºé»˜è®¤
            return all_tables[:5]
        
        # æå–å ä½ç¬¦ä¸­çš„å…³é”®è¯
        keywords = self._extract_business_keywords(placeholder_name)
        relevant_tables = []
        scores = {}
        
        # ä¸ºæ¯ä¸ªè¡¨è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
        for table in all_tables:
            score = 0
            table_lower = table.lower()
            
            # åŸºäºå…³é”®è¯åŒ¹é…è®¡ç®—åˆ†æ•°
            for keyword in keywords:
                if keyword in table_lower:
                    score += 10  # ç²¾ç¡®åŒ¹é…å¾—é«˜åˆ†
                elif any(keyword in part for part in table_lower.split('_')):
                    score += 5   # éƒ¨åˆ†åŒ¹é…å¾—ä¸­ç­‰åˆ†
            
            # åŸºäºè¡¨åå¸¸è§æ¨¡å¼çš„åˆ†æ•°è°ƒæ•´
            if any(pattern in table_lower for pattern in ['complain', 'complaint', 'feedback']):
                if any(kw in ['æŠ•è¯‰', 'åé¦ˆ', 'æ„è§'] for kw in keywords):
                    score += 8
            
            if any(pattern in table_lower for pattern in ['user', 'customer', 'tourist', 'client']):
                if any(kw in ['ç”¨æˆ·', 'å®¢æˆ·', 'æ¸¸å®¢', 'èº«ä»½è¯'] for kw in keywords):
                    score += 6
            
            if any(pattern in table_lower for pattern in ['order', 'travel', 'itinerary']):
                if any(kw in ['è®¢å•', 'è¡Œç¨‹', 'æ—…æ¸¸'] for kw in keywords):
                    score += 6
            
            scores[table] = score
        
        # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„è¡¨ï¼Œæœ€å¤šé€‰æ‹©3ä¸ª
        sorted_tables = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        relevant_tables = [table for table, score in sorted_tables if score > 0][:3]
        
        # å¦‚æœæ²¡æœ‰ç›¸å…³è¡¨ï¼Œè¿”å›å‰3ä¸ªè¡¨
        if not relevant_tables:
            relevant_tables = all_tables[:3]
        
        return relevant_tables
    
    def _extract_business_keywords(self, placeholder_name: str) -> List[str]:
        """ä»å ä½ç¬¦åç§°ä¸­æå–ä¸šåŠ¡å…³é”®è¯"""
        # ç§»é™¤å¸¸è§çš„ç»Ÿè®¡è¯æ±‡ï¼Œä¿ç•™ä¸šåŠ¡å…³é”®è¯
        business_terms = []
        name_lower = placeholder_name.lower()
        
        # ä¸šåŠ¡å…³é”®è¯æ˜ å°„
        keyword_mapping = {
            'æŠ•è¯‰': ['complaint', 'complain'],
            'åé¦ˆ': ['feedback'],
            'ç”¨æˆ·': ['user', 'customer'],
            'å®¢æˆ·': ['customer', 'client'], 
            'æ¸¸å®¢': ['tourist', 'visitor'],
            'èº«ä»½è¯': ['id_card', 'identity'],
            'å¾®ä¿¡': ['wechat', 'weixin'],
            'å°ç¨‹åº': ['miniprogram', 'applet'],
            'è®¢å•': ['order'],
            'è¡Œç¨‹': ['itinerary', 'travel'],
            'ä½å®¿': ['accommodation', 'hotel'],
            'æ™¯åŒº': ['scenic', 'attraction'],
            'å¯¼æ¸¸': ['guide'],
            'é€€è´¹': ['refund']
        }
        
        # æå–ä¸­æ–‡å…³é”®è¯
        for chinese, english_variants in keyword_mapping.items():
            if chinese in placeholder_name:
                business_terms.append(chinese)
                business_terms.extend(english_variants)
        
        return business_terms
    
    def _identify_text_fields(self, columns: List[Dict]) -> List[str]:
        """è¯†åˆ«æ–‡æœ¬å­—æ®µ"""
        text_fields = []
        for col in columns:
            col_type = col.get('type', '').lower()
            if any(t in col_type for t in ['varchar', 'text', 'char', 'string']):
                text_fields.append(col.get('name', ''))
        return text_fields
    
    async def _perform_ai_agent_analysis(
        self, 
        placeholder_name: str, 
        placeholder_type: str, 
        enhanced_schema: Dict, 
        data_source: Dict
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨AI Agentè¿›è¡Œæ·±åº¦å ä½ç¬¦åˆ†æ
        
        Args:
            placeholder_name: å ä½ç¬¦åç§°
            placeholder_type: å ä½ç¬¦ç±»å‹
            enhanced_schema: å¢å¼ºçš„æ•°æ®åº“ç»“æ„ä¿¡æ¯
            data_source: æ•°æ®æºä¿¡æ¯
            
        Returns:
            AIåˆ†æç»“æœ
        """
        try:
            # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
            context = {
                "placeholder_name": placeholder_name,
                "placeholder_type": placeholder_type,
                "available_tables": enhanced_schema.get('tables', []),  # ä¿®å¤ï¼štablesæ˜¯åˆ—è¡¨ï¼Œä¸æ˜¯å­—å…¸
                "table_relationships": enhanced_schema.get('relationships', []),
                "business_domain": enhanced_schema.get('business_domain', ''),
                "data_source_type": data_source.get('source_type', '')
            }
            
            # æ„å»ºAIåˆ†ææç¤º
            analysis_prompt = self._build_ai_analysis_prompt(context, enhanced_schema)
            
            # ä½¿ç”¨AIæœåŠ¡è¿›è¡Œåˆ†æ
            if not self.ai_service:
                raise Exception("AIæœåŠ¡æœªåˆå§‹åŒ–")
            
            # å¦‚æœAIæœåŠ¡æ”¯æŒç”¨æˆ·ç‰¹å®šé…ç½®ï¼Œè®°å½•ç”¨æˆ·ä¿¡æ¯
            if hasattr(self.ai_service, 'user_id') and self.ai_service.user_id:
                self.logger.info(f"ä½¿ç”¨ç”¨æˆ·ç‰¹å®šAIæœåŠ¡è¿›è¡Œåˆ†æ: {self.ai_service.user_id}")
            else:
                self.logger.info("ä½¿ç”¨ç³»ç»Ÿé»˜è®¤AIæœåŠ¡è¿›è¡Œåˆ†æ")
            
            response = await self.ai_service.analyze_with_context(
                context=str(context),
                prompt=analysis_prompt,
                task_type=f"placeholder_agent_analysis_{placeholder_type}",
                use_cache=True,
                use_rate_limiter=True
            )
            
            # è°ƒè¯•ï¼šæŸ¥çœ‹AIå“åº”å†…å®¹
            self.logger.info(f"AIå“åº”å†…å®¹: {response[:200]}...")  # åªæ˜¾ç¤ºå‰200ä¸ªå­—ç¬¦
            
            # è§£æAIå“åº”
            ai_result = None
            if response:
                try:
                    import json
                    # å°è¯•ç›´æ¥è§£æJSON
                    ai_result = {
                        "success": True,
                        "data": json.loads(response)
                    }
                    self.logger.info("AIå“åº”JSONè§£ææˆåŠŸ")
                except json.JSONDecodeError as e:
                    self.logger.warning(f"AIå“åº”JSONè§£æå¤±è´¥: {e}")
                    self.logger.warning(f"AIå“åº”åŸå§‹å†…å®¹: {response}")
                    
                    # å°è¯•æå–JSONéƒ¨åˆ†
                    json_start = response.find('{')
                    json_end = response.rfind('}') + 1
                    
                    if json_start != -1 and json_end > json_start:
                        try:
                            json_str = response[json_start:json_end]
                            ai_result = {
                                "success": True,
                                "data": json.loads(json_str)
                            }
                            self.logger.info("ä»å“åº”ä¸­æå–JSONæˆåŠŸ")
                        except json.JSONDecodeError:
                            self.logger.error("æå–çš„JSONä»ç„¶æ— æ•ˆ")
                            # å¦‚æœä¸æ˜¯JSONï¼Œå°è¯•è§£æä¸ºæ–‡æœ¬
                            ai_result = {
                                "success": True,
                                "data": {
                                    "intent": "statistical",
                                    "data_operation": "count",
                                    "reasoning": [response]
                                }
                            }
                    else:
                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°JSONï¼Œè¿”å›æ–‡æœ¬å“åº”
                        ai_result = {
                            "success": True,
                            "data": {
                                "intent": "statistical",
                                "data_operation": "count",
                                "reasoning": [response]
                            }
                        }
            
            if ai_result and ai_result.get("success"):
                analysis_data = ai_result.get("data", {})
                
                return {
                    "intent": analysis_data.get("intent", "statistical"),
                    "data_operation": analysis_data.get("data_operation", "count"),
                    "business_domain": analysis_data.get("business_domain", ""),
                    "target_metrics": analysis_data.get("target_metrics", []),
                    "time_dimension": analysis_data.get("time_dimension"),
                    "grouping_dimensions": analysis_data.get("grouping_dimensions", []),
                    "filters": analysis_data.get("filters", []),
                    "aggregations": analysis_data.get("aggregations", ["count"]),
                    "reasoning": analysis_data.get("reasoning", []),
                    "confidence": analysis_data.get("confidence", 0.8),
                    "optimizations": analysis_data.get("optimizations", [])
                }
            else:
                # AIåˆ†æå¤±è´¥æ—¶å›é€€åˆ°è¯­ä¹‰åˆ†æ
                self.logger.warning(f"AIåˆ†æå¤±è´¥ï¼Œå›é€€åˆ°è¯­ä¹‰åˆ†æ: {ai_result.get('error', 'Unknown error')}")
                return await self._analyze_placeholder_semantics(placeholder_name, placeholder_type, enhanced_schema)
                
        except Exception as e:
            self.logger.error(f"AI Agentåˆ†æå¤±è´¥: {e}")
            # å›é€€åˆ°è¯­ä¹‰åˆ†æ
            return await self._analyze_placeholder_semantics(placeholder_name, placeholder_type, enhanced_schema)
    
    def _build_ai_analysis_prompt(self, context: Dict, enhanced_schema: Dict) -> str:
        """æ„å»ºAIåˆ†ææç¤º - ä¼˜åŒ–ç‰ˆæœ¬"""
        
        placeholder_name = context["placeholder_name"]
        placeholder_type = context["placeholder_type"]
        available_tables = context["available_tables"]
        
        # è·å–è¡¨ç»“æ„è¯¦æƒ…
        table_details = []
        for table_name, table_info in enhanced_schema.get('table_schemas', {}).items():
            columns = table_info.get('columns', [])
            column_names = [col.get('name', col.get('Field', '')) for col in columns]
            table_details.append(f"- {table_name}: {', '.join(column_names[:10])}")  # é™åˆ¶æ˜¾ç¤ºçš„å­—æ®µæ•°é‡
        
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆã€‚è¯·åˆ†æä»¥ä¸‹å ä½ç¬¦çš„ä¸šåŠ¡éœ€æ±‚ï¼Œå¹¶è¿”å›JSONæ ¼å¼çš„åˆ†æç»“æœã€‚

å ä½ç¬¦ä¿¡æ¯ï¼š
- åç§°: {placeholder_name}
- ç±»å‹: {placeholder_type}
- ä¸šåŠ¡é¢†åŸŸ: {context.get('business_domain', 'æœªçŸ¥')}

å¯ç”¨æ•°æ®è¡¨ç»“æ„ï¼š
{chr(10).join(table_details[:5])}  # é™åˆ¶æ˜¾ç¤ºçš„è¡¨æ•°é‡

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡æœ¬ï¼š

{{
    "intent": "statistical",
    "data_operation": "count",
    "business_domain": "travel_service",
    "target_metrics": ["å¯¼æ¸¸æ•°é‡"],
    "time_dimension": null,
    "grouping_dimensions": [],
    "filters": ["city_id = 'æ˜†æ˜'"],
    "aggregations": ["count"],
    "reasoning": ["æ ¹æ®å ä½ç¬¦åç§°ï¼Œç›®æ ‡æ˜¯ç»Ÿè®¡æ˜†æ˜æ³¨å†Œçš„å¯¼æ¸¸æ•°é‡"],
    "confidence": 0.9,
    "optimizations": ["è€ƒè™‘å»ºç«‹ç´¢å¼•åœ¨city_idå­—æ®µä¸Š"]
}}

é‡è¦è¦æ±‚ï¼š
1. åªè¿”å›JSONå¯¹è±¡ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šã€æ³¨é‡Šæˆ–å…¶ä»–æ–‡æœ¬
2. ç¡®ä¿JSONè¯­æ³•å®Œå…¨æ­£ç¡®
3. å­—æ®µåå¿…é¡»æ˜¯æ•°æ®åº“ä¸­å®é™…å­˜åœ¨çš„å­—æ®µå
4. èšåˆå‡½æ•°å¿…é¡»æ˜¯æ ‡å‡†çš„SQLèšåˆå‡½æ•°

è¯·ç›´æ¥è¿”å›JSONå¯¹è±¡ï¼Œä¸è¦æœ‰ä»»ä½•å‰ç¼€æˆ–åç¼€ã€‚
"""
        
        return prompt
    
    def _build_sql_generation_prompt(self, semantic_analysis: Dict, target_selection: Dict, enhanced_schema: Dict) -> str:
        """æ„å»ºä¸“é—¨çš„SQLç”Ÿæˆæç¤º"""
        
        table_name = target_selection.get('table', '')
        fields = target_selection.get('fields', [])
        intent = semantic_analysis.get('intent', '')
        data_operation = semantic_analysis.get('data_operation', '')
        
        # è·å–è¡¨ç»“æ„ä¿¡æ¯
        table_schema = enhanced_schema.get('table_schemas', {}).get(table_name, {})
        columns = table_schema.get('columns', [])
        column_names = [col.get('name', '') for col in columns if col.get('name')]
        
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„SQLä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆå‡†ç¡®ã€æœ‰æ•ˆçš„SQLæŸ¥è¯¢è¯­å¥ã€‚

åˆ†æä¿¡æ¯ï¼š
- ä¸šåŠ¡æ„å›¾: {intent}
- æ•°æ®æ“ä½œ: {data_operation}
- ç›®æ ‡è¡¨: {table_name}
- å¯ç”¨å­—æ®µ: {', '.join(column_names[:20])}  # é™åˆ¶æ˜¾ç¤ºå­—æ®µæ•°é‡

SQLç”Ÿæˆè¦æ±‚ï¼š
1. ç”Ÿæˆæ ‡å‡†çš„SQL SELECTè¯­å¥
2. ä½¿ç”¨æ­£ç¡®çš„è¡¨åå’Œå­—æ®µå
3. æ ¹æ®ä¸šåŠ¡æ„å›¾é€‰æ‹©åˆé€‚çš„èšåˆå‡½æ•°
4. æ·»åŠ é€‚å½“çš„WHEREæ¡ä»¶ï¼ˆå¦‚æœéœ€è¦ï¼‰
5. ä½¿ç”¨LIMITé™åˆ¶ç»“æœæ•°é‡ï¼ˆå»ºè®®100è¡Œä»¥å†…ï¼‰

SQLæ¨¡æ¿ç¤ºä¾‹ï¼š
- ç»Ÿè®¡æŸ¥è¯¢: SELECT COUNT(*) as total_count FROM {table_name}
- æ±‚å’ŒæŸ¥è¯¢: SELECT SUM(amount_field) as total_amount FROM {table_name}
- åˆ†ç»„æŸ¥è¯¢: SELECT group_field, COUNT(*) as count FROM {table_name} GROUP BY group_field
- æ—¶é—´æŸ¥è¯¢: SELECT DATE(date_field) as date, COUNT(*) as count FROM {table_name} GROUP BY DATE(date_field)

è¯·åªè¿”å›SQLè¯­å¥ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šã€æ³¨é‡Šæˆ–å…¶ä»–å†…å®¹ã€‚ç¡®ä¿SQLè¯­æ³•å®Œå…¨æ­£ç¡®ã€‚
"""
        
        return prompt
    
    def _generate_semantic_optimizations(self, analysis: Dict) -> List[str]:
        """ç”Ÿæˆè¯­ä¹‰ä¼˜åŒ–å»ºè®®"""
        optimizations = []
        intent = analysis.get('intent', '')
        
        if intent == 'statistical':
            optimizations.append('è€ƒè™‘æ·»åŠ é€‚å½“çš„WHEREæ¡ä»¶è¿‡æ»¤æ— æ•ˆæ•°æ®')
            optimizations.append('å¯¹äºå¤§è¡¨å»ºè®®ä½¿ç”¨ç´¢å¼•ä¼˜åŒ–COUNTæŸ¥è¯¢')
        elif intent == 'dimensional':
            optimizations.append('è€ƒè™‘ä¸ºåˆ†ç»„å­—æ®µæ·»åŠ ç´¢å¼•')
            optimizations.append('é™åˆ¶åˆ†ç»„ç»“æœæ•°é‡é¿å…è¿‡å¤šç»´åº¦')
        elif intent == 'temporal':
            optimizations.append('ä¸ºæ—¥æœŸå­—æ®µæ·»åŠ ç´¢å¼•ä»¥ä¼˜åŒ–æ—¶é—´èŒƒå›´æŸ¥è¯¢')
            optimizations.append('è€ƒè™‘ä½¿ç”¨æ—¥æœŸåˆ†åŒºæå‡æŸ¥è¯¢æ€§èƒ½')
        
        return optimizations
    
    def _select_optimal_fields(self, semantic_analysis: Dict, schema: Dict) -> List[str]:
        """é€‰æ‹©æœ€ä¼˜å­—æ®µç»„åˆ"""
        enhanced_metadata = schema.get('enhanced_metadata', {})
        all_columns = [col.get('name') for col in schema.get('columns', []) if col.get('name')]
        
        intent = semantic_analysis.get('intent', 'general')
        
        # æ¸…ç†å­—æ®µåç§°çš„è¾…åŠ©å‡½æ•°
        def clean_field_names(fields):
            cleaned = []
            for field in fields:
                if field and isinstance(field, str):
                    # ç¡®ä¿å­—æ®µåç§°åªåŒ…å«æœ‰æ•ˆå­—ç¬¦
                    clean_field = ''.join(c for c in field if c.isalnum() or c in ['_'])
                    if clean_field and not clean_field.isdigit():  # é¿å…çº¯æ•°å­—å­—æ®µå
                        cleaned.append(clean_field)
            return cleaned
        
        if intent == 'statistical':
            # ç»Ÿè®¡æŸ¥è¯¢ä½¿ç”¨è®¡æ•°ï¼Œä½†åœ¨SQLç”Ÿæˆé˜¶æ®µå¤„ç†
            return []  # è¿”å›ç©ºï¼Œè®©SQLç”Ÿæˆé˜¶æ®µå†³å®šä½¿ç”¨COUNT(*)
        elif intent == 'dimensional':
            # ç»´åº¦æŸ¥è¯¢éœ€è¦åˆ†ç»„å­—æ®µ
            business_fields = enhanced_metadata.get('business_fields', [])
            fields = business_fields[:3] if business_fields else all_columns[:3]
            return clean_field_names(fields)
        elif intent == 'temporal':
            # æ—¶é—´æŸ¥è¯¢éœ€è¦æ—¥æœŸå­—æ®µ
            date_fields = enhanced_metadata.get('date_fields', [])
            fields = date_fields[:2] if date_fields else all_columns[:2]
            return clean_field_names(fields)
        else:
            # ä¸€èˆ¬æŸ¥è¯¢é€‰æ‹©å…³é”®å­—æ®µ
            key_fields = enhanced_metadata.get('key_fields', [])
            business_fields = enhanced_metadata.get('business_fields', [])
            selected = list(set(key_fields[:2] + business_fields[:3]))
            fields = selected if selected else all_columns[:5]
            return clean_field_names(fields)
    
    def _create_field_mapping(self, semantic_analysis: Dict, schema: Dict) -> Dict[str, str]:
        """åˆ›å»ºå­—æ®µæ˜ å°„"""
        enhanced_metadata = schema.get('enhanced_metadata', {})
        mapping = {}
        
        intent = semantic_analysis.get('intent', '')
        
        if intent == 'financial':
            numeric_fields = enhanced_metadata.get('numeric_fields', [])
            if numeric_fields:
                mapping['amount_field'] = numeric_fields[0]
        elif intent == 'dimensional':
            business_fields = enhanced_metadata.get('business_fields', [])
            if business_fields:
                mapping['group_field'] = business_fields[0]
        elif intent == 'temporal':
            date_fields = enhanced_metadata.get('date_fields', [])
            if date_fields:
                mapping['date_field'] = date_fields[0]
        
        return mapping
    
    def _estimate_query_time(self, schema: Dict, fields: List[str]) -> int:
        """ä¼°ç®—æŸ¥è¯¢æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰"""
        base_time = 100
        
        # åŸºäºå­—æ®µæ•°é‡
        field_count = len(fields)
        if field_count > 10:
            base_time += field_count * 10
        
        # åŸºäºè¡¨å¤§å°ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        estimated_rows = schema.get('estimated_rows', 1000)
        if estimated_rows > 100000:
            base_time += 500
        elif estimated_rows > 10000:
            base_time += 200
        
        return base_time
    
    def _calculate_enhanced_confidence_score(self, semantic_analysis: Dict, target_selection: Dict, sql: str) -> float:
        """è®¡ç®—å¢å¼ºçš„ç½®ä¿¡åº¦åˆ†æ•°"""
        base_score = 0.3
        
        # è¯­ä¹‰åˆ†æè´¨é‡
        if semantic_analysis.get('intent') != 'general':
            base_score += 0.2
        
        if semantic_analysis.get('business_domain'):
            base_score += 0.2
        
        # ç›®æ ‡é€‰æ‹©è´¨é‡
        table_score = target_selection.get('table_score', 0.0)
        if table_score > 2.0:
            base_score += 0.2
        elif table_score > 1.0:
            base_score += 0.1
        
        # SQLå¤æ‚åº¦
        if 'COUNT' in sql.upper() or 'SUM' in sql.upper():
            base_score += 0.1
        
        return min(1.0, base_score)
    
    async def _create_fallback_solution(self, placeholder_name: str, data_source_id: str) -> Dict[str, Any]:
        """åˆ›å»ºé™çº§è§£å†³æ–¹æ¡ˆ"""
        try:
            # ç®€å•çš„é™çº§SQL
            if 'ç»Ÿè®¡' in placeholder_name or 'count' in placeholder_name.lower():
                sql = "SELECT COUNT(*) as count FROM information_schema.tables"
                reasoning = "ä½¿ç”¨ç³»ç»Ÿè¡¨è¿›è¡Œè®¡æ•°æŸ¥è¯¢"
            elif 'æ—¶é—´' in placeholder_name or 'date' in placeholder_name.lower():
                sql = "SELECT NOW() as current_time"
                reasoning = "è¿”å›å½“å‰æ—¶é—´ä½œä¸ºæ—¶é—´å ä½ç¬¦"
            else:
                sql = "SELECT 'placeholder_value' as value"
                reasoning = "è¿”å›é™æ€å ä½ç¬¦å€¼"
            
            return {
                'sql': sql,
                'reasoning': reasoning
            }
        except Exception:
            return {
                'sql': "SELECT 1 as placeholder_value",
                'reasoning': "æœ€åŸºç¡€çš„é™çº§æ–¹æ¡ˆ"
            }
    
    def _basic_sql_validation(self, sql: str) -> bool:
        """åŸºç¡€SQLè¯­æ³•éªŒè¯"""
        sql_upper = sql.upper().strip()
        
        # å¿…é¡»ä»¥SELECTå¼€å§‹
        if not sql_upper.startswith('SELECT'):
            return False
        
        # å¿…é¡»åŒ…å«FROMï¼ˆé™¤éæ˜¯ç®€å•çš„å¸¸é‡æŸ¥è¯¢ï¼‰
        if 'FROM' not in sql_upper and 'NOW()' not in sql_upper and not any(op in sql_upper for op in ['1', '2', '3', '4', '5', '6', '7', '8', '9']):
            return False
        
        # ä¸èƒ½åŒ…å«å±é™©å…³é”®è¯
        dangerous = ['DROP', 'DELETE', 'INSERT', 'UPDATE', 'CREATE', 'ALTER', 'TRUNCATE']
        if any(keyword in sql_upper for keyword in dangerous):
            return False
        
        return True
    
    def _fix_basic_syntax(self, sql: str) -> str:
        """ä¿®å¤åŸºç¡€è¯­æ³•é”™è¯¯"""
        # ä¿®å¤å¸¸è§æ‹¼å†™é”™è¯¯
        corrections = {
            'SELET': 'SELECT',
            'SELCT': 'SELECT',
            'FORM': 'FROM',
            'WHRE': 'WHERE',
            'GRUP': 'GROUP',
            'OEDER': 'ORDER'
        }
        
        for wrong, correct in corrections.items():
            sql = sql.replace(wrong, correct)
        
        # ç¡®ä¿ä»¥SELECTå¼€å§‹
        if not sql.upper().strip().startswith('SELECT'):
            sql = f"SELECT COUNT(*) FROM ({sql}) as subquery"
        
        return sql
    
    def _validate_and_fix_table_names(self, sql: str, available_tables: List[str]) -> str:
        """éªŒè¯å’Œä¿®å¤è¡¨å"""
        import re
        
        # æå–è¡¨å
        table_pattern = r'FROM\s+([`"]?)(\w+)\1'
        matches = re.findall(table_pattern, sql, re.IGNORECASE)
        
        for quote, table_name in matches:
            if table_name not in available_tables:
                # æŸ¥æ‰¾æœ€ç›¸ä¼¼çš„è¡¨
                best_match = self._find_similar_table(table_name, available_tables)
                if best_match:
                    sql = sql.replace(f'FROM {quote}{table_name}{quote}', f'FROM {quote}{best_match}{quote}')
                    self.logger.info(f"è¡¨åä¿®å¤: {table_name} -> {best_match}")
        
        return sql
    
    def _validate_and_fix_field_names(self, sql: str, schema_info: Dict, target_selection: Dict) -> str:
        """éªŒè¯å’Œä¿®å¤å­—æ®µå"""
        available_fields = [col.get('name') for col in schema_info.get('columns', [])]
        
        # å¦‚æœæŸ¥è¯¢å­—æ®µä¸å­˜åœ¨ï¼Œæ›¿æ¢ä¸ºå®‰å…¨çš„å­—æ®µ
        if not available_fields:
            return sql
        
        # ç®€å•çš„å­—æ®µæ›¿æ¢ç­–ç•¥
        if 'SELECT *' not in sql.upper():
            # å¦‚æœä¸æ˜¯SELECT *ï¼Œç¡®ä¿ä½¿ç”¨å­˜åœ¨çš„å­—æ®µ
            safe_fields = available_fields[:3]  # ä½¿ç”¨å‰3ä¸ªå­—æ®µ
            field_pattern = r'SELECT\s+([^FROM]+)'
            match = re.search(field_pattern, sql, re.IGNORECASE)
            if match:
                sql = sql.replace(match.group(1), ', '.join(safe_fields))
        
        return sql
    
    def _optimize_sql_performance(self, sql: str, target_selection: Dict) -> str:
        """SQLæ€§èƒ½ä¼˜åŒ–"""
        # æ·»åŠ LIMITä»¥é˜²æ­¢å¤§ç»“æœé›†
        if 'LIMIT' not in sql.upper():
            if 'COUNT(' in sql.upper() or 'SUM(' in sql.upper() or 'AVG(' in sql.upper():
                # èšåˆæŸ¥è¯¢ä¸éœ€è¦LIMIT
                pass
            else:
                sql += ' LIMIT 100'
        
        return sql
    
    # ç§æœ‰æ–¹æ³•
    async def _ensure_metadata_discovered(self, data_source_id: str):
        """ç¡®ä¿å…ƒæ•°æ®å·²å‘ç°"""
        cache_key = f"metadata_discovered_{data_source_id}"
        
        if cache_key not in self._metadata_cache:
            self.logger.info(f"Discovering metadata for data source: {data_source_id}")
            await self.metadata_service.discover_data_source_metadata(data_source_id)
            self._metadata_cache[cache_key] = (True, datetime.now())
    
    async def _build_response(
        self,
        request: AgentQueryRequest,
        query_plan: QueryPlan,
        execution_result: ExecutionResult,
        start_time: datetime
    ) -> AgentQueryResponse:
        """æ„å»ºæŸ¥è¯¢å“åº”"""
        
        # ç”ŸæˆæŸ¥è¯¢è§£é‡Š
        explanation_parts = []
        explanation_parts.append(f"æŸ¥è¯¢æ„å›¾: {query_plan.estimated_complexity}")
        explanation_parts.append(f"æ¶‰åŠè¡¨æ•°: {len(query_plan.primary_tables + query_plan.join_tables)}")
        
        if query_plan.cross_database:
            explanation_parts.append("æ‰§è¡Œäº†è·¨æ•°æ®åº“æŸ¥è¯¢")
        
        if execution_result.success:
            explanation_parts.append(f"æˆåŠŸè¿”å› {execution_result.row_count} è¡Œæ•°æ®")
        
        explanation = "; ".join(explanation_parts)
        
        # æå–SQLæŸ¥è¯¢
        sql_queries = [execution_result.query_sql] if execution_result.query_sql else []
        
        # æ„å»ºå…ƒæ•°æ®ä¿¡æ¯
        metadata = {
            'query_complexity': query_plan.estimated_complexity,
            'cross_database': query_plan.cross_database,
            'tables_involved': len(query_plan.primary_tables + query_plan.join_tables),
            'joins_count': len(query_plan.join_conditions)
        }
        
        # æ€§èƒ½ç»Ÿè®¡
        total_time = (datetime.now() - start_time).total_seconds()
        performance_stats = {
            'total_time': total_time,
            'execution_time': execution_result.execution_time,
            'routing_time': total_time - execution_result.execution_time
        }
        
        if execution_result.performance_stats:
            performance_stats.update(execution_result.performance_stats)
        
        return AgentQueryResponse(
            success=execution_result.success,
            data=execution_result.data.to_dict('records') if execution_result.data is not None else None,
            explanation=explanation,
            sql_queries=sql_queries,
            metadata=metadata,
            performance_stats=performance_stats,
            errors=execution_result.errors
        )