"""
Multi-Database Agent

å¤šæ•°æ®åº“æ™ºèƒ½ä»£ç†ï¼Œè´Ÿè´£åˆ†æå ä½ç¬¦éœ€æ±‚ã€é€‰æ‹©ç›®æ ‡è¡¨å’Œç”ŸæˆSQLæŸ¥è¯¢
"""

import asyncio
import json
import logging
import re
from typing import Dict, List, Any, Optional
from datetime import datetime

from sqlalchemy.orm import Session

from app.services.connectors.base_connector import BaseConnector
from app.services.connectors.doris_connector import DorisConnector, DorisConfig
from app.core.ai_service_factory import UserAIServiceFactory

logger = logging.getLogger(__name__)


class MultiDatabaseAgent:
    """å¤šæ•°æ®åº“æ™ºèƒ½ä»£ç†"""
    
    def __init__(self, db_session=None, user_id=None):
        """åˆå§‹åŒ–å¤šæ•°æ®åº“ä»£ç†"""
        self.db_session = db_session
        self.user_id = user_id
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–AIæœåŠ¡
        try:
            from app.services.agents.core.ai_service import UnifiedAIService
            if user_id and db_session:
                from app.core.ai_service_factory import UserAIServiceFactory
                factory = UserAIServiceFactory()
                self.ai_service = factory.get_user_ai_service(user_id)
                self.logger.info(f"ä½¿ç”¨ç”¨æˆ·ç‰¹å®šAIæœåŠ¡: {user_id}")
            else:
                self.ai_service = UnifiedAIService(db_session=db_session)
                self.logger.info("ä½¿ç”¨ç³»ç»Ÿé»˜è®¤AIæœåŠ¡")
        except Exception as e:
            self.logger.warning(f"AIæœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            try:
                self.ai_service = UnifiedAIService(db_session=db_session)
                self.logger.info("å›é€€åˆ°ç³»ç»Ÿé»˜è®¤AIæœåŠ¡")
            except Exception as e2:
                self.logger.error(f"ç³»ç»Ÿé»˜è®¤AIæœåŠ¡ä¹Ÿå¤±è´¥: {e2}")
                self.ai_service = None
    
    async def analyze_placeholder_requirements(
        self, 
        agent_input: Dict[str, Any],
        execution_context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        åˆ†æå ä½ç¬¦éœ€æ±‚
        
        Args:
            agent_input: åŒ…å«å ä½ç¬¦ä¿¡æ¯çš„å­—å…¸
            execution_context: å¯é€‰æ‰§è¡Œä¸Šä¸‹æ–‡
        
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        start_time = datetime.now()
        
        try:
            placeholder_name = agent_input.get("placeholder_name", "")
            placeholder_type = agent_input.get("placeholder_type", "")
            data_source = agent_input.get("data_source", {})
            
            self.logger.info(f"ğŸš€ å¼€å§‹Agentåˆ†æå ä½ç¬¦: {placeholder_name}")
            self.logger.info(f"ğŸ“Š å ä½ç¬¦ç±»å‹: {placeholder_type}, æ•°æ®æº: {data_source.get('id', 'unknown')}")
            if execution_context:
                self.logger.debug(f"æ‰§è¡Œä¸Šä¸‹æ–‡: {execution_context}")
            
            # 1. è·å–æ•°æ®æºç»“æ„ä¿¡æ¯
            self.logger.info("ğŸ” è·å–æ•°æ®æºç»“æ„ä¿¡æ¯...")
            enhanced_schema = await self._get_enhanced_schema(data_source)
            
            if not enhanced_schema:
                return {
                    "success": False,
                    "error": "æ— æ³•è·å–æ•°æ®æºç»“æ„ä¿¡æ¯"
                }
            
            # 2. æ‰§è¡ŒAIè¯­ä¹‰åˆ†æ
            self.logger.info("ğŸ§  ä½¿ç”¨ ai_agent åˆ†ææ¨¡å¼")
            if self.user_id:
                self.logger.info(f"ä½¿ç”¨ç”¨æˆ·ç‰¹å®šAIæœåŠ¡è¿›è¡Œåˆ†æ: {self.user_id}")
            else:
                self.logger.info("ä½¿ç”¨ç³»ç»Ÿé»˜è®¤AIæœåŠ¡è¿›è¡Œåˆ†æ")
            
            semantic_analysis = await self._perform_ai_agent_analysis(
                placeholder_name, placeholder_type, enhanced_schema, data_source
            )
            
            if not semantic_analysis.get("success"):
                return {
                    "success": False,
                    "error": f"AI Agentåˆ†æå¤±è´¥: {semantic_analysis.get('error', 'æœªçŸ¥é”™è¯¯')}"
                }
            
            # 3. æ‰§è¡Œæ™ºèƒ½ç›®æ ‡é€‰æ‹©
            self.logger.info("ğŸ¯ æ‰§è¡Œæ™ºèƒ½ç›®æ ‡é€‰æ‹©...")
            target_selection = await self._perform_intelligent_target_selection(
                semantic_analysis, enhanced_schema
            )
            
            if not target_selection.get("success"):
                return {
                    "success": False,
                    "error": f"æ™ºèƒ½ç›®æ ‡é€‰æ‹©å¤±è´¥: {target_selection.get('error', 'æœªçŸ¥é”™è¯¯')}"
                }
            
            # 4. ç”Ÿæˆæ™ºèƒ½SQL
            self.logger.info("âš™ï¸ ç”Ÿæˆæ™ºèƒ½SQL...")
            generated_sql = await self._generate_intelligent_sql(
                semantic_analysis, target_selection, enhanced_schema
            )
            
            if not generated_sql:
                return {
                    "success": False,
                    "error": "AI SQLç”Ÿæˆå¤±è´¥: AIæœåŠ¡æœªåˆå§‹åŒ–ï¼Œè¯·æä¾›æœ‰æ•ˆçš„æ•°æ®åº“ä¼šè¯"
                }
            
            # 5. æ‰§è¡ŒSQLè´¨é‡éªŒè¯å’Œæ”¹è¿›
            self.logger.info("ğŸ”§ æ‰§è¡ŒSQLè´¨é‡éªŒè¯å’Œæ”¹è¿›...")
            final_sql = await self._self_validate_and_improve_sql(
                generated_sql, data_source.get("id"), target_selection
            )
            
            # 6. æ„å»ºåˆ†æå…ƒæ•°æ®
            analysis_metadata = {
                "analysis_mode": "ai_agent",
                "intent": semantic_analysis.get("intent", "unknown"),
                "data_operation": semantic_analysis.get("data_operation", "unknown"),
                "relevant_tables_count": len(enhanced_schema.get("tables", [])),
                "analysis_duration_seconds": (datetime.now() - start_time).total_seconds(),
                "ai_service_used": "user_specific" if self.user_id else "system_default"
            }
            
            return {
                "success": True,
                "target_database": data_source.get("name", ""),
                "target_table": target_selection.get("table", ""),
                "required_fields": target_selection.get("fields", []),
                "generated_sql": final_sql,
                "confidence_score": semantic_analysis.get("confidence", 0.8),
                "reasoning": semantic_analysis.get("reasoning", []),
                "optimizations": semantic_analysis.get("optimizations", []),
                "estimated_time_ms": 100,
                "analysis_metadata": analysis_metadata
            }
            
        except Exception as e:
            self.logger.error(f"å ä½ç¬¦éœ€æ±‚åˆ†æå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    async def _get_enhanced_schema(self, data_source: Dict[str, Any]) -> Dict[str, Any]:
        """ä»ç¼“å­˜ä¸­è·å–å¢å¼ºçš„æ•°æ®æºschemaä¿¡æ¯"""
        try:
            data_source_id = data_source.get("id")
            source_type = data_source.get("source_type")
            
            if not data_source_id or not self.db_session:
                self.logger.error("ç¼ºå°‘æ•°æ®æºIDæˆ–æ•°æ®åº“ä¼šè¯")
                return None
            
            # ä½¿ç”¨SchemaQueryServiceè·å–ç¼“å­˜çš„è¡¨ç»“æ„ä¿¡æ¯
            from app.services.schema_management.schema_query_service import SchemaQueryService
            schema_service = SchemaQueryService(self.db_session)
            
            # è·å–æ‰€æœ‰è¡¨ç»“æ„
            table_schemas = schema_service.get_table_schemas(data_source_id)
            
            if not table_schemas:
                self.logger.warning(f"æ•°æ®æº {data_source_id} æ²¡æœ‰ç¼“å­˜çš„è¡¨ç»“æ„ä¿¡æ¯ï¼Œè¯·å…ˆæ‰§è¡Œè¡¨ç»“æ„å‘ç°")
                return None
            
            # æ„å»ºè¡¨ç»“æ„è¯¦æƒ…
            table_details = {}
            tables = []
            
            for table_schema in table_schemas:
                table_name = table_schema.table_name
                tables.append(table_name)
                
                # è·å–è¡¨çš„åˆ—ä¿¡æ¯
                columns = schema_service.get_table_columns(table_schema.id)
                column_details = []
                
                for column in columns:
                    column_info = {
                        "name": column.column_name,
                        "type": column.column_type,
                        "normalized_type": column.normalized_type if column.normalized_type else "unknown",
                        "nullable": column.is_nullable,
                        "primary_key": column.is_primary_key,
                        "business_name": column.business_name,  # ä¸šåŠ¡ä¸­æ–‡å
                        "business_description": column.business_description,  # ä¸šåŠ¡æè¿°
                        "semantic_category": column.semantic_category,  # è¯­ä¹‰åˆ†ç±»
                        "sample_values": column.sample_values,  # æ ·æœ¬å€¼
                        "data_patterns": column.data_patterns  # æ•°æ®æ¨¡å¼
                    }
                    column_details.append(column_info)
                
                table_details[table_name] = {
                    "columns": column_details,
                    "business_category": table_schema.business_category,  # è¡¨çš„ä¸šåŠ¡åˆ†ç±»
                    "data_freshness": table_schema.data_freshness,  # æ•°æ®æ–°é²œåº¦
                    "update_frequency": table_schema.update_frequency,  # æ›´æ–°é¢‘ç‡
                    "estimated_row_count": table_schema.estimated_row_count,  # é¢„ä¼°è¡Œæ•°
                    "data_quality_score": table_schema.data_quality_score,  # æ•°æ®è´¨é‡è¯„åˆ†
                    "table_size_bytes": table_schema.table_size_bytes  # è¡¨å¤§å°
                }
            
            return {
                "source_type": source_type,
                "tables": tables,
                "table_details": table_details,
                "total_tables": len(tables),
                "total_columns": sum(len(details["columns"]) for details in table_details.values()),
                "schema_metadata": {
                    "business_categories": list(set([t.business_category for t in table_schemas if t.business_category])),
                    "semantic_categories": schema_service.get_semantic_categories(data_source_id),
                    "data_quality_avg": sum([t.data_quality_score or 0 for t in table_schemas]) / len(table_schemas) if table_schemas else 0
                }
            }
                
        except Exception as e:
            self.logger.error(f"ä»ç¼“å­˜è·å–å¢å¼ºschemaå¤±è´¥: {e}")
            return None
    
    async def _create_connector(self, data_source: Dict[str, Any]) -> Optional[BaseConnector]:
        """åˆ›å»ºæ•°æ®åº“è¿æ¥å™¨"""
        try:
            source_type = data_source.get("source_type")
            
            if source_type == "doris":
                # ä»æ•°æ®åº“è·å–æ•°æ®æºè¯¦ç»†ä¿¡æ¯
                from app.models.data_source import DataSource
                ds = self.db_session.query(DataSource).filter(
                    DataSource.id == data_source.get("id")
                ).first()
                
                if not ds:
                    return None
                
                config = DorisConfig(
                    source_type=ds.source_type,
                    name=ds.name,
                    fe_hosts=ds.doris_fe_hosts if isinstance(ds.doris_fe_hosts, list) else [ds.doris_fe_hosts],
                    query_port=ds.doris_query_port or 9030,
                    http_port=ds.doris_http_port or 8030,
                    database=ds.doris_database or "default",
                    username=ds.doris_username or "root",
                    password=ds.doris_password or ""
                )
                
                return DorisConnector(config)
            else:
                self.logger.warning(f"ä¸æ”¯æŒçš„æ•°æ®æºç±»å‹: {source_type}")
                return None
                
        except Exception as e:
            self.logger.error(f"åˆ›å»ºè¿æ¥å™¨å¤±è´¥: {e}")
            return None
    
    async def _perform_ai_agent_analysis(
        self, 
        placeholder_name: str, 
        placeholder_type: str, 
        enhanced_schema: Dict, 
        data_source: Dict
    ) -> Dict[str, Any]:
        """æ‰§è¡ŒAIä»£ç†åˆ†æ"""
        try:
            if not self.ai_service:
                return {
                    "success": False,
                    "error": "AIæœåŠ¡æœªåˆå§‹åŒ–ï¼Œè¯·æä¾›æœ‰æ•ˆçš„æ•°æ®åº“ä¼šè¯"
                }
            
            # æ„å»ºåˆ†æä¸Šä¸‹æ–‡
            context = {
                "placeholder_name": placeholder_name,
                "placeholder_type": placeholder_type,
                "data_source": data_source,
                "schema_info": enhanced_schema
            }
            
            # æ„å»ºAIåˆ†ææç¤º
            prompt = self._build_ai_analysis_prompt(context, enhanced_schema)
            
            # æ‰§è¡ŒAIåˆ†æ
            response = await self.ai_service.analyze_with_context(
                context=str(context), prompt=prompt, task_type="placeholder_analysis"
            )
            
            self.logger.info(f"AIå“åº”å†…å®¹: {response[:200]}...")  # Debugging line
            
            if response:
                try:
                    ai_result = {"success": True, "data": json.loads(response)}
                    self.logger.info("AIå“åº”JSONè§£ææˆåŠŸ")
                except json.JSONDecodeError as e:
                    self.logger.warning(f"AIå“åº”JSONè§£æå¤±è´¥: {e}")
                    self.logger.warning(f"AIå“åº”åŸå§‹å†…å®¹: {response}")
                    
                    # å°è¯•ä»å“åº”ä¸­æå–JSONï¼ˆå¤„ç†markdownåŒ…è£…ï¼‰
                    # å…ˆå°è¯•ç§»é™¤markdownä»£ç å—æ ‡è®°
                    json_content = response
                    if "```json" in response:
                        # æå–markdownä»£ç å—ä¸­çš„JSON
                        start_marker = "```json"
                        end_marker = "```"
                        start_idx = response.find(start_marker)
                        if start_idx != -1:
                            start_idx += len(start_marker)
                            end_idx = response.find(end_marker, start_idx)
                            if end_idx != -1:
                                json_content = response[start_idx:end_idx].strip()
                    
                    # ç°åœ¨å°è¯•è§£ææ¸…ç†åçš„JSON
                    json_start = json_content.find('{')
                    json_end = json_content.rfind('}') + 1
                    if json_start != -1 and json_end > json_start:
                        try:
                            json_str = json_content[json_start:json_end]
                            ai_result = {"success": True, "data": json.loads(json_str)}
                            self.logger.info("ä»å“åº”ä¸­æå–JSONæˆåŠŸ")
                        except json.JSONDecodeError as e2:
                            self.logger.error(f"æå–çš„JSONä»ç„¶æ— æ•ˆ: {e2}")
                            self.logger.error(f"å°è¯•è§£æçš„JSON: {json_str[:200]}...")
                            ai_result = {"success": True, "data": {"intent": "statistical", "data_operation": "count", "reasoning": [response]}}
                    else:
                        ai_result = {"success": True, "data": {"intent": "statistical", "data_operation": "count", "reasoning": [response]}}
            else:
                ai_result = {"success": True, "data": {"intent": "statistical", "data_operation": "count", "reasoning": ["AIåˆ†æå¤±è´¥"]}}
            
            if ai_result.get("success"):
                analysis_data = ai_result.get("data", {})
                
                # ä»"åˆ†æç»“æœ"ä¸­æå–ä¿¡æ¯ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä»æ ¹çº§åˆ«æå–
                analysis_result = analysis_data.get("åˆ†æç»“æœ", analysis_data)
                
                return {
                    "success": True,
                    "intent": analysis_result.get("intent", "statistical"),
                    "data_operation": analysis_result.get("data_operation", "count"),
                    "business_domain": analysis_result.get("business_domain", ""),
                    "target_table": analysis_result.get("target_table", ""),  # æ·»åŠ ç›®æ ‡è¡¨
                    "target_fields": analysis_result.get("target_fields", []),  # æ·»åŠ ç›®æ ‡å­—æ®µ
                    "target_metrics": analysis_result.get("target_metrics", []),
                    "time_dimension": analysis_result.get("time_dimension"),
                    "grouping_dimensions": analysis_result.get("grouping_dimensions", []),
                    "filters": analysis_result.get("filters", []),
                    "aggregations": analysis_result.get("aggregations", []),
                    "reasoning": analysis_data.get("reasoning", []),
                    "confidence": analysis_data.get("confidence", 0.8),
                    "optimizations": analysis_data.get("optimizations", [])
                }
            else:
                return {
                    "success": False,
                    "error": "AIåˆ†æå¤±è´¥"
                }
                
        except Exception as e:
            self.logger.error(f"AIä»£ç†åˆ†æå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def _build_ai_analysis_prompt(self, context: Dict, enhanced_schema: Dict) -> str:
        """æ„å»ºAIåˆ†ææç¤º - åŸºäºçœŸå®ç¼“å­˜schemaçš„å¤šè½®æ€è€ƒåˆ†æ"""
        placeholder_name = context.get("placeholder_name", "")
        placeholder_type = context.get("placeholder_type", "")
        data_source = context.get("data_source", {})
        
        # åŸºäºçœŸå®ç¼“å­˜çš„è¡¨ç»“æ„æ„å»ºè¯¦ç»†ä¿¡æ¯
        tables_info = []
        business_mappings = []
        
        for table_name, table_data in enhanced_schema.get("table_details", {}).items():
            columns = table_data.get("columns", [])
            business_category = table_data.get("business_category", "")
            estimated_rows = table_data.get("estimated_row_count", 0)
            
            # æ„å»ºåˆ—ä¿¡æ¯ï¼ŒåŒ…å«ä¸šåŠ¡å«ä¹‰
            column_descriptions = []
            for col in columns[:15]:  # é™åˆ¶åˆ—æ•°é¿å…æç¤ºè¯è¿‡é•¿
                col_desc = col.get("name", "")
                if col.get("business_name"):
                    col_desc += f" ({col['business_name']})"
                if col.get("semantic_category"):
                    col_desc += f" [{col['semantic_category']}]"
                column_descriptions.append(col_desc)
            
            table_info = f"""è¡¨å: {table_name}
ä¸šåŠ¡åˆ†ç±»: {business_category or 'æœªåˆ†ç±»'}
è®°å½•æ•°: çº¦{estimated_rows:,}æ¡
å…³é”®å­—æ®µ: {', '.join(column_descriptions)}"""
            tables_info.append(table_info)
            
            # æ„å»ºä¸šåŠ¡æ˜ å°„å…³ç³»
            if business_category:
                business_mappings.append(f"- {business_category}ç›¸å…³æ•°æ® â†’ {table_name}è¡¨")
            
            # åŸºäºåˆ—çš„ä¸šåŠ¡åç§°å’Œè¯­ä¹‰åˆ†ç±»æ„å»ºæ˜ å°„
            for col in columns:
                if col.get("business_name") or col.get("semantic_category"):
                    business_name = col.get("business_name", col.get("name", ""))
                    semantic_cat = col.get("semantic_category", "")
                    if business_name:
                        business_mappings.append(f"- '{business_name}'{f'({semantic_cat})' if semantic_cat else ''} â†’ {table_name}.{col.get('name', '')}")
        
        tables_text = "\n\n".join(tables_info)
        business_mappings_text = "\n".join(list(set(business_mappings))[:20])  # å»é‡å¹¶é™åˆ¶æ•°é‡
        
        # è·å–ä¸šåŠ¡é¢†åŸŸä¿¡æ¯
        schema_metadata = enhanced_schema.get("schema_metadata", {})
        business_categories = schema_metadata.get("business_categories", [])
        semantic_categories = schema_metadata.get("semantic_categories", [])
        
        prompt = f"""
ä½ æ˜¯ä¸“ä¸šçš„æ•°æ®åˆ†æAIï¼Œéœ€è¦åˆ†æä¸­æ–‡å ä½ç¬¦çš„ä¸šåŠ¡éœ€æ±‚å¹¶é€‰æ‹©æœ€åˆé€‚çš„æ•°æ®è¡¨ã€‚è¯·è¿›è¡Œå¤šè½®æ€è€ƒåˆ†æã€‚

ã€æ•°æ®æºæ¦‚å†µã€‘
- æ•°æ®æº: {data_source.get('name', 'unknown')}
- ä¸šåŠ¡é¢†åŸŸ: {', '.join(business_categories) if business_categories else 'é€šç”¨ä¸šåŠ¡'}
- è¯­ä¹‰åˆ†ç±»: {', '.join(semantic_categories[:10]) if semantic_categories else 'æœªåˆ†ç±»'}
- è¡¨æ€»æ•°: {enhanced_schema.get('total_tables', 0)}
- å­—æ®µæ€»æ•°: {enhanced_schema.get('total_columns', 0)}

ã€å½“å‰åˆ†æä»»åŠ¡ã€‘
å ä½ç¬¦: {placeholder_name}
ç±»å‹: {placeholder_type}

ã€å¯ç”¨æ•°æ®è¡¨è¯¦æƒ…ã€‘
{tables_text}

ã€ä¸šåŠ¡è¯­ä¹‰æ˜ å°„ã€‘
{business_mappings_text}

ã€åˆ†æè¦æ±‚ã€‘
è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤è¿›è¡Œå¤šè½®æ€è€ƒï¼š

1. ã€è¯­ä¹‰ç†è§£ã€‘- åˆ†æå ä½ç¬¦çš„ä¸­æ–‡å«ä¹‰ï¼Œè¯†åˆ«å…³é”®ä¸šåŠ¡æ¦‚å¿µ
2. ã€è¡¨é€‰æ‹©ã€‘- åŸºäºçœŸå®è¡¨ç»“æ„å’Œä¸šåŠ¡åˆ†ç±»ï¼Œé€‰æ‹©æœ€åˆé€‚çš„æ•°æ®è¡¨
3. ã€å­—æ®µæ˜ å°„ã€‘- æ ¹æ®åˆ—çš„ä¸šåŠ¡åç§°å’Œè¯­ä¹‰åˆ†ç±»ï¼Œç¡®å®šç›®æ ‡å­—æ®µ
4. ã€æ“ä½œç¡®å®šã€‘- ç¡®å®šéœ€è¦çš„æ•°æ®æ“ä½œï¼ˆç»Ÿè®¡ã€å»é‡ã€èšåˆç­‰ï¼‰
5. ã€ç»“æœéªŒè¯ã€‘- éªŒè¯é€‰æ‹©çš„è¡¨å’Œå­—æ®µæ˜¯å¦èƒ½æ»¡è¶³ä¸šåŠ¡éœ€æ±‚

è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š

{{
    "æ€è€ƒè¿‡ç¨‹": {{
        "è¯­ä¹‰ç†è§£": "å¯¹å ä½ç¬¦ä¸­æ–‡å«ä¹‰çš„ç†è§£",
        "å…³é”®æ¦‚å¿µ": ["è¯†åˆ«å‡ºçš„ä¸šåŠ¡æ¦‚å¿µ1", "æ¦‚å¿µ2"],
        "è¡¨é€‰æ‹©æ¨ç†": "ä¸ºä»€ä¹ˆé€‰æ‹©è¿™ä¸ªè¡¨çš„è¯¦ç»†æ¨ç†",
        "å­—æ®µåŒ¹é…": "å­—æ®µé€‰æ‹©çš„ä¾æ®",
        "æ“ä½œç¡®å®š": "æ•°æ®æ“ä½œç±»å‹çš„ç¡®å®šç†ç”±"
    }},
    "åˆ†æç»“æœ": {{
        "intent": "statistical/analytical/reporting",
        "data_operation": "count/sum/avg/count_distinct/group_byç­‰",
        "business_domain": "ä¸šåŠ¡é¢†åŸŸ",
        "target_table": "é€‰å®šçš„ç›®æ ‡è¡¨å",
        "target_fields": ["å­—æ®µ1", "å­—æ®µ2"],
        "target_metrics": ["æŒ‡æ ‡1", "æŒ‡æ ‡2"],
        "time_dimension": "æ—¶é—´ç»´åº¦å­—æ®µæˆ–null",
        "grouping_dimensions": ["åˆ†ç»„å­—æ®µ"],
        "filters": ["è¿‡æ»¤æ¡ä»¶"],
        "aggregations": ["èšåˆæ“ä½œ"],
        "confidence": 0.0-1.0,
        "reasoning": ["è¯¦ç»†æ¨ç†æ­¥éª¤"],
        "optimizations": ["æ€§èƒ½ä¼˜åŒ–å»ºè®®"]
    }}
}}

ã€å…³é”®è¦æ±‚ã€‘
1. ä»”ç»†ç†è§£ä¸­æ–‡å ä½ç¬¦çš„ä¸šåŠ¡å«ä¹‰
2. é€‰æ‹©æœ€ç¬¦åˆä¸šåŠ¡é€»è¾‘çš„æ•°æ®è¡¨  
3. åªè¿”å›JSONï¼Œä¸è¦ä»»ä½•å…¶ä»–æ–‡æœ¬
4. confidenceå€¼åº”åæ˜ åˆ†æçš„ç¡®å®šç¨‹åº¦
5. ç‰¹åˆ«æ³¨æ„"å»é‡"ã€"åŒæ¯”"ã€"å æ¯”"ç­‰ç»Ÿè®¡éœ€æ±‚çš„å‡†ç¡®ç†è§£
"""
        return prompt
    
    
    async def _perform_intelligent_target_selection(
        self, 
        semantic_analysis: Dict, 
        enhanced_schema: Dict
    ) -> Dict[str, Any]:
        """åŸºäºAIåˆ†æç»“æœæ‰§è¡Œç›®æ ‡é€‰æ‹©"""
        try:
            # ä»AIåˆ†æç»“æœä¸­è·å–ç›®æ ‡è¡¨
            if "åˆ†æç»“æœ" in semantic_analysis:
                analysis_result = semantic_analysis["åˆ†æç»“æœ"]
                target_table = analysis_result.get("target_table")
                target_fields = analysis_result.get("target_fields", [])
            else:
                # å…¼å®¹è€æ ¼å¼
                target_table = semantic_analysis.get("target_table")
                target_fields = semantic_analysis.get("target_fields", [])
            
            if target_table:
                # éªŒè¯ç›®æ ‡è¡¨æ˜¯å¦å­˜åœ¨
                if target_table in enhanced_schema.get("table_details", {}):
                    self.logger.info(f"âœ… AIé€‰æ‹©çš„ç›®æ ‡è¡¨: {target_table}")
                    
                    return {
                        "success": True,
                        "table": target_table,
                        "fields": target_fields,
                        "field_mapping": {},
                        "relevance_score": semantic_analysis.get("confidence", 0.8),
                        "alternative_tables": []
                    }
                else:
                    self.logger.warning(f"AIé€‰æ‹©çš„è¡¨ {target_table} ä¸å­˜åœ¨ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨è¡¨")
            
            # å¦‚æœAIæ²¡æœ‰æŒ‡å®šè¡¨æˆ–è¡¨ä¸å­˜åœ¨ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨è¡¨
            available_tables = list(enhanced_schema.get("table_details", {}).keys())
            if available_tables:
                fallback_table = available_tables[0]
                self.logger.info(f"ä½¿ç”¨å¤‡ç”¨è¡¨: {fallback_table}")
                
                return {
                    "success": True,
                    "table": fallback_table,
                    "fields": target_fields,
                    "field_mapping": {},
                    "relevance_score": 0.5,
                    "alternative_tables": available_tables[1:3]
                }
            else:
                return {
                    "success": False,
                    "error": "æ²¡æœ‰å¯ç”¨çš„æ•°æ®è¡¨"
                }
                
        except Exception as e:
            self.logger.error(f"ç›®æ ‡é€‰æ‹©å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    
    async def _generate_intelligent_sql(
        self, 
        semantic_analysis: Dict, 
        target_selection: Dict, 
        enhanced_schema: Dict
    ) -> str:
        """åŸºäºè¯­ä¹‰åˆ†æå’Œç›®æ ‡é€‰æ‹©ç”Ÿæˆæ™ºèƒ½SQL - ä¼˜åŒ–ç‰ˆæœ¬"""
        try:
            # é¦–å…ˆå°è¯•AIé©±åŠ¨çš„SQLç”Ÿæˆ
            ai_generated_sql = await self._generate_sql_with_ai(semantic_analysis, target_selection, enhanced_schema)
            if ai_generated_sql and self._validate_sql_syntax(ai_generated_sql):
                return ai_generated_sql
            
            # å¦‚æœAIç”Ÿæˆå¤±è´¥æˆ–SQLæ— æ•ˆï¼Œä½¿ç”¨æ¨¡æ¿åŒ–ç”Ÿæˆ
            table_name = self._sanitize_identifier(target_selection.get('table', 'default_table'))
            fields = target_selection.get('fields', [])
            field_mapping = target_selection.get('field_mapping', {})
            
            sql = self._generate_sql_by_template(semantic_analysis.get('intent', 'general'),
                                                semantic_analysis.get('data_operation', 'select'),
                                                table_name, fields, field_mapping)
            
            if not self._validate_sql_syntax(sql):
                self.logger.warning(f"ç”Ÿæˆçš„SQLè¯­æ³•æ— æ•ˆï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ: {sql}")
                sql = self._generate_fallback_sql(table_name)
            
            return sql
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆæ™ºèƒ½SQLå¤±è´¥: {e}")
            return self._generate_fallback_sql()
    
    async def _generate_sql_with_ai(
        self, 
        semantic_analysis: Dict, 
        target_selection: Dict, 
        enhanced_schema: Dict
    ) -> str:
        """ä½¿ç”¨AIç”ŸæˆSQL"""
        try:
            if not self.ai_service:
                return None
            
            prompt = self._build_sql_generation_prompt(semantic_analysis, target_selection, enhanced_schema)
            
            response = await self.ai_service.analyze_with_context(
                context="", prompt=prompt, task_type="sql_generation"
            )
            
            if response:
                sql = self._extract_sql_from_response(response)
                if sql and self._validate_sql_syntax(sql):
                    return sql
            
            return None
            
        except Exception as e:
            self.logger.error(f"AI SQLç”Ÿæˆå¤±è´¥: {e}")
            return None
    
    def _build_sql_generation_prompt(
        self, 
        semantic_analysis: Dict, 
        target_selection: Dict, 
        enhanced_schema: Dict
    ) -> str:
        """æ„å»ºSQLç”Ÿæˆæç¤º"""
        table_name = target_selection.get('table', '')
        fields = target_selection.get('fields', [])
        
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªSQLä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆæ ‡å‡†çš„SQLæŸ¥è¯¢è¯­å¥ï¼š

åˆ†æç»“æœ:
- æ„å›¾: {semantic_analysis.get('intent', '')}
- æ•°æ®æ“ä½œ: {semantic_analysis.get('data_operation', '')}
- ä¸šåŠ¡é¢†åŸŸ: {semantic_analysis.get('business_domain', '')}
- ç›®æ ‡æŒ‡æ ‡: {semantic_analysis.get('target_metrics', [])}
- è¿‡æ»¤æ¡ä»¶: {semantic_analysis.get('filters', [])}
- èšåˆå‡½æ•°: {semantic_analysis.get('aggregations', [])}

ç›®æ ‡è¡¨: {table_name}
ç›¸å…³å­—æ®µ: {fields}

è¯·ç”Ÿæˆä¸€ä¸ªæ ‡å‡†çš„SQLæŸ¥è¯¢è¯­å¥ï¼Œåªè¿”å›SQLè¯­å¥ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæˆ–å…¶ä»–æ–‡æœ¬ã€‚
ç¡®ä¿SQLè¯­æ³•å®Œå…¨æ­£ç¡®ã€‚
"""
        return prompt
    
    def _extract_sql_from_response(self, response: str) -> str:
        """ä»AIå“åº”ä¸­æå–SQL"""
        # æŸ¥æ‰¾SQLè¯­å¥
        sql_patterns = [
            r'SELECT\s+.*?FROM\s+.*?(?:WHERE\s+.*?)?(?:GROUP BY\s+.*?)?(?:ORDER BY\s+.*?)?(?:LIMIT\s+\d+)?;?',
            r'SELECT\s+.*?FROM\s+.*?(?:WHERE\s+.*?)?(?:GROUP BY\s+.*?)?(?:ORDER BY\s+.*?)?(?:LIMIT\s+\d+)?',
        ]
        
        for pattern in sql_patterns:
            match = re.search(pattern, response, re.IGNORECASE | re.DOTALL)
            if match:
                return match.group(0).strip()
        
        return None
    
    def _generate_sql_by_template(
        self, 
        intent: str, 
        operation: str, 
        table_name: str, 
        fields: List[str], 
        field_mapping: Dict
    ) -> str:
        """åŸºäºæ¨¡æ¿ç”ŸæˆSQL"""
        if operation == "count":
            return f"SELECT COUNT(*) as total_count FROM {table_name}"
        elif operation == "sum":
            if fields:
                field = fields[0]
                return f"SELECT SUM({field}) as total_sum FROM {table_name}"
            else:
                return f"SELECT COUNT(*) as total_count FROM {table_name}"
        else:
            return f"SELECT COUNT(*) as total_count FROM {table_name}"
    
    def _sanitize_identifier(self, identifier: str) -> str:
        """æ¸…ç†æ ‡è¯†ç¬¦"""
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œåªä¿ç•™å­—æ¯ã€æ•°å­—å’Œä¸‹åˆ’çº¿
        return re.sub(r'[^a-zA-Z0-9_]', '', identifier)
    
    def _is_valid_field_name(self, field_name: str) -> bool:
        """æ£€æŸ¥å­—æ®µåæ˜¯å¦æœ‰æ•ˆ"""
        if not field_name:
            return False
        # æ£€æŸ¥æ˜¯å¦åªåŒ…å«å­—æ¯ã€æ•°å­—å’Œä¸‹åˆ’çº¿
        return bool(re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*$', field_name))
    
    def _clean_field_names(self, fields: List[str]) -> List[str]:
        """æ¸…ç†å­—æ®µååˆ—è¡¨"""
        cleaned_fields = []
        for field in fields:
            if self._is_valid_field_name(field):
                cleaned_fields.append(field)
        return cleaned_fields
    
    def _validate_sql_syntax(self, sql: str) -> bool:
        """éªŒè¯SQLè¯­æ³•"""
        if not sql:
            return False
        
        sql_upper = sql.upper()
        
        # åŸºæœ¬è¯­æ³•æ£€æŸ¥
        if not sql_upper.startswith('SELECT'):
            return False
        
        if 'FROM' not in sql_upper:
            return False
        
        # æ£€æŸ¥FROMåé¢æ˜¯å¦æœ‰è¡¨å
        if not re.search(r'FROM\s+\w+', sql, re.IGNORECASE):
            return False
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„è¯­æ³•é”™è¯¯
        if 'SELECTSELECT' in sql_upper or 'FROMFROM' in sql_upper:
            return False
        
        return True
    
    def _fix_sql_syntax_errors(self, sql: str, table_name: str = "default_table") -> str:
        """ä¿®å¤SQLè¯­æ³•é”™è¯¯"""
        if not sql:
            return f"SELECT COUNT(*) as total_count FROM {table_name}"
        
        # ä¿®å¤å¸¸è§çš„AIç”Ÿæˆé”™è¯¯
        sql = re.sub(r'SELECT\s+([^F]+)FROM', r'SELECT \1 FROM', sql, flags=re.IGNORECASE)
        sql = re.sub(r'SELECT\s+([^F]+)T\s+([^F]+)FROM', r'SELECT \1 FROM', sql, flags=re.IGNORECASE)
        sql = re.sub(r'SELECT\s+([^F]+)OUNT\s*\(\s*\*\s*\)', r'SELECT COUNT(*)', sql, flags=re.IGNORECASE)
        
        # ç¡®ä¿åŸºæœ¬ç»“æ„æ­£ç¡®
        if not sql.upper().startswith('SELECT'):
            sql = f"SELECT {sql}"
        
        # æ£€æŸ¥æ˜¯å¦ç¼ºå°‘è¡¨å
        if 'FROM' in sql.upper() and not re.search(r'FROM\s+\w+', sql, re.IGNORECASE):
            # å¦‚æœFROMåé¢æ²¡æœ‰è¡¨åï¼Œæ·»åŠ è¡¨å
            sql = re.sub(r'FROM\s*$', f'FROM {table_name}', sql, flags=re.IGNORECASE)
        elif 'FROM' not in sql.upper():
            sql = f"{sql} FROM {table_name}"
        
        return sql
    
    def _generate_fallback_sql(self, table_name: str = "default_table") -> str:
        """ç”Ÿæˆå¤‡ç”¨SQL"""
        return f"SELECT COUNT(*) as total_count FROM {table_name}"
    
    async def _self_validate_and_improve_sql(
        self, 
        sql: str, 
        data_source_id: str, 
        target_selection: Dict
    ) -> str:
        """SQLè´¨é‡éªŒè¯å’Œè‡ªæˆ‘æ”¹è¿› - å¢å¼ºç‰ˆæœ¬"""
        try:
            # åˆå§‹è¯­æ³•æ£€æŸ¥
            table_name = target_selection.get('table', 'default_table')
            if not self._validate_sql_syntax(sql):
                sql = self._fix_sql_syntax_errors(sql, table_name)
                if not self._validate_sql_syntax(sql):
                    return self._generate_fallback_sql(table_name)
            
            # å­—æ®µåéªŒè¯
            table_name = target_selection.get('table', 'default_table')
            fields = target_selection.get('fields', [])
            
            # æ¸…ç†å­—æ®µå
            cleaned_fields = self._clean_field_names(fields)
            if cleaned_fields != fields:
                self.logger.info(f"å­—æ®µåå·²æ¸…ç†: {fields} -> {cleaned_fields}")
            
            # æœ€ç»ˆè¯­æ³•æ£€æŸ¥
            if not self._validate_sql_syntax(sql):
                return self._generate_fallback_sql(table_name)
            
            return sql
            
        except Exception as e:
            self.logger.error(f"SQLéªŒè¯å’Œæ”¹è¿›å¤±è´¥: {e}")
            return self._generate_fallback_sql(target_selection.get('table', 'default_table'))
