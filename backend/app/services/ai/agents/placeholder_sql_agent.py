"""
å ä½ç¬¦SQLæ„å»ºAgent

ç‹¬ç«‹çš„Agentï¼Œä¸“é—¨è´Ÿè´£åŸºäºå ä½ç¬¦å’Œæ•°æ®åº“è¡¨ç»“æ„æ„å»ºSQLæŸ¥è¯¢èƒ½åŠ›
åŸºäºä¸Šä¸‹æ–‡å·¥ç¨‹è®¾è®¡ï¼Œæ”¯æŒä»»åŠ¡æ¿å—å’Œæ¨¡ç‰ˆå ä½ç¬¦åˆ†ææ¿å—è°ƒç”¨
"""

import logging
import json
from typing import Dict, Any, List, Optional
from datetime import datetime
from dataclasses import dataclass, asdict
from sqlalchemy.orm import Session

from ..core import (
    ContextAwareAgent, AgentContext, ContextScope,
    get_context_manager, get_agent_registry
)
from ...domain.placeholder.semantic_analyzer import (
    PlaceholderSemanticAnalyzer, create_semantic_analyzer, SemanticAnalysisResult
)
from ...domain.placeholder.intelligent_sql_generator import (
    IntelligentSQLGenerator, create_intelligent_sql_generator, 
    TableContext, SQLGenerationResult
)
from ..enhanced_placeholder_analyzer import (
    EnhancedPlaceholderAnalyzer, create_enhanced_placeholder_analyzer,
    EnhancedSemanticAnalysisResult, DataSourceContext
)

logger = logging.getLogger(__name__)


@dataclass
class PlaceholderAnalysisRequest:
    """å ä½ç¬¦åˆ†æè¯·æ±‚"""
    placeholder_id: str
    placeholder_text: str
    placeholder_type: str
    data_source_id: str
    template_id: Optional[str] = None
    template_context: Optional[Dict[str, Any]] = None
    force_reanalyze: bool = False
    store_result: bool = True
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


@dataclass
class PlaceholderAnalysisResult:
    """å ä½ç¬¦åˆ†æç»“æœ"""
    success: bool
    placeholder_id: str
    generated_sql: Optional[str] = None
    confidence: float = 0.0
    semantic_type: Optional[str] = None
    semantic_subtype: Optional[str] = None
    data_intent: Optional[str] = None
    target_table: Optional[str] = None
    explanation: Optional[str] = None
    suggestions: Optional[List[str]] = None
    metadata: Optional[Dict[str, Any]] = None
    error_message: Optional[str] = None
    error_context: Optional[Dict[str, Any]] = None
    analysis_timestamp: datetime = None
    
    def __post_init__(self):
        if self.analysis_timestamp is None:
            self.analysis_timestamp = datetime.now()
        if self.suggestions is None:
            self.suggestions = []
        if self.metadata is None:
            self.metadata = {}
        if self.error_context is None:
            self.error_context = {}
    
    def to_dict(self) -> Dict[str, Any]:
        result = asdict(self)
        result['analysis_timestamp'] = self.analysis_timestamp.isoformat()
        return result


class PlaceholderSQLAgent(ContextAwareAgent):
    """å ä½ç¬¦SQLæ„å»ºAgent - ç‹¬ç«‹çš„ã€åŸºäºä¸Šä¸‹æ–‡å·¥ç¨‹çš„Agent"""
    
    def __init__(self, db_session: Session = None, user_id: str = None):
        # åˆå§‹åŒ–ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        context_manager = get_context_manager()
        
        super().__init__("placeholder_sql_agent", context_manager)
        
        self.db_session = db_session
        self.user_id = user_id
        
        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        self.semantic_analyzer = create_semantic_analyzer()
        self.sql_generator = create_intelligent_sql_generator()
        
        # åˆå§‹åŒ–LLMå¢å¼ºåˆ†æå™¨
        try:
            self.enhanced_analyzer = create_enhanced_placeholder_analyzer(
                db_session=db_session, user_id=user_id
            )
            logger.info("LLMå¢å¼ºåˆ†æå™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"LLMå¢å¼ºåˆ†æå™¨åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°†å›é€€åˆ°è§„åˆ™åˆ†æå™¨")
            self.enhanced_analyzer = None
        
        # æ³¨å†ŒAgentèƒ½åŠ›
        self._register_capabilities()
        
        # å£°æ˜æ‰€éœ€çš„ä¸Šä¸‹æ–‡ï¼ˆä½†æ˜¯è¿™äº›åœ¨å®é™…ä½¿ç”¨ä¸­æ˜¯å¯é€‰çš„ï¼‰
        # æ³¨é‡Šæ‰å¿…éœ€ä¸Šä¸‹æ–‡çš„å£°æ˜ï¼Œå› ä¸ºæˆ‘ä»¬ä¼šåœ¨è·å–schemaæ—¶åŠ¨æ€è·å–
        # self.require_context(
        #     'data_source_schemas',
        #     'table_structures', 
        #     'placeholder_history',
        #     'analysis_patterns'
        # )
        
        logger.info(f"PlaceholderSQLAgent initialized for user: {user_id}")
    
    def _register_capabilities(self):
        """æ³¨å†ŒAgentèƒ½åŠ›"""
        
        # ä¸»è¦èƒ½åŠ›ï¼šå ä½ç¬¦åˆ†æä¸SQLç”Ÿæˆ
        self.register_capability(
            "analyze_placeholder_sql",
            "åˆ†æå ä½ç¬¦å¹¶ç”ŸæˆSQLæŸ¥è¯¢",
            metadata={
                "input_schema": {
                    "request": {
                        "type": "object",
                        "properties": {
                            "placeholder_id": {"type": "string", "required": True},
                            "placeholder_text": {"type": "string", "required": True},
                            "data_source_id": {"type": "string", "required": True},
                            "force_reanalyze": {"type": "boolean", "default": False},
                            "store_result": {"type": "boolean", "default": True}
                        }
                    }
                },
                "output_schema": {
                    "result": {
                        "type": "object",
                        "properties": {
                            "success": {"type": "boolean"},
                            "generated_sql": {"type": "string"},
                            "confidence": {"type": "number"},
                            "semantic_type": {"type": "string"},
                            "target_table": {"type": "string"}
                        }
                    }
                },
                "tags": ["sql", "placeholder", "analysis"],
                "requirements": ["database_access", "schema_access"]
            }
        )
        
        # æ‰¹é‡åˆ†æèƒ½åŠ›
        self.register_capability(
            "batch_analyze_placeholders",
            "æ‰¹é‡åˆ†æå¤šä¸ªå ä½ç¬¦",
            metadata={
                "input_schema": {
                    "requests": {
                        "type": "array",
                        "items": {"type": "object"}
                    }
                },
                "output_schema": {
                    "results": {
                        "type": "array", 
                        "items": {"type": "object"}
                    }
                },
                "tags": ["sql", "placeholder", "batch"],
                "requirements": ["database_access", "schema_access"]
            }
        )
        
        # æ£€æŸ¥å·²å­˜å‚¨SQLèƒ½åŠ›
        self.register_capability(
            "check_stored_sql",
            "æ£€æŸ¥å ä½ç¬¦æ˜¯å¦å·²æœ‰å­˜å‚¨çš„SQL",
            metadata={
                "input_schema": {
                    "placeholder_id": {"type": "string", "required": True}
                },
                "output_schema": {
                    "has_sql": {"type": "boolean"},
                    "sql": {"type": "string"},
                    "last_analysis_at": {"type": "string"}
                },
                "tags": ["sql", "storage", "check"],
                "requirements": ["database_access"]
            }
        )
    
    async def _execute_action(self, context: AgentContext, action: str, 
                            parameters: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå…·ä½“çš„Agentæ“ä½œ"""
        
        if action == "analyze_placeholder_sql":
            return await self._analyze_placeholder_sql(context, parameters)
        elif action == "batch_analyze_placeholders":
            return await self._batch_analyze_placeholders(context, parameters)
        elif action == "check_stored_sql":
            return await self._check_stored_sql(context, parameters)
        else:
            raise ValueError(f"Unknown action: {action}")
    
    async def _analyze_placeholder_sql(self, context: AgentContext, 
                                     parameters: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æå ä½ç¬¦å¹¶ç”ŸæˆSQL"""
        
        try:
            # æ„å»ºåˆ†æè¯·æ±‚
            request = PlaceholderAnalysisRequest(
                placeholder_id=parameters.get('placeholder_id'),
                placeholder_text=parameters.get('placeholder_text'),
                placeholder_type=parameters.get('placeholder_type', 'unknown'),
                data_source_id=parameters.get('data_source_id'),
                template_id=parameters.get('template_id'),
                template_context=parameters.get('template_context'),
                force_reanalyze=parameters.get('force_reanalyze', False),
                store_result=parameters.get('store_result', True)
            )
            
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰å­˜å‚¨çš„SQLï¼ˆå¦‚æœä¸å¼ºåˆ¶é‡æ–°åˆ†æï¼‰
            if not request.force_reanalyze:
                stored_result = await self._get_stored_sql(request.placeholder_id)
                if stored_result:
                    self.update_context(
                        context, 'last_operation', 
                        'retrieved_stored_sql', 
                        ContextScope.REQUEST
                    )
                    return {"result": stored_result.to_dict()}
            
            # æ›´æ–°ä¸Šä¸‹æ–‡
            self.update_context(
                context, 'current_analysis_request', 
                request.to_dict(), 
                ContextScope.REQUEST
            )
            
            # è·å–æ•°æ®æºSchemaä¿¡æ¯
            schema_info = await self._get_schema_info(context, request.data_source_id)
            if not schema_info:
                return {
                    "result": PlaceholderAnalysisResult(
                        success=False,
                        placeholder_id=request.placeholder_id,
                        error_message="æ— æ³•è·å–æ•°æ®æºschemaä¿¡æ¯",
                        error_context={"data_source_id": request.data_source_id}
                    ).to_dict()
                }
            
            # å°è¯•ä½¿ç”¨LLMå¢å¼ºåˆ†æå™¨
            if self.enhanced_analyzer:
                try:
                    enhanced_result = await self._analyze_with_llm(
                        request, schema_info, context
                    )
                    if enhanced_result.success:
                        logger.info(f"LLMåˆ†ææˆåŠŸ: {request.placeholder_id}")
                        return {"result": enhanced_result.to_dict()}
                    else:
                        logger.warning(f"LLMåˆ†æå¤±è´¥ï¼Œå›é€€åˆ°è§„åˆ™åˆ†æå™¨: {request.placeholder_id}")
                except Exception as e:
                    logger.error(f"LLMåˆ†æå¼‚å¸¸: {e}ï¼Œå›é€€åˆ°è§„åˆ™åˆ†æå™¨")
            
            # å›é€€åˆ°è§„åˆ™åˆ†æå™¨
            logger.info(f"ä½¿ç”¨è§„åˆ™åˆ†æå™¨å¤„ç†: {request.placeholder_id}")
            
            # è¯­ä¹‰åˆ†æ
            semantic_result = self.semantic_analyzer.analyze(
                request.placeholder_text,
                context={
                    'template_context': request.template_context or {},
                    'data_source_id': request.data_source_id
                }
            )
            
            # æ„å»ºè¡¨ä¸Šä¸‹æ–‡
            table_context = await self._build_table_context(schema_info, semantic_result)
            if not table_context:
                return {
                    "result": PlaceholderAnalysisResult(
                        success=False,
                        placeholder_id=request.placeholder_id,
                        error_message="æ— æ³•æ„å»ºè¡¨ä¸Šä¸‹æ–‡",
                        error_context={"schema_info": "no_suitable_table"}
                    ).to_dict()
                }
            
            # SQLç”Ÿæˆ
            sql_result = self.sql_generator.generate_sql(
                semantic_result=semantic_result,
                table_context=table_context,
                placeholder_name=request.placeholder_text,
                additional_context=request.template_context or {}
            )
            
            # æ„å»ºç»“æœ
            analysis_result = PlaceholderAnalysisResult(
                success=True,
                placeholder_id=request.placeholder_id,
                generated_sql=sql_result.sql,
                confidence=sql_result.confidence,
                semantic_type=semantic_result.primary_type.value,
                semantic_subtype=semantic_result.sub_type,
                data_intent=semantic_result.data_intent,
                target_table=table_context.table_name,
                explanation=sql_result.explanation,
                suggestions=sql_result.suggestions,
                metadata={
                    'semantic_analysis': {
                        'primary_type': semantic_result.primary_type.value,
                        'sub_type': semantic_result.sub_type,
                        'keywords': semantic_result.keywords,
                        'confidence': semantic_result.confidence
                    },
                    'sql_generation': {
                        'parameters': sql_result.parameters,
                        'metadata': sql_result.metadata
                    },
                    'table_context': {
                        'table_name': table_context.table_name,
                        'business_category': table_context.business_category,
                        'data_quality_score': table_context.data_quality_score
                    }
                }
            )
            
            # å­˜å‚¨ç»“æœåˆ°æ•°æ®åº“ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if request.store_result:
                await self._store_analysis_result(analysis_result)
            
            # æ›´æ–°ä¸Šä¸‹æ–‡
            self.update_context(
                context, 'last_analysis_result', 
                analysis_result.to_dict(), 
                ContextScope.TASK
            )
            
            return {"result": analysis_result.to_dict()}
            
        except Exception as e:
            logger.error(f"å ä½ç¬¦åˆ†æå¤±è´¥: {e}", exc_info=True)
            
            error_result = PlaceholderAnalysisResult(
                success=False,
                placeholder_id=parameters.get('placeholder_id', 'unknown'),
                error_message=str(e),
                error_context={'exception_type': type(e).__name__}
            )
            
            return {"result": error_result.to_dict()}
    
    async def _batch_analyze_placeholders(self, context: AgentContext, 
                                        parameters: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰¹é‡åˆ†æå ä½ç¬¦"""
        
        requests = parameters.get('requests', [])
        results = []
        
        for request_data in requests:
            try:
                result = await self._analyze_placeholder_sql(context, request_data)
                results.append(result['result'])
            except Exception as e:
                error_result = PlaceholderAnalysisResult(
                    success=False,
                    placeholder_id=request_data.get('placeholder_id', 'unknown'),
                    error_message=str(e)
                )
                results.append(error_result.to_dict())
        
        return {
            "results": results,
            "total_count": len(requests),
            "success_count": len([r for r in results if r.get('success')])
        }
    
    async def _check_stored_sql(self, context: AgentContext, 
                              parameters: Dict[str, Any]) -> Dict[str, Any]:
        """æ£€æŸ¥å·²å­˜å‚¨çš„SQL"""
        
        placeholder_id = parameters.get('placeholder_id')
        stored_result = await self._get_stored_sql(placeholder_id)
        
        if stored_result and stored_result.success and stored_result.generated_sql:
            return {
                "has_sql": True,
                "sql": stored_result.generated_sql,
                "last_analysis_at": stored_result.analysis_timestamp.isoformat(),
                "confidence": stored_result.confidence,
                "target_table": stored_result.target_table
            }
        else:
            return {
                "has_sql": False,
                "sql": None,
                "last_analysis_at": None
            }
    
    async def _get_schema_info(self, context: AgentContext, 
                             data_source_id: str) -> Optional[Dict[str, Any]]:
        """è·å–æ•°æ®æºSchemaä¿¡æ¯"""
        
        try:
            from ...data.schemas.schema_query_service import SchemaQueryService
            from ...data.schemas.schema_discovery_service import SchemaDiscoveryService
            
            if not self.db_session:
                logger.error("æ•°æ®åº“ä¼šè¯æœªåˆå§‹åŒ–")
                return None
                
            schema_service = SchemaQueryService(self.db_session)
            table_schemas = schema_service.get_table_schemas(data_source_id)
            
            if not table_schemas:
                logger.warning(f"æ•°æ®æº {data_source_id} æ²¡æœ‰ç¼“å­˜çš„è¡¨ç»“æ„ä¿¡æ¯ï¼Œå°è¯•è‡ªåŠ¨è·å–")
                
                # è‡ªåŠ¨è§¦å‘schemaå‘ç°
                try:
                    discovery_service = SchemaDiscoveryService(self.db_session)
                    discovery_result = await discovery_service.discover_and_store_schemas(data_source_id)
                    
                    if discovery_result.get("success"):
                        logger.info(f"è‡ªåŠ¨schemaå‘ç°æˆåŠŸï¼Œå‘ç° {discovery_result.get('tables_count', 0)} ä¸ªè¡¨")
                        # é‡æ–°è·å–è¡¨ç»“æ„
                        table_schemas = schema_service.get_table_schemas(data_source_id)
                    else:
                        logger.error(f"è‡ªåŠ¨schemaå‘ç°å¤±è´¥: {discovery_result.get('error')}")
                        return None
                        
                except Exception as discovery_e:
                    logger.error(f"è‡ªåŠ¨schemaå‘ç°å¼‚å¸¸: {discovery_e}")
                    return None
                
            if not table_schemas:
                return None
            
            # æ„å»ºschemaä¿¡æ¯
            schema_info = {
                'data_source_id': data_source_id,
                'tables': [],
                'table_details': {}
            }
            
            for table_schema in table_schemas:
                table_name = table_schema.table_name
                schema_info['tables'].append(table_name)
                
                # è·å–è¡¨çš„åˆ—ä¿¡æ¯
                columns = schema_service.get_table_columns(table_schema.id)
                column_details = []
                
                for column in columns:
                    column_info = {
                        "name": column.column_name,
                        "type": column.column_type,
                        "normalized_type": column.normalized_type or "unknown",
                        "nullable": column.is_nullable,
                        "primary_key": column.is_primary_key,
                        "business_name": column.business_name,
                        "business_description": column.business_description,
                        "semantic_category": column.semantic_category,
                        "sample_values": column.sample_values,
                        "data_patterns": column.data_patterns
                    }
                    column_details.append(column_info)
                
                schema_info['table_details'][table_name] = {
                    "columns": column_details,
                    "business_category": table_schema.business_category,
                    "data_freshness": table_schema.data_freshness,
                    "update_frequency": table_schema.update_frequency,
                    "estimated_row_count": table_schema.estimated_row_count,
                    "data_quality_score": table_schema.data_quality_score
                }
            
            # æ›´æ–°ä¸Šä¸‹æ–‡
            self.update_context(
                context, 'current_schema_info', 
                schema_info, 
                ContextScope.REQUEST
            )
            
            return schema_info
            
        except Exception as e:
            logger.error(f"è·å–Schemaä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    async def _build_table_context(self, schema_info: Dict[str, Any], 
                                 semantic_result: SemanticAnalysisResult) -> Optional[TableContext]:
        """åŸºäºè¯­ä¹‰åˆ†æç»“æœæ„å»ºæœ€ä½³è¡¨ä¸Šä¸‹æ–‡"""
        
        try:
            if not schema_info.get('tables'):
                return None
            
            # æ™ºèƒ½é€‰æ‹©æœ€ä½³è¡¨
            best_table = self._select_best_table(schema_info, semantic_result)
            if not best_table:
                return None
            
            table_info = schema_info['table_details'].get(best_table, {})
            columns = table_info.get("columns", [])
            
            # æå–ä¸»é”®ä¸æ—¥æœŸåˆ—
            primary_keys = []
            date_columns = []
            
            for col in columns:
                col_name = col.get("name", "")
                normalized_type = (col.get("normalized_type") or "").lower()
                raw_type = (col.get("type") or "").lower()
                
                if col.get("primary_key"):
                    primary_keys.append(col_name)
                
                # è¯†åˆ«æ—¥æœŸåˆ—
                if (any(k in col_name.lower() for k in ["date", "dt", "time", "day", "month", "year"]) or
                    any(k in normalized_type for k in ["date", "time"]) or
                    any(k in raw_type for k in ["date", "time", "timestamp"])):
                    date_columns.append(col_name)
            
            # æ„å»ºTableContext - å®‰å…¨å¤„ç† None å€¼
            def safe_float(value, default=0.0):
                """å®‰å…¨åœ°å°†å€¼è½¬æ¢ä¸ºfloatï¼Œå¤„ç†Noneæƒ…å†µ"""
                if value is None:
                    return default
                try:
                    return float(value)
                except (ValueError, TypeError):
                    return default
            
            def safe_int(value, default=0):
                """å®‰å…¨åœ°å°†å€¼è½¬æ¢ä¸ºintï¼Œå¤„ç†Noneæƒ…å†µ"""
                if value is None:
                    return default
                try:
                    return int(value)
                except (ValueError, TypeError):
                    return default
            
            return TableContext(
                table_name=best_table,
                columns=columns,
                business_category=table_info.get("business_category"),
                estimated_rows=safe_int(table_info.get("estimated_row_count"), 0),
                primary_keys=primary_keys,
                date_columns=date_columns,
                data_freshness=table_info.get("data_freshness"),
                update_frequency=table_info.get("update_frequency"),
                data_quality_score=safe_float(table_info.get("data_quality_score"), 0.0),
                completeness_rate=safe_float(table_info.get("completeness_rate"), 0.0),
                accuracy_rate=safe_float(table_info.get("accuracy_rate"), 0.0)
            )
            
        except Exception as e:
            logger.error(f"æ„å»ºè¡¨ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return None
    
    def _select_best_table(self, schema_info: Dict[str, Any], 
                          semantic_result: SemanticAnalysisResult) -> Optional[str]:
        """æ™ºèƒ½é€‰æ‹©æœ€ä½³ç›®æ ‡è¡¨"""
        
        tables = schema_info.get('tables', [])
        if not tables:
            return None
        
        table_details = schema_info.get('table_details', {})
        
        # æ ¹æ®è¯­ä¹‰ç±»å‹è¿›è¡Œè¡¨é€‰æ‹©
        if semantic_result.primary_type.value == "temporal":
            # æ—¶é—´ç›¸å…³æŸ¥è¯¢ï¼Œä¼˜å…ˆé€‰æ‹©æœ‰æ—¶é—´å­—æ®µçš„è¡¨
            for table_name in tables:
                table_info = table_details.get(table_name, {})
                columns = table_info.get("columns", [])
                for col in columns:
                    col_name = col.get("name", "").lower()
                    if any(k in col_name for k in ["date", "time", "year", "dt"]):
                        return table_name
        
        elif semantic_result.primary_type.value == "dimensional":
            # ç»´åº¦æŸ¥è¯¢ï¼Œä¼˜å…ˆé€‰æ‹©æœ‰åœ°åŒºã€åˆ†ç±»å­—æ®µçš„è¡¨
            for table_name in tables:
                table_info = table_details.get(table_name, {})
                columns = table_info.get("columns", [])
                for col in columns:
                    col_name = col.get("name", "").lower()
                    if any(k in col_name for k in ["region", "area", "city", "category", "type"]):
                        return table_name
        
        # ä¼˜å…ˆé€‰æ‹©æ•°æ®è´¨é‡é«˜çš„è¡¨
        best_table = None
        best_score = -1
        
        for table_name in tables:
            table_info = table_details.get(table_name, {})
            # å®‰å…¨åœ°å¤„ç†å¯èƒ½çš„ None å€¼
            quality_score_raw = table_info.get("data_quality_score", 0.0)
            if quality_score_raw is None:
                quality_score = 0.0
            else:
                try:
                    quality_score = float(quality_score_raw)
                except (ValueError, TypeError):
                    quality_score = 0.0
            
            if quality_score > best_score:
                best_score = quality_score
                best_table = table_name
        
        # å¦‚æœæœ‰æ•°æ®è´¨é‡è¯„åˆ†ï¼Œè¿”å›æœ€ä½³è¡¨
        if best_table and best_score > 0:
            return best_table
        
        # é»˜è®¤è¿”å›ç¬¬ä¸€ä¸ªè¡¨
        return tables[0]
    
    async def _get_stored_sql(self, placeholder_id: str) -> Optional[PlaceholderAnalysisResult]:
        """è·å–å·²å­˜å‚¨çš„SQLåˆ†æç»“æœ"""
        
        try:
            from ....models.template_placeholder import TemplatePlaceholder
            
            if not self.db_session:
                return None
                
            placeholder = (
                self.db_session.query(TemplatePlaceholder)
                .filter(TemplatePlaceholder.id == placeholder_id)
                .first()
            )
            
            if placeholder and placeholder.generated_sql:
                # ä»æ•°æ®åº“è®°å½•æ„å»ºç»“æœ
                metadata = placeholder.parsing_metadata or {}
                semantic_analysis = metadata.get('semantic_analysis', {})
                
                return PlaceholderAnalysisResult(
                    success=True,
                    placeholder_id=placeholder_id,
                    generated_sql=placeholder.generated_sql,
                    confidence=metadata.get('confidence', 0.8),
                    semantic_type=semantic_analysis.get('primary_type'),
                    semantic_subtype=semantic_analysis.get('sub_type'),
                    data_intent=semantic_analysis.get('data_intent'),
                    target_table=metadata.get('target_table'),
                    explanation=metadata.get('explanation', 'ä»å­˜å‚¨è®°å½•æ¢å¤'),
                    metadata=metadata,
                    analysis_timestamp=placeholder.analyzed_at or datetime.now()
                )
            
            return None
            
        except Exception as e:
            logger.error(f"è·å–å­˜å‚¨SQLå¤±è´¥: {e}")
            return None
    
    async def _store_analysis_result(self, result: PlaceholderAnalysisResult):
        """å­˜å‚¨åˆ†æç»“æœåˆ°æ•°æ®åº“"""
        
        try:
            from ....models.template_placeholder import TemplatePlaceholder
            
            if not self.db_session or not result.success:
                return
            
            placeholder = (
                self.db_session.query(TemplatePlaceholder)
                .filter(TemplatePlaceholder.id == result.placeholder_id)
                .first()
            )
            
            if placeholder:
                placeholder.generated_sql = result.generated_sql
                placeholder.analyzed_at = result.analysis_timestamp
                placeholder.parsing_metadata = result.metadata
                
                self.db_session.commit()
                
                logger.info(f"ğŸ’¾ åˆ†æç»“æœå·²å­˜å‚¨: {result.placeholder_id}")
            
        except Exception as e:
            logger.error(f"å­˜å‚¨åˆ†æç»“æœå¤±è´¥: {e}")
            if self.db_session:
                self.db_session.rollback()
    
    async def _analyze_with_llm(
        self,
        request: PlaceholderAnalysisRequest,
        schema_info: Dict[str, Any],
        context: AgentContext
    ) -> PlaceholderAnalysisResult:
        """ä½¿ç”¨LLMè¿›è¡Œå¢å¼ºåˆ†æ"""
        
        try:
            # æ„å»ºæ•°æ®æºä¸Šä¸‹æ–‡
            data_source_context = await self._build_data_source_context(
                request.data_source_id, schema_info
            )
            
            # è°ƒç”¨LLMå¢å¼ºåˆ†æå™¨
            enhanced_result = await self.enhanced_analyzer.analyze_placeholder(
                placeholder_text=request.placeholder_text,
                data_source_context=data_source_context,
                template_context=request.template_context
            )
            
            # è½¬æ¢ä¸ºPlaceholderAnalysisResultæ ¼å¼
            analysis_result = PlaceholderAnalysisResult(
                success=True,
                placeholder_id=request.placeholder_id,
                generated_sql=enhanced_result.sql_query,
                confidence=enhanced_result.confidence,
                semantic_type=enhanced_result.primary_type,
                semantic_subtype=enhanced_result.sub_type,
                data_intent=enhanced_result.data_intent,
                target_table=enhanced_result.target_table,
                explanation=enhanced_result.explanation,
                suggestions=enhanced_result.suggestions,
                metadata={
                    'llm_enhanced': True,
                    'analyzer_type': 'llm',
                    'keywords': enhanced_result.keywords,
                    'target_columns': enhanced_result.target_columns,
                    'llm_metadata': enhanced_result.metadata
                }
            )
            
            # å­˜å‚¨ç»“æœåˆ°æ•°æ®åº“ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if request.store_result:
                await self._store_analysis_result(analysis_result)
            
            # æ›´æ–°ä¸Šä¸‹æ–‡
            self.update_context(
                context, 'last_llm_analysis_result', 
                analysis_result.to_dict(), 
                ContextScope.TASK
            )
            
            return analysis_result
            
        except Exception as e:
            logger.error(f"LLMåˆ†æå¤±è´¥: {e}")
            return PlaceholderAnalysisResult(
                success=False,
                placeholder_id=request.placeholder_id,
                error_message=f"LLMåˆ†æå¤±è´¥: {str(e)}",
                error_context={'llm_error': True}
            )
    
    async def _build_data_source_context(
        self, 
        data_source_id: str, 
        schema_info: Dict[str, Any]
    ) -> DataSourceContext:
        """æ„å»ºæ•°æ®æºä¸Šä¸‹æ–‡ä¿¡æ¯"""
        
        try:
            from app import crud
            
            if not self.db_session:
                raise ValueError("æ•°æ®åº“ä¼šè¯æœªåˆå§‹åŒ–")
            
            # è·å–æ•°æ®æºä¿¡æ¯
            data_source = crud.data_source.get(self.db_session, id=data_source_id)
            if not data_source:
                raise ValueError(f"æ•°æ®æº {data_source_id} ä¸å­˜åœ¨")
            
            # æ„å»ºè¿æ¥ä¿¡æ¯
            connection_info = {
                'source_type': data_source.source_type,
                'database': getattr(data_source, 'doris_database', ''),
                'hosts': getattr(data_source, 'doris_fe_hosts', []),
                'query_port': getattr(data_source, 'doris_query_port', 9030)
            }
            
            # æ„å»ºè¡¨ä¿¡æ¯åˆ—è¡¨
            tables_info = []
            for table_name in schema_info.get('tables', []):
                table_detail = schema_info.get('table_details', {}).get(table_name, {})
                
                table_info = {
                    'table_name': table_name,
                    'business_category': table_detail.get('business_category'),
                    'data_quality_score': table_detail.get('data_quality_score'),
                    'estimated_row_count': table_detail.get('estimated_row_count'),
                    'data_freshness': table_detail.get('data_freshness'),
                    'update_frequency': table_detail.get('update_frequency'),
                    'columns': table_detail.get('columns', [])
                }
                tables_info.append(table_info)
            
            return DataSourceContext(
                data_source_id=data_source_id,
                data_source_name=data_source.name,
                data_source_type=data_source.source_type,
                tables=tables_info,
                connection_info=connection_info
            )
            
        except Exception as e:
            logger.error(f"æ„å»ºæ•°æ®æºä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            # è¿”å›æœ€å°åŒ–çš„ä¸Šä¸‹æ–‡
            return DataSourceContext(
                data_source_id=data_source_id,
                data_source_name="unknown",
                data_source_type="unknown",
                tables=[],
                connection_info={}
            )


# åˆ›å»ºå¹¶æ³¨å†ŒAgentçš„å·¥å‚å‡½æ•°
def create_and_register_placeholder_sql_agent(
    db_session: Session = None, 
    user_id: str = None
) -> str:
    """åˆ›å»ºå¹¶æ³¨å†Œå ä½ç¬¦SQLæ„å»ºAgent"""
    
    # åˆ›å»ºAgentå®ä¾‹
    agent = PlaceholderSQLAgent(db_session=db_session, user_id=user_id)
    
    # è·å–æ³¨å†Œè¡¨å¹¶æ³¨å†Œ
    registry = get_agent_registry()
    agent_id = registry.register_agent(agent, priority=1, max_concurrent=3)
    
    logger.info(f"PlaceholderSQLAgent registered with ID: {agent_id}")
    return agent_id


# ä¾¿æ·è°ƒç”¨æ¥å£
class PlaceholderSQLAnalyzer:
    """å ä½ç¬¦SQLåˆ†æå™¨ - æä¾›ç®€å•çš„è°ƒç”¨æ¥å£"""
    
    def __init__(self, db_session: Session = None, user_id: str = None):
        self.agent_id = create_and_register_placeholder_sql_agent(db_session, user_id)
        self.context_manager = get_context_manager()
        self.registry = get_agent_registry()
    
    async def analyze_placeholder(self, 
                                placeholder_id: str,
                                placeholder_text: str, 
                                data_source_id: str,
                                placeholder_type: str = "unknown",
                                template_id: str = None,
                                template_context: Dict[str, Any] = None,
                                force_reanalyze: bool = False) -> PlaceholderAnalysisResult:
        """åˆ†æå•ä¸ªå ä½ç¬¦"""
        
        session_id = f"placeholder_analysis_{placeholder_id}_{datetime.now().timestamp()}"
        
        try:
            result = await self.registry.execute_capability(
                'analyze_placeholder_sql',
                session_id,
                {
                    'placeholder_id': placeholder_id,
                    'placeholder_text': placeholder_text,
                    'placeholder_type': placeholder_type,
                    'data_source_id': data_source_id,
                    'template_id': template_id,
                    'template_context': template_context,
                    'force_reanalyze': force_reanalyze
                }
            )
            
            result_data = result.get('result', {})
            return PlaceholderAnalysisResult(**result_data)
            
        except Exception as e:
            logger.error(f"å ä½ç¬¦åˆ†æå¤±è´¥: {e}")
            return PlaceholderAnalysisResult(
                success=False,
                placeholder_id=placeholder_id,
                error_message=str(e)
            )
    
    async def check_stored_sql(self, placeholder_id: str) -> Dict[str, Any]:
        """æ£€æŸ¥æ˜¯å¦å·²æœ‰å­˜å‚¨çš„SQL"""
        
        session_id = f"check_sql_{placeholder_id}_{datetime.now().timestamp()}"
        
        try:
            result = await self.registry.execute_capability(
                'check_stored_sql',
                session_id,
                {'placeholder_id': placeholder_id}
            )
            
            return result
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥å­˜å‚¨SQLå¤±è´¥: {e}")
            return {
                "has_sql": False,
                "sql": None,
                "error": str(e)
            }
    
    async def batch_analyze(self, requests: List[Dict[str, Any]]) -> List[PlaceholderAnalysisResult]:
        """æ‰¹é‡åˆ†æå ä½ç¬¦"""
        
        session_id = f"batch_analysis_{datetime.now().timestamp()}"
        
        try:
            result = await self.registry.execute_capability(
                'batch_analyze_placeholders',
                session_id,
                {'requests': requests}
            )
            
            results = result.get('results', [])
            return [PlaceholderAnalysisResult(**r) for r in results]
            
        except Exception as e:
            logger.error(f"æ‰¹é‡åˆ†æå¤±è´¥: {e}")
            return [PlaceholderAnalysisResult(
                success=False,
                placeholder_id="batch",
                error_message=str(e)
            )]

    async def analyze_and_execute(self, request) -> Any:
        """åˆ†æå¹¶æ‰§è¡Œå ä½ç¬¦è¯·æ±‚ - å…¼å®¹æ¥å£"""
        from ...domain.placeholder.models import AgentExecutionResult
        
        try:
            # è½¬æ¢è¯·æ±‚å‚æ•°
            result = await self.analyze_placeholder(
                placeholder_id=request.placeholder_id,
                placeholder_text=request.placeholder_name, 
                data_source_id=request.data_source_id,
                placeholder_type=getattr(request, 'placeholder_type', 'unknown'),
                template_id=getattr(request, 'template_id', None),
                template_context=getattr(request, 'metadata', None),
                force_reanalyze=getattr(request, 'force_reanalyze', False)
            )
            
            # è½¬æ¢ç»“æœæ ¼å¼
            if result.success:
                return AgentExecutionResult(
                    success=True,
                    raw_data=result.generated_sql,
                    formatted_value=f"SQL: {result.generated_sql[:100]}..." if result.generated_sql else "æ— SQLç”Ÿæˆ",
                    confidence=result.confidence,
                    execution_time_ms=0,  # è¿™é‡Œæ²¡æœ‰å…·ä½“çš„æ‰§è¡Œæ—¶é—´
                    row_count=0,
                    metadata={
                        'semantic_type': result.semantic_type,
                        'target_table': result.target_table,
                        'explanation': result.explanation,
                        'analysis_metadata': result.metadata
                    },
                    error_message=None,
                    error_context={}
                )
            else:
                return AgentExecutionResult(
                    success=False,
                    raw_data=None,
                    formatted_value="åˆ†æå¤±è´¥",
                    confidence=0.0,
                    execution_time_ms=0,
                    row_count=0,
                    metadata={},
                    error_message=result.error_message,
                    error_context=result.error_context or {}
                )
                
        except Exception as e:
            logger.error(f"analyze_and_execute å¤±è´¥: {e}")
            return AgentExecutionResult(
                success=False,
                raw_data=None,
                formatted_value="æ‰§è¡Œå¼‚å¸¸",
                confidence=0.0,
                execution_time_ms=0,
                row_count=0,
                metadata={},
                error_message=str(e),
                error_context={'exception_type': type(e).__name__}
            )