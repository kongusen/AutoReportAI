"""
Task SQL Execution Agent - ç»Ÿä¸€çš„ä»»åŠ¡SQLæ‰§è¡ŒAgent

å®ç°ä¸å ä½ç¬¦åˆ†æAgentç›¸åŒçš„æ¶æ„æ¨¡å¼ï¼Œç”¨äºä»»åŠ¡SQLæ‰§è¡Œçš„ç»Ÿä¸€ç®¡ç†
"""

import asyncio
import logging
import time
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Tuple
from enum import Enum
from dataclasses import dataclass
from uuid import uuid4
from sqlalchemy.orm import Session

from app.services.data.processing.etl.intelligent_etl_executor import IntelligentETLExecutor
from app.services.data.connectors.connector_factory import create_connector
from app.models.data_source import DataSource
from ..core.intelligent_cache import IntelligentCacheManager
# from ..core.sql_quality_assessor import SQLQualityAssessor  # ä¸´æ—¶æ³¨é‡Šæ‰

logger = logging.getLogger(__name__)


class TaskExecutionMode(Enum):
    """ä»»åŠ¡æ‰§è¡Œæ¨¡å¼"""
    DIRECT_SQL = "direct_sql"                    # ç›´æ¥SQLæ‰§è¡Œ
    CACHED_EXECUTION = "cached_execution"        # ç¼“å­˜ä¼˜å…ˆæ‰§è¡Œ
    INTELLIGENT_ANALYSIS = "intelligent_analysis" # æ™ºèƒ½åˆ†ææ‰§è¡Œ
    RECOVERY_MODE = "recovery_mode"              # æ•…éšœæ¢å¤æ¨¡å¼
    HYBRID_MODE = "hybrid_mode"                  # æ··åˆæ¨¡å¼


class ExecutionPriority(Enum):
    """æ‰§è¡Œä¼˜å…ˆçº§"""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


@dataclass
class TaskExecutionContext:
    """ä»»åŠ¡æ‰§è¡Œä¸Šä¸‹æ–‡"""
    task_id: int
    etl_instruction: Dict[str, Any]
    data_source_id: str
    execution_mode: TaskExecutionMode = TaskExecutionMode.DIRECT_SQL
    priority: ExecutionPriority = ExecutionPriority.MEDIUM
    retry_count: int = 0
    max_retries: int = 3
    timeout_seconds: int = 300
    enable_cache: bool = True
    enable_recovery: bool = True
    agent_id: str = None
    
    def __post_init__(self):
        if self.agent_id is None:
            self.agent_id = str(uuid4())


@dataclass
class ExecutionResult:
    """æ‰§è¡Œç»“æœ"""
    success: bool
    data: Any = None
    execution_time: float = 0.0
    mode_used: TaskExecutionMode = None
    cache_hit: bool = False
    error_message: str = None
    retry_count: int = 0
    metadata: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


class TaskSQLExecutionAgent:
    """
    ç»Ÿä¸€çš„ä»»åŠ¡SQLæ‰§è¡ŒAgent
    
    æä¾›ä¸å ä½ç¬¦åˆ†æAgentç›¸åŒæ¶æ„çš„ä»»åŠ¡æ‰§è¡ŒæœåŠ¡
    """
    
    def __init__(self, db: Session, config: Dict[str, Any] = None):
        self.db = db
        self.config = config or {}
        self.agent_id = str(uuid4())
        
        # ç»„ä»¶åˆå§‹åŒ–
        self.etl_executor = IntelligentETLExecutor(db)
        self.cache_manager = IntelligentCacheManager(db_session=db)
        # self.quality_assessor = SQLQualityAssessor(db_session=db)  # ä¸´æ—¶æ³¨é‡Šæ‰
        
        # é…ç½®å‚æ•°
        self.enable_intelligent_analysis = config.get('enable_intelligent_analysis', True)
        self.enable_cache_optimization = config.get('enable_cache_optimization', True)
        self.enable_recovery_mode = config.get('enable_recovery_mode', True)
        self.default_timeout = config.get('default_timeout', 300)
        
        logger.info(f"TaskSQLExecutionAgentåˆå§‹åŒ–å®Œæˆ: {self.agent_id}")
    
    async def execute_task(self, context: TaskExecutionContext) -> ExecutionResult:
        """
        æ‰§è¡Œä»»åŠ¡SQL
        
        Args:
            context: ä»»åŠ¡æ‰§è¡Œä¸Šä¸‹æ–‡
            
        Returns:
            æ‰§è¡Œç»“æœ
        """
        start_time = time.time()
        
        try:
            logger.info(f"ğŸš€ å¼€å§‹ä»»åŠ¡SQLæ‰§è¡Œ: task_id={context.task_id}, mode={context.execution_mode.value}")
            
            # 1. æ‰§è¡Œæ¨¡å¼æ™ºèƒ½åˆ¤æ–­
            optimal_mode = await self._determine_execution_mode(context)
            if optimal_mode != context.execution_mode:
                logger.info(f"ğŸ“‹ æ‰§è¡Œæ¨¡å¼ä¼˜åŒ–: {context.execution_mode.value} â†’ {optimal_mode.value}")
                context.execution_mode = optimal_mode
            
            # 2. æ ¹æ®æ¨¡å¼æ‰§è¡Œä»»åŠ¡
            result = await self._execute_by_mode(context)
            
            # 3. ç»“æœåå¤„ç†
            result = await self._post_process_result(result, context)
            
            execution_time = time.time() - start_time
            result.execution_time = execution_time
            result.mode_used = context.execution_mode
            
            logger.info(f"âœ… ä»»åŠ¡SQLæ‰§è¡Œå®Œæˆ: {execution_time:.2f}s, mode={context.execution_mode.value}")
            return result
            
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"âŒ ä»»åŠ¡SQLæ‰§è¡Œå¤±è´¥: {e}")
            
            # é”™è¯¯æ¢å¤
            if context.enable_recovery and context.retry_count < context.max_retries:
                return await self._handle_execution_failure(e, context)
            
            return ExecutionResult(
                success=False,
                error_message=str(e),
                execution_time=execution_time,
                mode_used=context.execution_mode,
                retry_count=context.retry_count
            )
    
    async def _determine_execution_mode(self, context: TaskExecutionContext) -> TaskExecutionMode:
        """æ™ºèƒ½ç¡®å®šæ‰§è¡Œæ¨¡å¼"""
        try:
            etl_instruction = context.etl_instruction
            
            # 1. æ£€æŸ¥ç¼“å­˜å¯ç”¨æ€§
            if context.enable_cache:
                # ç®€åŒ–ç¼“å­˜æ£€æŸ¥é€»è¾‘ï¼ˆä¸´æ—¶å®ç°ï¼‰
                cache_status = {'available': False, 'confidence': 0.0}
                if cache_status['available'] and cache_status['confidence'] > 0.8:
                    logger.info("ğŸ’¾ æ£€æµ‹åˆ°é«˜è´¨é‡ç¼“å­˜ï¼Œä½¿ç”¨ç¼“å­˜æ‰§è¡Œæ¨¡å¼")
                    return TaskExecutionMode.CACHED_EXECUTION
            
            # 2. SQLå¤æ‚åº¦åˆ†æ
            if self.enable_intelligent_analysis:
                complexity = await self._analyze_sql_complexity(etl_instruction)
                if complexity['level'] == 'high':
                    logger.info("ğŸ§  æ£€æµ‹åˆ°å¤æ‚SQLï¼Œä½¿ç”¨æ™ºèƒ½åˆ†ææ¨¡å¼")
                    return TaskExecutionMode.INTELLIGENT_ANALYSIS
            
            # 3. æ•°æ®æºçŠ¶æ€æ£€æŸ¥
            data_source_health = await self._check_data_source_health(context.data_source_id)
            if not data_source_health['healthy']:
                logger.warning("âš ï¸ æ•°æ®æºçŠ¶æ€å¼‚å¸¸ï¼Œä½¿ç”¨æ¢å¤æ¨¡å¼")
                return TaskExecutionMode.RECOVERY_MODE
            
            # 4. é»˜è®¤ä½¿ç”¨ç›´æ¥æ‰§è¡Œ
            return TaskExecutionMode.DIRECT_SQL
            
        except Exception as e:
            logger.warning(f"æ‰§è¡Œæ¨¡å¼åˆ¤æ–­å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å¼: {e}")
            return TaskExecutionMode.DIRECT_SQL
    
    async def _execute_by_mode(self, context: TaskExecutionContext) -> ExecutionResult:
        """æ ¹æ®æ‰§è¡Œæ¨¡å¼æ‰§è¡Œä»»åŠ¡"""
        mode_handlers = {
            TaskExecutionMode.DIRECT_SQL: self._execute_direct_sql,
            TaskExecutionMode.CACHED_EXECUTION: self._execute_cached,
            TaskExecutionMode.INTELLIGENT_ANALYSIS: self._execute_intelligent_analysis,
            TaskExecutionMode.RECOVERY_MODE: self._execute_recovery_mode,
            TaskExecutionMode.HYBRID_MODE: self._execute_hybrid_mode
        }
        
        handler = mode_handlers.get(context.execution_mode, self._execute_direct_sql)
        return await handler(context)
    
    async def _execute_direct_sql(self, context: TaskExecutionContext) -> ExecutionResult:
        """ç›´æ¥SQLæ‰§è¡Œæ¨¡å¼"""
        logger.info("ğŸ”§ æ‰§è¡Œç›´æ¥SQLæ¨¡å¼")
        
        try:
            # ä½¿ç”¨ç°æœ‰çš„ETLæ‰§è¡Œå™¨
            result = self.etl_executor.execute_instruction(
                context.etl_instruction, 
                context.data_source_id
            )
            
            # ç¼“å­˜ç»“æœï¼ˆä¸´æ—¶ç®€åŒ–å®ç°ï¼‰
            if context.enable_cache:
                try:
                    # ç®€åŒ–çš„ç¼“å­˜é€»è¾‘
                    logger.debug("ç¼“å­˜ç»“æœå·²è®°å½•ï¼ˆç®€åŒ–å®ç°ï¼‰")
                except Exception as cache_error:
                    logger.warning(f"ç¼“å­˜å­˜å‚¨å¤±è´¥: {cache_error}")
            
            return ExecutionResult(
                success=True,
                data=result,
                cache_hit=False
            )
            
        except Exception as e:
            logger.error(f"ç›´æ¥SQLæ‰§è¡Œå¤±è´¥: {e}")
            raise
    
    async def _execute_cached(self, context: TaskExecutionContext) -> ExecutionResult:
        """ç¼“å­˜æ‰§è¡Œæ¨¡å¼"""
        logger.info("ğŸ’¾ æ‰§è¡Œç¼“å­˜æ¨¡å¼")
        
        try:
            # å°è¯•ä»ç¼“å­˜è·å–ç»“æœï¼ˆä¸´æ—¶ç®€åŒ–å®ç°ï¼‰
            cached_result = None  # ç®€åŒ–å®ç°
            
            if cached_result:
                logger.info("âœ… ç¼“å­˜å‘½ä¸­")
                return ExecutionResult(
                    success=True,
                    data=cached_result['data'],
                    cache_hit=True,
                    metadata=cached_result.get('metadata', {})
                )
            
            # ç¼“å­˜æœªå‘½ä¸­ï¼Œå›é€€åˆ°ç›´æ¥æ‰§è¡Œ
            logger.info("âŒ ç¼“å­˜æœªå‘½ä¸­ï¼Œå›é€€åˆ°ç›´æ¥æ‰§è¡Œ")
            context.execution_mode = TaskExecutionMode.DIRECT_SQL
            return await self._execute_direct_sql(context)
            
        except Exception as e:
            logger.error(f"ç¼“å­˜æ‰§è¡Œå¤±è´¥: {e}")
            # å›é€€åˆ°ç›´æ¥æ‰§è¡Œ
            context.execution_mode = TaskExecutionMode.DIRECT_SQL
            return await self._execute_direct_sql(context)
    
    async def _execute_intelligent_analysis(self, context: TaskExecutionContext) -> ExecutionResult:
        """æ™ºèƒ½åˆ†ææ‰§è¡Œæ¨¡å¼"""
        logger.info("ğŸ§  æ‰§è¡Œæ™ºèƒ½åˆ†ææ¨¡å¼")
        
        try:
            # 1. SQLè´¨é‡è¯„ä¼°ï¼ˆä¸´æ—¶ç®€åŒ–å®ç°ï¼‰
            quality_report = {
                'optimization_suggestions': [],
                'overall_score': 80.0,
                'issues_found': []
            }
            
            # 2. æ ¹æ®è´¨é‡æŠ¥å‘Šä¼˜åŒ–æ‰§è¡Œç­–ç•¥
            if quality_report['optimization_suggestions']:
                logger.info("ğŸ”§ åº”ç”¨SQLä¼˜åŒ–å»ºè®®")
                optimized_instruction = await self._apply_optimizations(
                    context.etl_instruction,
                    quality_report['optimization_suggestions']
                )
                context.etl_instruction = optimized_instruction
            
            # 3. æ‰§è¡Œä¼˜åŒ–åçš„æŒ‡ä»¤
            result = self.etl_executor.execute_instruction(
                context.etl_instruction,
                context.data_source_id
            )
            
            return ExecutionResult(
                success=True,
                data=result,
                cache_hit=False,
                metadata={
                    'quality_report': quality_report,
                    'optimizations_applied': len(quality_report['optimization_suggestions'])
                }
            )
            
        except Exception as e:
            logger.error(f"æ™ºèƒ½åˆ†ææ‰§è¡Œå¤±è´¥: {e}")
            # å›é€€åˆ°ç›´æ¥æ‰§è¡Œ
            context.execution_mode = TaskExecutionMode.DIRECT_SQL
            return await self._execute_direct_sql(context)
    
    async def _execute_recovery_mode(self, context: TaskExecutionContext) -> ExecutionResult:
        """æ•…éšœæ¢å¤æ‰§è¡Œæ¨¡å¼"""
        logger.info("ğŸ› ï¸ æ‰§è¡Œæ•…éšœæ¢å¤æ¨¡å¼")
        
        try:
            # 1. å°è¯•ç¼“å­˜æ¢å¤ï¼ˆä¸´æ—¶ç®€åŒ–å®ç°ï¼‰
            cached_result = None  # ç®€åŒ–å®ç°
            
            if cached_result:
                logger.info("ğŸ’¾ ä½¿ç”¨è¿‡æœŸç¼“å­˜æ•°æ®æ¢å¤")
                return ExecutionResult(
                    success=True,
                    data=cached_result['data'],
                    cache_hit=True,
                    metadata={
                        'recovery_method': 'stale_cache',
                        'cache_age': cached_result.get('age_hours', 0)
                    }
                )
            
            # 2. å°è¯•å†å²æ•°æ®æ¢å¤
            historical_data = await self._get_historical_data(context)
            if historical_data:
                logger.info("ğŸ“š ä½¿ç”¨å†å²æ•°æ®æ¢å¤")
                return ExecutionResult(
                    success=True,
                    data=historical_data,
                    cache_hit=False,
                    metadata={'recovery_method': 'historical_data'}
                )
            
            # 3. æœ€å°åŒ–åŠŸèƒ½æ‰§è¡Œ
            minimal_result = await self._execute_minimal_functionality(context)
            return ExecutionResult(
                success=True,
                data=minimal_result,
                cache_hit=False,
                metadata={'recovery_method': 'minimal_functionality'}
            )
            
        except Exception as e:
            logger.error(f"æ•…éšœæ¢å¤æ‰§è¡Œå¤±è´¥: {e}")
            return ExecutionResult(
                success=False,
                error_message=f"æ‰€æœ‰æ¢å¤å°è¯•å¤±è´¥: {str(e)}",
                metadata={'recovery_method': 'failed'}
            )
    
    async def _execute_hybrid_mode(self, context: TaskExecutionContext) -> ExecutionResult:
        """æ··åˆæ‰§è¡Œæ¨¡å¼"""
        logger.info("ğŸ”€ æ‰§è¡Œæ··åˆæ¨¡å¼")
        
        try:
            # æ ¹æ®æ•°æ®ç‰¹ç‚¹é€‰æ‹©æœ€ä½³ç­–ç•¥ç»„åˆ
            strategies = await self._determine_hybrid_strategies(context)
            
            results = []
            for strategy in strategies:
                try:
                    temp_context = context
                    temp_context.execution_mode = strategy
                    result = await self._execute_by_mode(temp_context)
                    if result.success:
                        results.append(result)
                except Exception as e:
                    logger.warning(f"æ··åˆç­–ç•¥{strategy.value}å¤±è´¥: {e}")
            
            if results:
                # é€‰æ‹©æœ€ä½³ç»“æœ
                best_result = max(results, key=lambda r: self._calculate_result_score(r))
                best_result.metadata['hybrid_strategies_tried'] = len(strategies)
                return best_result
            
            # æ‰€æœ‰ç­–ç•¥å¤±è´¥ï¼Œå›é€€åˆ°ç›´æ¥æ‰§è¡Œ
            context.execution_mode = TaskExecutionMode.DIRECT_SQL
            return await self._execute_direct_sql(context)
            
        except Exception as e:
            logger.error(f"æ··åˆæ¨¡å¼æ‰§è¡Œå¤±è´¥: {e}")
            context.execution_mode = TaskExecutionMode.DIRECT_SQL
            return await self._execute_direct_sql(context)
    
    async def _handle_execution_failure(self, 
                                      error: Exception, 
                                      context: TaskExecutionContext) -> ExecutionResult:
        """å¤„ç†æ‰§è¡Œå¤±è´¥"""
        logger.warning(f"ğŸ”„ å¤„ç†æ‰§è¡Œå¤±è´¥ï¼Œé‡è¯• {context.retry_count + 1}/{context.max_retries}")
        
        # å¢åŠ é‡è¯•è®¡æ•°
        context.retry_count += 1
        
        # æ ¹æ®é”™è¯¯ç±»å‹é€‰æ‹©æ¢å¤ç­–ç•¥
        error_type = type(error).__name__
        
        if 'Connection' in error_type or 'Network' in error_type:
            # ç½‘ç»œé—®é¢˜ï¼Œå°è¯•ç¼“å­˜æ‰§è¡Œ
            context.execution_mode = TaskExecutionMode.CACHED_EXECUTION
        elif 'Timeout' in error_type:
            # è¶…æ—¶é—®é¢˜ï¼Œä½¿ç”¨æ¢å¤æ¨¡å¼
            context.execution_mode = TaskExecutionMode.RECOVERY_MODE
        else:
            # å…¶ä»–é—®é¢˜ï¼Œå°è¯•æ™ºèƒ½åˆ†æ
            context.execution_mode = TaskExecutionMode.INTELLIGENT_ANALYSIS
        
        # å»¶è¿Ÿé‡è¯•
        await asyncio.sleep(2 ** context.retry_count)
        
        return await self.execute_task(context)
    
    # è¾…åŠ©æ–¹æ³•
    
    async def _analyze_sql_complexity(self, etl_instruction: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æSQLå¤æ‚åº¦"""
        sql = etl_instruction.get('sql_query', '')
        if not sql:
            return {'level': 'low', 'score': 0.1}
        
        # ç®€å•çš„å¤æ‚åº¦è¯„ä¼°
        complexity_score = 0.0
        
        # æ£€æŸ¥JOIN
        if 'JOIN' in sql.upper():
            complexity_score += 0.3
        
        # æ£€æŸ¥å­æŸ¥è¯¢
        if '(' in sql and 'SELECT' in sql[sql.find('('):]:
            complexity_score += 0.4
        
        # æ£€æŸ¥èšåˆå‡½æ•°
        aggregates = ['COUNT', 'SUM', 'AVG', 'MIN', 'MAX', 'GROUP BY']
        for agg in aggregates:
            if agg in sql.upper():
                complexity_score += 0.2
                break
        
        level = 'high' if complexity_score > 0.6 else 'medium' if complexity_score > 0.3 else 'low'
        
        return {'level': level, 'score': complexity_score}
    
    async def _check_data_source_health(self, data_source_id: str) -> Dict[str, Any]:
        """æ£€æŸ¥æ•°æ®æºå¥åº·çŠ¶æ€"""
        try:
            data_source = self.db.query(DataSource).filter(
                DataSource.id == data_source_id
            ).first()
            
            if not data_source:
                return {'healthy': False, 'reason': 'data_source_not_found'}
            
            # ç®€å•çš„è¿æ¥æµ‹è¯•
            connector = create_connector(data_source)
            await connector.connect()
            await connector.disconnect()
            
            return {'healthy': True, 'last_check': datetime.now()}
            
        except Exception as e:
            return {'healthy': False, 'reason': str(e)}
    
    async def _post_process_result(self, 
                                 result: ExecutionResult, 
                                 context: TaskExecutionContext) -> ExecutionResult:
        """ç»“æœåå¤„ç†"""
        # æ·»åŠ æ‰§è¡Œå…ƒæ•°æ®
        result.metadata.update({
            'agent_id': self.agent_id,
            'task_id': context.task_id,
            'execution_timestamp': datetime.now().isoformat(),
            'data_source_id': context.data_source_id
        })
        
        return result
    
    async def _apply_optimizations(self, 
                                 instruction: Dict[str, Any], 
                                 suggestions: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åº”ç”¨ä¼˜åŒ–å»ºè®®"""
        # ç®€å•çš„ä¼˜åŒ–åº”ç”¨é€»è¾‘
        optimized = instruction.copy()
        
        for suggestion in suggestions:
            if suggestion['type'] == 'add_limit' and 'LIMIT' not in optimized.get('sql_query', '').upper():
                optimized['sql_query'] = f"{optimized['sql_query']} LIMIT 1000"
        
        return optimized
    
    async def _get_historical_data(self, context: TaskExecutionContext) -> Any:
        """è·å–å†å²æ•°æ®"""
        # å®ç°å†å²æ•°æ®è·å–é€»è¾‘
        return None
    
    async def _execute_minimal_functionality(self, context: TaskExecutionContext) -> Any:
        """æ‰§è¡Œæœ€å°åŒ–åŠŸèƒ½"""
        # è¿”å›åŸºæœ¬çš„ç©ºç»“æœ
        return {'message': 'æœ€å°åŒ–åŠŸèƒ½æ‰§è¡Œ', 'data': []}
    
    async def _determine_hybrid_strategies(self, 
                                         context: TaskExecutionContext) -> List[TaskExecutionMode]:
        """ç¡®å®šæ··åˆç­–ç•¥"""
        return [TaskExecutionMode.CACHED_EXECUTION, TaskExecutionMode.DIRECT_SQL]
    
    def _calculate_result_score(self, result: ExecutionResult) -> float:
        """è®¡ç®—ç»“æœåˆ†æ•°"""
        score = 0.0
        
        if result.success:
            score += 1.0
        
        if result.cache_hit:
            score += 0.5  # ç¼“å­˜å‘½ä¸­æœ‰é¢å¤–åˆ†æ•°
        
        # æ‰§è¡Œæ—¶é—´è¶ŠçŸ­åˆ†æ•°è¶Šé«˜
        if result.execution_time < 1.0:
            score += 0.3
        elif result.execution_time < 5.0:
            score += 0.1
        
        return score


# ä¾¿æ·å‡½æ•°

async def execute_task_sql(db: Session,
                          task_id: int,
                          etl_instruction: Dict[str, Any],
                          data_source_id: str,
                          config: Dict[str, Any] = None) -> ExecutionResult:
    """æ‰§è¡Œä»»åŠ¡SQLçš„ä¾¿æ·å‡½æ•°"""
    
    agent = TaskSQLExecutionAgent(db, config)
    context = TaskExecutionContext(
        task_id=task_id,
        etl_instruction=etl_instruction,
        data_source_id=data_source_id
    )
    
    return await agent.execute_task(context)