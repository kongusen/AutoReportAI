"""
AIé›†æˆæœåŠ¡
ä½¿ç”¨é…ç½®çš„AIä¾›åº”å•†è¿›è¡Œæ™ºèƒ½åˆ†æå’ŒæŠ¥å‘Šç”Ÿæˆ
"""

import asyncio
import json
import logging
import time
from typing import Any, Dict, List, Optional, Union
from dataclasses import dataclass

import aiohttp
import requests
from tenacity import retry, stop_after_attempt, wait_exponential

from ..config.ai_provider_config import get_ai_config

logger = logging.getLogger(__name__)

@dataclass
class LLMRequest:
    """LLMè¯·æ±‚æ•°æ®ç±»"""
    prompt: str
    system_prompt: Optional[str] = None
    max_tokens: Optional[int] = None
    temperature: Optional[float] = None
    model: Optional[str] = None

@dataclass 
class LLMResponse:
    """LLMå“åº”æ•°æ®ç±»"""
    content: str
    usage: Dict[str, int]
    model: str
    response_time: float
    success: bool
    error: Optional[str] = None

class AIService:
    """AIæœåŠ¡é›†æˆç±»"""
    
    def __init__(self, scenario: str = "default"):
        self.config = get_ai_config(scenario)
        self.session = None
        
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        connector = aiohttp.TCPConnector(
            verify_ssl=self.config.get('verify_ssl', True),
            timeout=aiohttp.ClientTimeout(total=self.config.get('timeout', 60))
        )
        self.session = aiohttp.ClientSession(connector=connector)
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        if self.session:
            await self.session.close()
    
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
    async def call_llm_async(self, request: LLMRequest) -> LLMResponse:
        """å¼‚æ­¥è°ƒç”¨LLM"""
        if not self.session:
            raise RuntimeError("AIService must be used as async context manager")
        
        # æ„å»ºæ¶ˆæ¯
        messages = []
        if request.system_prompt:
            messages.append({"role": "system", "content": request.system_prompt})
        messages.append({"role": "user", "content": request.prompt})
        
        # æ„å»ºè¯·æ±‚payload
        payload = {
            "model": request.model or self.config["model"],
            "messages": messages,
            "max_tokens": request.max_tokens or self.config["max_tokens"],
            "temperature": request.temperature or self.config["temperature"],
            "stream": self.config.get("stream", False)
        }
        
        headers = {
            "Authorization": f"Bearer {self.config['api_key']}",
            "Content-Type": "application/json"
        }
        
        start_time = time.time()
        
        try:
            async with self.session.post(
                self.config["api_base_url"],
                json=payload,
                headers=headers
            ) as response:
                
                response_time = time.time() - start_time
                
                if response.status == 200:
                    data = await response.json()
                    
                    return LLMResponse(
                        content=data["choices"][0]["message"]["content"],
                        usage=data.get("usage", {}),
                        model=data.get("model", payload["model"]),
                        response_time=response_time,
                        success=True
                    )
                else:
                    error_text = await response.text()
                    logger.error(f"LLM APIé”™è¯¯ {response.status}: {error_text}")
                    
                    return LLMResponse(
                        content="",
                        usage={},
                        model=payload["model"],
                        response_time=response_time,
                        success=False,
                        error=f"HTTP {response.status}: {error_text}"
                    )
                    
        except asyncio.TimeoutError:
            return LLMResponse(
                content="",
                usage={},
                model=payload["model"],
                response_time=time.time() - start_time,
                success=False,
                error="Request timeout"
            )
        except Exception as e:
            logger.error(f"LLMè°ƒç”¨å¼‚å¸¸: {e}")
            return LLMResponse(
                content="",
                usage={},
                model=payload["model"],
                response_time=time.time() - start_time,
                success=False,
                error=str(e)
            )
    
    def call_llm_sync(self, request: LLMRequest) -> LLMResponse:
        """åŒæ­¥è°ƒç”¨LLMï¼ˆç”¨äºéå¼‚æ­¥ç¯å¢ƒï¼‰"""
        # æ„å»ºæ¶ˆæ¯
        messages = []
        if request.system_prompt:
            messages.append({"role": "system", "content": request.system_prompt})
        messages.append({"role": "user", "content": request.prompt})
        
        # æ„å»ºè¯·æ±‚payload
        payload = {
            "model": request.model or self.config["model"],
            "messages": messages,
            "max_tokens": request.max_tokens or self.config["max_tokens"],
            "temperature": request.temperature or self.config["temperature"]
        }
        
        headers = {
            "Authorization": f"Bearer {self.config['api_key']}",
            "Content-Type": "application/json"
        }
        
        start_time = time.time()
        
        try:
            response = requests.post(
                self.config["api_base_url"],
                json=payload,
                headers=headers,
                timeout=self.config.get("timeout", 60),
                verify=self.config.get('verify_ssl', True)
            )
            
            response_time = time.time() - start_time
            
            if response.status_code == 200:
                data = response.json()
                
                return LLMResponse(
                    content=data["choices"][0]["message"]["content"],
                    usage=data.get("usage", {}),
                    model=data.get("model", payload["model"]),
                    response_time=response_time,
                    success=True
                )
            else:
                logger.error(f"LLM APIé”™è¯¯ {response.status_code}: {response.text}")
                
                return LLMResponse(
                    content="",
                    usage={},
                    model=payload["model"],
                    response_time=response_time,
                    success=False,
                    error=f"HTTP {response.status_code}: {response.text}"
                )
                
        except requests.exceptions.Timeout:
            return LLMResponse(
                content="",
                usage={},
                model=payload["model"],
                response_time=time.time() - start_time,
                success=False,
                error="Request timeout"
            )
        except Exception as e:
            logger.error(f"LLMè°ƒç”¨å¼‚å¸¸: {e}")
            return LLMResponse(
                content="",
                usage={},
                model=payload["model"],
                response_time=time.time() - start_time,
                success=False,
                error=str(e)
            )

class BigDataAIAnalyzer:
    """å¤§æ•°æ®AIåˆ†æå™¨ - ä¸“é—¨ç”¨äºå¤§æ•°æ®åˆ†æåœºæ™¯"""
    
    def __init__(self):
        self.ai_service = AIService("data_analysis")
    
    async def analyze_placeholder_requirements(self, template: str, placeholders: List[Dict]) -> Dict[str, Any]:
        """åˆ†æå ä½ç¬¦éœ€æ±‚"""
        
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªå¤§æ•°æ®åˆ†æä¸“å®¶ï¼Œä¸“é—¨åˆ†ææŠ¥å‘Šæ¨¡æ¿ä¸­çš„å ä½ç¬¦éœ€æ±‚ã€‚
        
ä½ éœ€è¦ï¼š
1. åˆ†ææ¯ä¸ªå ä½ç¬¦çš„æ•°æ®éœ€æ±‚
2. ç¡®å®šéœ€è¦çš„æ•°æ®è¡¨å’Œå­—æ®µ
3. åˆ¤æ–­èšåˆè®¡ç®—ç±»å‹
4. è¯„ä¼°æŸ¥è¯¢å¤æ‚åº¦
5. æä¾›ä¼˜åŒ–å»ºè®®

è¯·ç”¨JSONæ ¼å¼è¿”å›ç»“æ„åŒ–çš„åˆ†æç»“æœã€‚"""
        
        user_prompt = f"""
è¯·åˆ†æä»¥ä¸‹æŠ¥å‘Šæ¨¡æ¿ä¸­çš„å ä½ç¬¦éœ€æ±‚ï¼š

æ¨¡æ¿å†…å®¹ï¼š
{template}

å ä½ç¬¦åˆ—è¡¨ï¼š
{json.dumps(placeholders, ensure_ascii=False, indent=2)}

è¯·åˆ†ææ¯ä¸ªå ä½ç¬¦çš„ï¼š
1. æ•°æ®éœ€æ±‚ï¼ˆéœ€è¦ä»€ä¹ˆæ•°æ®ï¼‰
2. è®¡ç®—ç±»å‹ï¼ˆç»Ÿè®¡ã€èšåˆã€æ—¶é—´åˆ†æç­‰ï¼‰
3. å¤æ‚åº¦è¯„ä¼°
4. å»ºè®®çš„SQLæŸ¥è¯¢ç­–ç•¥

è¿”å›JSONæ ¼å¼çš„åˆ†æç»“æœã€‚
"""
        
        request = LLMRequest(
            prompt=user_prompt,
            system_prompt=system_prompt,
            max_tokens=3000,
            temperature=0.1
        )
        
        async with self.ai_service as ai:
            response = await ai.call_llm_async(request)
            
            if response.success:
                try:
                    analysis = json.loads(response.content)
                    return {
                        "success": True,
                        "analysis": analysis,
                        "response_time": response.response_time,
                        "token_usage": response.usage
                    }
                except json.JSONDecodeError:
                    return {
                        "success": False,
                        "error": "AIè¿”å›çš„å†…å®¹ä¸æ˜¯æœ‰æ•ˆJSON",
                        "raw_content": response.content
                    }
            else:
                return {
                    "success": False,
                    "error": response.error
                }
    
    async def generate_sql_queries(self, placeholder_requirements: Dict, table_schemas: Dict) -> Dict[str, Any]:
        """æ ¹æ®å ä½ç¬¦éœ€æ±‚ç”ŸæˆSQLæŸ¥è¯¢"""
        
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªSQLä¸“å®¶ï¼Œä¸“é—¨ä¸ºå¤§æ•°æ®åˆ†æç”Ÿæˆé«˜æ•ˆçš„SQLæŸ¥è¯¢ã€‚

ä½ éœ€è¦ï¼š
1. æ ¹æ®å ä½ç¬¦éœ€æ±‚ç”Ÿæˆå¯¹åº”çš„SQLæŸ¥è¯¢
2. ä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½ï¼ˆä½¿ç”¨ç´¢å¼•ã€å‡å°‘JOINç­‰ï¼‰
3. è€ƒè™‘å¤§æ•°æ®åœºæ™¯ä¸‹çš„æŸ¥è¯¢ä¼˜åŒ–
4. æä¾›æ‰¹å¤„ç†å»ºè®®

è¯·ç”Ÿæˆå¯æ‰§è¡Œçš„SQLæŸ¥è¯¢ï¼Œå¹¶ç”¨JSONæ ¼å¼è¿”å›ã€‚"""
        
        user_prompt = f"""
åŸºäºä»¥ä¸‹ä¿¡æ¯ç”ŸæˆSQLæŸ¥è¯¢ï¼š

å ä½ç¬¦éœ€æ±‚ï¼š
{json.dumps(placeholder_requirements, ensure_ascii=False, indent=2)}

æ•°æ®åº“è¡¨ç»“æ„ï¼š
{json.dumps(table_schemas, ensure_ascii=False, indent=2)}

è¯·ä¸ºæ¯ä¸ªå ä½ç¬¦ç”Ÿæˆå¯¹åº”çš„SQLæŸ¥è¯¢ï¼Œè¦æ±‚ï¼š
1. æŸ¥è¯¢è¦é«˜æ•ˆï¼ˆé€‚åˆå¤§æ•°æ®åœºæ™¯ï¼‰
2. åŒ…å«å¿…è¦çš„WHEREæ¡ä»¶å’ŒGROUP BY
3. è€ƒè™‘ä½¿ç”¨LIMITå‡å°‘æ•°æ®é‡
4. æä¾›æŸ¥è¯¢å¤æ‚åº¦è¯„ä¼°

è¿”å›JSONæ ¼å¼ï¼ŒåŒ…å«æ¯ä¸ªå ä½ç¬¦å¯¹åº”çš„SQLæŸ¥è¯¢ã€‚
"""
        
        request = LLMRequest(
            prompt=user_prompt,
            system_prompt=system_prompt,
            max_tokens=4000,
            temperature=0.2
        )
        
        async with self.ai_service as ai:
            response = await ai.call_llm_async(request)
            
            if response.success:
                try:
                    queries = json.loads(response.content)
                    return {
                        "success": True,
                        "queries": queries,
                        "response_time": response.response_time,
                        "token_usage": response.usage
                    }
                except json.JSONDecodeError:
                    return {
                        "success": False,
                        "error": "AIç”Ÿæˆçš„SQLæŸ¥è¯¢æ ¼å¼é”™è¯¯",
                        "raw_content": response.content
                    }
            else:
                return {
                    "success": False,
                    "error": response.error
                }
    
    async def optimize_report_template(self, template: str, data_analysis_results: Dict) -> Dict[str, Any]:
        """æ ¹æ®æ•°æ®åˆ†æç»“æœä¼˜åŒ–æŠ¥å‘Šæ¨¡æ¿"""
        
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªæŠ¥å‘Šä¼˜åŒ–ä¸“å®¶ï¼Œä¸“é—¨ä¼˜åŒ–å¤§æ•°æ®åˆ†ææŠ¥å‘Šçš„å±•ç¤ºæ•ˆæœã€‚

ä½ éœ€è¦ï¼š
1. æ ¹æ®æ•°æ®åˆ†æç»“æœä¼˜åŒ–æŠ¥å‘Šç»“æ„
2. æ”¹è¿›æ•°æ®å±•ç¤ºæ–¹å¼
3. å¢åŠ æœ‰ä»·å€¼çš„æ´å¯Ÿå’Œå»ºè®®
4. ç¡®ä¿æŠ¥å‘Šå†…å®¹å‡†ç¡®ä¸”æœ‰ä»·å€¼

è¯·è¿”å›ä¼˜åŒ–åçš„æŠ¥å‘Šæ¨¡æ¿ã€‚"""
        
        user_prompt = f"""
è¯·ä¼˜åŒ–ä»¥ä¸‹æŠ¥å‘Šæ¨¡æ¿ï¼š

åŸå§‹æ¨¡æ¿ï¼š
{template}

æ•°æ®åˆ†æç»“æœï¼š
{json.dumps(data_analysis_results, ensure_ascii=False, indent=2)}

è¯·ä¼˜åŒ–ï¼š
1. æ”¹è¿›æ•°æ®å±•ç¤ºç»“æ„
2. æ·»åŠ æ•°æ®æ´å¯Ÿ
3. å®Œå–„å›¾è¡¨å»ºè®®
4. å¢åŠ ä¸šåŠ¡å»ºè®®

è¿”å›ä¼˜åŒ–åçš„å®Œæ•´æŠ¥å‘Šæ¨¡æ¿ã€‚
"""
        
        request = LLMRequest(
            prompt=user_prompt,
            system_prompt=system_prompt,
            max_tokens=4000,
            temperature=0.4
        )
        
        async with self.ai_service as ai:
            response = await ai.call_llm_async(request)
            
            return {
                "success": response.success,
                "optimized_template": response.content if response.success else "",
                "error": response.error if not response.success else None,
                "response_time": response.response_time,
                "token_usage": response.usage
            }

# æµ‹è¯•AIé…ç½®çš„å‡½æ•°
async def test_ai_configuration():
    """æµ‹è¯•AIé…ç½®æ˜¯å¦æ­£å¸¸å·¥ä½œ"""
    print("ğŸ§ª æµ‹è¯•AIä¾›åº”å•†é…ç½®...")
    
    ai_service = AIService("placeholder_analysis")
    
    test_request = LLMRequest(
        prompt="è¯·å›ç­”ï¼šä½ æ˜¯ä»€ä¹ˆAIæ¨¡å‹ï¼Ÿè¯·ç®€è¦ä»‹ç»ä½ çš„èƒ½åŠ›ã€‚",
        system_prompt="ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©ç”¨æˆ·è¿›è¡Œæ•°æ®åˆ†æã€‚",
        max_tokens=200,
        temperature=0.3
    )
    
    async with ai_service as ai:
        response = await ai.call_llm_async(test_request)
        
        if response.success:
            print("âœ… AIé…ç½®æµ‹è¯•æˆåŠŸ!")
            print(f"å“åº”æ—¶é—´: {response.response_time:.2f}ç§’")
            print(f"Tokenä½¿ç”¨: {response.usage}")
            print(f"æ¨¡å‹: {response.model}")
            print(f"å›ç­”: {response.content}")
            return True
        else:
            print("âŒ AIé…ç½®æµ‹è¯•å¤±è´¥!")
            print(f"é”™è¯¯: {response.error}")
            return False

if __name__ == "__main__":
    # æµ‹è¯•AIé…ç½®
    import asyncio
    asyncio.run(test_ai_configuration())