"""
æ•°æ®æºä¸Šä¸‹æ–‡åº”ç”¨æœåŠ¡

ç”¨äºè·å–æ•°æ®åº“è¡¨ç»“æ„çš„å®Œæ•´ä¿¡æ¯ï¼Œå¹¶æ„é€ Agentä¾¿äºç†è§£çš„ä¸Šä¸‹æ–‡ç»“æ„
æ”¯æŒå®æ—¶è·å–è¡¨ç»“æ„ä¿¡æ¯ï¼Œç¡®ä¿æ•°æ®æºä¿¡æ¯ä¸ä¼šè¿‡æ—¶

åŸºäºbackup2çš„åŸå§‹å®ç°ï¼Œé€‚é…å½“å‰ç³»ç»Ÿæ¶æ„
"""
from __future__ import annotations

import logging
from dataclasses import dataclass
from datetime import datetime
from typing import Dict, Any, List, Optional
from enum import Enum

logger = logging.getLogger(__name__)


class ColumnType(Enum):
    """åˆ—ç±»å‹æšä¸¾"""
    STRING = "string"
    INTEGER = "integer"
    DECIMAL = "decimal"
    DATE = "date"
    DATETIME = "datetime"
    TIMESTAMP = "timestamp"
    BOOLEAN = "boolean"
    JSON = "json"
    TEXT = "text"
    UNKNOWN = "unknown"


@dataclass
class ColumnInfo:
    """åˆ—ä¿¡æ¯"""
    name: str
    type: str
    nullable: bool
    default_value: Optional[str] = None
    comment: Optional[str] = None
    key: Optional[str] = None  # PRI, UNI, MULç­‰
    extra: Optional[str] = None  # auto_incrementç­‰
    max_length: Optional[int] = None
    precision: Optional[int] = None
    scale: Optional[int] = None

    def to_agent_format(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºAgentæ˜“äºç†è§£çš„æ ¼å¼"""
        return {
            "name": self.name,
            "type": self._normalize_type(),
            "nullable": self.nullable,
            "description": self.comment or f"{self.name}å­—æ®µ",
            "is_primary_key": self.key == "PRI",
            "is_unique": self.key in ["PRI", "UNI"],
            "is_indexed": self.key in ["PRI", "UNI", "MUL"],
            "is_auto_increment": "auto_increment" in (self.extra or "").lower(),
            "default_value": self.default_value,
            "constraints": self._get_constraints(),
            "business_meaning": self._infer_business_meaning(),
            "data_characteristics": self._analyze_data_characteristics()
        }

    def _normalize_type(self) -> str:
        """æ ‡å‡†åŒ–æ•°æ®ç±»å‹"""
        type_lower = self.type.lower()

        if "varchar" in type_lower or "char" in type_lower or "text" in type_lower:
            return ColumnType.STRING.value
        elif "int" in type_lower or "bigint" in type_lower or "smallint" in type_lower:
            return ColumnType.INTEGER.value
        elif "decimal" in type_lower or "numeric" in type_lower or "float" in type_lower or "double" in type_lower:
            return ColumnType.DECIMAL.value
        elif "date" in type_lower and "time" not in type_lower:
            return ColumnType.DATE.value
        elif "datetime" in type_lower:
            return ColumnType.DATETIME.value
        elif "timestamp" in type_lower:
            return ColumnType.TIMESTAMP.value
        elif "boolean" in type_lower or "bool" in type_lower or "tinyint(1)" in type_lower:
            return ColumnType.BOOLEAN.value
        elif "json" in type_lower:
            return ColumnType.JSON.value
        else:
            return ColumnType.UNKNOWN.value

    def _get_constraints(self) -> List[str]:
        """è·å–çº¦æŸä¿¡æ¯"""
        constraints = []

        if not self.nullable:
            constraints.append("NOT NULL")
        if self.key == "PRI":
            constraints.append("PRIMARY KEY")
        elif self.key == "UNI":
            constraints.append("UNIQUE")
        if "auto_increment" in (self.extra or "").lower():
            constraints.append("AUTO_INCREMENT")

        return constraints

    def _infer_business_meaning(self) -> str:
        """æ¨æ–­ä¸šåŠ¡å«ä¹‰"""
        name_lower = self.name.lower()

        # æ—¶é—´ç›¸å…³
        if any(keyword in name_lower for keyword in ["created", "create_time", "created_at"]):
            return "åˆ›å»ºæ—¶é—´"
        elif any(keyword in name_lower for keyword in ["updated", "update_time", "updated_at", "modified"]):
            return "æ›´æ–°æ—¶é—´"
        elif any(keyword in name_lower for keyword in ["deleted", "delete_time", "deleted_at"]):
            return "åˆ é™¤æ—¶é—´"
        elif "time" in name_lower or "date" in name_lower:
            return "æ—¶é—´å­—æ®µ"

        # IDç›¸å…³
        elif name_lower == "id" or name_lower.endswith("_id"):
            return "æ ‡è¯†ç¬¦"

        # çŠ¶æ€ç›¸å…³
        elif "status" in name_lower or "state" in name_lower:
            return "çŠ¶æ€å­—æ®µ"

        # é‡‘é¢ç›¸å…³
        elif any(keyword in name_lower for keyword in ["amount", "price", "cost", "fee", "money"]):
            return "é‡‘é¢å­—æ®µ"

        # æ•°é‡ç›¸å…³
        elif any(keyword in name_lower for keyword in ["count", "num", "quantity", "qty"]):
            return "æ•°é‡å­—æ®µ"

        # åç§°ç›¸å…³
        elif "name" in name_lower or "title" in name_lower:
            return "åç§°å­—æ®µ"

        else:
            return self.comment or f"{self.name}å­—æ®µ"

    def _analyze_data_characteristics(self) -> Dict[str, bool]:
        """åˆ†ææ•°æ®ç‰¹å¾"""
        name_lower = self.name.lower()
        type_normalized = self._normalize_type()

        return {
            "suitable_for_grouping": (
                type_normalized in [ColumnType.STRING.value, ColumnType.INTEGER.value, ColumnType.DATE.value] or
                any(keyword in name_lower for keyword in ["status", "type", "category", "country", "region"])
            ),
            "suitable_for_aggregation": (
                type_normalized in [ColumnType.INTEGER.value, ColumnType.DECIMAL.value] and
                any(keyword in name_lower for keyword in ["amount", "price", "count", "quantity", "num"])
            ),
            "suitable_for_filtering": (
                self.key in ["PRI", "UNI", "MUL"] or
                type_normalized in [ColumnType.DATE.value, ColumnType.DATETIME.value, ColumnType.TIMESTAMP.value]
            ),
            "suitable_for_ordering": (
                type_normalized in [ColumnType.DATE.value, ColumnType.DATETIME.value, ColumnType.TIMESTAMP.value, ColumnType.INTEGER.value, ColumnType.DECIMAL.value]
            ),
            "is_measure": (
                type_normalized in [ColumnType.INTEGER.value, ColumnType.DECIMAL.value] and
                any(keyword in name_lower for keyword in ["amount", "price", "count", "quantity", "total", "sum"])
            ),
            "is_dimension": (
                type_normalized == ColumnType.STRING.value or
                any(keyword in name_lower for keyword in ["name", "type", "category", "status", "country", "region"])
            )
        }


@dataclass
class TableInfo:
    """è¡¨ä¿¡æ¯"""
    name: str
    columns: List[ColumnInfo]
    comment: Optional[str] = None
    engine: Optional[str] = None
    charset: Optional[str] = None
    row_count: Optional[int] = None
    last_updated: Optional[datetime] = None

    def to_agent_format(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºAgentæ˜“äºç†è§£çš„æ ¼å¼"""
        # åˆ†ç±»åˆ—ä¿¡æ¯
        primary_keys = [col.name for col in self.columns if col.key == "PRI"]
        foreign_keys = [col.name for col in self.columns if col.name.endswith("_id") and col.key != "PRI"]
        time_columns = [col.name for col in self.columns if any(t in col.type.lower() for t in ["date", "time", "timestamp"])]
        measure_columns = [col.name for col in self.columns if col._analyze_data_characteristics()["is_measure"]]
        dimension_columns = [col.name for col in self.columns if col._analyze_data_characteristics()["is_dimension"]]

        return {
            "table_name": self.name,
            "description": self.comment or f"{self.name}è¡¨",
            "row_count": self.row_count,
            "total_columns": len(self.columns),
            "last_updated": self.last_updated.isoformat() if self.last_updated else None,
            "business_domain": self._infer_business_domain(),

            # å…³é”®å­—æ®µåˆ†ç±»
            "primary_keys": primary_keys,
            "foreign_keys": foreign_keys,
            "time_columns": time_columns,
            "measure_columns": measure_columns,
            "dimension_columns": dimension_columns,

            # è¯¦ç»†åˆ—ä¿¡æ¯
            "columns": [col.to_agent_format() for col in self.columns],

            # æŸ¥è¯¢å»ºè®®
            "query_suggestions": self._generate_query_suggestions(),

            # æ€§èƒ½æç¤º
            "performance_hints": self._generate_performance_hints()
        }

    def _infer_business_domain(self) -> str:
        """æ¨æ–­ä¸šåŠ¡åŸŸ"""
        table_name = self.name.lower()

        if "order" in table_name or "retail" in table_name or "sales" in table_name:
            return "sales"
        elif "user" in table_name or "customer" in table_name:
            return "customer"
        elif "product" in table_name or "goods" in table_name or "item" in table_name:
            return "product"
        elif "payment" in table_name or "pay" in table_name:
            return "payment"
        elif "refund" in table_name:
            return "refund"
        elif "log" in table_name:
            return "logging"
        elif any(prefix in table_name for prefix in ["ods_", "dwd_", "dws_", "ads_"]):
            return "datawarehouse"
        else:
            return "general"

    def _generate_query_suggestions(self) -> List[str]:
        """ç”ŸæˆæŸ¥è¯¢å»ºè®®"""
        suggestions = []

        # åŸºäºåº¦é‡å­—æ®µçš„å»ºè®®
        measure_cols = [col for col in self.columns if col._analyze_data_characteristics()["is_measure"]]
        if measure_cols:
            suggestions.append(f"èšåˆåˆ†æ: å¯å¯¹ {', '.join([col.name for col in measure_cols])} è¿›è¡Œ SUM/AVG/COUNT èšåˆ")

        # åŸºäºæ—¶é—´å­—æ®µçš„å»ºè®®
        time_cols = [col for col in self.columns if any(t in col.type.lower() for t in ["date", "time", "timestamp"])]
        if time_cols:
            suggestions.append(f"æ—¶é—´åºåˆ—åˆ†æ: å¯æŒ‰ {', '.join([col.name for col in time_cols])} è¿›è¡Œæ—¶é—´è¶‹åŠ¿åˆ†æ")

        # åŸºäºç»´åº¦å­—æ®µçš„å»ºè®®
        dim_cols = [col for col in self.columns if col._analyze_data_characteristics()["is_dimension"]]
        if dim_cols:
            suggestions.append(f"åˆ†ç»„åˆ†æ: å¯æŒ‰ {', '.join([col.name for col in dim_cols[:3]])} è¿›è¡Œåˆ†ç»„ç»Ÿè®¡")

        # åŸºäºä¸»é”®çš„å»ºè®®
        pk_cols = [col for col in self.columns if col.key == "PRI"]
        if pk_cols:
            suggestions.append(f"ç²¾ç¡®æŸ¥è¯¢: ä½¿ç”¨ä¸»é”® {', '.join([col.name for col in pk_cols])} è¿›è¡Œç²¾ç¡®å®šä½")

        return suggestions

    def _generate_performance_hints(self) -> List[str]:
        """ç”Ÿæˆæ€§èƒ½æç¤º"""
        hints = []

        if self.row_count:
            if self.row_count > 1000000:
                hints.append("å¤§è¡¨(>100ä¸‡è¡Œ)ï¼Œå»ºè®®ä½¿ç”¨ç´¢å¼•å­—æ®µè¿‡æ»¤ï¼Œé™åˆ¶è¿”å›è¡Œæ•°")
            elif self.row_count > 100000:
                hints.append("ä¸­ç­‰å¤§å°è¡¨(>10ä¸‡è¡Œ)ï¼Œæ³¨æ„æŸ¥è¯¢æ€§èƒ½ï¼Œå»ºè®®æ·»åŠ WHEREæ¡ä»¶")

        # ç´¢å¼•æç¤º
        indexed_cols = [col for col in self.columns if col.key in ["PRI", "UNI", "MUL"]]
        if indexed_cols:
            hints.append(f"å·²å»ºç´¢å¼•å­—æ®µ: {', '.join([col.name for col in indexed_cols])}")

        return hints


@dataclass
class DataSourceContextInfo:
    """æ•°æ®æºä¸Šä¸‹æ–‡ä¿¡æ¯"""
    tables: List[TableInfo]
    database_name: str
    database_type: str
    last_refresh: Optional[datetime] = None

    def to_agent_format(self, lightweight: bool = False) -> Dict[str, Any]:
        """
        è½¬æ¢ä¸ºAgentå‹å¥½çš„æ ¼å¼

        Args:
            lightweight: å¦‚æœä¸ºTrueï¼Œåªè¿”å›è¡¨ååˆ—è¡¨ï¼ˆèŠ‚çœtokenï¼‰
        """
        # ğŸš€ è½»é‡çº§æ¨¡å¼ï¼šä»…è¿”å›è¡¨ååˆ—è¡¨ï¼ˆå‡å°‘99%ä¸Šä¸‹æ–‡å¤§å°ï¼‰
        if lightweight:
            return {
                "database_name": self.database_name,
                "database_type": self.database_type,
                "total_tables": len(self.tables),
                "tables": [{"table_name": table.name} for table in self.tables]
            }

        # å®Œæ•´æ¨¡å¼ï¼šè¿”å›æ‰€æœ‰å…ƒæ•°æ®
        return {
            "database_name": self.database_name,
            "database_type": self.database_type,
            "last_refresh": self.last_refresh.isoformat() if self.last_refresh else None,
            "total_tables": len(self.tables),

            "tables": [table.to_agent_format() for table in self.tables],

            "statistics": {
                "total_columns": sum(len(table.columns) for table in self.tables),
                "total_rows": sum(table.row_count or 0 for table in self.tables),
                "avg_columns_per_table": round(sum(len(table.columns) for table in self.tables) / len(self.tables), 1) if self.tables else 0,
                "domain_distribution": self._get_domain_distribution()
            },

            "relationships": self._infer_relationships(),
            "common_patterns": self._identify_common_patterns()
        }

    def _get_domain_distribution(self) -> Dict[str, int]:
        """è·å–ä¸šåŠ¡åŸŸåˆ†å¸ƒ"""
        domains = {}
        for table in self.tables:
            domain = table._infer_business_domain()
            domains[domain] = domains.get(domain, 0) + 1
        return domains

    def _infer_relationships(self) -> List[Dict[str, str]]:
        """æ¨æ–­è¡¨å…³ç³»"""
        relationships = []

        for table in self.tables:
            for col in table.columns:
                if col.name.endswith("_id") and col.key != "PRI":
                    # å¯»æ‰¾å¯èƒ½çš„ä¸»è¡¨
                    entity_name = col.name[:-3]  # å»æ‰ "_id"
                    for other_table in self.tables:
                        if entity_name in other_table.name.lower() or other_table.name.lower().endswith(entity_name):
                            relationships.append({
                                "from_table": table.name,
                                "from_column": col.name,
                                "to_table": other_table.name,
                                "to_column": "id",
                                "type": "foreign_key"
                            })
                            break

        return relationships

    def _identify_common_patterns(self) -> List[str]:
        """è¯†åˆ«å¸¸è§æŸ¥è¯¢æ¨¡å¼"""
        patterns = []

        # æ£€æŸ¥æ—¶é—´åºåˆ—æ¨¡å¼
        time_tables = [table for table in self.tables if any(
            any(t in col.type.lower() for t in ["date", "time", "timestamp"])
            for col in table.columns
        )]
        if time_tables:
            patterns.append("æ”¯æŒæ—¶é—´åºåˆ—åˆ†æ")

        # æ£€æŸ¥èšåˆæ¨¡å¼
        measure_tables = [table for table in self.tables if any(
            col._analyze_data_characteristics()["is_measure"]
            for col in table.columns
        )]
        if measure_tables:
            patterns.append("æ”¯æŒèšåˆç»Ÿè®¡åˆ†æ")

        # æ£€æŸ¥å…³è”æ¨¡å¼
        if len(self.tables) > 1:
            patterns.append("æ”¯æŒå¤šè¡¨å…³è”æŸ¥è¯¢")

        return patterns


class DataSourceContextBuilder:
    """æ•°æ®æºä¸Šä¸‹æ–‡æ„å»ºå™¨"""

    def __init__(self, container=None):
        self.container = container
        self._table_cache: Dict[str, TableInfo] = {}
        self._cache_ttl = 300  # 5åˆ†é’Ÿç¼“å­˜

    async def build_context_info(self,
                                tables_info: List[TableInfo],
                                database_name: str,
                                database_type: str = "mysql") -> DataSourceContextInfo:
        """ä»TableInfoåˆ—è¡¨æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯"""
        return DataSourceContextInfo(
            tables=tables_info,
            database_name=database_name,
            database_type=database_type,
            last_refresh=datetime.now()
        )

    async def build_data_source_context(
        self,
        user_id: str,
        data_source_id: str,
        required_tables: Optional[List[str]] = None,
        force_refresh: bool = False,
        names_only: bool = False
    ) -> Dict[str, Any]:
        """æ„å»ºæ•°æ®æºä¸Šä¸‹æ–‡ï¼ˆbackup2åŸå§‹æ¥å£ï¼‰"""

        try:
            # 1. è·å–æ•°æ®æºåŸºæœ¬ä¿¡æ¯
            if self.container and hasattr(self.container, 'user_data_source_service'):
                data_source = await self.container.user_data_source_service.get_user_data_source(
                    user_id=user_id,
                    data_source_id=data_source_id
                )

                if not data_source:
                    raise ValueError(f"Data source {data_source_id} not found")

                # 2. è·å–è¡¨ä¿¡æ¯
                if required_tables:
                    table_infos = []
                    for table_name in required_tables:
                        table_info = await self.get_table_info(
                            data_source_id=data_source_id,
                            table_name=table_name,
                            connection_config=data_source.connection_config,
                            force_refresh=force_refresh
                        )
                        if table_info:
                            table_infos.append(table_info)
                else:
                    if names_only:
                        # ä»…è·å–è¡¨åï¼Œåˆ—ä¿¡æ¯ç•™å¾…åç»­æŒ‰éœ€è·å–
                        table_names = await self._get_all_table_names(connection_config=data_source.connection_config)
                        table_infos = [TableInfo(name=t, columns=[]) for t in table_names]
                    else:
                        # è·å–æ‰€æœ‰è¡¨çš„è¯¦ç»†ä¿¡æ¯
                        table_infos = await self.get_all_tables_info(
                            data_source_id=data_source_id,
                            connection_config=data_source.connection_config,
                            force_refresh=force_refresh
                        )

                # 3. æ„å»ºAgentä¸Šä¸‹æ–‡
                context_info = DataSourceContextInfo(
                    tables=table_infos,
                    database_name=data_source.connection_config.get("database", "unknown"),
                    database_type=data_source.source_type,
                    last_refresh=datetime.now()
                )

                # ğŸš€ ä½¿ç”¨è½»é‡çº§æ ¼å¼ä»¥å‡å°‘ä¸Šä¸‹æ–‡å¤§å°
                ctx = context_info.to_agent_format(lightweight=names_only)
                ctx["success"] = True
                return ctx

            else:
                logger.warning("Container or user_data_source_service not available, using fallback mode")
                return await self._build_fallback_data_source_context(user_id, data_source_id, names_only=names_only)

        except Exception as e:
            logger.error(f"æ„å»ºæ•°æ®æºä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return {"success": False, "error": str(e)}

    async def _build_fallback_data_source_context(
        self,
        user_id: str,
        data_source_id: str,
        names_only: bool = False
    ) -> Dict[str, Any]:
        """å›é€€æ¨¡å¼æ„å»ºæ•°æ®æºä¸Šä¸‹æ–‡ - è¿æ¥çœŸå®æ•°æ®æºè·å–è¡¨ç»“æ„"""

        try:
            # ç›´æ¥æŸ¥è¯¢æ•°æ®åº“è·å–æ•°æ®æºä¿¡æ¯
            from app.db.session import get_db_session
            from app.models.data_source import DataSource

            with get_db_session() as db:
                data_source = db.query(DataSource).filter(
                    DataSource.id == data_source_id,
                    DataSource.user_id == user_id
                ).first()

                if not data_source:
                    logger.warning(f"Data source {data_source_id} not found for user {user_id}")
                    return {"success": False, "error": "data_source_not_found"}

                # è·å–è¡¨ç»“æ„æˆ–ä»…è¡¨å
                if names_only:
                    # ä»…æ ¹æ®æ•°æ®æºç±»å‹è·å–è¡¨åï¼ˆä¸ä¾èµ– connection_config å­—æ®µï¼‰
                    if data_source.source_type and getattr(data_source.source_type, 'value', None) == 'doris':
                        table_names = await self._get_doris_table_names(data_source)
                    else:
                        table_names = []
                    real_tables = [TableInfo(name=t, columns=[]) for t in table_names]
                else:
                    real_tables = await self._get_real_table_info(data_source)

                if not real_tables:
                    # å¦‚æœè·å–çœŸå®è¡¨ç»“æ„å¤±è´¥ï¼Œç›´æ¥è¿”å›é”™è¯¯ï¼Œä¸ä½¿ç”¨æ¨¡æ‹Ÿè¡¨ç»“æ„
                    logger.error(f"Failed to get real table info for {data_source_id}, no fallback available")
                    return {
                        "success": False,
                        "error": "no_real_tables_available",
                        "message": f"Unable to retrieve table structures from data source {data_source_id}",
                        "fallback_mode": True
                    }

                # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
                context_info = DataSourceContextInfo(
                    tables=real_tables,
                    database_name=data_source.display_name or "fallback_database",
                    database_type=data_source.source_type.value if data_source.source_type else "mysql",
                    last_refresh=datetime.now()
                )

                # ğŸš€ ä½¿ç”¨è½»é‡çº§æ ¼å¼ä»¥å‡å°‘ä¸Šä¸‹æ–‡å¤§å°
                ctx = context_info.to_agent_format(lightweight=names_only)
                ctx["success"] = True
                ctx["fallback_mode"] = True
                ctx["real_tables"] = len(real_tables) > 0
                logger.info(f"Successfully built fallback data source context for {data_source_id} with {len(real_tables)} tables")
                return ctx

        except Exception as e:
            logger.error(f"Fallback data source context build failed: {e}")
            return {
                "success": False,
                "error": f"fallback_failed: {str(e)}",
                "fallback_mode": True
            }


    async def _get_real_table_info(self, data_source) -> List[TableInfo]:
        """è¿æ¥çœŸå®æ•°æ®æºè·å–è¡¨ç»“æ„ä¿¡æ¯"""

        try:
            # æ ¹æ®æ•°æ®æºç±»å‹é€‰æ‹©è¿æ¥å™¨
            if data_source.source_type.value == "doris":
                return await self._get_doris_table_info(data_source)
            elif data_source.source_type.value == "sql":
                return await self._get_sql_table_info(data_source)
            else:
                logger.warning(f"Unsupported data source type: {data_source.source_type}")
                return []

        except Exception as e:
            logger.error(f"Failed to get real table info: {e}")
            return []

    async def _get_doris_table_info(self, data_source, max_retries: int = 3) -> List[TableInfo]:
        """è·å–Dorisæ•°æ®æºçš„è¡¨ç»“æ„ - å¸¦é‡è¯•æœºåˆ¶"""

        for attempt in range(max_retries):
            try:
                from app.services.data.connectors.doris_connector import DorisConnector, DorisConfig
                from app.core.data_source_utils import DataSourcePasswordManager

                # æ„å»ºDorisé…ç½®
                password = ""
                if data_source.doris_password:
                    password = DataSourcePasswordManager.get_password(data_source.doris_password)

                config = DorisConfig(
                    source_type="doris",
                    name=data_source.name,
                    fe_hosts=data_source.doris_fe_hosts or ["localhost"],
                    http_port=data_source.doris_http_port or 8030,
                    query_port=data_source.doris_query_port or 9030,
                    database=data_source.doris_database or "default",
                    username=data_source.doris_username or "root",
                    password=password,
                    use_mysql_protocol=False  # ä½¿ç”¨HTTP APIæ›´ç¨³å®š
                )

                connector = DorisConnector(config)
                tables = []

                try:
                    await connector.__aenter__()

                    # è·å–æ‰€æœ‰è¡¨å
                    tables_result = await connector.execute_query("SHOW TABLES")
                    table_names = []

                    for row in tables_result.to_dict().get('data', []):
                        if isinstance(row, dict):
                            table_name = list(row.values())[0]
                        elif isinstance(row, list):
                            table_name = row[0]
                        else:
                            table_name = str(row)
                        table_names.append(table_name)

                    logger.info(f"Found {len(table_names)} tables in Doris: {table_names}")

                    # ä¸€æ¬¡æ€§æŠ“å–æ‰€æœ‰è¡¨çš„åˆ—ä¿¡æ¯ï¼ˆåˆ†æ‰¹ï¼Œå¸¦å›é€€ï¼‰ï¼Œé¿å…ä¸Šä¸‹æ–‡æ— åˆ—
                    batch_size = 10
                    def _append_table(name: str, cols: List[ColumnInfo]):
                        tables.append(TableInfo(
                            name=name,
                            columns=cols,
                            comment=f"{name}è¡¨",
                            engine="Doris"
                        ))

                    for i in range(0, len(table_names), batch_size):
                        batch = table_names[i:i+batch_size]
                        for table_name in batch:
                            try:
                                desc_result = await connector.execute_query(f"DESCRIBE {table_name}")
                                columns: List[ColumnInfo] = []

                                for row in desc_result.to_dict().get('data', []):
                                    if isinstance(row, dict):
                                        field = row.get('Field', '')
                                        type_info = row.get('Type', '')
                                        null_info = row.get('Null', 'YES')
                                        key_info = row.get('Key', '')
                                        default = row.get('Default')
                                        extra = row.get('Extra', '')
                                    elif isinstance(row, list) and len(row) >= 4:
                                        field = row[0] or ''
                                        type_info = row[1] or ''
                                        null_info = row[2] or 'YES'
                                        key_info = row[3] or ''
                                        default = row[4] if len(row) > 4 else None
                                        extra = row[5] if len(row) > 5 else ''
                                    else:
                                        continue

                                    columns.append(ColumnInfo(
                                        name=field,
                                        type=type_info,
                                        nullable=null_info.upper() != 'NO',
                                        default_value=str(default) if default is not None else None,
                                        key=key_info if key_info else None,
                                        extra=extra if extra else None
                                    ))

                                _append_table(table_name, columns)
                            except Exception as e:
                                logger.warning(f"Failed to get structure for table {table_name}: {e}")
                                _append_table(table_name, [])

                finally:
                    await connector.__aexit__(None, None, None)

                logger.info(f"Successfully retrieved {len(tables)} table structures from Doris")
                return tables

            except Exception as e:
                if attempt < max_retries - 1:
                    import asyncio
                    delay = 2 ** attempt  # æŒ‡æ•°é€€é¿
                    logger.warning(f"Schemaè·å–ç¬¬{attempt+1}æ¬¡å¤±è´¥ï¼Œ{delay}ç§’åé‡è¯•: {e}")
                    await asyncio.sleep(delay)
                else:
                    logger.error(f"Schemaè·å–å¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡: {e}")
                    return []

        return []  # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥

    async def _get_doris_table_names(self, data_source) -> List[str]:
        """ä»…è·å–Dorisæ•°æ®æºçš„è¡¨åï¼ˆè½»é‡æ¨¡å¼ï¼‰ã€‚"""
        try:
            from app.services.data.connectors.doris_connector import DorisConnector, DorisConfig
            from app.core.data_source_utils import DataSourcePasswordManager

            password = ""
            if data_source.doris_password:
                password = DataSourcePasswordManager.get_password(data_source.doris_password)

            config = DorisConfig(
                source_type="doris",
                name=data_source.name,
                fe_hosts=data_source.doris_fe_hosts or ["localhost"],
                http_port=data_source.doris_http_port or 8030,
                query_port=data_source.doris_query_port or 9030,
                database=data_source.doris_database or "default",
                username=data_source.doris_username or "root",
                password=password,
                use_mysql_protocol=False
            )

            connector = DorisConnector(config)
            names: List[str] = []
            try:
                await connector.__aenter__()
                tables_result = await connector.execute_query("SHOW TABLES")
                for row in tables_result.to_dict().get('data', []):
                    if isinstance(row, dict):
                        table_name = list(row.values())[0]
                    elif isinstance(row, list) and row:
                        table_name = row[0]
                    else:
                        table_name = str(row)
                    if table_name:
                        names.append(str(table_name))
            finally:
                await connector.__aexit__(None, None, None)
            return names
        except Exception as e:
            logger.error(f"Failed to get Doris table names: {e}")
            return []

    async def _get_sql_table_info(self, data_source) -> List[TableInfo]:
        """è·å–SQLæ•°æ®æºçš„è¡¨ç»“æ„ï¼ˆPostgreSQL/MySQLç­‰ï¼‰"""

        # TODO: å®ç°SQLæ•°æ®æºçš„è¡¨ç»“æ„è·å–
        logger.info("SQL data source table info retrieval not implemented yet")
        return []

    async def get_table_info(
        self,
        data_source_id: str,
        table_name: str,
        connection_config: Dict[str, Any],
        force_refresh: bool = False
    ) -> Optional[TableInfo]:
        """è·å–å•ä¸ªè¡¨çš„è¯¦ç»†ä¿¡æ¯"""

        cache_key = f"{data_source_id}:{table_name}"

        # æ£€æŸ¥ç¼“å­˜
        if not force_refresh and cache_key in self._table_cache:
            cached_table = self._table_cache[cache_key]
            if cached_table.last_updated and (datetime.now() - cached_table.last_updated).seconds < self._cache_ttl:
                return cached_table

        try:
            # æ‰§è¡ŒSHOW FULL COLUMNSæŸ¥è¯¢
            columns_sql = f"SHOW FULL COLUMNS FROM {table_name}"
            columns_result = await self._execute_sql(connection_config, columns_sql)

            if not columns_result.get("success"):
                return None

            # è§£æåˆ—ä¿¡æ¯
            columns = []
            for row in columns_result.get("rows", []):
                column = ColumnInfo(
                    name=row.get("Field", ""),
                    type=row.get("Type", ""),
                    nullable=row.get("Null", "NO") == "YES",
                    default_value=row.get("Default"),
                    comment=row.get("Comment"),
                    key=row.get("Key"),
                    extra=row.get("Extra")
                )
                columns.append(column)

            # è·å–è¡¨æ³¨é‡Šå’Œè¡Œæ•°
            table_status_sql = f"SHOW TABLE STATUS LIKE '{table_name}'"
            status_result = await self._execute_sql(connection_config, table_status_sql)

            table_comment = None
            row_count = None
            engine = None
            charset = None

            if status_result.get("success") and status_result.get("rows"):
                status_row = status_result["rows"][0]
                table_comment = status_row.get("Comment")
                row_count = status_row.get("Rows")
                engine = status_row.get("Engine")
                charset = status_row.get("Collation")

            # åˆ›å»ºè¡¨ä¿¡æ¯å¯¹è±¡
            table_info = TableInfo(
                name=table_name,
                columns=columns,
                comment=table_comment,
                engine=engine,
                charset=charset,
                row_count=row_count,
                last_updated=datetime.now()
            )

            # ç¼“å­˜ç»“æœ
            self._table_cache[cache_key] = table_info

            return table_info

        except Exception as e:
            logger.error(f"Error getting table info for {table_name}: {str(e)}")
            return None

    async def get_all_tables_info(
        self,
        data_source_id: str,
        connection_config: Dict[str, Any],
        force_refresh: bool = False
    ) -> List[TableInfo]:
        """è·å–æ‰€æœ‰è¡¨çš„ä¿¡æ¯"""

        try:
            # é¦–å…ˆè·å–æ‰€æœ‰è¡¨å
            tables_sql = "SHOW TABLES"
            tables_result = await self._execute_sql(connection_config, tables_sql)

            if not tables_result.get("success"):
                return []

            table_names = [list(row.values())[0] for row in tables_result.get("rows", [])]

            # è·å–æ¯ä¸ªè¡¨çš„è¯¦ç»†ä¿¡æ¯
            table_infos = []
            for table_name in table_names:
                table_info = await self.get_table_info(
                    data_source_id=data_source_id,
                    table_name=table_name,
                    connection_config=connection_config,
                    force_refresh=force_refresh
                )
                if table_info:
                    table_infos.append(table_info)

            return table_infos

        except Exception as e:
            logger.error(f"Error getting all tables info: {str(e)}")
            return []

    async def _get_all_table_names(self, connection_config: Dict[str, Any]) -> List[str]:
        """ä»…è·å–æ‰€æœ‰è¡¨åï¼ˆè½»é‡æ¨¡å¼ï¼‰"""
        try:
            tables_result = await self._execute_sql(connection_config, "SHOW TABLES")
            names: List[str] = []
            rows = tables_result.get("rows", []) or tables_result.get("data", []) or []
            for row in rows:
                if isinstance(row, dict):
                    names.append(list(row.values())[0])
                elif isinstance(row, list) and len(row) > 0:
                    names.append(str(row[0]))
                elif isinstance(row, str):
                    names.append(row)
            return [n for n in names if n]
        except Exception as e:
            logger.error(f"Error getting table names: {str(e)}")
            return []

    async def _execute_sql(self, connection_config: Dict[str, Any], sql: str) -> Dict[str, Any]:
        """æ‰§è¡ŒSQLæŸ¥è¯¢"""
        try:
            if self.container and hasattr(self.container, 'data_source'):
                # ä½¿ç”¨å®¹å™¨ä¸­çš„æ•°æ®æºæœåŠ¡æ‰§è¡ŒæŸ¥è¯¢
                result = await self.container.data_source.run_query(
                    connection_config=connection_config,
                    sql=sql,
                    limit=1000
                )
                return result
            else:
                logger.warning("Container or data_source service not available")
                return {"success": False, "error": "Data source service not available"}
        except Exception as e:
            return {"success": False, "error": str(e)}

    def _create_fallback_context(self, database_name: str, error: Optional[str] = None) -> Dict[str, Any]:
        """åˆ›å»ºé™çº§ä¸Šä¸‹æ–‡"""
        return {
            "database_name": database_name,
            "database_type": "unknown",
            "last_refresh": datetime.now().isoformat(),
            "total_tables": 0,
            "tables": [],
            "statistics": {
                "total_columns": 0,
                "total_rows": 0,
                "avg_columns_per_table": 0,
                "domain_distribution": {}
            },
            "relationships": [],
            "common_patterns": [],
            "error": error,
            "fallback": True
        }

    async def refresh_table_cache(self, data_source_id: str, table_name: Optional[str] = None):
        """åˆ·æ–°è¡¨ç¼“å­˜"""
        if table_name:
            cache_key = f"{data_source_id}:{table_name}"
            if cache_key in self._table_cache:
                del self._table_cache[cache_key]
        else:
            # æ¸…é™¤è¯¥æ•°æ®æºçš„æ‰€æœ‰ç¼“å­˜
            keys_to_remove = [key for key in self._table_cache.keys() if key.startswith(f"{data_source_id}:")]
            for key in keys_to_remove:
                del self._table_cache[key]
