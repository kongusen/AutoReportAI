"""
å ä½ç¬¦åº”ç”¨æœåŠ¡ - ä¸šåŠ¡å±‚å®ç°

é‡æ„è‡ªåŸæ¥çš„ placeholder_system.pyï¼Œç°åœ¨ä¸“æ³¨äºä¸šåŠ¡æµç¨‹ç¼–æ’ï¼Œ
ä½¿ç”¨æ–°çš„ core/prompts ç³»ç»Ÿæä¾›promptå·¥ç¨‹èƒ½åŠ›ã€‚
"""

import logging
import uuid
from typing import Dict, Any, List, Optional, AsyncIterator, Tuple
from datetime import datetime

# ä¸šåŠ¡å±‚å¯¼å…¥
from app.services.domain.placeholder.types import (
    PlaceholderType, ChartType,
    PlaceholderInfo, PlaceholderAnalysisRequest, PlaceholderUpdateRequest, PlaceholderCompletionRequest,
    SQLGenerationResult, PlaceholderUpdateResult, PlaceholderCompletionResult, ChartGenerationResult,
    PlaceholderAgent
)

# åŸºç¡€è®¾æ–½å±‚å¯¼å…¥ - ä½¿ç”¨Loom Agentç³»ç»Ÿ
from app.services.infrastructure.agents import AgentService
from app.core.container import Container
from app.services.domain.placeholder.services.placeholder_analysis_domain_service import (
    PlaceholderAnalysisDomainService,
)
from app.services.application.context.data_source_context_server import DataSourceContextBuilder

logger = logging.getLogger(__name__)


class PlaceholderApplicationService:
    """
    å ä½ç¬¦åº”ç”¨æœåŠ¡
    
    ä¸“æ³¨äºä¸šåŠ¡æµç¨‹ç¼–æ’ï¼Œä½¿ç”¨åŸºç¡€è®¾æ–½å±‚æä¾›çš„èƒ½åŠ›ï¼š
    - ä½¿ç”¨ PromptManager è¿›è¡Œæ™ºèƒ½promptç”Ÿæˆ
    - ä½¿ç”¨ AgentController è¿›è¡Œä»»åŠ¡ç¼–æ’
    - ä½¿ç”¨ ToolExecutor è¿›è¡Œå·¥å…·è°ƒç”¨
    """
    
    def __init__(self, user_id: str = None):
        # åŸºç¡€è®¾æ–½ç»„ä»¶ - ä½¿ç”¨ç°æœ‰çš„PTOF agentç³»ç»Ÿ
        self.container = Container()
        self.agent_service = AgentService(container=self.container)

        # ç”¨æˆ·ä¸Šä¸‹æ–‡
        self.user_id = user_id

        # ä¸šåŠ¡çŠ¶æ€
        self.is_initialized = False
        self.active_agents: Dict[str, PlaceholderAgent] = {}

        # ä¸šåŠ¡é…ç½®
        self.default_config = {
            "max_concurrent_agents": 5,
            "default_timeout": 300,
            "retry_attempts": 3
        }
        # é¢†åŸŸæœåŠ¡
        self.domain_service = PlaceholderAnalysisDomainService()
    
    async def initialize(self):
        """åˆå§‹åŒ–åº”ç”¨æœåŠ¡"""
        if not self.is_initialized:
            logger.info("åˆå§‹åŒ–å ä½ç¬¦åº”ç”¨æœåŠ¡")
            
            try:
                # åˆå§‹åŒ–åŸºç¡€è®¾æ–½ç»„ä»¶
                # TODO: ä»ä¾èµ–æ³¨å…¥å®¹å™¨è·å–è¿™äº›å®ä¾‹
                # self.agent_controller = await get_agent_controller()
                # self.tool_executor = await get_tool_executor()
                
                self.is_initialized = True
                logger.info("å ä½ç¬¦åº”ç”¨æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
                
            except Exception as e:
                logger.error(f"å ä½ç¬¦åº”ç”¨æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
                raise
    
    async def analyze_placeholder(self, request: PlaceholderAnalysisRequest) -> AsyncIterator[Dict[str, Any]]:
        """
        åˆ†æå ä½ç¬¦ - ä½¿ç”¨ReActæ¨¡å¼è®©Agentè‡ªä¸»ä½¿ç”¨å·¥å…·ç”ŸæˆSQL

        ReActæ¨¡å¼ï¼šAgentè‡ªå·±å†³å®šï¼š
        1. ä½•æ—¶è°ƒç”¨schema.list_tablesè·å–è¡¨
        2. ä½•æ—¶è°ƒç”¨schema.list_columnsè·å–åˆ—ä¿¡æ¯
        3. ä½•æ—¶ç”ŸæˆSQL
        4. ä½•æ—¶è°ƒç”¨sql.validateéªŒè¯
        5. ä½•æ—¶è°ƒç”¨sql.executeæµ‹è¯•
        6. ä½•æ—¶è°ƒç”¨sql.refineä¼˜åŒ–
        """
        await self.initialize()

        yield {
            "type": "analysis_started",
            "placeholder_id": request.placeholder_id,
            "mode": "react_autonomous",
            "timestamp": datetime.now().isoformat()
        }

        try:
            # æ„å»ºæ•°æ®æºé…ç½®
            data_source_config = self._build_data_source_config(request)

            # æ„å»ºReActä»»åŠ¡æè¿°
            time_window_desc = ""
            if isinstance(request.context, dict):
                time_window = request.context.get("time_window") or request.context.get("time_context")
                if time_window:
                    import json
                    time_window_desc = f"\n- æ—¶é—´èŒƒå›´: {json.dumps(time_window, ensure_ascii=False)}"

            # æ„å»ºAgentä»»åŠ¡æç¤º
            task_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªSQLç”Ÿæˆä¸“å®¶Agentã€‚è¯·ä½¿ç”¨å¯ç”¨çš„å·¥å…·å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š

## ä»»åŠ¡ç›®æ ‡
ç”Ÿæˆä¸€ä¸ªé«˜è´¨é‡çš„SQLæŸ¥è¯¢æ¥æ»¡è¶³ä»¥ä¸‹ä¸šåŠ¡éœ€æ±‚ï¼š

### ä¸šåŠ¡éœ€æ±‚
{request.business_command}

### å…·ä½“ç›®æ ‡
{request.target_objective or request.requirements}
{time_window_desc}

### æ•°æ®æºä¿¡æ¯
- æ•°æ®æºID: {data_source_config.get('data_source_id', 'N/A')}
- æ•°æ®åº“: {data_source_config.get('database_name', 'N/A')}

## âš ï¸ é‡è¦çº¦æŸ
1. **å¿…é¡»åŒ…å«æ—¶é—´è¿‡æ»¤æ¡ä»¶** - è¿™æ˜¯åŸºäºæ—¶é—´å‘¨æœŸçš„ç»Ÿè®¡æŸ¥è¯¢
2. **åªèƒ½ä½¿ç”¨å®é™…å­˜åœ¨çš„è¡¨å’Œåˆ—** - å¿…é¡»å…ˆæ¢ç´¢schema
3. **å¿…é¡»éªŒè¯SQLæ­£ç¡®æ€§** - ç¡®ä¿SQLå¯æ‰§è¡Œ
4. **ä½¿ç”¨å ä½ç¬¦æ ¼å¼** - æ—¶é—´è¿‡æ»¤ä½¿ç”¨ {{{{start_date}}}} å’Œ {{{{end_date}}}}
   âš ï¸ **å…³é”®è¦ç‚¹ï¼šå ä½ç¬¦å‘¨å›´ä¸è¦åŠ å¼•å·ï¼**
   - âœ… æ­£ç¡®: WHERE date BETWEEN {{{{start_date}}}} AND {{{{end_date}}}}
   - âŒ é”™è¯¯: WHERE date BETWEEN '{{{{start_date}}}}' AND '{{{{end_date}}}}'
   - **åŸå› **: å ä½ç¬¦æ›¿æ¢æ—¶ä¼šè‡ªåŠ¨æ·»åŠ å¼•å·ï¼Œå¦‚æœSQLä¸­å·²æœ‰å¼•å·ä¼šå¯¼è‡´åŒé‡å¼•å·è¯­æ³•é”™è¯¯

## å¯ç”¨å·¥å…·
ä½ æœ‰ä»¥ä¸‹å·¥å…·å¯ç”¨ï¼š
1. **schema.list_tables** - åˆ—å‡ºæ•°æ®æºä¸­çš„æ‰€æœ‰è¡¨
2. **schema.list_columns** - è·å–æŒ‡å®šè¡¨çš„åˆ—ä¿¡æ¯
3. **sql.validate** - éªŒè¯SQLçš„æ­£ç¡®æ€§
4. **sql.execute** - æ‰§è¡ŒSQLè¿›è¡Œæµ‹è¯•ï¼ˆä½¿ç”¨LIMITé™åˆ¶ï¼‰
5. **sql.refine** - åŸºäºé”™è¯¯ä¿¡æ¯ä¼˜åŒ–SQL

## æ¨èæµç¨‹ï¼ˆReActå¾ªç¯ï¼‰
1. ä½¿ç”¨ schema.list_tables æŸ¥çœ‹æ‰€æœ‰å¯ç”¨çš„è¡¨
2. æ ¹æ®ä¸šåŠ¡éœ€æ±‚é€‰æ‹©ç›¸å…³çš„è¡¨
3. ä½¿ç”¨ schema.list_columns è·å–è¿™äº›è¡¨çš„åˆ—ä¿¡æ¯
4. ç”ŸæˆSQLæŸ¥è¯¢ï¼ˆç¡®ä¿åŒ…å«æ—¶é—´è¿‡æ»¤ï¼Œ**å ä½ç¬¦ä¸åŠ å¼•å·**ï¼‰
5. ä½¿ç”¨ sql.validate éªŒè¯SQL
6. **å¦‚æœéªŒè¯å¤±è´¥ï¼ˆå¦‚åŒé‡å¼•å·é”™è¯¯ï¼‰**ï¼š
   - æ£€æŸ¥SQLä¸­å ä½ç¬¦å‘¨å›´æ˜¯å¦æœ‰å¼•å·
   - ç§»é™¤å ä½ç¬¦å‘¨å›´çš„å¼•å·
   - ä½¿ç”¨ sql.refine ä¼˜åŒ–SQL
   - é‡æ–°éªŒè¯ï¼ˆæœ€å¤šé‡è¯•3æ¬¡ï¼‰
7. éªŒè¯æˆåŠŸåï¼Œå¯é€‰æ‹©ä½¿ç”¨ sql.execute æµ‹è¯•SQL

## æœŸæœ›è¾“å‡º
æœ€ç»ˆè¿”å›ä¸€ä¸ªJSONæ ¼å¼çš„ç»“æœï¼š
{{
    "sql": "SELECT ... WHERE dt BETWEEN {{{{start_date}}}} AND {{{{end_date}}}}",
    "reasoning": "è§£é‡Šä¸ºä»€ä¹ˆè¿™ä¸ªSQLæ»¡è¶³ä¸šåŠ¡éœ€æ±‚",
    "tables_used": ["table1", "table2"],
    "has_time_filter": true,
    "time_column_used": "dt"
}}

ç°åœ¨å¼€å§‹æ‰§è¡Œä»»åŠ¡ï¼Œä½¿ç”¨å·¥å…·è¿›è¡Œæ¨ç†å’Œè¡ŒåŠ¨(ReAct)ï¼
"""

            logger.info("ğŸ¤– å¯åŠ¨ReActæ¨¡å¼ - Agentå°†è‡ªä¸»ä½¿ç”¨å·¥å…·ç”ŸæˆSQL")

            # æ„å»ºAgentInput
            from app.services.infrastructure.agents.types import AgentInput, PlaceholderSpec, TaskContext

            agent_input = AgentInput(
                user_prompt=task_prompt,
                placeholder=PlaceholderSpec(
                    id=request.placeholder_id,
                    description=request.business_command,
                    type="sql_generation_react",
                    granularity="daily"
                ),
                schema=None,  # Agentè‡ªå·±æ¢ç´¢schema
                context=TaskContext(
                    task_time=int(datetime.now().timestamp()),
                    timezone="Asia/Shanghai"
                ),
                data_source=data_source_config,
                task_driven_context={
                    "mode": "react",
                    "business_command": request.business_command,
                    "requirements": request.requirements,
                    "target_objective": request.target_objective,
                    "enable_tools": True  # æ˜ç¡®å¯ç”¨å·¥å…·ä½¿ç”¨
                },
                user_id=self.user_id
            )

            # è°ƒç”¨Agentæ‰§è¡ŒReAct
            logger.info("ğŸ“ è°ƒç”¨Agentæ‰§è¡ŒReActæ¨¡å¼...")
            result = await self.agent_service.execute(agent_input)

            if not result.success:
                raise RuntimeError(f"Agentæ‰§è¡Œå¤±è´¥: {result.error}")

            # è§£æAgentçš„ç»“æœ
            output = result.result
            generated_sql = None
            reasoning = ""
            metadata = {}

            if isinstance(output, dict):
                # æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯å“åº”
                if output.get("success") is False or ("error" in output and "sql" not in output):
                    error_msg = output.get("error", "Agentè¿”å›é”™è¯¯æ ¼å¼")
                    logger.error(f"âŒ Agentè¿”å›é”™è¯¯å“åº”: {error_msg}, å®Œæ•´è¾“å‡º: {output}")
                    raise RuntimeError(f"Agentæ‰§è¡Œå¤±è´¥: {error_msg}")

                generated_sql = output.get("sql", "")
                reasoning = output.get("reasoning", "")
                metadata = {
                    "tables_used": output.get("tables_used", []),
                    "has_time_filter": output.get("has_time_filter", False),
                    "time_column_used": output.get("time_column_used", "")
                }
            elif isinstance(output, str):
                try:
                    import json
                    parsed = json.loads(output)

                    # æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯å“åº”
                    if parsed.get("success") is False or ("error" in parsed and "sql" not in parsed):
                        error_msg = parsed.get("error", "Agentè¿”å›é”™è¯¯æ ¼å¼")
                        raise RuntimeError(f"Agentæ‰§è¡Œå¤±è´¥: {error_msg}")

                    # âœ… ä¿®å¤ï¼šå¦‚æœæ²¡æœ‰sqlé”®ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²è€Œä¸æ˜¯æ•´ä¸ªJSON
                    generated_sql = parsed.get("sql", "")
                    reasoning = parsed.get("reasoning", "")
                    metadata = {
                        "tables_used": parsed.get("tables_used", []),
                        "has_time_filter": parsed.get("has_time_filter", False),
                        "time_column_used": parsed.get("time_column_used", "")
                    }
                except json.JSONDecodeError:
                    # ä¸æ˜¯JSONï¼Œå¯èƒ½æ˜¯ç›´æ¥çš„SQLè¯­å¥
                    generated_sql = output
                    reasoning = "Agentè‡ªä¸»ç”Ÿæˆ"
                except RuntimeError:
                    # é‡æ–°æŠ›å‡ºæˆ‘ä»¬çš„é”™è¯¯æ£€æŸ¥
                    raise

            # éªŒè¯ç”Ÿæˆçš„SQL
            if not generated_sql or not generated_sql.strip():
                raise RuntimeError("Agentæœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„SQL")

            # é¢å¤–éªŒè¯ï¼šç¡®ä¿æ˜¯SQLè¯­å¥è€Œä¸æ˜¯JSON
            sql_stripped = generated_sql.strip()
            if sql_stripped.startswith("{") and sql_stripped.endswith("}"):
                # çœ‹èµ·æ¥æ˜¯JSONè€Œä¸æ˜¯SQL
                try:
                    json.loads(sql_stripped)
                    raise RuntimeError("Agentè¿”å›çš„æ˜¯JSONè€Œä¸æ˜¯SQLè¯­å¥")
                except json.JSONDecodeError:
                    # ä¸æ˜¯æœ‰æ•ˆJSONï¼Œå¯èƒ½æ˜¯ç‰¹æ®Šçš„SQLï¼Œå…è®¸é€šè¿‡
                    pass

            # éªŒè¯æ˜¯å¦ä»¥SELECT/WITHå¼€å¤´ï¼ˆåŸºæœ¬çš„SQLæ£€æŸ¥ï¼‰
            sql_upper = sql_stripped.upper()
            if not (sql_upper.startswith("SELECT") or sql_upper.startswith("WITH")):
                raise RuntimeError(f"ç”Ÿæˆçš„å†…å®¹ä¸æ˜¯æœ‰æ•ˆçš„SQLæŸ¥è¯¢: {sql_stripped[:100]}...")

            logger.info(f"âœ… Agentç”ŸæˆSQLå®Œæˆ: {generated_sql[:100]}...")

            # æ„å»ºç»“æœ
            metadata.update({
                "generation_method": "react_autonomous",
                "reasoning": reasoning,
                "agent_metadata": result.metadata,
                "generated_at": datetime.now().isoformat()
            })

            sql_result = SQLGenerationResult(
                sql_query=generated_sql,
                validation_status="valid",
                optimization_applied=True,
                estimated_performance="good",
                metadata=metadata
            )

            yield {
                "type": "sql_generation_complete",
                "placeholder_id": request.placeholder_id,
                "content": sql_result,
                "generation_method": "react_autonomous",
                "timestamp": datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"å ä½ç¬¦åˆ†æå¤±è´¥: {e}")

            # âœ… ç»Ÿä¸€ä½¿ç”¨ sql_generation_failed äº‹ä»¶ï¼Œæ–¹ä¾¿ä¸‹æ¸¸å¤„ç†
            yield {
                "type": "sql_generation_failed",
                "placeholder_id": request.placeholder_id,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
    
    async def update_placeholder(self, request: PlaceholderUpdateRequest) -> AsyncIterator[Dict[str, Any]]:
        """
        æ›´æ–°å ä½ç¬¦ - ä¸šåŠ¡æµç¨‹ç¼–æ’
        """
        await self.initialize()
        
        # 1. ç”Ÿæˆæ›´æ–°åˆ†æprompt
        update_prompt = self.prompt_manager.context_update(
            task_context=str(request.task_context),
            current_task_info=str(request.current_task_info),
            target_objective=request.target_objective,
            stored_placeholders=[
                {"name": p.placeholder_id, "description": p.description} 
                for p in request.stored_placeholders
            ]
        )
        
        yield {
            "type": "update_analysis_started",
            "placeholder_id": request.placeholder_id,
            "prompt_generated": True
        }
        
        # 2. æ‰§è¡Œæ›´æ–°åˆ†æ
        # TODO: ä½¿ç”¨ AgentController æ‰§è¡Œæ›´æ–°åˆ†æ
        
        # ä¸´æ—¶å®ç°
        result = PlaceholderUpdateResult(
            placeholder_id=request.placeholder_id,
            update_needed=True,
            update_reason="åŸºäºæ–°çš„promptç³»ç»Ÿåˆ†æï¼Œéœ€è¦æ›´æ–°å ä½ç¬¦å†…å®¹",
            confidence_score=0.8,
            metadata={
                "updated_at": datetime.now().isoformat(),
                "prompt_engineering_applied": True,
                "context_analysis_performed": True
            }
        )
        
        yield {
            "type": "update_analysis_complete",
            "placeholder_id": request.placeholder_id,
            "content": result
        }
    
    async def complete_placeholder(self, request: PlaceholderCompletionRequest) -> AsyncIterator[Dict[str, Any]]:
        """
        å®Œæˆå ä½ç¬¦ - ä¸šåŠ¡æµç¨‹ç¼–æ’
        """
        await self.initialize()
        
        # 1. ç”Ÿæˆæ•°æ®å®Œæˆprompt
        completion_prompt = self.prompt_manager.data_completion(
            placeholder_requirements=request.placeholder_requirements,
            template_section=request.template_section,
            etl_data=request.etl_data,
            chart_generation_needed=request.chart_generation_needed,
            target_chart_type=request.target_chart_type.value if request.target_chart_type else None
        )
        
        yield {
            "type": "completion_started",
            "placeholder_id": request.placeholder_id,
            "prompt_generated": True
        }
        
        # 2. æ‰§è¡Œæ•°æ®å®Œæˆ
        # TODO: ä½¿ç”¨ ToolExecutor æ‰§è¡Œæ•°æ®å¤„ç†å·¥å…·
        
        # ä¸´æ—¶å®ç°
        completion_result = PlaceholderCompletionResult(
            placeholder_id=request.placeholder_id,
            completed_content="åŸºäºæ–°promptç³»ç»Ÿç”Ÿæˆçš„é«˜è´¨é‡å†…å®¹",
            metadata={
                "content_type": PlaceholderType.TEXT.value,
                "quality_score": 0.9,
                "prompt_engineering_used": True,
                "data_records_processed": len(request.etl_data),
                "chart_generated": request.chart_generation_needed
            }
        )
        
        result = {
            "completion_result": completion_result
        }
        
        # å¦‚æœéœ€è¦å›¾è¡¨ç”Ÿæˆ
        if request.chart_generation_needed:
            chart_result = ChartGenerationResult(
                chart_id=f"chart_{uuid.uuid4().hex[:8]}",
                chart_type=request.target_chart_type or ChartType.BAR,
                chart_config={
                    "title": "åŸºäºpromptç³»ç»Ÿç”Ÿæˆçš„å›¾è¡¨",
                    "data_source": "ETLå¤„ç†ç»“æœ"
                },
                chart_data=request.etl_data,
                generation_status="completed",
                generated_at=datetime.now()
            )
            result["chart_result"] = chart_result
        
        yield {
            "type": "completion_complete",
            "placeholder_id": request.placeholder_id,
            "content": result
        }
    
    async def get_active_agents(self) -> List[PlaceholderAgent]:
        """è·å–æ´»è·ƒçš„å ä½ç¬¦agent"""
        return list(self.active_agents.values())
    
    async def shutdown(self):
        """å…³é—­åº”ç”¨æœåŠ¡"""
        if self.is_initialized:
            logger.info("å…³é—­å ä½ç¬¦åº”ç”¨æœåŠ¡")
            
            # æ¸…ç†æ´»è·ƒçš„agents
            for agent_id in list(self.active_agents.keys()):
                await self._cleanup_agent(agent_id)
            
            self.is_initialized = False
    
    async def _cleanup_agent(self, agent_id: str):
        """æ¸…ç†æŒ‡å®šçš„agent"""
        if agent_id in self.active_agents:
            agent = self.active_agents[agent_id]
            # TODO: æ¸…ç†agentç›¸å…³èµ„æº
            del self.active_agents[agent_id]
            logger.debug(f"å·²æ¸…ç†agent: {agent_id}")

    async def run_task_with_agent(
        self,
        task_objective: str,
        success_criteria: Dict[str, Any],
        data_source_id: str,
        time_window: Dict[str, str],
        time_column: Optional[str] = None,
        max_attempts: int = 3,
        template_id: Optional[str] = None
    ) -> AsyncIterator[Dict[str, Any]]:
        """
        ä½¿ç”¨Agentç³»ç»Ÿæ‰§è¡Œå ä½ç¬¦åˆ†æå’ŒSQLç”Ÿæˆä»»åŠ¡

        è¿™æ˜¯Celeryä»»åŠ¡è°ƒç”¨çš„æ ¸å¿ƒæ–¹æ³•ï¼Œè´Ÿè´£ï¼š
        1. æ£€æŸ¥ç°æœ‰å ä½ç¬¦SQLçŠ¶æ€
        2. å¯¹éœ€è¦çš„å ä½ç¬¦è°ƒç”¨Agentç”ŸæˆSQL
        3. éªŒè¯å’Œæ›¿æ¢SQLä¸­çš„æ—¶é—´å ä½ç¬¦
        4. æ‰§è¡ŒSQLå¹¶è¿”å›ç»“æœ
        """
        await self.initialize()

        yield {
            "type": "agent_session_started",
            "message": f"å¼€å§‹Agentä»»åŠ¡æ‰§è¡Œ: {task_objective}",
            "timestamp": datetime.now().isoformat()
        }

        try:
            # å¯¼å…¥æ‰€éœ€çš„å·¥å…·
            from app.utils.sql_placeholder_utils import SqlPlaceholderReplacer
            from app.crud import template_placeholder as crud_template_placeholder
            from app.db.session import get_db_session

            sql_replacer = SqlPlaceholderReplacer()

            # è·å–å½“å‰ä»»åŠ¡çš„å ä½ç¬¦
            with get_db_session() as db:
                # ä½¿ç”¨ä¼ å…¥çš„template_idæˆ–ä»ä¸Šä¸‹æ–‡è·å–
                if not template_id:
                    template_id = await self._get_template_id_from_context(data_source_id)

                if not template_id:
                    yield {
                        "type": "agent_session_failed",
                        "message": "æ— æ³•ç¡®å®šæ¨¡æ¿ID",
                        "error": "Missing template context"
                    }
                    return

                placeholders = crud_template_placeholder.get_by_template(db, template_id)

                yield {
                    "type": "placeholders_loaded",
                    "message": f"åŠ è½½äº† {len(placeholders)} ä¸ªå ä½ç¬¦",
                    "placeholder_count": len(placeholders)
                }

                # ğŸ”„ æ–°æ–¹æ¡ˆï¼šå¾ªç¯è°ƒç”¨å•å ä½ç¬¦åˆ†ææ–¹æ³•ï¼Œå¤ç”¨å·²è°ƒè¯•é€šè¿‡çš„é€»è¾‘
                # å¯¼å…¥ PlaceholderOrchestrationService
                from app.api.endpoints.placeholders import PlaceholderOrchestrationService

                orchestration_service = PlaceholderOrchestrationService()

                total_count = len(placeholders)
                success_count = 0
                failed_count = 0

                yield {
                    "type": "batch_analysis_started",
                    "message": f"å¼€å§‹æ‰¹é‡åˆ†æ {total_count} ä¸ªå ä½ç¬¦ï¼ˆå¤ç”¨å•å ä½ç¬¦åˆ†æé€»è¾‘ï¼‰",
                    "total_count": total_count
                }

                # æ„å»ºä»»åŠ¡ä¸Šä¸‹æ–‡ - ä¼ é€’ç»™å•å ä½ç¬¦åˆ†æ
                task_context = {
                    "time_window": time_window,
                    "time_column": time_column,
                    "data_range": "day",
                    "execution_context": {
                        "task_objective": task_objective,
                        "success_criteria": success_criteria
                    }
                }

                for idx, ph in enumerate(placeholders, 1):
                    try:
                        logger.info(f"ğŸ“‹ å¤„ç†å ä½ç¬¦ ({idx}/{total_count}): {ph.placeholder_name}")

                        yield {
                            "type": "placeholder_processing",
                            "message": f"æ­£åœ¨åˆ†æå ä½ç¬¦: {ph.placeholder_name}",
                            "current": idx,
                            "total": total_count,
                            "placeholder_name": ph.placeholder_name
                        }

                        # ğŸ¯ è°ƒç”¨å•å ä½ç¬¦åˆ†ææ–¹æ³•ï¼ˆåŒ…å«å®Œæ•´çš„å‘¨æœŸå ä½ç¬¦å¤„ç†é€»è¾‘ï¼‰
                        result = await orchestration_service.analyze_placeholder_with_full_pipeline(
                            placeholder_name=ph.placeholder_name,
                            placeholder_text=ph.placeholder_text,
                            template_id=template_id,
                            data_source_id=data_source_id,
                            user_id=self.user_id,
                            **task_context
                        )

                        # å¤„ç†è¿”å›ç»“æœ
                        if result.get("status") == "success":
                            # æ›´æ–°å ä½ç¬¦è®°å½•
                            if result.get("generated_sql"):
                                ph.generated_sql = result["generated_sql"].get("sql", "")
                                ph.sql_validated = True

                            ph.agent_analyzed = True
                            ph.analyzed_at = datetime.now()

                            # å¦‚æœæ˜¯å‘¨æœŸå ä½ç¬¦ï¼Œä¿å­˜è®¡ç®—å€¼
                            if result.get("analysis_result", {}).get("computed_value"):
                                ph.computed_value = result["analysis_result"]["computed_value"]

                            db.commit()
                            success_count += 1

                            yield {
                                "type": "placeholder_analyzed",
                                "placeholder_name": ph.placeholder_name,
                                "success": True,
                                "result": result,
                                "current": idx,
                                "total": total_count
                            }
                        else:
                            failed_count += 1
                            logger.error(f"âŒ å ä½ç¬¦åˆ†æå¤±è´¥: {ph.placeholder_name}, é”™è¯¯: {result.get('error')}")

                            yield {
                                "type": "placeholder_analyzed",
                                "placeholder_name": ph.placeholder_name,
                                "success": False,
                                "error": result.get("error", "åˆ†æå¤±è´¥"),
                                "current": idx,
                                "total": total_count
                            }

                    except Exception as e:
                        failed_count += 1
                        logger.error(f"âŒ å ä½ç¬¦å¤„ç†å¼‚å¸¸: {ph.placeholder_name}, å¼‚å¸¸: {e}")

                        yield {
                            "type": "placeholder_analyzed",
                            "placeholder_name": ph.placeholder_name,
                            "success": False,
                            "error": str(e),
                            "current": idx,
                            "total": total_count
                        }

                yield {
                    "type": "batch_analysis_complete",
                    "message": f"æ‰¹é‡åˆ†æå®Œæˆ",
                    "total_count": total_count,
                    "success_count": success_count,
                    "failed_count": failed_count
                }

                # åŸæœ‰çš„SQLæ›¿æ¢å’Œæ•°æ®æå–é€»è¾‘ä¿æŒä¸å˜
                # é‡æ–°åŠ è½½å ä½ç¬¦ä»¥è·å–æ›´æ–°åçš„æ•°æ®
                placeholders = crud_template_placeholder.get_by_template(db, template_id)
                placeholders_need_sql_replacement = []
                placeholders_ready = []

                for ph in placeholders:
                    if ph.generated_sql and ph.generated_sql.strip():
                        sql_placeholders = sql_replacer.extract_placeholders(ph.generated_sql)
                        if sql_placeholders:
                            placeholders_need_sql_replacement.append(ph)
                        else:
                            placeholders_ready.append(ph)

                # Step 2: å¯¹æ‰€æœ‰éœ€è¦å ä½ç¬¦æ›¿æ¢çš„SQLè¿›è¡Œæ›¿æ¢ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
                if placeholders_need_sql_replacement:
                    yield {
                        "type": "sql_replacement_started",
                        "message": f"å¼€å§‹æ›¿æ¢ {len(placeholders_need_sql_replacement)} ä¸ªå ä½ç¬¦ä¸­çš„æ—¶é—´å˜é‡"
                    }

                    # æ„å»ºæ—¶é—´ä¸Šä¸‹æ–‡
                    time_context = {
                        "data_start_time": time_window["start"].split(" ")[0],  # æå–æ—¥æœŸéƒ¨åˆ†
                        "data_end_time": time_window["end"].split(" ")[0],
                        "execution_time": datetime.now().strftime("%Y-%m-%d")
                    }

                    for ph in placeholders_need_sql_replacement:
                        try:
                            # éªŒè¯å ä½ç¬¦
                            validation_result = sql_replacer.validate_placeholders(ph.generated_sql, time_context)

                            if validation_result["valid"]:
                                # æ‰§è¡Œå ä½ç¬¦æ›¿æ¢
                                replaced_sql = sql_replacer.replace_time_placeholders(ph.generated_sql, time_context)

                                yield {
                                    "type": "sql_replaced",
                                    "placeholder_name": ph.placeholder_name,
                                    "original_sql": ph.generated_sql,
                                    "replaced_sql": replaced_sql,
                                    "replacements": validation_result["placeholder_details"]
                                }

                                # å°†æ›¿æ¢åçš„SQLæ·»åŠ åˆ°å‡†å¤‡å°±ç»ªåˆ—è¡¨
                                ph._final_sql = replaced_sql  # ä¸´æ—¶å­˜å‚¨æœ€ç»ˆSQL
                                placeholders_ready.append(ph)
                            else:
                                yield {
                                    "type": "sql_replacement_failed",
                                    "placeholder_name": ph.placeholder_name,
                                    "missing_placeholders": validation_result["missing_placeholders"],
                                    "warnings": validation_result["warnings"]
                                }

                        except Exception as e:
                            logger.error(f"å ä½ç¬¦æ›¿æ¢å¤±è´¥ {ph.placeholder_name}: {e}")
                            yield {
                                "type": "sql_replacement_failed",
                                "placeholder_name": ph.placeholder_name,
                                "error": str(e)
                            }

                # Step 3: æ‰§è¡Œæ•°æ®æå–ï¼ˆå¯é€‰ï¼Œæ ¹æ®éœ€è¦ï¼‰
                if success_criteria.get("execute_queries", False):
                    yield {
                        "type": "data_extraction_started",
                        "message": f"å¼€å§‹æ‰§è¡Œ {len(placeholders_ready)} ä¸ªå ä½ç¬¦çš„æ•°æ®æŸ¥è¯¢"
                    }

                    for ph in placeholders_ready:
                        try:
                            final_sql = getattr(ph, '_final_sql', ph.generated_sql)
                            # è¿™é‡Œå¯ä»¥è°ƒç”¨å®é™…çš„æ•°æ®åº“æ‰§è¡Œé€»è¾‘
                            # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿç»“æœ
                            yield {
                                "type": "data_extracted",
                                "placeholder_name": ph.placeholder_name,
                                "row_count": 1,  # æ¨¡æ‹Ÿç»“æœ
                                "execution_time_ms": 100
                            }
                        except Exception as e:
                            logger.error(f"æ•°æ®æå–å¤±è´¥ {ph.placeholder_name}: {e}")
                            yield {
                                "type": "data_extraction_failed",
                                "placeholder_name": ph.placeholder_name,
                                "error": str(e)
                            }

                # æœ€ç»ˆç»“æœ
                total_processed = len(placeholders_ready)
                total_failed = len(placeholders) - total_processed

                yield {
                    "type": "agent_session_complete",
                    "success": total_failed == 0,
                    "message": f"ä»»åŠ¡å®Œæˆ: {total_processed} ä¸ªå ä½ç¬¦å¤„ç†æˆåŠŸ, {total_failed} ä¸ªå¤±è´¥",
                    "total_placeholders": len(placeholders),
                    "processed_successfully": total_processed,
                    "failed": total_failed,
                    "time_window": time_window
                }

        except Exception as e:
            logger.error(f"Agentä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {e}")
            yield {
                "type": "agent_session_failed",
                "message": "Agentä»»åŠ¡æ‰§è¡Œå¼‚å¸¸",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    async def _get_template_id_from_context(self, data_source_id: str) -> Optional[str]:
        """ä»ä¸Šä¸‹æ–‡ä¸­è·å–æ¨¡æ¿ID"""
        # å®é™…ä¸Šè¿™ä¸ªæ–¹æ³•åº”è¯¥ä»å¤–éƒ¨ä¼ å…¥template_id
        # åœ¨run_task_with_agentæ–¹æ³•ä¸­æ·»åŠ template_idå‚æ•°
        return None

    def set_template_context(self, template_id: str):
        """è®¾ç½®æ¨¡æ¿ä¸Šä¸‹æ–‡"""
        self.template_id = template_id

    async def _generate_sql_with_agent(
        self,
        placeholder,
        data_source_id: str,
        task_objective: str,
        success_criteria: Dict[str, Any],
        db,
        task_context: Optional[Dict[str, Any]] = None  # ğŸ‘ˆ æ–°å¢ï¼šä»»åŠ¡ä¸Šä¸‹æ–‡å‚æ•°
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨Agentç”Ÿæˆå ä½ç¬¦çš„SQL

        Args:
            placeholder: å ä½ç¬¦å¯¹è±¡
            data_source_id: æ•°æ®æºID
            task_objective: ä»»åŠ¡ç›®æ ‡
            success_criteria: æˆåŠŸæ ‡å‡†
            db: æ•°æ®åº“ä¼šè¯
            task_context: ä»»åŠ¡ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
                - å•å ä½ç¬¦APIï¼šNoneï¼ˆä½¿ç”¨é»˜è®¤å€¼ï¼‰
                - ä»»åŠ¡ä¸­å¤šå ä½ç¬¦ï¼šçœŸå®çš„ä»»åŠ¡ä¿¡æ¯
        """
        try:
            # ğŸ‘‡ è·å–æ•°æ®æºä¿¡æ¯å¹¶æ„å»ºç»Ÿä¸€ä¸Šä¸‹æ–‡
            data_source_info = await self._get_data_source_info(data_source_id)

            user_id = (task_context or {}).get("user_id") or self.user_id
            template_id = getattr(placeholder, "template_id", None)
            template_id_str = str(template_id) if template_id else None
            template_content = ""
            template_snippet = ""
            if template_id_str:
                template_content = self._load_template_content(template_id_str)
                template_snippet = self._extract_placeholder_snippet(
                    template_content,
                    getattr(placeholder, "placeholder_text", ""),
                    getattr(placeholder, "placeholder_name", "")
                )

            raw_schedule = (task_context or {}).get("schedule")
            if raw_schedule and not isinstance(raw_schedule, dict):
                normalized_schedule = {"cron_expression": raw_schedule}
            else:
                normalized_schedule = raw_schedule or {}

            raw_time_window = (task_context or {}).get("time_window") or {}
            normalized_time_window, normalized_time_range = self._normalize_time_window(
                raw_time_window, normalized_schedule
            )

            business_context = {
                "template_id": template_id_str,
                "data_source_id": data_source_id,
                "template_context": {"snippet": template_snippet} if template_snippet else {},
                "execution_context": task_context or {},
                "time_column": normalized_time_window.get("time_column"),
                "data_range": normalized_time_window.get("data_range") or "day",
                "time_window": normalized_time_window,
                "time_range": normalized_time_range,
            }

            business_requirements = await self.domain_service.analyze_placeholder_business_requirements(
                placeholder_text=placeholder.placeholder_text,
                business_context=business_context,
                user_id=user_id
            )
            semantic_type = self._map_business_to_semantic_type(business_requirements)
            schema_context = await self._get_schema_context(user_id, data_source_id)

            data_source_info["semantic_type"] = semantic_type
            data_source_info["business_requirements"] = business_requirements

            context = self._build_unified_context(
                placeholder=placeholder,
                data_source_id=data_source_id,
                success_criteria=success_criteria,
                task_context=task_context,
                data_source_info=data_source_info,
                business_requirements=business_requirements,
                semantic_type=semantic_type,
                template_context={"snippet": template_snippet} if template_snippet else {},
                template_snippet=template_snippet,
                schema_context=schema_context,
                business_context=business_context,
                template_content=template_content,
                normalized_time_window=normalized_time_window,
                normalized_time_range=normalized_time_range,
                schedule=normalized_schedule,
            )

            # æ„å»ºAgentè¾“å…¥
            agent_request = PlaceholderAnalysisRequest(
                placeholder_id=str(placeholder.id),
                business_command=placeholder.placeholder_text,
                requirements=placeholder.description or task_objective,
                context=context,  # ğŸ‘ˆ ä½¿ç”¨ç»Ÿä¸€æ„å»ºçš„ context
                target_objective=task_objective,
                data_source_info=data_source_info
            )

            # è°ƒç”¨å ä½ç¬¦åˆ†æï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
            MAX_RETRIES = 3
            retry_count = 0
            last_error = None

            while retry_count < MAX_RETRIES:
                sql_result = None
                async for event in self.analyze_placeholder(agent_request):
                    logger.debug(f"æ”¶åˆ°äº‹ä»¶: type={event.get('type')}, placeholder_id={event.get('placeholder_id')}")

                    if event.get("type") == "sql_generation_complete":
                        sql_result = event.get("content")
                        logger.info(f"âœ… SQLç”ŸæˆæˆåŠŸ (å°è¯• {retry_count + 1}/{MAX_RETRIES}): placeholder={agent_request.placeholder_id}")
                        break
                    elif event.get("type") == "sql_generation_failed":
                        logger.error(f"âŒ SQLç”Ÿæˆå¤±è´¥ (å°è¯• {retry_count + 1}/{MAX_RETRIES}): error={event.get('error')}")
                        last_error = event.get("error", "SQLç”Ÿæˆå¤±è´¥")
                        break

                # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†SQL
                if not sql_result or not hasattr(sql_result, 'sql_query'):
                    retry_count += 1
                    logger.warning(f"âš ï¸ SQLç”Ÿæˆæœªè¿”å›æœ‰æ•ˆç»“æœï¼Œå‡†å¤‡é‡è¯• ({retry_count}/{MAX_RETRIES})")
                    if retry_count < MAX_RETRIES:
                        # æ›´æ–°agent_requestï¼Œæ·»åŠ é‡è¯•æç¤º
                        agent_request.requirements = f"{agent_request.requirements}\n\nâš ï¸ é‡è¯• {retry_count}: ä¸Šæ¬¡ç”Ÿæˆå¤±è´¥ï¼Œè¯·é‡æ–°å°è¯•"
                        continue
                    else:
                        return {
                            "success": False,
                            "error": last_error or "Agentæœªè¿”å›æœ‰æ•ˆçš„SQLç»“æœ"
                        }

                # éªŒè¯ç”Ÿæˆçš„SQLï¼ˆæ£€æŸ¥åŒé‡å¼•å·ç­‰é—®é¢˜ï¼‰
                generated_sql = sql_result.sql_query
                validation_issues = self._validate_sql_placeholders(generated_sql)

                if validation_issues:
                    logger.warning(f"âš ï¸ SQLéªŒè¯å‘ç°é—®é¢˜ (å°è¯• {retry_count + 1}/{MAX_RETRIES}): {validation_issues}")
                    retry_count += 1

                    if retry_count < MAX_RETRIES:
                        # å°è¯•è‡ªåŠ¨ä¿®å¤
                        fixed_sql = self._fix_sql_placeholder_quotes(generated_sql)
                        if fixed_sql != generated_sql:
                            logger.info(f"âœ… è‡ªåŠ¨ä¿®å¤SQLå ä½ç¬¦å¼•å·é—®é¢˜")
                            return {
                                "success": True,
                                "sql": fixed_sql,
                                "confidence": sql_result.metadata.get('confidence_level', 0.9),
                                "auto_fixed": True
                            }

                        # æ— æ³•è‡ªåŠ¨ä¿®å¤ï¼Œè¯·æ±‚Agenté‡æ–°ç”Ÿæˆ
                        agent_request.requirements = f"""{agent_request.requirements}

âš ï¸ é‡è¯• {retry_count}: ä¸Šæ¬¡ç”Ÿæˆçš„SQLå­˜åœ¨é—®é¢˜: {validation_issues}
è¯·ç‰¹åˆ«æ³¨æ„ï¼š
1. å ä½ç¬¦ {{{{start_date}}}} å’Œ {{{{end_date}}}} å‘¨å›´**ä¸è¦**åŠ å¼•å·
2. æ­£ç¡®æ ¼å¼: WHERE date BETWEEN {{{{start_date}}}} AND {{{{end_date}}}}
3. é”™è¯¯æ ¼å¼: WHERE date BETWEEN '{{{{start_date}}}}' AND '{{{{end_date}}}}'"""
                        continue
                    else:
                        # è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œå°è¯•æœ€åä¸€æ¬¡è‡ªåŠ¨ä¿®å¤
                        fixed_sql = self._fix_sql_placeholder_quotes(generated_sql)
                        return {
                            "success": True,
                            "sql": fixed_sql,
                            "confidence": sql_result.metadata.get('confidence_level', 0.7),
                            "auto_fixed": True,
                            "warning": f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œä½¿ç”¨è‡ªåŠ¨ä¿®å¤çš„SQL: {validation_issues}"
                        }

                # SQLéªŒè¯é€šè¿‡
                logger.info(f"âœ… SQLéªŒè¯é€šè¿‡: placeholder={agent_request.placeholder_id}")
                return {
                    "success": True,
                    "sql": generated_sql,
                    "confidence": sql_result.metadata.get('confidence_level', 0.9),
                    "validated": True
                }

            # è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
            return {
                "success": False,
                "error": f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({MAX_RETRIES})ï¼Œæœ€åé”™è¯¯: {last_error or 'æœªçŸ¥é”™è¯¯'}"
            }

        except Exception as e:
            logger.error(f"Agent SQLç”Ÿæˆå¼‚å¸¸: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    def _validate_sql_placeholders(self, sql: str) -> Optional[str]:
        """
        éªŒè¯SQLä¸­çš„å ä½ç¬¦æ ¼å¼ï¼Œæ£€æŸ¥æ˜¯å¦å­˜åœ¨åŒé‡å¼•å·ç­‰é—®é¢˜

        Args:
            sql: å¾…éªŒè¯çš„SQL

        Returns:
            å¦‚æœæœ‰é—®é¢˜è¿”å›é”™è¯¯æè¿°ï¼Œå¦åˆ™è¿”å›None
        """
        import re

        # æ£€æŸ¥æ˜¯å¦æœ‰å¸¦å¼•å·çš„å ä½ç¬¦: '{{...}}' æˆ– "{{...}}"
        quoted_placeholder_pattern = r"""['"]{{[^}]+}}['"]"""
        matches = re.findall(quoted_placeholder_pattern, sql)

        if matches:
            return f"å‘ç°å ä½ç¬¦å‘¨å›´æœ‰å¼•å·: {matches}ï¼Œè¿™ä¼šå¯¼è‡´åŒé‡å¼•å·é”™è¯¯"

        return None

    def _fix_sql_placeholder_quotes(self, sql: str) -> str:
        """
        è‡ªåŠ¨ä¿®å¤SQLä¸­å ä½ç¬¦å‘¨å›´çš„å¼•å·é—®é¢˜

        ç§»é™¤å ä½ç¬¦å‘¨å›´çš„å•å¼•å·æˆ–åŒå¼•å·ï¼Œå› ä¸ºå ä½ç¬¦æ›¿æ¢æ—¶ä¼šè‡ªåŠ¨æ·»åŠ å¼•å·

        Args:
            sql: åŸå§‹SQL

        Returns:
            ä¿®å¤åçš„SQL
        """
        import re

        # ç§»é™¤å ä½ç¬¦å‘¨å›´çš„å¼•å·
        # åŒ¹é…æ¨¡å¼: '{{placeholder}}' -> {{placeholder}}
        # æˆ–: "{{placeholder}}" -> {{placeholder}}
        fixed_sql = re.sub(r"""['"](\{\{[^}]+\}\})['"]""", r'\1', sql)

        if fixed_sql != sql:
            logger.info(f"ğŸ”§ è‡ªåŠ¨ä¿®å¤SQLå ä½ç¬¦å¼•å·")
            logger.debug(f"   åŸSQL: {sql[:200]}...")
            logger.debug(f"   ä¿®å¤å: {fixed_sql[:200]}...")

        return fixed_sql

    def _build_unified_context(
        self,
        placeholder,
        data_source_id: str,
        success_criteria: Dict[str, Any],
        task_context: Optional[Dict[str, Any]] = None,
        data_source_info: Optional[Dict[str, Any]] = None,
        business_requirements: Optional[Dict[str, Any]] = None,
        semantic_type: Optional[str] = None,
        template_context: Optional[Dict[str, Any]] = None,
        template_snippet: Optional[str] = None,
        schema_context: Optional[Dict[str, Any]] = None,
        business_context: Optional[Dict[str, Any]] = None,
        template_content: Optional[str] = None,
        normalized_time_window: Optional[Dict[str, Any]] = None,
        normalized_time_range: Optional[Dict[str, Any]] = None,
        schedule: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """
        æ„å»ºç»Ÿä¸€çš„ contextï¼ˆåŒºåˆ†çœŸå®å€¼ vs é»˜è®¤å€¼ï¼‰

        Args:
            placeholder: å ä½ç¬¦å¯¹è±¡
            data_source_id: æ•°æ®æºID
            success_criteria: æˆåŠŸæ ‡å‡†
            task_context: ä»»åŠ¡ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
                - Noneï¼šå•å ä½ç¬¦APIåœºæ™¯ï¼Œä½¿ç”¨é»˜è®¤å€¼
                - Dictï¼šä»»åŠ¡åœºæ™¯ï¼Œä½¿ç”¨çœŸå®å€¼

        Returns:
            ç»Ÿä¸€çš„ context å­—å…¸
        """
        # ğŸ”¹ åŸºç¡€å­—æ®µï¼ˆä¸¤ç§åœºæ™¯éƒ½æœ‰ï¼‰
        context = {
            "data_source_id": data_source_id,
            "placeholder_type": getattr(placeholder, "placeholder_type", None),
            "placeholder_name": getattr(placeholder, "placeholder_name", None),
            "content_type": getattr(placeholder, "content_type", None),
            "template_id": str(getattr(placeholder, "template_id", "") or "") or None,
        }

        # ğŸ”¹ ä» success_criteria æå–ä¿¡æ¯
        context["required_fields"] = success_criteria.get("required_fields", [])
        context["quality_threshold"] = success_criteria.get("quality_threshold", 0.6)

        # ğŸ”¹ ä» placeholder å¯¹è±¡æå–æ›´å¤šä¿¡æ¯
        context["execution_order"] = getattr(placeholder, "execution_order", 0)
        context["is_required"] = getattr(placeholder, "is_required", True)
        context["confidence_score"] = getattr(placeholder, "confidence_score", 0.0)

        # æå–è§£æå…ƒæ•°æ®
        parsing_metadata = getattr(placeholder, "parsing_metadata", None)
        if isinstance(parsing_metadata, dict):
            context["parsing_metadata"] = parsing_metadata

        # ğŸ”¹ æ•°æ®æºä¸Šä¸‹æ–‡
        if data_source_info:
            ds_id = data_source_info.get("data_source_id") or data_source_info.get("id") or data_source_id
            normalized_ds = {
                "id": str(ds_id) if ds_id else None,
                "data_source_id": str(ds_id) if ds_id else None,
                "name": data_source_info.get("name"),
                "source_type": data_source_info.get("source_type"),
                "connection_config": data_source_info.get("connection_config") or data_source_info
            }
            context["data_source"] = normalized_ds
            context["data_source_id"] = normalized_ds["data_source_id"]
            context["data_source_info"] = data_source_info
            if data_source_info.get("name"):
                context.setdefault("data_source_name", data_source_info.get("name"))

        # ğŸ”¹ ä»»åŠ¡ä¸Šä¸‹æ–‡å¤„ç†ï¼ˆåŒºåˆ†çœŸå®å€¼ vs é»˜è®¤å€¼ï¼‰
        if task_context:
            # âœ… ä»»åŠ¡åœºæ™¯ï¼šä½¿ç”¨çœŸå®çš„ä»»åŠ¡ä¿¡æ¯
            logger.info(f"ğŸ“¦ ä½¿ç”¨çœŸå®ä»»åŠ¡ä¸Šä¸‹æ–‡: task_id={task_context.get('task_id')}")

            context["task_id"] = task_context.get("task_id")
            context["task_name"] = task_context.get("task_name")
            context["report_period"] = task_context.get("report_period")
            context["user_id"] = task_context.get("user_id")

            raw_schedule = schedule if schedule is not None else task_context.get("schedule")
            if raw_schedule and not isinstance(raw_schedule, dict):
                schedule_dict = {"cron_expression": raw_schedule}
            else:
                schedule_dict = raw_schedule or {}
            context["schedule"] = schedule_dict

            raw_time_window = task_context.get("time_window") or {}
            if normalized_time_window is None or normalized_time_range is None:
                normalized_time_window, normalized_time_range = self._normalize_time_window(
                    raw_time_window,
                    schedule_dict
                )
            context["time_window"] = normalized_time_window
            context["time_range"] = normalized_time_range

            context["time_context"] = task_context.get("time_context")  # å®Œæ•´æ—¶é—´ä¸Šä¸‹æ–‡

            # æ‰§è¡Œä¸Šä¸‹æ–‡ï¼ˆçœŸå®å€¼ï¼‰
            context["execution_trigger"] = task_context.get("execution_trigger", "scheduled")
            context["execution_id"] = task_context.get("execution_id")

        else:
            # âš ï¸ APIåœºæ™¯ï¼šæ„é€ é»˜è®¤å€¼
            logger.info("ğŸ“¦ ä½¿ç”¨é»˜è®¤ä¸Šä¸‹æ–‡ï¼ˆå•å ä½ç¬¦APIåœºæ™¯ï¼‰")

            context["template_id"] = str(placeholder.template_id) if hasattr(placeholder, "template_id") else None
            context["execution_trigger"] = "manual"  # API æ‰‹åŠ¨è§¦å‘

            # æ—¶é—´ä¿¡æ¯ï¼ˆé»˜è®¤å€¼ï¼‰
            from datetime import datetime
            from app.utils.time_context import TimeContextManager

            # æ„é€ é»˜è®¤çš„æ—¶é—´ä¸Šä¸‹æ–‡ï¼ˆæ¯å¤© 9ç‚¹ï¼ŒæŸ¥è¯¢æ˜¨å¤©çš„æ•°æ®ï¼‰
            default_cron = "0 9 * * *"
            time_manager = TimeContextManager()
            default_time_ctx = time_manager.build_task_time_context(default_cron, datetime.now())

            schedule_dict = {
                "cron_expression": default_cron,
                "timezone": default_time_ctx.get("timezone", "Asia/Shanghai"),
            }
            context["schedule"] = schedule_dict
            default_raw_time_window = {
                "start": default_time_ctx.get("data_start_time") and f"{default_time_ctx.get('data_start_time')} 00:00:00",
                "end": default_time_ctx.get("data_end_time") and f"{default_time_ctx.get('data_end_time')} 23:59:59",
                "time_column": None,
                "timezone": default_time_ctx.get("timezone", "Asia/Shanghai"),
                "data_range": "day",
            }
            normalized_default_window, normalized_default_range = self._normalize_time_window(
                default_raw_time_window,
                schedule_dict
            )
            context["time_window"] = normalized_default_window
            context["time_range"] = normalized_default_range
            context["time_context"] = default_time_ctx
            logger.info(f"ğŸ•’ ä½¿ç”¨é»˜è®¤æ—¶é—´çª—å£: {context['time_window']}")

        # ğŸ”¹ ä¸šåŠ¡ä¸è¯­ä¹‰ä¿¡æ¯
        if business_context:
            context["business_context"] = business_context
        if business_requirements:
            context["business_requirements"] = business_requirements
            if business_requirements.get("top_n") is not None:
                context.setdefault("top_n", business_requirements.get("top_n"))
        if semantic_type:
            context["semantic_type"] = semantic_type
            context["placeholder_type"] = semantic_type

        # ğŸ”¹ æ¨¡æ¿ä¸Šä¸‹æ–‡
        if template_context:
            context["template_context"] = template_context
        if template_snippet:
            context["placeholder_context_snippet"] = template_snippet
        if template_content:
            context["template_content_preview"] = template_content[:500]

        # ğŸ”¹ Schemaä¸Šä¸‹æ–‡
        if schema_context:
            context["schema_context"] = schema_context

        # é»˜è®¤çš„è§„åˆ’æç¤ºå ä½
        context.setdefault("planning_hints", {})

        return context

    def _normalize_time_window(
        self,
        raw_time_window: Optional[Dict[str, Any]],
        schedule: Optional[Dict[str, Any]] = None,
        default_timezone: str = "Asia/Shanghai",
    ) -> Tuple[Dict[str, Any], Dict[str, Any]]:
        """è§„èŒƒåŒ–æ—¶é—´çª—å£ä¿¡æ¯ï¼Œç”Ÿæˆç»Ÿä¸€çš„time_windowä¸time_rangeç»“æ„"""
        raw_time_window = raw_time_window or {}
        schedule = schedule or {}
        if isinstance(schedule, str):
            schedule = {"cron_expression": schedule}

        timezone = (
            raw_time_window.get("timezone")
            or schedule.get("timezone")
            or default_timezone
        )
        start_dt = raw_time_window.get("start") or raw_time_window.get("start_datetime")
        end_dt = raw_time_window.get("end") or raw_time_window.get("end_datetime")
        start_date = raw_time_window.get("start_date")
        end_date = raw_time_window.get("end_date")

        if start_dt and not start_date:
            start_date = start_dt.split(" ")[0]
        if end_dt and not end_date:
            end_date = end_dt.split(" ")[0]

        normalized_time_window = {
            "start": start_dt,
            "end": end_dt,
            "start_date": start_date,
            "end_date": end_date,
            "time_column": raw_time_window.get("time_column"),
            "timezone": timezone,
            "data_range": raw_time_window.get("data_range"),
        }

        normalized_time_range = {
            "start_date": start_date,
            "end_date": end_date,
            "time_column": raw_time_window.get("time_column"),
            "timezone": timezone,
        }

        return normalized_time_window, normalized_time_range

    def _map_business_to_semantic_type(self, business_requirements: Optional[Dict[str, Any]]) -> str:
        """å°†ä¸šåŠ¡éœ€æ±‚æ˜ å°„åˆ°Agentå·¥å…·çš„è¯­ä¹‰ç±»å‹"""
        if not business_requirements:
            return "stat"

        business_type = str(business_requirements.get("business_type", "")).lower()
        semantic_intent = str(business_requirements.get("semantic_intent", "")).lower()

        if "ranking" in business_type or "top" in semantic_intent or "æ’è¡Œ" in semantic_intent:
            return "ranking"
        if "compare" in business_type or "å¯¹æ¯”" in semantic_intent or "æ¯”è¾ƒ" in semantic_intent:
            return "compare"
        if "period" in business_type or "å‘¨æœŸ" in semantic_intent or "æ—¶é—´" in semantic_intent:
            return "period"
        if "chart" in business_type or "å›¾è¡¨" in semantic_intent:
            return "chart"
        return "stat"

    def _extract_placeholder_snippet(self, template_text: str, placeholder_text: str, placeholder_name: str) -> str:
        """ä»æ¨¡æ¿ä¸­æå–åŒ…å«å ä½ç¬¦çš„æ®µè½"""
        try:
            if not template_text:
                return ""
            lines = template_text.splitlines()
            keys = [k for k in [placeholder_text, placeholder_name, placeholder_name and f"{{{{{placeholder_name}}}}}"] if k]
            hit_idx = -1
            for i, line in enumerate(lines):
                for key in keys:
                    if key and key in line:
                        hit_idx = i
                        break
                if hit_idx >= 0:
                    break

            if hit_idx < 0:
                preview = template_text[:500]
                return preview + ("â€¦" if len(template_text) > 500 else "")

            start = hit_idx
            while start > 0 and lines[start].strip() != "" and lines[start - 1].strip() != "":
                start -= 1
            end = hit_idx
            while end + 1 < len(lines) and lines[end].strip() != "" and lines[end + 1].strip() != "":
                end += 1
            snippet_lines = lines[start:end + 1]
            if start > 0:
                snippet_lines.insert(0, lines[start - 1])
            if end + 1 < len(lines):
                snippet_lines.append(lines[end + 1])
            return "\n".join(snippet_lines).strip()
        except Exception:
            preview = template_text[:500]
            return preview + ("â€¦" if len(template_text) > 500 else "")

    def _load_template_content(self, template_id: str) -> str:
        """åŠ è½½æ¨¡æ¿å†…å®¹"""
        try:
            from app.db.session import get_db_session
            from app import crud

            with get_db_session() as db_session:
                template_obj = crud.template.get(db_session, id=template_id)
                if template_obj and getattr(template_obj, "content", None):
                    return template_obj.content
        except Exception as e:
            logger.warning(f"åŠ è½½æ¨¡æ¿å†…å®¹å¤±è´¥: {e}")
        return ""

    async def _get_schema_context(self, user_id: Optional[str], data_source_id: str) -> Dict[str, Any]:
        """è·å–æ•°æ®æºSchemaä¿¡æ¯ï¼Œç”¨äºæŒ‡å¯¼Agentç”ŸæˆSQL"""
        if not data_source_id:
            return {"available_tables": [], "table_count": 0}

        try:
            builder = DataSourceContextBuilder(container=self.container)
            context_result = await builder.build_data_source_context(
                user_id=user_id or "system",
                data_source_id=data_source_id,
                required_tables=None,
                force_refresh=False,
                names_only=True
            )
            if context_result and context_result.get("success"):
                tables_payload = context_result.get("tables", [])
                tables: List[str] = []
                for table_info in tables_payload:
                    if isinstance(table_info, dict):
                        name = table_info.get("table_name")
                        if name:
                            tables.append(name)
                return {
                    "available_tables": tables,
                    "table_count": len(tables)
                }
        except Exception as e:
            logger.warning(f"è·å–Schemaä¸Šä¸‹æ–‡å¤±è´¥: {e}")
        return {"available_tables": [], "table_count": 0}

    async def _get_data_source_info(self, data_source_id: str) -> Dict[str, Any]:
        """è·å–æ•°æ®æºä¿¡æ¯"""
        try:
            if not data_source_id:
                return {}

            normalized_id = str(data_source_id)

            # ä¼˜å…ˆä½¿ç”¨å®¹å™¨æä¾›çš„ç”¨æˆ·æ•°æ®æºæœåŠ¡ï¼ˆæ”¯æŒå¯†ç è§£å¯†ï¼‰
            user_id = self.user_id or ""
            user_ds_service = getattr(self.container, "user_data_source_service", None)
            if user_ds_service and user_id:
                try:
                    ds_obj = await user_ds_service.get_user_data_source(user_id, normalized_id)
                    if ds_obj and hasattr(ds_obj, "connection_config"):
                        cfg = dict(ds_obj.connection_config or {})
                        # è¡¥å…¨åŸºç¡€å­—æ®µ
                        source_type = getattr(ds_obj, "source_type", cfg.get("source_type"))
                        if hasattr(source_type, "value"):
                            source_type = source_type.value
                        if isinstance(source_type, str):
                            cfg.setdefault("source_type", source_type.split(".")[-1])
                        cfg.setdefault("name", getattr(ds_obj, "name", ""))
                        cfg.setdefault("id", normalized_id)
                        cfg.setdefault("data_source_id", normalized_id)
                        # å…¼å®¹å¸¸ç”¨å­—æ®µ
                        cfg.setdefault("database_name", cfg.get("database") or cfg.get("schema"))
                        return cfg
                except Exception as svc_error:
                    logger.warning(f"ä½¿ç”¨ç”¨æˆ·æ•°æ®æºæœåŠ¡è·å–é…ç½®å¤±è´¥: {svc_error}")

            # å›é€€ï¼šç›´æ¥è¯»å–æ•°æ®æºè®°å½•
            from app.crud import data_source as crud_data_source
            from app.db.session import get_db_session
            from app.models.data_source import DataSourceType
            from app.core.security_utils import decrypt_data
            from app.core.data_source_utils import DataSourcePasswordManager

            with get_db_session() as db:
                data_source = crud_data_source.get(db, id=normalized_id)
                if not data_source:
                    return {}

                info: Dict[str, Any] = {
                    "id": normalized_id,
                    "data_source_id": normalized_id,
                    "name": data_source.name,
                }

                if data_source.source_type == DataSourceType.doris:
                    info.update({
                        "source_type": "doris",
                        "database": getattr(data_source, "doris_database", "default"),
                        "database_name": getattr(data_source, "doris_database", "default"),
                        "fe_hosts": list(getattr(data_source, "doris_fe_hosts", []) or ["localhost"]),
                        "be_hosts": list(getattr(data_source, "doris_be_hosts", []) or ["localhost"]),
                        "http_port": getattr(data_source, "doris_http_port", 8030),
                        "query_port": getattr(data_source, "doris_query_port", 9030),
                        "username": getattr(data_source, "doris_username", "root"),
                        "password": DataSourcePasswordManager.get_password(data_source.doris_password) if getattr(data_source, "doris_password", None) else "",
                        "timeout": 30
                    })
                elif data_source.source_type == DataSourceType.sql:
                    conn_str = data_source.connection_string
                    try:
                        if conn_str:
                            conn_str = decrypt_data(conn_str)
                    except Exception:
                        pass
                    info.update({
                        "source_type": "sql",
                        "connection_string": conn_str,
                        "database": getattr(data_source, "database_name", None),
                        "database_name": getattr(data_source, "database_name", None),
                        "host": getattr(data_source, "host", None),
                        "port": getattr(data_source, "port", None),
                        "username": getattr(data_source, "username", None),
                        "password": getattr(data_source, "password", None),
                    })
                else:
                    source_type = data_source.source_type.value if hasattr(data_source.source_type, "value") else str(data_source.source_type)
                    info.setdefault("source_type", source_type)

                return info

        except Exception as e:
            logger.error(f"è·å–æ•°æ®æºä¿¡æ¯å¤±è´¥: {e}")
            return {}

    # ==================== å¤šæ­¥éª¤SQLç”Ÿæˆè¾…åŠ©æ–¹æ³• ====================

    def _build_data_source_config(self, request: PlaceholderAnalysisRequest) -> Dict[str, Any]:
        """æ„å»ºæ•°æ®æºé…ç½®"""
        if not request.data_source_info:
            return {}

        ds_config = dict(request.data_source_info)
        ds_id = ds_config.get('id') or ds_config.get('data_source_id')
        if ds_id:
            ds_config.setdefault("id", str(ds_id))
            ds_config.setdefault("data_source_id", str(ds_id))

        return ds_config

    async def _discover_schema(self, data_source_config: Dict[str, Any], request: PlaceholderAnalysisRequest):
        """
        Schema Discovery - æ¢ç´¢æ•°æ®åº“schema

        æ­¥éª¤ï¼š
        1. è·å–æ‰€æœ‰è¡¨
        2. ä½¿ç”¨Agentåˆ†æä¸šåŠ¡éœ€æ±‚ï¼Œé€‰æ‹©ç›¸å…³è¡¨
        3. è·å–ç›¸å…³è¡¨çš„åˆ—ä¿¡æ¯
        """
        from app.services.infrastructure.agents.tools.schema_tools import SchemaListTablesTool, SchemaListColumnsTool
        from app.services.infrastructure.agents.types import SchemaInfo

        # 1. è·å–æ‰€æœ‰è¡¨
        schema_list_tables_tool = SchemaListTablesTool(container=self.container)
        tables_result = await schema_list_tables_tool.execute({
            "data_source": data_source_config
        })

        if not tables_result.get("success"):
            raise RuntimeError(f"è·å–è¡¨åˆ—è¡¨å¤±è´¥: {tables_result.get('error')}")

        all_tables = tables_result.get("tables", [])
        logger.info(f"ğŸ“Š å‘ç° {len(all_tables)} ä¸ªè¡¨: {all_tables[:10]}...")

        # 2. é€‰æ‹©ç›¸å…³è¡¨ï¼ˆç®€åŒ–ç‰ˆï¼šä½¿ç”¨å…³é”®è¯åŒ¹é…æˆ–ä½¿ç”¨æ‰€æœ‰è¡¨ï¼‰
        relevant_tables = await self._select_relevant_tables(
            all_tables,
            request.business_command,
            request.target_objective or request.requirements
        )

        logger.info(f"ğŸ¯ é€‰æ‹©äº† {len(relevant_tables)} ä¸ªç›¸å…³è¡¨: {relevant_tables}")

        # 3. è·å–ç›¸å…³è¡¨çš„åˆ—ä¿¡æ¯
        schema_list_columns_tool = SchemaListColumnsTool(container=self.container)
        columns_result = await schema_list_columns_tool.execute({
            "data_source": data_source_config,
            "tables": relevant_tables
        })

        if not columns_result.get("success"):
            raise RuntimeError(f"è·å–åˆ—ä¿¡æ¯å¤±è´¥: {columns_result.get('error')}")

        # æ„å»ºSchemaInfo
        column_details = columns_result.get("column_details", {})
        schema_info = SchemaInfo(
            tables=relevant_tables,
            columns={
                table: [col.get("name") for col in cols if col.get("name")]
                for table, cols in column_details.items()
            }
        )

        return schema_info

    async def _select_relevant_tables(
        self,
        all_tables: List[str],
        business_command: str,
        target_objective: str
    ) -> List[str]:
        """
        é€‰æ‹©ç›¸å…³è¡¨

        ä½¿ç”¨å…³é”®è¯åŒ¹é…é€‰æ‹©ä¸ä¸šåŠ¡éœ€æ±‚ç›¸å…³çš„è¡¨
        """
        combined_text = f"{business_command} {target_objective}".lower()

        # è®¡ç®—è¡¨çš„ç›¸å…³æ€§å¾—åˆ†
        scored_tables = []
        for table in all_tables:
            score = 0
            table_lower = table.lower()

            # å®Œå…¨åŒ¹é…
            if table_lower in combined_text:
                score += 10

            # éƒ¨åˆ†åŒ¹é…
            for word in table_lower.split('_'):
                if len(word) > 2 and word in combined_text:
                    score += 1

            if score > 0:
                scored_tables.append((table, score))

        # æ’åºå¹¶å–å‰5ä¸ª
        scored_tables.sort(key=lambda x: x[1], reverse=True)
        selected = [table for table, _ in scored_tables[:5]]

        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ï¼Œä½¿ç”¨å‰3ä¸ªè¡¨
        if not selected and all_tables:
            selected = all_tables[:3]

        return selected or all_tables

    async def _generate_sql_with_schema(
        self,
        request: PlaceholderAnalysisRequest,
        schema_info,
        data_source_config: Dict[str, Any]
    ) -> Tuple[str, str]:
        """
        åŸºäºç²¾ç¡®schemaç”ŸæˆSQL

        è¿”å›: (sql, reasoning)
        """
        import json
        from app.services.infrastructure.agents.types import AgentInput, PlaceholderSpec, TaskContext

        # æ„å»ºschemaæç¤º
        schema_prompt = self._build_schema_prompt(schema_info)

        # è¯†åˆ«æ—¶é—´å­—æ®µ
        time_columns = self._identify_time_columns(schema_info)

        # æ„å»ºæ—¶é—´ä¿¡æ¯å’Œè¦æ±‚
        time_window = None
        time_requirement = ""
        if isinstance(request.context, dict):
            time_window = request.context.get("time_window") or request.context.get("time_context")

        if time_columns:
            time_col_list = ", ".join(time_columns)
            if time_window:
                time_requirement = f"""
## âš ï¸ æ—¶é—´è¿‡æ»¤è¦æ±‚ï¼ˆå¿…é¡»éµå®ˆï¼‰
**è¿™æ˜¯ä¸€ä¸ªåŸºäºæ—¶é—´å‘¨æœŸçš„ç»Ÿè®¡æŸ¥è¯¢ï¼Œå¿…é¡»åŒ…å«æ—¶é—´è¿‡æ»¤æ¡ä»¶ï¼**

- å¯ç”¨çš„æ—¶é—´å­—æ®µ: {time_col_list}
- æ—¶é—´èŒƒå›´: {json.dumps(time_window, ensure_ascii=False) if time_window else "å‘¨æœŸå†…æ•°æ®"}
- **å¿…é¡»åœ¨WHEREå­å¥ä¸­ä½¿ç”¨æ—¶é—´å­—æ®µè¿›è¡Œè¿‡æ»¤**
- æ¨èä½¿ç”¨å ä½ç¬¦æ ¼å¼: WHERE {time_columns[0]} = '{{{{start_date}}}}'
- æˆ–ä½¿ç”¨BETWEEN: WHERE {time_columns[0]} BETWEEN '{{{{start_date}}}}' AND '{{{{end_date}}}}'
"""
            else:
                time_requirement = f"""
## âš ï¸ æ—¶é—´è¿‡æ»¤è¦æ±‚ï¼ˆå¿…é¡»éµå®ˆï¼‰
**è¿™æ˜¯ä¸€ä¸ªåŸºäºæ—¶é—´å‘¨æœŸçš„ç»Ÿè®¡æŸ¥è¯¢ï¼Œå¿…é¡»åŒ…å«æ—¶é—´è¿‡æ»¤æ¡ä»¶ï¼**

- å¯ç”¨çš„æ—¶é—´å­—æ®µ: {time_col_list}
- **å¿…é¡»åœ¨WHEREå­å¥ä¸­ä½¿ç”¨æ—¶é—´å­—æ®µè¿›è¡Œè¿‡æ»¤**
- ä½¿ç”¨å ä½ç¬¦æ ¼å¼: WHERE {time_columns[0]} = '{{{{start_date}}}}'
"""

        # æ„å»ºprompt
        user_prompt = f"""
è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯ç”ŸæˆSQLæŸ¥è¯¢ï¼š

## ä¸šåŠ¡éœ€æ±‚
{request.business_command}

## ç›®æ ‡
{request.target_objective or request.requirements}

## å¯ç”¨çš„æ•°æ®åº“Schemaï¼ˆè¯·ä¸¥æ ¼ä½¿ç”¨ä»¥ä¸‹è¡¨å’Œåˆ—ï¼‰
{schema_prompt}
{time_requirement}

## è¦æ±‚
1. **åªèƒ½ä½¿ç”¨ä¸Šè¿°Schemaä¸­å­˜åœ¨çš„è¡¨å’Œåˆ—**
2. åˆ—åå¿…é¡»å®Œå…¨åŒ¹é…
3. ç”Ÿæˆæ ‡å‡†çš„SQLæŸ¥è¯¢è¯­å¥
4. **âœ… å¿…é¡»åŒ…å«æ—¶é—´è¿‡æ»¤æ¡ä»¶**ï¼ˆä½¿ç”¨ä¸Šè¿°æ—¶é—´å­—æ®µï¼‰
5. è¿”å›JSONæ ¼å¼: {{"sql": "SELECT ...", "reasoning": "è§£é‡ŠSQLç”Ÿæˆçš„æ€è·¯", "time_filter_applied": true}}

è¯·ç”ŸæˆSQLï¼š
"""

        # æ„å»ºAgentInput
        agent_input = AgentInput(
            user_prompt=user_prompt,
            placeholder=PlaceholderSpec(
                id=request.placeholder_id,
                description=request.business_command,
                type="sql_generation",
                granularity="daily"
            ),
            schema=schema_info,
            context=TaskContext(
                task_time=int(datetime.now().timestamp()),
                timezone="Asia/Shanghai"
            ),
            data_source=data_source_config,
            task_driven_context={
                "stage": "sql_generation",
                "business_command": request.business_command,
                "requirements": request.requirements,
                "target_objective": request.target_objective
            },
            user_id=self.user_id
        )

        # è°ƒç”¨Agentç”ŸæˆSQL
        result = await self.agent_service.execute(agent_input)

        if not result.success:
            raise RuntimeError(f"SQLç”Ÿæˆå¤±è´¥: {result.error}")

        # è§£æç»“æœ
        output = result.result
        sql = ""
        reasoning = ""

        if isinstance(output, dict):
            sql = output.get("sql", "")
            reasoning = output.get("reasoning", "")
        elif isinstance(output, str):
            try:
                parsed = json.loads(output)
                sql = parsed.get("sql", output)
                reasoning = parsed.get("reasoning", "")
            except:
                sql = output
                reasoning = "ç›´æ¥ç”Ÿæˆ"

        return sql, reasoning

    def _identify_time_columns(self, schema_info) -> List[str]:
        """
        è¯†åˆ«Schemaä¸­çš„æ—¶é—´å­—æ®µ

        å¸¸è§æ—¶é—´å­—æ®µåæ¨¡å¼:
        - dt, date, time, datetime, timestamp
        - created_at, updated_at, create_time, update_time
        - *_date, *_time, *_at
        """
        time_patterns = [
            'dt', 'date', 'time', 'datetime', 'timestamp',
            'created', 'updated', 'deleted',
            'create_time', 'update_time', 'delete_time',
            'created_at', 'updated_at', 'deleted_at',
            'start_date', 'end_date', 'start_time', 'end_time'
        ]

        time_columns = []
        for table, columns in schema_info.columns.items():
            for col in columns:
                col_lower = col.lower()
                # å®Œå…¨åŒ¹é…
                if col_lower in time_patterns:
                    time_columns.append(col)
                # åç¼€åŒ¹é…
                elif any(col_lower.endswith(suffix) for suffix in ['_date', '_time', '_at', '_datetime', '_timestamp']):
                    time_columns.append(col)
                # å‰ç¼€åŒ¹é…
                elif any(col_lower.startswith(prefix) for prefix in ['date_', 'time_', 'dt_']):
                    time_columns.append(col)

        # å»é‡å¹¶ä¿æŒé¡ºåº
        seen = set()
        unique_time_columns = []
        for col in time_columns:
            if col not in seen:
                seen.add(col)
                unique_time_columns.append(col)

        return unique_time_columns

    def _build_schema_prompt(self, schema_info) -> str:
        """æ„å»ºSchemaæç¤ºæ–‡æœ¬"""
        lines = []

        for table in schema_info.tables:
            columns = schema_info.columns.get(table, [])
            if columns:
                col_list = ", ".join(columns)
                lines.append(f"### è¡¨: {table}")
                lines.append(f"åˆ—: {col_list}")
                lines.append("")

        return "\n".join(lines)

    async def _validate_sql(self, sql: str, schema_info, require_time_filter: bool = True) -> Dict[str, Any]:
        """
        éªŒè¯SQLæ­£ç¡®æ€§

        æ£€æŸ¥ï¼š
        1. SQLæ˜¯å¦ä¸ºç©º
        2. æ˜¯å¦åŒ…å«SELECT/WITHè¯­å¥
        3. æ˜¯å¦ä½¿ç”¨äº†ä¸å­˜åœ¨çš„åˆ—ï¼ˆåŸºæœ¬æ£€æŸ¥ï¼‰
        4. **æ˜¯å¦åŒ…å«æ—¶é—´è¿‡æ»¤æ¡ä»¶**ï¼ˆé‡è¦ï¼ï¼‰
        """
        import re

        validation_result = {
            "is_valid": True,
            "errors": [],
            "warnings": [],
            "suggestions": [],
            "has_time_filter": False
        }

        # 1. æ£€æŸ¥SQLæ˜¯å¦ä¸ºç©º
        if not sql or not sql.strip():
            validation_result["is_valid"] = False
            validation_result["errors"].append("SQLä¸ºç©º")
            return validation_result

        # 2. åŸºæœ¬è¯­æ³•æ£€æŸ¥
        sql_upper = sql.upper()
        if not any(keyword in sql_upper for keyword in ["SELECT", "WITH"]):
            validation_result["is_valid"] = False
            validation_result["errors"].append("SQLå¿…é¡»åŒ…å«SELECTæˆ–WITHè¯­å¥")

        # 3. è·å–æ‰€æœ‰æœ‰æ•ˆåˆ—å
        all_valid_columns = set()
        for columns in schema_info.columns.values():
            all_valid_columns.update(columns)

        # 4. æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†ä¸å­˜åœ¨çš„åˆ—ï¼ˆç®€åŒ–æ£€æŸ¥ï¼‰
        # æå–æ½œåœ¨çš„åˆ—å
        potential_columns = re.findall(r'\b([a-zA-Z_][a-zA-Z0-9_]*)\b', sql)

        # SQLå…³é”®å­—
        sql_keywords = {
            'SELECT', 'FROM', 'WHERE', 'AND', 'OR', 'NOT', 'IN', 'BETWEEN', 'LIKE',
            'ORDER', 'BY', 'GROUP', 'HAVING', 'JOIN', 'LEFT', 'RIGHT', 'INNER', 'OUTER',
            'AS', 'ON', 'LIMIT', 'OFFSET', 'COUNT', 'SUM', 'AVG', 'MAX', 'MIN',
            'DISTINCT', 'CASE', 'WHEN', 'THEN', 'ELSE', 'END', 'NULL', 'IS', 'ASC', 'DESC',
            'UNION', 'ALL', 'EXISTS', 'ANY', 'SOME', 'CAST', 'CONVERT'
        }

        # æ£€æŸ¥æœªçŸ¥åˆ—
        unknown_columns = []
        for col in potential_columns:
            if col.upper() not in sql_keywords and col not in all_valid_columns:
                # è¿›ä¸€æ­¥æ£€æŸ¥æ˜¯å¦çœŸçš„åœ¨SQLä¸­ä½œä¸ºåˆ—ä½¿ç”¨
                pattern = rf'\b{re.escape(col)}\b\s*(=|<|>|!=|IN|BETWEEN|LIKE|,|\))'
                if re.search(pattern, sql, re.IGNORECASE):
                    unknown_columns.append(col)

        if unknown_columns:
            validation_result["is_valid"] = False
            validation_result["errors"].append(f"ä½¿ç”¨äº†ä¸å­˜åœ¨çš„åˆ—: {', '.join(unknown_columns)}")
            validation_result["suggestions"].append(
                f"è¯·æ£€æŸ¥åˆ—åï¼Œå¯ç”¨çš„åˆ—åŒ…æ‹¬: {', '.join(sorted(list(all_valid_columns))[:20])}..."
            )

        # 5. âš ï¸ æ£€æŸ¥æ—¶é—´è¿‡æ»¤æ¡ä»¶ï¼ˆå…³é”®ï¼ï¼‰
        if require_time_filter:
            time_columns = self._identify_time_columns(schema_info)

            if time_columns:
                # æ£€æŸ¥SQLä¸­æ˜¯å¦ä½¿ç”¨äº†ä»»ä½•æ—¶é—´å­—æ®µ
                has_time_filter = False
                used_time_column = None

                for time_col in time_columns:
                    # æ£€æŸ¥WHEREå­å¥ä¸­æ˜¯å¦ä½¿ç”¨äº†æ—¶é—´å­—æ®µ
                    # åŒ¹é…æ¨¡å¼: time_col = / time_col BETWEEN / time_col >= / time_col <=
                    patterns = [
                        rf'\bWHERE\b.*\b{re.escape(time_col)}\b\s*=',
                        rf'\bWHERE\b.*\b{re.escape(time_col)}\b\s*BETWEEN',
                        rf'\bWHERE\b.*\b{re.escape(time_col)}\b\s*>=',
                        rf'\bWHERE\b.*\b{re.escape(time_col)}\b\s*<=',
                        rf'\bAND\b.*\b{re.escape(time_col)}\b\s*=',
                        rf'\bAND\b.*\b{re.escape(time_col)}\b\s*BETWEEN',
                        rf'\bAND\b.*\b{re.escape(time_col)}\b\s*>=',
                        rf'\bAND\b.*\b{re.escape(time_col)}\b\s*<=',
                    ]

                    for pattern in patterns:
                        if re.search(pattern, sql, re.IGNORECASE):
                            has_time_filter = True
                            used_time_column = time_col
                            break

                    if has_time_filter:
                        break

                validation_result["has_time_filter"] = has_time_filter
                validation_result["used_time_column"] = used_time_column

                if not has_time_filter:
                    validation_result["is_valid"] = False
                    validation_result["errors"].append("âš ï¸ ç¼ºå°‘æ—¶é—´è¿‡æ»¤æ¡ä»¶ï¼è¿™æ˜¯åŸºäºæ—¶é—´å‘¨æœŸçš„ç»Ÿè®¡æŸ¥è¯¢")
                    validation_result["suggestions"].append(
                        f"è¯·æ·»åŠ æ—¶é—´è¿‡æ»¤ï¼Œå¯ç”¨çš„æ—¶é—´å­—æ®µ: {', '.join(time_columns)}"
                    )
                    validation_result["suggestions"].append(
                        f"ç¤ºä¾‹: WHERE {time_columns[0]} = '{{{{start_date}}}}'"
                    )
                    validation_result["suggestions"].append(
                        f"æˆ–: WHERE {time_columns[0]} BETWEEN '{{{{start_date}}}}' AND '{{{{end_date}}}}'"
                    )

        return validation_result

    async def _refine_sql_add_time_filter(
        self,
        original_sql: str,
        schema_info,
        validation_result: Dict[str, Any],
        request: PlaceholderAnalysisRequest
    ) -> Tuple[str, str]:
        """
        ä¼˜åŒ–SQLï¼šæ·»åŠ æ—¶é—´è¿‡æ»¤æ¡ä»¶

        è¿”å›: (refined_sql, reasoning)
        """
        import json
        from app.services.infrastructure.agents.types import AgentInput, PlaceholderSpec, TaskContext

        time_columns = self._identify_time_columns(schema_info)

        if not time_columns:
            return original_sql, "æœªæ‰¾åˆ°æ—¶é—´å­—æ®µ"

        # æ„å»ºrefinement prompt
        errors = validation_result.get("errors", [])
        suggestions = validation_result.get("suggestions", [])

        refinement_prompt = f"""
ä½ ä¹‹å‰ç”Ÿæˆçš„SQLç¼ºå°‘æ—¶é—´è¿‡æ»¤æ¡ä»¶ï¼Œéœ€è¦ä¼˜åŒ–ï¼š

## åŸå§‹SQL
```sql
{original_sql}
```

## é—®é¢˜
{json.dumps(errors, ensure_ascii=False)}

## å»ºè®®
{json.dumps(suggestions, ensure_ascii=False)}

## å¯ç”¨çš„æ—¶é—´å­—æ®µ
{', '.join(time_columns)}

## Schemaä¿¡æ¯
{self._build_schema_prompt(schema_info)}

## è¦æ±‚
1. åœ¨åŸSQLåŸºç¡€ä¸Šæ·»åŠ æ—¶é—´è¿‡æ»¤æ¡ä»¶
2. ä½¿ç”¨WHEREå­å¥æˆ–åœ¨ç°æœ‰WHEREå­å¥ä¸­æ·»åŠ ANDæ¡ä»¶
3. æ¨èä½¿ç”¨ç¬¬ä¸€ä¸ªæ—¶é—´å­—æ®µ: {time_columns[0]}
4. ä½¿ç”¨å ä½ç¬¦æ ¼å¼: WHERE {time_columns[0]} = '{{{{start_date}}}}'
5. ä¿æŒåŸSQLçš„å…¶ä»–é€»è¾‘ä¸å˜
6. è¿”å›JSONæ ¼å¼: {{"sql": "ä¼˜åŒ–åçš„SQL", "reasoning": "è¯´æ˜æ·»åŠ äº†ä»€ä¹ˆæ—¶é—´è¿‡æ»¤æ¡ä»¶"}}

è¯·ä¼˜åŒ–SQLï¼š
"""

        # æ„å»ºAgentInput
        agent_input = AgentInput(
            user_prompt=refinement_prompt,
            placeholder=PlaceholderSpec(
                id=request.placeholder_id,
                description="SQLä¼˜åŒ– - æ·»åŠ æ—¶é—´è¿‡æ»¤",
                type="sql_refinement",
                granularity="daily"
            ),
            schema=schema_info,
            context=TaskContext(
                task_time=int(datetime.now().timestamp()),
                timezone="Asia/Shanghai"
            ),
            data_source=self._build_data_source_config(request),
            task_driven_context={
                "stage": "sql_refinement",
                "refinement_type": "add_time_filter",
                "original_sql": original_sql
            },
            user_id=self.user_id
        )

        # è°ƒç”¨Agentä¼˜åŒ–SQL
        try:
            result = await self.agent_service.execute(agent_input)

            if not result.success:
                return original_sql, f"ä¼˜åŒ–å¤±è´¥: {result.error}"

            # è§£æç»“æœ
            output = result.result
            refined_sql = ""
            reasoning = ""

            if isinstance(output, dict):
                refined_sql = output.get("sql", "")
                reasoning = output.get("reasoning", "")
            elif isinstance(output, str):
                try:
                    parsed = json.loads(output)
                    refined_sql = parsed.get("sql", output)
                    reasoning = parsed.get("reasoning", "")
                except:
                    refined_sql = output
                    reasoning = "è‡ªåŠ¨æ·»åŠ æ—¶é—´è¿‡æ»¤"

            return refined_sql if refined_sql else original_sql, reasoning

        except Exception as e:
            logger.error(f"SQL refinementå¤±è´¥: {e}")
            return original_sql, f"ä¼˜åŒ–å¼‚å¸¸: {str(e)}"

    async def _test_sql(self, sql: str, data_source_config: Dict[str, Any]) -> Dict[str, Any]:
        """
        æµ‹è¯•SQLæ‰§è¡Œ

        åœ¨æ•°æ®åº“ä¸Šæ‰§è¡ŒSQLï¼ˆLIMIT 5å°æ ·æœ¬æµ‹è¯•ï¼‰
        """
        # æ·»åŠ LIMITä»¥é™åˆ¶è¿”å›ç»“æœ
        test_sql = sql
        if "LIMIT" not in test_sql.upper():
            test_sql += " LIMIT 5"

        # è·å–æ•°æ®æºadapter
        ds_adapter = None
        for attr in ("data_source", "data_source_service"):
            if hasattr(self.container, attr):
                ds_adapter = getattr(self.container, attr)
                break

        if not ds_adapter:
            return {
                "success": False,
                "error": "æ•°æ®æºadapterä¸å¯ç”¨"
            }

        # æ‰§è¡Œæµ‹è¯•æŸ¥è¯¢
        try:
            test_result = await ds_adapter.run_query(
                data_source_config,
                test_sql,
                limit=5
            )

            return {
                "success": test_result.get("success", False),
                "error": test_result.get("error"),
                "row_count": len(test_result.get("rows", [])) if test_result.get("success") else 0,
                "columns": test_result.get("columns", [])
            }
        except Exception as e:
            logger.error(f"SQLæµ‹è¯•å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e)
            }


# å…¨å±€æœåŠ¡å®ä¾‹ç®¡ç†
_global_service = None


async def get_placeholder_service() -> PlaceholderApplicationService:
    """è·å–å…¨å±€å ä½ç¬¦åº”ç”¨æœåŠ¡å®ä¾‹"""
    global _global_service
    if _global_service is None:
        _global_service = PlaceholderApplicationService()
        await _global_service.initialize()
    return _global_service


async def shutdown_placeholder_service():
    """å…³é—­å…¨å±€å ä½ç¬¦åº”ç”¨æœåŠ¡"""
    global _global_service
    if _global_service:
        await _global_service.shutdown()
        _global_service = None


# å…¼å®¹æ€§å‡½æ•° - ä¿æŒå‘åå…¼å®¹
async def analyze_placeholder_simple(
    placeholder_id: str,
    business_command: str,
    requirements: str,
    context: Optional[Dict[str, Any]] = None,
    target_objective: str = "",
    data_source_info: Optional[Dict[str, Any]] = None,
    existing_sql: Optional[str] = None
) -> SQLGenerationResult:
    """
    ç®€åŒ–çš„å ä½ç¬¦åˆ†ææ¥å£ - ä½¿ç”¨ä»»åŠ¡éªŒè¯æ™ºèƒ½æ¨¡å¼

    Args:
        placeholder_id: å ä½ç¬¦ID
        business_command: ä¸šåŠ¡å‘½ä»¤
        requirements: éœ€æ±‚æè¿°
        context: ä¸Šä¸‹æ–‡ä¿¡æ¯
        target_objective: ç›®æ ‡è¦æ±‚
        data_source_info: æ•°æ®æºä¿¡æ¯
        existing_sql: ç°æœ‰SQLï¼ˆå¦‚æœå­˜åœ¨ï¼‰

    Returns:
        SQLç”Ÿæˆç»“æœ
    """

    service = await get_placeholder_service()

    # å¦‚æœæä¾›äº†existing_sqlï¼ŒåŠ å…¥åˆ°contextä¸­
    if existing_sql:
        context = context or {}
        context["current_sql"] = existing_sql

    request = PlaceholderAnalysisRequest(
        placeholder_id=placeholder_id,
        business_command=business_command,
        requirements=requirements,
        context=context or {},
        target_objective=target_objective,
        data_source_info=data_source_info
    )

    result = None
    async for response in service.analyze_placeholder(request):
        if response["type"] == "sql_generation_complete":
            result = response["content"]
            break
        elif response["type"] == "sql_generation_failed":
            # è¿”å›å¤±è´¥çš„ç»“æœ
            result = response["content"]
            break

    return result


async def update_placeholder_simple(
    placeholder_id: str,
    task_context: Dict[str, Any],
    current_task_info: Dict[str, Any],
    target_objective: str,
    stored_placeholders: List[PlaceholderInfo]
) -> PlaceholderUpdateResult:
    """ç®€åŒ–çš„å ä½ç¬¦æ›´æ–°æ¥å£ - å…¼å®¹æ€§å‡½æ•°"""
    
    service = await get_placeholder_service()
    
    request = PlaceholderUpdateRequest(
        placeholder_id=placeholder_id,
        task_context=task_context,
        current_task_info=current_task_info,
        target_objective=target_objective,
        stored_placeholders=stored_placeholders
    )
    
    result = None
    async for response in service.update_placeholder(request):
        if response["type"] == "update_analysis_complete":
            result = response["content"]
            break
    
    return result


async def complete_placeholder_simple(
    placeholder_id: str,
    etl_data: List[Dict[str, Any]],
    placeholder_requirements: str,
    template_section: str,
    chart_generation_needed: bool = False,
    target_chart_type: Optional[ChartType] = None
) -> Dict[str, Any]:
    """ç®€åŒ–çš„å ä½ç¬¦å®Œæˆæ¥å£ - å…¼å®¹æ€§å‡½æ•°"""
    
    service = await get_placeholder_service()
    
    request = PlaceholderCompletionRequest(
        placeholder_id=placeholder_id,
        etl_data=etl_data,
        placeholder_requirements=placeholder_requirements,
        template_section=template_section,
        chart_generation_needed=chart_generation_needed,
        target_chart_type=target_chart_type
    )
    
    result = None
    async for response in service.complete_placeholder(request):
        if response["type"] == "completion_complete":
            result = response["content"]
            break
    
    return result


# å…¼å®¹æ€§åˆ«å
analyze_placeholder = analyze_placeholder_simple
update_placeholder = update_placeholder_simple  
complete_placeholder = complete_placeholder_simple
