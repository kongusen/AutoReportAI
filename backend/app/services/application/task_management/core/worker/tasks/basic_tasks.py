"""
Basic Celery Tasks

åŸºç¡€Celeryä»»åŠ¡å®šä¹‰ï¼ŒåŒ…æ‹¬ï¼š
- æ¨¡æ¿è§£æ
- å ä½ç¬¦åˆ†æ
- æ•°æ®æŸ¥è¯¢
- å†…å®¹å¡«å……
- æŠ¥å‘Šç”Ÿæˆ
- ETLä½œä¸šæ‰§è¡Œ
"""

import asyncio
import logging
from typing import Any, Dict, List

from celery import chord, group
from celery.exceptions import MaxRetriesExceededError, Retry
from sqlalchemy.orm import Session

from app import crud, schemas
from app.core.time_utils import now, format_iso
from app.db.session import SessionLocal
from app.services.ai.integration.ai_service_enhanced import EnhancedAIService
# AgentOrchestrator not available, using placeholder
class AgentOrchestrator:
    def __init__(self):
        pass
    
    async def execute(self, agent_input, context):
        # Placeholder implementation
        return type('obj', (object,), {
            'success': True,
            'data': type('obj', (object,), {
                'results': {
                    'fetch_data': type('obj', (object,), {
                        'success': True,
                        'data': {'etl_instruction': 'SELECT * FROM placeholder_table'}
                    })()
                }
            })()
        })()
from app.services.data.processing.etl.intelligent_etl_executor import IntelligentETLExecutor
from app.services.infrastructure.notification.notification_service import NotificationService
from app.services.domain.template.services.template_domain_service import TemplateParser
from app.services.domain.reporting.word_generator_service import WordGeneratorService
from ..config.celery_app import celery_app
from ..utils.progress_utils import update_task_progress, send_error_notification

# æ·»åŠ WebSocketé€šçŸ¥å·¥å…·å‡½æ•°
async def send_progress_notification(task_id: int, progress: int, message: str, user_id: str = None):
    """å‘é€è¿›åº¦é€šçŸ¥åˆ°å‰ç«¯"""
    try:
        from app.websocket.manager import manager, NotificationMessage
        
        if not user_id:
            # ä»ä»»åŠ¡ä¸­è·å–ç”¨æˆ·ID
            db = SessionLocal()
            try:
                task = crud.task.get(db, id=task_id)
                if task:
                    user_id = str(task.owner_id)
            finally:
                db.close()
        
        if user_id:
            notification = NotificationMessage(
                type="info",
                title="ä»»åŠ¡è¿›åº¦æ›´æ–°",
                message=message,
                data={
                    "task_id": task_id,
                    "progress": progress,
                    "action": "task_progress"
                },
                user_id=user_id
            )
            await manager.send_to_user(user_id, notification.to_dict())
            logger.debug(f"å‘é€è¿›åº¦é€šçŸ¥: ä»»åŠ¡{task_id}, è¿›åº¦{progress}%")
    except Exception as e:
        logger.warning(f"å‘é€è¿›åº¦é€šçŸ¥å¤±è´¥: {e}")

def send_progress_notification_sync(task_id: int, progress: int, message: str, user_id: str = None):
    """åŒæ­¥ç‰ˆæœ¬çš„è¿›åº¦é€šçŸ¥"""
    import asyncio
    try:
        # åœ¨æ–°çš„äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œå¼‚æ­¥å‡½æ•°
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        loop.run_until_complete(send_progress_notification(task_id, progress, message, user_id))
        loop.close()
    except Exception as e:
        logger.warning(f"åŒæ­¥è¿›åº¦é€šçŸ¥å¤±è´¥: {e}")

logger = logging.getLogger(__name__)


@celery_app.task(name='app.services.application.task_management.core.worker.tasks.basic_tasks.template_parsing')
def template_parsing(template_id: str, task_id: int) -> List[Dict[str, Any]]:
    """æ¨¡æ¿è§£æä»»åŠ¡ - æå–æ‰€æœ‰å ä½ç¬¦"""
    logger.info(f"å¼€å§‹è§£ææ¨¡æ¿ï¼Œæ¨¡æ¿ID: {template_id}, ä»»åŠ¡ID: {task_id}")
    
    # å‘é€å¼€å§‹é€šçŸ¥
    send_progress_notification_sync(task_id, 10, "å¼€å§‹è§£ææ¨¡æ¿...")
    
    db = SessionLocal()
    try:
        template = crud.template.get(db=db, id=template_id)
        if not template:
            raise ValueError(f"æ¨¡æ¿ {template_id} ä¸å­˜åœ¨")
        
        send_progress_notification_sync(task_id, 30, "æ­£åœ¨æå–å ä½ç¬¦...")
        
        parser = TemplateParser()
        placeholders = parser.extract_placeholders(template.content)
        
        send_progress_notification_sync(task_id, 50, f"æ¨¡æ¿è§£æå®Œæˆï¼Œå‘ç° {len(placeholders)} ä¸ªå ä½ç¬¦")
        
        logger.info(f"æ¨¡æ¿è§£æå®Œæˆï¼Œæ‰¾åˆ° {len(placeholders)} ä¸ªå ä½ç¬¦")
        return placeholders
        
    except Exception as e:
        logger.error(f"æ¨¡æ¿è§£æå¤±è´¥: {str(e)}")
        raise
    finally:
        db.close()


@celery_app.task(bind=True, name='app.services.application.task_management.core.worker.tasks.basic_tasks.placeholder_analysis',
                autoretry_for=(Exception,), 
                retry_kwargs={'max_retries': 3, 'countdown': 30})
def placeholder_analysis(self, placeholder_data: Dict[str, Any], 
                        data_source_id: str, task_id: int) -> Dict[str, Any]:
    """å ä½ç¬¦åˆ†æä»»åŠ¡ - ä½¿ç”¨Agentç³»ç»Ÿè¿›è¡Œæ™ºèƒ½åˆ†æ"""
    placeholder_name = placeholder_data.get('name', 'unknown')
    logger.info(f"å¼€å§‹åˆ†æå ä½ç¬¦: {placeholder_name}, ä»»åŠ¡ID: {task_id}")
    
    try:
        # ä½¿ç”¨Agentç¼–æ’å™¨è¿›è¡Œæ™ºèƒ½åˆ†æ
        orchestrator = AgentOrchestrator()
        
        # å‡†å¤‡å ä½ç¬¦æ•°æ®ï¼Œæ·»åŠ å¿…è¦å­—æ®µ
        agent_input = {
            "placeholder_type": placeholder_data.get('type', 'ç»Ÿè®¡'),
            "description": placeholder_data.get('description', placeholder_name),
            "data_source_id": data_source_id,
            "name": placeholder_name,
            **placeholder_data
        }
        
        # æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
        context = {
            "task_id": task_id,
            "processing_mode": "placeholder_analysis"
        }
        
        # æ‰§è¡Œå¼‚æ­¥åˆ†æï¼ˆåœ¨åŒæ­¥å‡½æ•°ä¸­è¿è¡Œå¼‚æ­¥ä»£ç ï¼‰
        try:
            # å°è¯•è·å–å·²å­˜åœ¨çš„äº‹ä»¶å¾ªç¯
            loop = asyncio.get_event_loop()
        except RuntimeError:
            # å¦‚æœæ²¡æœ‰äº‹ä»¶å¾ªç¯ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        # è¿è¡ŒAgentåˆ†æ
        if loop.is_running():
            # å¦‚æœå¾ªç¯å·²åœ¨è¿è¡Œï¼Œä½¿ç”¨run_coroutine_threadsafe
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(asyncio.run, orchestrator.execute(agent_input, context))
                agent_result = future.result(timeout=120)
        else:
            # å¦‚æœå¾ªç¯æœªè¿è¡Œï¼Œç›´æ¥ä½¿ç”¨run
            agent_result = loop.run_until_complete(orchestrator.execute(agent_input, context))
        
        if agent_result.success:
            logger.info(f"Agentå ä½ç¬¦åˆ†æå®Œæˆ: {placeholder_name}")
            
            # ä»Agentç»“æœä¸­æå–ETLæŒ‡ä»¤
            workflow_result = agent_result.data
            if workflow_result and hasattr(workflow_result, 'results'):
                # æŸ¥æ‰¾æ•°æ®æŸ¥è¯¢ç»“æœ
                data_query_result = workflow_result.results.get('fetch_data')
                if data_query_result and data_query_result.success:
                    return {
                        "placeholder": placeholder_data,
                        "etl_instruction": data_query_result.data.get('etl_instruction'),
                        "data_source_id": data_source_id,
                        "analysis_time": format_iso(),
                        "agent_workflow": True
                    }
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å…·ä½“çš„ETLæŒ‡ä»¤ï¼Œè¿”å›Agentç»“æœ
            return {
                "placeholder": placeholder_data,
                "etl_instruction": {
                    "query_type": "agent_processed",
                    "agent_result": agent_result.data,
                    "description": f"é€šè¿‡Agentç³»ç»Ÿå¤„ç†å ä½ç¬¦: {placeholder_name}"
                },
                "data_source_id": data_source_id,
                "analysis_time": format_iso(),
                "agent_workflow": True
            }
        else:
            # Agentåˆ†æå¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•
            logger.warning(f"Agentåˆ†æå¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»ŸAIæœåŠ¡: {placeholder_name}")
            return _fallback_ai_analysis(placeholder_data, data_source_id)
            
    except Exception as e:
        logger.error(f"Agentå ä½ç¬¦åˆ†æå¼‚å¸¸: {placeholder_name}, é”™è¯¯: {str(e)}")
        
        # å›é€€åˆ°ä¼ ç»ŸAIæœåŠ¡
        logger.info(f"å›é€€åˆ°ä¼ ç»ŸAIåˆ†ææ–¹æ³•: {placeholder_name}")
        try:
            return _fallback_ai_analysis(placeholder_data, data_source_id)
        except Exception as fallback_error:
            logger.error(f"ä¼ ç»ŸAIåˆ†æä¹Ÿå¤±è´¥: {fallback_error}")
            
            if self.request.retries < self.max_retries:
                logger.warning(f"é‡è¯•å ä½ç¬¦åˆ†æ: {placeholder_name}, ç¬¬{self.request.retries + 1}æ¬¡é‡è¯•")
                raise self.retry(countdown=30 * (self.request.retries + 1))
            
            # è¿”å›é»˜è®¤ç»“æœè€Œä¸æ˜¯å¤±è´¥
            return {
                "placeholder": placeholder_data,
                "etl_instruction": None,
                "error": str(e),
                "fallback_error": str(fallback_error)
            }


def _fallback_ai_analysis(placeholder_data: Dict[str, Any], data_source_id: str) -> Dict[str, Any]:
    """å›é€€çš„AIåˆ†ææ–¹æ³•"""
    db = SessionLocal()
    try:
        ai_service = EnhancedAIService(db)
        return asyncio.run(ai_service.analyze_placeholder_requirements(placeholder_data, data_source_id))
    finally:
        db.close()


@celery_app.task(bind=True, name='app.services.application.task_management.core.worker.tasks.basic_tasks.data_query',
                autoretry_for=(Exception,), 
                retry_kwargs={'max_retries': 2, 'countdown': 60})
def data_query(self, etl_instruction: Dict[str, Any], 
               data_source_id: str, task_id: int) -> Dict[str, Any]:
    """æ•°æ®æŸ¥è¯¢ä»»åŠ¡ - ä½¿ç”¨ç»Ÿä¸€çš„Agentæ‰§è¡Œç³»ç»Ÿ"""
    logger.info(f"ğŸš€ å¼€å§‹Agentæ•°æ®æŸ¥è¯¢æ‰§è¡Œï¼Œä»»åŠ¡ID: {task_id}")
    
    # å‘é€å¼€å§‹é€šçŸ¥
    send_progress_notification_sync(task_id, 60, "å¼€å§‹æ‰§è¡Œæ•°æ®æŸ¥è¯¢...")
    
    db = SessionLocal()
    try:
        # ä½¿ç”¨TaskSQLExecutionAgentæ›¿ä»£ç›´æ¥çš„ETLæ‰§è¡Œå™¨
        from app.services.ai.agents.task_sql_execution_agent import (
            TaskSQLExecutionAgent, 
            TaskExecutionContext,
            TaskExecutionMode,
            ExecutionPriority
        )
        
        # åˆ›å»ºAgentå®ä¾‹
        agent_config = {
            'enable_intelligent_analysis': True,
            'enable_cache_optimization': True,
            'enable_recovery_mode': True,
            'default_timeout': 120
        }
        
        sql_agent = TaskSQLExecutionAgent(db, agent_config)
        
        # æ„å»ºæ‰§è¡Œä¸Šä¸‹æ–‡
        context = TaskExecutionContext(
            task_id=task_id,
            etl_instruction=etl_instruction,
            data_source_id=data_source_id,
            execution_mode=TaskExecutionMode.DIRECT_SQL,  # é»˜è®¤æ¨¡å¼ï¼ŒAgentä¼šæ™ºèƒ½ä¼˜åŒ–
            priority=ExecutionPriority.MEDIUM,
            retry_count=self.request.retries,
            max_retries=self.max_retries,
            timeout_seconds=120,
            enable_cache=True,
            enable_recovery=True
        )
        
        # æ‰§è¡ŒAgentä»»åŠ¡
        try:
            # è·å–æˆ–åˆ›å»ºäº‹ä»¶å¾ªç¯
            try:
                loop = asyncio.get_event_loop()
            except RuntimeError:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
            
            # å¼‚æ­¥æ‰§è¡ŒAgentä»»åŠ¡
            if loop.is_running():
                # å¦‚æœå¾ªç¯å·²åœ¨è¿è¡Œï¼Œä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œ
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(asyncio.run, sql_agent.execute_task(context))
                    agent_result = future.result(timeout=context.timeout_seconds)
            else:
                # å¦‚æœå¾ªç¯æœªè¿è¡Œï¼Œç›´æ¥ä½¿ç”¨run_until_complete
                agent_result = loop.run_until_complete(sql_agent.execute_task(context))
            
            if agent_result.success:
                logger.info(f"âœ… Agentæ•°æ®æŸ¥è¯¢æˆåŠŸï¼Œä»»åŠ¡ID: {task_id}, æ¨¡å¼: {agent_result.mode_used.value if agent_result.mode_used else 'unknown'}, æ‰§è¡Œæ—¶é—´: {agent_result.execution_time:.2f}s")
                
                # å‘é€æˆåŠŸé€šçŸ¥
                send_progress_notification_sync(task_id, 80, f"æ•°æ®æŸ¥è¯¢å®Œæˆï¼Œè€—æ—¶ {agent_result.execution_time:.2f}s")
                
                # è½¬æ¢Agentç»“æœä¸ºæ ‡å‡†æ ¼å¼
                query_result = {
                    "success": True,
                    "data": agent_result.data,
                    "execution_time": agent_result.execution_time,
                    "cache_hit": agent_result.cache_hit,
                    "agent_metadata": {
                        "execution_mode": agent_result.mode_used.value if agent_result.mode_used else None,
                        "retry_count": agent_result.retry_count,
                        "agent_used": True,
                        **agent_result.metadata
                    }
                }
                
                return query_result
            else:
                # Agentæ‰§è¡Œå¤±è´¥ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯
                error_message = agent_result.error_message or "Agentæ‰§è¡Œå¤±è´¥"
                logger.error(f"âŒ Agentæ•°æ®æŸ¥è¯¢å¤±è´¥ï¼Œä»»åŠ¡ID: {task_id}, é”™è¯¯: {error_message}")
                
                # å¦‚æœè¿˜æœ‰é‡è¯•æ¬¡æ•°ï¼ŒæŠ›å‡ºå¼‚å¸¸è§¦å‘é‡è¯•
                if self.request.retries < self.max_retries:
                    logger.warning(f"ğŸ”„ Agentä»»åŠ¡é‡è¯• {self.request.retries + 1}/{self.max_retries}: {task_id}")
                    raise Exception(f"Agentæ‰§è¡Œå¤±è´¥: {error_message}")
                
                # é‡è¯•æ¬¡æ•°ç”¨å°½ï¼Œè¿”å›å¤±è´¥ç»“æœ
                return {
                    "success": False,
                    "error": error_message,
                    "agent_metadata": {
                        "execution_mode": agent_result.mode_used.value if agent_result.mode_used else None,
                        "retry_count": agent_result.retry_count,
                        "agent_used": True,
                        "final_failure": True,
                        **agent_result.metadata
                    }
                }
                
        except Exception as agent_error:
            logger.error(f"âš ï¸ Agentæ‰§è¡Œå¼‚å¸¸ï¼Œä»»åŠ¡ID: {task_id}, é”™è¯¯: {str(agent_error)}")
            
            # Agentç³»ç»Ÿå¼‚å¸¸ï¼Œå›é€€åˆ°ä¼ ç»ŸETLæ‰§è¡Œå™¨
            logger.info(f"ğŸ”„ Agentå¼‚å¸¸å›é€€åˆ°ä¼ ç»Ÿæ‰§è¡Œå™¨ï¼Œä»»åŠ¡ID: {task_id}")
            try:
                etl_executor = IntelligentETLExecutor(db)
                traditional_result = etl_executor.execute_instruction(etl_instruction, data_source_id)
                
                logger.info(f"âœ… ä¼ ç»Ÿæ‰§è¡Œå™¨æˆåŠŸæ‰§è¡Œï¼Œä»»åŠ¡ID: {task_id}")
                return {
                    "success": True,
                    "data": traditional_result,
                    "agent_metadata": {
                        "agent_used": False,
                        "fallback_reason": f"Agentå¼‚å¸¸: {str(agent_error)}",
                        "fallback_success": True
                    }
                }
                
            except Exception as fallback_error:
                logger.error(f"âŒ ä¼ ç»Ÿæ‰§è¡Œå™¨ä¹Ÿå¤±è´¥ï¼Œä»»åŠ¡ID: {task_id}, é”™è¯¯: {str(fallback_error)}")
                
                # å¦‚æœè¿˜æœ‰é‡è¯•æ¬¡æ•°ï¼ŒæŠ›å‡ºå¼‚å¸¸è§¦å‘é‡è¯•
                if self.request.retries < self.max_retries:
                    raise self.retry(countdown=60 * (self.request.retries + 1))
                
                # å®Œå…¨å¤±è´¥
                raise Exception(f"Agentå’Œä¼ ç»Ÿæ‰§è¡Œå™¨éƒ½å¤±è´¥: Agent={str(agent_error)}, Fallback={str(fallback_error)}")
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®æŸ¥è¯¢ä»»åŠ¡å®Œå…¨å¤±è´¥ï¼Œä»»åŠ¡ID: {task_id}, é”™è¯¯: {str(e)}")
        
        # æœ€åçš„é‡è¯•æœºåˆ¶
        if self.request.retries < self.max_retries:
            logger.warning(f"ğŸ”„ ä»»åŠ¡æœ€ç»ˆé‡è¯• {self.request.retries + 1}/{self.max_retries}: {task_id}")
            raise self.retry(countdown=60 * (self.request.retries + 1))
        
        raise
    finally:
        db.close()


@celery_app.task(name='app.services.application.task_management.core.worker.tasks.basic_tasks.content_filling')
def content_filling(template_content: str, placeholders: List[Dict[str, Any]], 
                   query_results: List[Dict[str, Any]], task_id: int) -> str:
    """å†…å®¹å¡«å……ä»»åŠ¡ - å°†æŸ¥è¯¢ç»“æœå¡«å…¥æ¨¡æ¿"""
    logger.info(f"å¼€å§‹å¡«å……å†…å®¹ï¼Œä»»åŠ¡ID: {task_id}")
    
    # å‘é€å¼€å§‹é€šçŸ¥
    send_progress_notification_sync(task_id, 85, "å¼€å§‹å¡«å……å†…å®¹åˆ°æ¨¡æ¿...")
    
    try:
        # åˆ›å»ºå ä½ç¬¦åˆ°ç»“æœçš„æ˜ å°„
        result_mapping = {}
        for i, result in enumerate(query_results):
            if i < len(placeholders):
                placeholder_name = placeholders[i].get('name')
                if placeholder_name and result:
                    result_mapping[placeholder_name] = result
        
        # æ›¿æ¢æ¨¡æ¿ä¸­çš„å ä½ç¬¦
        filled_content = template_content
        for placeholder_name, result_data in result_mapping.items():
            placeholder_pattern = f"{{{{{placeholder_name}}}}}"
            
            # æ ¹æ®ç»“æœç±»å‹è¿›è¡Œä¸åŒçš„å¤„ç†
            if isinstance(result_data.get('data'), list):
                # å¦‚æœæ˜¯åˆ—è¡¨æ•°æ®ï¼Œå–ç¬¬ä¸€ä¸ªå€¼æˆ–æ ¼å¼åŒ–æ˜¾ç¤º
                data_list = result_data['data']
                replacement = str(data_list[0]) if data_list else "æ— æ•°æ®"
            elif isinstance(result_data.get('data'), dict):
                # å¦‚æœæ˜¯å­—å…¸æ•°æ®ï¼Œå–ç¬¬ä¸€ä¸ªå€¼
                data_dict = result_data['data']
                replacement = str(next(iter(data_dict.values()))) if data_dict else "æ— æ•°æ®"
            else:
                replacement = str(result_data.get('data', 'æ— æ•°æ®'))
            
            filled_content = filled_content.replace(placeholder_pattern, replacement)
        
        # å‘é€å®Œæˆé€šçŸ¥
        send_progress_notification_sync(task_id, 90, "å†…å®¹å¡«å……å®Œæˆ")
        
        logger.info(f"å†…å®¹å¡«å……å®Œæˆï¼Œä»»åŠ¡ID: {task_id}")
        return filled_content
        
    except Exception as e:
        logger.error(f"å†…å®¹å¡«å……å¤±è´¥ï¼Œä»»åŠ¡ID: {task_id}, é”™è¯¯: {str(e)}")
        raise


@celery_app.task(name='app.services.application.task_management.core.worker.tasks.basic_tasks.report_generation')
def report_generation(template_content: str, output_config: Dict[str, Any], 
                     task_id: int) -> Dict[str, Any]:
    """æŠ¥å‘Šç”Ÿæˆä»»åŠ¡ - æœ€ç»ˆç”ŸæˆæŠ¥å‘Šæ–‡ä»¶"""
    logger.info(f"å¼€å§‹ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶ï¼Œä»»åŠ¡ID: {task_id}")
    
    # å‘é€å¼€å§‹é€šçŸ¥
    send_progress_notification_sync(task_id, 95, "å¼€å§‹ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶...")
    
    db = SessionLocal()
    try:
        # ç”ŸæˆWordæ–‡æ¡£
        word_generator = WordGeneratorService()
        report_path = word_generator.generate_report(
            content=template_content,
            title=output_config.get('title', 'è‡ªåŠ¨ç”ŸæˆæŠ¥å‘Š'),
            format=output_config.get('format', 'docx')
        )
        
        # ä¿å­˜æŠ¥å‘Šè®°å½•åˆ°æ•°æ®åº“
        report_data = {
            "task_id": task_id,
            "content": template_content,
            "file_path": report_path,
            "status": "completed",
            "generated_at": now()
        }
        
        report_record = crud.report_history.create(
            db=db, 
            obj_in=schemas.ReportHistoryCreate(**report_data)
        )
        
        # åœ¨ä¼šè¯å…³é—­å‰è·å–ID
        report_id = report_record.id
        
        logger.info(f"æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼Œä»»åŠ¡ID: {task_id}, è·¯å¾„: {report_path}")
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå·²å®Œæˆ
        task = crud.task.get(db, id=task_id)
        if task:
            from app.models.task import TaskStatus
            crud.task.update_status(db, task_id=task_id, status=TaskStatus.COMPLETED)
            
            # å‘é€WebSocketé€šçŸ¥åˆ°å‰ç«¯
            try:
                import asyncio
                from app.websocket.manager import manager, NotificationMessage
                
                notification = NotificationMessage(
                    type="success",
                    title="æŠ¥å‘Šç”Ÿæˆå®Œæˆ",
                    message=f"ä»»åŠ¡ '{task.name}' çš„æŠ¥å‘Šå·²ç”Ÿæˆå®Œæˆï¼Œå¯ä»¥ä¸‹è½½äº†",
                    data={
                        "task_id": task_id,
                        "report_id": report_id,
                        "report_path": report_path,
                        "action": "task_completed",
                        "download_url": f"/api/v1/reports/{report_id}/download"
                    },
                    user_id=str(task.owner_id)
                )
                
                # ä½¿ç”¨åŒæ­¥æ–¹å¼å‘é€å¼‚æ­¥é€šçŸ¥
                try:
                    loop = asyncio.get_event_loop()
                except RuntimeError:
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                
                if loop.is_running():
                    import concurrent.futures
                    with concurrent.futures.ThreadPoolExecutor() as executor:
                        future = executor.submit(
                            asyncio.run, 
                            manager.send_to_user(str(task.owner_id), notification.to_dict())
                        )
                        future.result(timeout=5)
                else:
                    loop.run_until_complete(
                        manager.send_to_user(str(task.owner_id), notification.to_dict())
                    )
                
                logger.info(f"å·²å‘é€ä»»åŠ¡å®Œæˆé€šçŸ¥åˆ°ç”¨æˆ·: {task.owner_id}")
                
            except Exception as notification_error:
                logger.error(f"å‘é€å®Œæˆé€šçŸ¥å¤±è´¥: {notification_error}")
        
        return {
            "report_path": report_path,
            "report_id": report_id,
            "task_id": task_id,
            "status": "completed",
            "notification_sent": True
        }
        
    except Exception as e:
        logger.error(f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼Œä»»åŠ¡ID: {task_id}, é”™è¯¯: {str(e)}")
        raise
    finally:
        db.close()


@celery_app.task(bind=True, name='app.services.application.task_management.core.worker.tasks.basic_tasks.execute_etl_job',
                autoretry_for=(Exception,), 
                retry_kwargs={'max_retries': 3, 'countdown': 60})
def execute_etl_job(self, job_id: str, job_config: Dict[str, Any]) -> Dict[str, Any]:
    """
    æ‰§è¡ŒETLä½œä¸šä»»åŠ¡
    
    Args:
        job_id: ETLä½œä¸šID
        job_config: ä½œä¸šé…ç½®
        
    Returns:
        æ‰§è¡Œç»“æœ
    """
    logger.info(f"å¼€å§‹æ‰§è¡ŒETLä½œä¸š: {job_id}")
    
    db = SessionLocal()
    try:
        # è·å–ä½œä¸šä¿¡æ¯
        etl_job = crud.etl_job.get(db, id=job_id)
        if not etl_job:
            raise ValueError(f"ETLä½œä¸šä¸å­˜åœ¨: {job_id}")
        
        # æ£€æŸ¥ä½œä¸šæ˜¯å¦å¯ç”¨
        if not etl_job.enabled:
            logger.warning(f"ETLä½œä¸šå·²ç¦ç”¨: {job_id}")
            return {"status": "skipped", "message": "ä½œä¸šå·²ç¦ç”¨"}
        
        # åˆ›å»ºETLæ‰§è¡Œå™¨
        etl_executor = IntelligentETLExecutor(db)
        
        # å‡†å¤‡æ‰§è¡Œå‚æ•°
        data_source_id = str(etl_job.data_source_id)
        etl_config = etl_job.config or {}
        
        # æ„å»ºETLæŒ‡ä»¤
        etl_instruction = {
            "query_type": etl_config.get("query_type", "sql"),
            "table": etl_config.get("table_name", ""),
            "fields": etl_config.get("fields", []),
            "filters": etl_config.get("filters", []),
            "group_by": etl_config.get("group_by", []),
            "order_by": etl_config.get("order_by", []),
            "limit": etl_config.get("limit"),
            "sql_query": etl_config.get("sql_query", "")
        }
        
        # æ‰§è¡ŒETLä½œä¸š
        start_time = now()
        result = etl_executor.execute_instruction(etl_instruction, data_source_id)
        end_time = now()
        
        # æ›´æ–°ä½œä¸šæ‰§è¡ŒçŠ¶æ€
        crud.etl_job.update(
            db, 
            db_obj=etl_job,
            obj_in={
                "last_run_at": start_time,
                "status": "completed",
                "execution_time": (end_time - start_time).total_seconds()
            }
        )
        
        # å‘é€é€šçŸ¥ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
        if etl_config.get("notify_on_completion", False):
            notification_service = NotificationService()
            notification_service.send_etl_completion_notification(
                job_id=job_id,
                job_name=etl_job.name,
                status="success",
                execution_time=(end_time - start_time).total_seconds(),
                result_summary=f"å¤„ç†äº†{len(result) if isinstance(result, list) else 1}æ¡è®°å½•"
            )
        
        logger.info(f"ETLä½œä¸šæ‰§è¡Œå®Œæˆ: {job_id}")
        
        return {
            "status": "completed",
            "job_id": job_id,
            "job_name": etl_job.name,
            "execution_time": (end_time - start_time).total_seconds(),
            "records_processed": len(result) if isinstance(result, list) else 1,
            "started_at": start_time.isoformat(),
            "completed_at": end_time.isoformat()
        }
        
    except Exception as e:
        logger.error(f"ETLä½œä¸šæ‰§è¡Œå¤±è´¥: {job_id}, é”™è¯¯: {str(e)}")
        
        # æ›´æ–°ä½œä¸šæ‰§è¡ŒçŠ¶æ€ä¸ºå¤±è´¥
        try:
            etl_job = crud.etl_job.get(db, id=job_id)
            if etl_job:
                crud.etl_job.update(
                    db,
                    db_obj=etl_job,
                    obj_in={
                        "last_run_at": now(),
                        "status": "failed",
                        "error_message": str(e)
                    }
                )
        except Exception as update_error:
            logger.error(f"æ›´æ–°ETLä½œä¸šçŠ¶æ€å¤±è´¥: {update_error}")
        
        # å‘é€å¤±è´¥é€šçŸ¥
        try:
            etl_job = crud.etl_job.get(db, id=job_id)
            if etl_job and etl_job.config and etl_job.config.get("notify_on_failure", True):
                notification_service = NotificationService()
                notification_service.send_etl_failure_notification(
                    job_id=job_id,
                    job_name=etl_job.name if etl_job else job_id,
                    error_message=str(e)
                )
        except Exception as notify_error:
            logger.error(f"å‘é€ETLå¤±è´¥é€šçŸ¥å¤±è´¥: {notify_error}")
        
        # å¦‚æœè¿˜æœ‰é‡è¯•æ¬¡æ•°ï¼Œåˆ™é‡è¯•
        if self.request.retries < self.max_retries:
            logger.warning(f"é‡è¯•ETLä½œä¸š: {job_id}, ç¬¬{self.request.retries + 1}æ¬¡é‡è¯•")
            raise self.retry(countdown=60 * (self.request.retries + 1))
        
        # é‡è¯•æ¬¡æ•°ç”¨å°½ï¼Œè¿”å›å¤±è´¥ç»“æœ
        return {
            "status": "failed",
            "job_id": job_id,
            "error": str(e),
            "retries": self.request.retries
        }
        
    finally:
        db.close()


@celery_app.task(name='app.services.application.task_management.core.worker.tasks.basic_tasks.test_celery_task')
def test_celery_task(word: str) -> str:
    """æµ‹è¯•ä»»åŠ¡"""
    return f"æµ‹è¯•ä»»åŠ¡æˆåŠŸæ‰§è¡Œï¼Œæ”¶åˆ°çš„å‚æ•°æ˜¯: {word}"
