"""
å¢å¼ºçš„ä¸¤é˜¶æ®µæµæ°´çº¿ - ä¿®å¤æ¡ä»¶åˆ†æ”¯ç¼ºé™·

ä¿®å¤çš„é—®é¢˜ï¼š
1. æ–°å¢æ‰§è¡Œæ¨¡å¼ï¼šPARTIAL_ANALYSIS, INCREMENTAL_UPDATE, RECOVERY_MODE
2. å®Œå–„æ™ºèƒ½åˆ¤æ–­é€»è¾‘ï¼šå¤„ç†éƒ¨åˆ†å°±ç»ªã€å¼‚å¸¸ç»†åˆ†ç­‰åœºæ™¯
3. å¢å¼ºå ä½ç¬¦SQLè·å–ï¼šå¤„ç†SQLæŸåã€å¹¶å‘å†²çªç­‰è¾¹ç•Œæ¡ä»¶
4. ä¼˜åŒ–å¼‚å¸¸å¤„ç†ï¼šåŒºåˆ†ä¸´æ—¶æ€§å’Œç»“æ„æ€§é—®é¢˜
"""

import logging
import asyncio
import time
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Tuple
from enum import Enum
from dataclasses import dataclass, field
from uuid import uuid4
from sqlalchemy.orm import Session

from app.db.session import SessionLocal
# Enhanced template parser disabled - using alternative approach in DAG architecture
# from app.services.domain.template.enhanced_template_parser import EnhancedTemplateParser
from ..core.progress_manager import update_task_progress_dict
# æ™ºèƒ½ä»»åŠ¡è°ƒåº¦å’ŒåŠ¨æ€è´Ÿè½½å‡è¡¡åŠŸèƒ½å·²å†…ç½®

logger = logging.getLogger(__name__)


class EnhancedExecutionMode(Enum):
    """å¢å¼ºçš„æ‰§è¡Œæ¨¡å¼"""
    FULL_PIPELINE = "full_pipeline"           # å®Œæ•´ä¸¤é˜¶æ®µæµæ°´çº¿
    PHASE_1_ONLY = "phase_1_only"            # ä»…æ‰§è¡Œé˜¶æ®µ1ï¼ˆåˆ†æï¼‰
    PHASE_2_ONLY = "phase_2_only"            # ä»…æ‰§è¡Œé˜¶æ®µ2ï¼ˆæ‰§è¡Œï¼‰
    SMART_EXECUTION = "smart_execution"      # æ™ºèƒ½æ‰§è¡Œï¼ˆæ ¹æ®çŠ¶æ€å†³å®šï¼‰
    
    # æ–°å¢æ‰§è¡Œæ¨¡å¼
    PARTIAL_ANALYSIS = "partial_analysis"     # éƒ¨åˆ†åˆ†ææ¨¡å¼ï¼ˆå¤„ç†éƒ¨åˆ†å·²åˆ†ææƒ…å†µï¼‰
    INCREMENTAL_UPDATE = "incremental_update" # å¢é‡æ›´æ–°æ¨¡å¼ï¼ˆåªåˆ†ææœªå®Œæˆçš„å ä½ç¬¦ï¼‰
    RECOVERY_MODE = "recovery_mode"           # æ•…éšœæ¢å¤æ¨¡å¼ï¼ˆä¸´æ—¶æ€§å¼‚å¸¸æ—¶ä½¿ç”¨ï¼‰
    CACHED_EXECUTION = "cached_execution"     # ç¼“å­˜æ‰§è¡Œæ¨¡å¼ï¼ˆä¼˜å…ˆä½¿ç”¨ç¼“å­˜ï¼‰


class AnalysisCompleteness(Enum):
    """åˆ†æå®Œæˆåº¦ç­‰çº§"""
    NONE = "none"           # æœªåˆ†æï¼ˆ0%ï¼‰
    MINIMAL = "minimal"     # æœ€å°‘åˆ†æï¼ˆ1-25%ï¼‰
    PARTIAL = "partial"     # éƒ¨åˆ†åˆ†æï¼ˆ26-75%ï¼‰
    SUBSTANTIAL = "substantial"  # å¤§éƒ¨åˆ†åˆ†æï¼ˆ76-99%ï¼‰
    COMPLETE = "complete"   # å®Œå…¨åˆ†æï¼ˆ100%ï¼‰


class ExceptionSeverity(Enum):
    """å¼‚å¸¸ä¸¥é‡ç¨‹åº¦"""
    TEMPORARY = "temporary"     # ä¸´æ—¶æ€§å¼‚å¸¸ï¼ˆç½‘ç»œã€è¶…æ—¶ç­‰ï¼‰
    RECOVERABLE = "recoverable" # å¯æ¢å¤å¼‚å¸¸ï¼ˆæƒé™ã€é…ç½®ç­‰ï¼‰
    CRITICAL = "critical"       # ä¸¥é‡å¼‚å¸¸ï¼ˆæ•°æ®æŸåã€ç³»ç»Ÿé”™è¯¯ç­‰ï¼‰


@dataclass
class TemplateReadinessAnalysis:
    """æ¨¡æ¿å°±ç»ªåº¦åˆ†æ"""
    template_id: str
    total_placeholders: int = 0
    analyzed_placeholders: int = 0
    validated_placeholders: int = 0
    failed_placeholders: int = 0
    
    # è´¨é‡æŒ‡æ ‡
    avg_confidence_score: float = 0.0
    min_confidence_score: float = 0.0
    max_confidence_score: float = 0.0
    
    # æ—¶é—´ä¿¡æ¯
    last_analysis_time: Optional[datetime] = None
    analysis_age_hours: float = 0.0
    
    # é—®é¢˜ç»Ÿè®¡
    syntax_issues: int = 0
    performance_issues: int = 0
    business_logic_issues: int = 0
    
    @property
    def completion_ratio(self) -> float:
        """åˆ†æå®Œæˆæ¯”ä¾‹"""
        return self.analyzed_placeholders / self.total_placeholders if self.total_placeholders > 0 else 0.0
    
    @property
    def validation_ratio(self) -> float:
        """éªŒè¯å®Œæˆæ¯”ä¾‹"""  
        return self.validated_placeholders / self.total_placeholders if self.total_placeholders > 0 else 0.0
    
    @property
    def completeness_level(self) -> AnalysisCompleteness:
        """å®Œæˆåº¦ç­‰çº§"""
        ratio = self.completion_ratio
        if ratio == 0:
            return AnalysisCompleteness.NONE
        elif ratio <= 0.25:
            return AnalysisCompleteness.MINIMAL
        elif ratio <= 0.75:
            return AnalysisCompleteness.PARTIAL
        elif ratio < 1.0:
            return AnalysisCompleteness.SUBSTANTIAL
        else:
            return AnalysisCompleteness.COMPLETE
    
    @property
    def is_ready_for_execution(self) -> bool:
        """æ˜¯å¦å‡†å¤‡å¥½æ‰§è¡Œ"""
        return (
            self.completion_ratio >= 1.0 and
            self.validation_ratio >= 0.9 and
            self.avg_confidence_score >= 0.6 and
            self.critical_issues_count == 0
        )
    
    @property
    def is_partially_ready(self) -> bool:
        """æ˜¯å¦éƒ¨åˆ†å‡†å¤‡å¥½"""
        return (
            self.completion_ratio >= 0.5 and
            self.avg_confidence_score >= 0.5 and
            self.critical_issues_count == 0
        )
    
    @property
    def critical_issues_count(self) -> int:
        """ä¸¥é‡é—®é¢˜æ•°é‡"""
        return self.failed_placeholders + self.syntax_issues
    
    @property
    def requires_reanalysis(self) -> bool:
        """æ˜¯å¦éœ€è¦é‡æ–°åˆ†æ"""
        return (
            self.analysis_age_hours > 24 or  # åˆ†æç»“æœè¿‡æœŸ
            self.avg_confidence_score < 0.5 or  # ç½®ä¿¡åº¦å¤ªä½
            self.critical_issues_count > 0  # å­˜åœ¨ä¸¥é‡é—®é¢˜
        )


class EnhancedTwoPhasePipeline:
    """å¢å¼ºçš„ä¸¤é˜¶æ®µæµæ°´çº¿ - é›†æˆæ™ºèƒ½è°ƒåº¦å’Œè´Ÿè½½å‡è¡¡"""
    
    def __init__(self, config=None):
        self.config = config or {}
        self.pipeline_id = str(uuid4())
        
        # å¢å¼ºé…ç½®
        self.enable_recovery_mode = config.get('enable_recovery_mode', True)
        self.enable_partial_analysis = config.get('enable_partial_analysis', True) 
        self.enable_incremental_update = config.get('enable_incremental_update', True)
        self.max_retry_attempts = config.get('max_retry_attempts', 3)
        self.cache_preference_threshold = config.get('cache_preference_threshold', 0.8)
        
        # æ–°å¢ï¼šæ™ºèƒ½è°ƒåº¦å’Œè´Ÿè½½å‡è¡¡ç»„ä»¶
        self.intelligent_scheduler = IntelligentTaskScheduler()
        self.load_balancer = DynamicLoadBalancer()
        self.enable_intelligent_scheduling = config.get('enable_intelligent_scheduling', True)
        self.enable_load_balancing = config.get('enable_load_balancing', True)
        
        logger.info(f"EnhancedTwoPhasePipeline initialized with intelligent scheduling: {self.pipeline_id}")
    
    async def execute(self,
                     task_id: int,
                     user_id: str,
                     template_id: Optional[str] = None,
                     data_source_id: Optional[str] = None,
                     db: Optional[Session] = None,
                     execution_context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """æ‰§è¡Œå¢å¼ºçš„ä¸¤é˜¶æ®µæµæ°´çº¿"""
        start_time = time.time()
        
        try:
            logger.info(f"ğŸš€ å¼€å§‹å¢å¼ºæµæ°´çº¿æ‰§è¡Œ: task_id={task_id}, template_id={template_id}")
            
            # 1. åˆå§‹åŒ–å’Œä¸Šä¸‹æ–‡æ„å»º
            context = await self._build_enhanced_context(
                task_id, user_id, template_id, data_source_id, execution_context
            )
            
            # 2. æ™ºèƒ½ä»»åŠ¡è°ƒåº¦ï¼ˆæ–°å¢ï¼‰
            execution_plan = None
            if self.enable_intelligent_scheduling:
                try:
                    execution_plan = await self._create_intelligent_execution_plan(context, db)
                    context['execution_plan'] = execution_plan
                    logger.info(f"ğŸ§  æ™ºèƒ½è°ƒåº¦å®Œæˆ: å¹¶å‘åº¦={execution_plan.strategy.parallel_degree}, "
                               f"æ¨¡å¼={execution_plan.strategy.execution_mode}")
                except Exception as scheduling_error:
                    logger.warning(f"æ™ºèƒ½è°ƒåº¦å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨é»˜è®¤æ¨¡å¼: {scheduling_error}")
            
            # 3. å¢å¼ºçš„æ™ºèƒ½æ‰§è¡Œæ¨¡å¼åˆ¤æ–­
            execution_mode = await self._determine_enhanced_execution_mode(context, db)
            logger.info(f"ğŸ“‹ ç¡®å®šæ‰§è¡Œæ¨¡å¼: {execution_mode.value}")
            
            # 4. ä»»åŠ¡åˆ†è§£å’Œè´Ÿè½½å‡è¡¡ï¼ˆæ–°å¢ï¼‰
            load_balancing_result = None
            if self.enable_load_balancing and execution_plan:
                try:
                    subtasks = await self._decompose_task_for_balancing(context, execution_plan)
                    load_balancing_result = await self.load_balancer.distribute_task(task_id, subtasks)
                    context['load_balancing_result'] = load_balancing_result
                    logger.info(f"âš–ï¸ è´Ÿè½½å‡è¡¡å®Œæˆ: æˆåŠŸåˆ†é…={len(load_balancing_result.allocations)}, "
                               f"å‡è¡¡åˆ†æ•°={load_balancing_result.load_balance_score:.2f}")
                except Exception as load_balancing_error:
                    logger.warning(f"è´Ÿè½½å‡è¡¡å¤±è´¥ï¼Œç»§ç»­æ­£å¸¸æ‰§è¡Œ: {load_balancing_error}")
            
            # 5. æ ¹æ®æ‰§è¡Œæ¨¡å¼æ‰§è¡Œç›¸åº”æµç¨‹
            result = await self._execute_by_mode(execution_mode, context, db)
            
            # 6. ç»“æœåå¤„ç†å’Œä¼˜åŒ–
            enhanced_result = await self._enhance_result(result, execution_mode, context, execution_plan, load_balancing_result)
            
            total_time = time.time() - start_time
            enhanced_result['total_execution_time'] = total_time
            enhanced_result['execution_mode'] = execution_mode.value
            enhanced_result['pipeline_id'] = self.pipeline_id
            
            logger.info(f"âœ… å¢å¼ºæµæ°´çº¿æ‰§è¡Œå®Œæˆ: {total_time:.2f}s, æ¨¡å¼: {execution_mode.value}")
            return enhanced_result
            
        except Exception as e:
            total_time = time.time() - start_time
            logger.error(f"âŒ å¢å¼ºæµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}")
            
            # å¢å¼ºçš„é”™è¯¯æ¢å¤
            recovery_result = await self._handle_pipeline_failure(e, context, db)
            recovery_result.update({
                'total_execution_time': total_time,
                'pipeline_id': self.pipeline_id,
                'error': str(e)
            })
            
            return recovery_result
    
    async def _determine_enhanced_execution_mode(self, 
                                               context: Dict[str, Any], 
                                               db: Session) -> EnhancedExecutionMode:
        """å¢å¼ºçš„æ™ºèƒ½æ‰§è¡Œæ¨¡å¼åˆ¤æ–­ - ä¿®å¤åŸæœ‰ç¼ºé™·"""
        template_id = context.get("template_id")
        
        if not template_id:
            logger.warning("æ¨¡æ¿IDç¼ºå¤±ï¼Œä½¿ç”¨å®Œæ•´æµæ°´çº¿æ¨¡å¼")
            return EnhancedExecutionMode.FULL_PIPELINE
        
        try:
            # 1. æ·±åº¦å°±ç»ªåº¦åˆ†æ
            readiness_analysis = await self._analyze_template_readiness(template_id, db)
            logger.info(f"ğŸ“Š æ¨¡æ¿å°±ç»ªåº¦: {readiness_analysis.completeness_level.value} " +
                       f"({readiness_analysis.completion_ratio:.1%})")
            
            # 2. å¼ºåˆ¶é‡æ–°åˆ†ææ£€æŸ¥
            if context.get('force_reanalyze', False):
                logger.info("ğŸ”„ å¼ºåˆ¶é‡æ–°åˆ†æï¼Œä½¿ç”¨å®Œæ•´æµæ°´çº¿æ¨¡å¼")
                return EnhancedExecutionMode.FULL_PIPELINE
            
            # 3. å®Œå…¨å°±ç»ª - ç›´æ¥æ‰§è¡Œ
            if readiness_analysis.is_ready_for_execution:
                logger.info("âœ… æ¨¡æ¿å®Œå…¨å°±ç»ªï¼Œä½¿ç”¨é˜¶æ®µ2æ‰§è¡Œæ¨¡å¼")
                return EnhancedExecutionMode.PHASE_2_ONLY
            
            # 4. éƒ¨åˆ†å°±ç»ª - æ–°å¢å¤„ç†é€»è¾‘
            elif readiness_analysis.is_partially_ready and self.enable_partial_analysis:
                if readiness_analysis.completeness_level == AnalysisCompleteness.SUBSTANTIAL:
                    logger.info("ğŸ”¶ æ¨¡æ¿å¤§éƒ¨åˆ†å°±ç»ªï¼Œä½¿ç”¨å¢é‡æ›´æ–°æ¨¡å¼")
                    return EnhancedExecutionMode.INCREMENTAL_UPDATE
                else:
                    logger.info("ğŸ”¸ æ¨¡æ¿éƒ¨åˆ†å°±ç»ªï¼Œä½¿ç”¨éƒ¨åˆ†åˆ†ææ¨¡å¼")
                    return EnhancedExecutionMode.PARTIAL_ANALYSIS
            
            # 5. éœ€è¦é‡æ–°åˆ†ææ£€æŸ¥
            elif readiness_analysis.requires_reanalysis:
                logger.info("ğŸ” æ¨¡æ¿éœ€è¦é‡æ–°åˆ†æï¼Œä½¿ç”¨å®Œæ•´æµæ°´çº¿æ¨¡å¼")
                return EnhancedExecutionMode.FULL_PIPELINE
            
            # 6. å®Œå…¨æœªå‡†å¤‡
            else:
                logger.info("ğŸ“‹ æ¨¡æ¿æœªåˆ†æï¼Œä½¿ç”¨å®Œæ•´æµæ°´çº¿æ¨¡å¼")
                return EnhancedExecutionMode.FULL_PIPELINE
                
        except ConnectionError as e:
            # ç½‘ç»œè¿æ¥å¼‚å¸¸ - ä¼˜å…ˆä½¿ç”¨ç¼“å­˜
            logger.warning(f"ğŸŒ ç½‘ç»œè¿æ¥å¼‚å¸¸ï¼Œå°è¯•ç¼“å­˜æ‰§è¡Œæ¨¡å¼: {e}")
            if self.enable_recovery_mode:
                return EnhancedExecutionMode.CACHED_EXECUTION
            else:
                return EnhancedExecutionMode.RECOVERY_MODE
                
        except TimeoutError as e:
            # è¶…æ—¶å¼‚å¸¸ - ä½¿ç”¨æ¢å¤æ¨¡å¼
            logger.warning(f"â° æ“ä½œè¶…æ—¶ï¼Œä½¿ç”¨æ¢å¤æ¨¡å¼: {e}")
            return EnhancedExecutionMode.RECOVERY_MODE
            
        except PermissionError as e:
            # æƒé™å¼‚å¸¸ - å°è¯•æ¢å¤
            logger.warning(f"ğŸ” æƒé™å¼‚å¸¸ï¼Œå°è¯•æ¢å¤æ¨¡å¼: {e}")
            return EnhancedExecutionMode.RECOVERY_MODE
            
        except Exception as e:
            # å…¶ä»–ä¸¥é‡å¼‚å¸¸ - é™çº§å¤„ç†
            severity = self._classify_exception_severity(e)
            logger.warning(f"âš ï¸ å¼‚å¸¸({severity.value}): {e}")
            
            if severity == ExceptionSeverity.TEMPORARY and self.enable_recovery_mode:
                return EnhancedExecutionMode.RECOVERY_MODE
            elif severity == ExceptionSeverity.RECOVERABLE:
                return EnhancedExecutionMode.CACHED_EXECUTION
            else:
                return EnhancedExecutionMode.FULL_PIPELINE
    
    async def _analyze_template_readiness(self, 
                                        template_id: str, 
                                        db: Session) -> TemplateReadinessAnalysis:
        """æ·±åº¦åˆ†ææ¨¡æ¿å°±ç»ªåº¦"""
        try:
            # template_parser = EnhancedTemplateParser(db)  # Disabled
            # In DAG architecture, use IntelligentPlaceholderService instead
            from app.services.domain.placeholder import IntelligentPlaceholderService
            template_parser = IntelligentPlaceholderService()
            
            # è·å–åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
            stats_result = await template_parser.get_placeholder_analysis_statistics(template_id)
            
            # è·å–è´¨é‡ä¿¡æ¯
            quality_info = await self._get_placeholder_quality_info(template_id, db)
            
            # è®¡ç®—åˆ†ææ—¶æ•ˆæ€§
            last_analysis = await self._get_last_analysis_time(template_id, db)
            analysis_age = 0.0
            if last_analysis:
                analysis_age = (datetime.now() - last_analysis).total_seconds() / 3600
            
            analysis = TemplateReadinessAnalysis(
                template_id=template_id,
                total_placeholders=stats_result.get('total_placeholders', 0),
                analyzed_placeholders=stats_result.get('analyzed_placeholders', 0),
                validated_placeholders=stats_result.get('validated_placeholders', 0),
                failed_placeholders=stats_result.get('failed_placeholders', 0),
                avg_confidence_score=quality_info.get('avg_confidence', 0.0),
                min_confidence_score=quality_info.get('min_confidence', 0.0),
                max_confidence_score=quality_info.get('max_confidence', 0.0),
                last_analysis_time=last_analysis,
                analysis_age_hours=analysis_age,
                syntax_issues=quality_info.get('syntax_issues', 0),
                performance_issues=quality_info.get('performance_issues', 0),
                business_logic_issues=quality_info.get('business_logic_issues', 0)
            )
            
            return analysis
            
        except Exception as e:
            logger.error(f"æ¨¡æ¿å°±ç»ªåº¦åˆ†æå¤±è´¥: {e}")
            # è¿”å›å®‰å…¨çš„é»˜è®¤å€¼
            return TemplateReadinessAnalysis(
                template_id=template_id,
                total_placeholders=0,
                analyzed_placeholders=0
            )
    
    async def _execute_by_mode(self, 
                             execution_mode: EnhancedExecutionMode,
                             context: Dict[str, Any],
                             db: Session) -> Dict[str, Any]:
        """æ ¹æ®æ‰§è¡Œæ¨¡å¼æ‰§è¡Œç›¸åº”æµç¨‹"""
        
        if execution_mode == EnhancedExecutionMode.FULL_PIPELINE:
            return await self._execute_full_pipeline(context, db)
            
        elif execution_mode == EnhancedExecutionMode.PHASE_1_ONLY:
            return await self._execute_phase_1_only(context, db)
            
        elif execution_mode == EnhancedExecutionMode.PHASE_2_ONLY:
            return await self._execute_phase_2_only(context, db)
            
        elif execution_mode == EnhancedExecutionMode.PARTIAL_ANALYSIS:
            return await self._execute_partial_analysis(context, db)
            
        elif execution_mode == EnhancedExecutionMode.INCREMENTAL_UPDATE:
            return await self._execute_incremental_update(context, db)
            
        elif execution_mode == EnhancedExecutionMode.RECOVERY_MODE:
            return await self._execute_recovery_mode(context, db)
            
        elif execution_mode == EnhancedExecutionMode.CACHED_EXECUTION:
            return await self._execute_cached_mode(context, db)
            
        else:
            # é»˜è®¤å›é€€åˆ°å®Œæ•´æµæ°´çº¿
            logger.warning(f"æœªçŸ¥æ‰§è¡Œæ¨¡å¼: {execution_mode}, å›é€€åˆ°å®Œæ•´æµæ°´çº¿")
            return await self._execute_full_pipeline(context, db)
    
    async def _execute_partial_analysis(self, 
                                      context: Dict[str, Any], 
                                      db: Session) -> Dict[str, Any]:
        """æ‰§è¡Œéƒ¨åˆ†åˆ†ææ¨¡å¼ - æ–°å¢åŠŸèƒ½"""
        logger.info("ğŸ”¸ å¼€å§‹éƒ¨åˆ†åˆ†ææ¨¡å¼æ‰§è¡Œ")
        
        try:
            template_id = context["template_id"]
            
            # 1. è¯†åˆ«æœªåˆ†æçš„å ä½ç¬¦
            unanalyzed_placeholders = await self._get_unanalyzed_placeholders(template_id, db)
            logger.info(f"ğŸ“ å‘ç° {len(unanalyzed_placeholders)} ä¸ªæœªåˆ†æå ä½ç¬¦")
            
            # 2. ä¼˜å…ˆåˆ†æå…³é”®å ä½ç¬¦
            critical_placeholders = self._prioritize_placeholders(unanalyzed_placeholders)
            
            # 3. åˆ†æå…³é”®å ä½ç¬¦
            analysis_results = await self._analyze_critical_placeholders(critical_placeholders, context, db)
            
            # 4. è¯„ä¼°æ˜¯å¦å¯ä»¥æ‰§è¡Œ
            execution_feasibility = await self._assess_execution_feasibility(template_id, db)
            
            if execution_feasibility['feasible']:
                # 5. æ‰§è¡Œé˜¶æ®µ2
                execution_result = await self._execute_phase_2_with_partial_data(context, db)
                
                return {
                    'success': True,
                    'mode': 'partial_analysis',
                    'analysis_results': analysis_results,
                    'execution_result': execution_result,
                    'analyzed_count': len(critical_placeholders),
                    'remaining_count': len(unanalyzed_placeholders) - len(critical_placeholders)
                }
            else:
                return {
                    'success': False,
                    'mode': 'partial_analysis',
                    'analysis_results': analysis_results,
                    'reason': 'insufficient_analysis_for_execution',
                    'recommendation': 'run_full_pipeline'
                }
                
        except Exception as e:
            logger.error(f"éƒ¨åˆ†åˆ†ææ¨¡å¼æ‰§è¡Œå¤±è´¥: {e}")
            return {
                'success': False,
                'mode': 'partial_analysis',
                'error': str(e),
                'fallback_required': True
            }
    
    async def _execute_incremental_update(self, 
                                        context: Dict[str, Any], 
                                        db: Session) -> Dict[str, Any]:
        """æ‰§è¡Œå¢é‡æ›´æ–°æ¨¡å¼ - æ–°å¢åŠŸèƒ½"""
        logger.info("ğŸ”¶ å¼€å§‹å¢é‡æ›´æ–°æ¨¡å¼æ‰§è¡Œ")
        
        try:
            template_id = context["template_id"]
            
            # 1. è¯†åˆ«éœ€è¦æ›´æ–°çš„å ä½ç¬¦
            update_candidates = await self._identify_update_candidates(template_id, db)
            logger.info(f"ğŸ”„ è¯†åˆ«åˆ° {len(update_candidates)} ä¸ªéœ€è¦æ›´æ–°çš„å ä½ç¬¦")
            
            if not update_candidates:
                # æ— éœ€æ›´æ–°ï¼Œç›´æ¥æ‰§è¡Œ
                logger.info("âœ… æ— éœ€å¢é‡æ›´æ–°ï¼Œç›´æ¥æ‰§è¡Œé˜¶æ®µ2")
                return await self._execute_phase_2_only(context, db)
            
            # 2. å¢é‡åˆ†æ
            update_results = await self._perform_incremental_analysis(update_candidates, context, db)
            
            # 3. éªŒè¯æ›´æ–°ç»“æœ
            validation_result = await self._validate_incremental_updates(update_results, db)
            
            if validation_result['valid']:
                # 4. æ‰§è¡Œé˜¶æ®µ2
                execution_result = await self._execute_phase_2_only(context, db)
                
                return {
                    'success': True,
                    'mode': 'incremental_update',
                    'update_results': update_results,
                    'execution_result': execution_result,
                    'updated_count': len(update_candidates)
                }
            else:
                return {
                    'success': False,
                    'mode': 'incremental_update',
                    'update_results': update_results,
                    'validation_issues': validation_result['issues'],
                    'recommendation': 'run_full_analysis'
                }
                
        except Exception as e:
            logger.error(f"å¢é‡æ›´æ–°æ¨¡å¼æ‰§è¡Œå¤±è´¥: {e}")
            return {
                'success': False,
                'mode': 'incremental_update',
                'error': str(e),
                'fallback_required': True
            }
    
    async def _execute_recovery_mode(self, 
                                   context: Dict[str, Any], 
                                   db: Session) -> Dict[str, Any]:
        """æ‰§è¡Œæ•…éšœæ¢å¤æ¨¡å¼ - æ–°å¢åŠŸèƒ½"""
        logger.info("ğŸ› ï¸ å¼€å§‹æ•…éšœæ¢å¤æ¨¡å¼æ‰§è¡Œ")
        
        try:
            # 1. å°è¯•ä»ç¼“å­˜æ¢å¤
            cache_result = await self._attempt_cache_recovery(context, db)
            if cache_result['success']:
                logger.info("âœ… ä»ç¼“å­˜æˆåŠŸæ¢å¤")
                return cache_result
            
            # 2. å°è¯•ä½¿ç”¨å†å²æ•°æ®
            history_result = await self._attempt_history_recovery(context, db)
            if history_result['success']:
                logger.info("âœ… ä»å†å²æ•°æ®æˆåŠŸæ¢å¤")
                return history_result
            
            # 3. é™çº§åˆ°æœ€å°åŒ–åŠŸèƒ½
            minimal_result = await self._attempt_minimal_execution(context, db)
            if minimal_result['success']:
                logger.info("âœ… æœ€å°åŒ–åŠŸèƒ½æ‰§è¡ŒæˆåŠŸ")
                return minimal_result
            
            # 4. å®Œå…¨å¤±è´¥
            logger.error("âŒ æ‰€æœ‰æ¢å¤å°è¯•éƒ½å¤±è´¥")
            return {
                'success': False,
                'mode': 'recovery_mode',
                'recovery_attempts': ['cache', 'history', 'minimal'],
                'error': 'all_recovery_attempts_failed',
                'recommendation': 'manual_intervention_required'
            }
            
        except Exception as e:
            logger.error(f"æ•…éšœæ¢å¤æ¨¡å¼æ‰§è¡Œå¤±è´¥: {e}")
            return {
                'success': False,
                'mode': 'recovery_mode',
                'error': str(e),
                'critical_failure': True
            }
    
    async def _execute_cached_mode(self, 
                                 context: Dict[str, Any], 
                                 db: Session) -> Dict[str, Any]:
        """æ‰§è¡Œç¼“å­˜ä¼˜å…ˆæ¨¡å¼ - æ–°å¢åŠŸèƒ½"""
        logger.info("ğŸ’¾ å¼€å§‹ç¼“å­˜ä¼˜å…ˆæ¨¡å¼æ‰§è¡Œ")
        
        try:
            # 1. æ£€æŸ¥å¯ç”¨ç¼“å­˜
            cache_inventory = await self._inventory_available_cache(context, db)
            logger.info(f"ğŸ“¦ å‘ç° {cache_inventory['total_items']} ä¸ªç¼“å­˜é¡¹")
            
            # 2. è¯„ä¼°ç¼“å­˜å®Œæ•´æ€§
            completeness_score = cache_inventory['completeness_score']
            
            if completeness_score >= self.cache_preference_threshold:
                # 3. ä½¿ç”¨ç¼“å­˜æ•°æ®æ‰§è¡Œ
                logger.info(f"âœ… ç¼“å­˜å®Œæ•´æ€§è‰¯å¥½({completeness_score:.1%})ï¼Œä½¿ç”¨ç¼“å­˜æ‰§è¡Œ")
                return await self._execute_with_cache_data(cache_inventory, context, db)
            else:
                # 4. ç¼“å­˜ä¸è¶³ï¼Œæ··åˆæ‰§è¡Œ
                logger.info(f"ğŸ”¸ ç¼“å­˜å®Œæ•´æ€§ä¸è¶³({completeness_score:.1%})ï¼Œæ‰§è¡Œæ··åˆæ¨¡å¼")
                return await self._execute_hybrid_cache_mode(cache_inventory, context, db)
                
        except Exception as e:
            logger.error(f"ç¼“å­˜ä¼˜å…ˆæ¨¡å¼æ‰§è¡Œå¤±è´¥: {e}")
            return {
                'success': False,
                'mode': 'cached_execution',
                'error': str(e),
                'fallback_required': True
            }
    
    # è¾…åŠ©æ–¹æ³•
    
    async def _build_enhanced_context(self, 
                                    task_id: int,
                                    user_id: str,
                                    template_id: str,
                                    data_source_id: str,
                                    execution_context: Dict[str, Any]) -> Dict[str, Any]:
        """æ„å»ºå¢å¼ºä¸Šä¸‹æ–‡"""
        context = {
            'task_id': task_id,
            'user_id': user_id,
            'template_id': template_id,
            'data_source_id': data_source_id,
            'execution_context': execution_context or {},
            'pipeline_id': self.pipeline_id,
            'start_time': datetime.now(),
            'retry_count': 0,
            'enhancement_enabled': True,
            'intelligent_scheduling_enabled': self.enable_intelligent_scheduling,
            'load_balancing_enabled': self.enable_load_balancing
        }
        return context
    
    def _classify_exception_severity(self, exception: Exception) -> ExceptionSeverity:
        """åˆ†ç±»å¼‚å¸¸ä¸¥é‡ç¨‹åº¦"""
        exception_type = type(exception).__name__
        
        # ä¸´æ—¶æ€§å¼‚å¸¸
        temporary_types = ['ConnectionError', 'TimeoutError', 'RequestException', 'NetworkError']
        if exception_type in temporary_types:
            return ExceptionSeverity.TEMPORARY
        
        # å¯æ¢å¤å¼‚å¸¸  
        recoverable_types = ['PermissionError', 'AuthenticationError', 'ConfigurationError']
        if exception_type in recoverable_types:
            return ExceptionSeverity.RECOVERABLE
        
        # ä¸¥é‡å¼‚å¸¸
        return ExceptionSeverity.CRITICAL
    
    async def _get_unanalyzed_placeholders(self, template_id: str, db: Session) -> List[Dict[str, Any]]:
        """è·å–æœªåˆ†æçš„å ä½ç¬¦åˆ—è¡¨"""
        # å®ç°è·å–æœªåˆ†æå ä½ç¬¦çš„é€»è¾‘
        # è¿™é‡Œè¿”å›æ¨¡æ‹Ÿæ•°æ®
        return [
            {'id': 'ph1', 'text': 'placeholder1', 'priority': 'high'},
            {'id': 'ph2', 'text': 'placeholder2', 'priority': 'medium'},
        ]
    
    def _prioritize_placeholders(self, placeholders: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å¯¹å ä½ç¬¦è¿›è¡Œä¼˜å…ˆçº§æ’åº"""
        return sorted(placeholders, key=lambda p: {'high': 3, 'medium': 2, 'low': 1}.get(p.get('priority', 'low'), 1), reverse=True)
    
    async def _handle_pipeline_failure(self, 
                                     exception: Exception, 
                                     context: Dict[str, Any], 
                                     db: Session) -> Dict[str, Any]:
        """å¤„ç†æµæ°´çº¿å¤±è´¥"""
        severity = self._classify_exception_severity(exception)
        
        if severity == ExceptionSeverity.TEMPORARY and context.get('retry_count', 0) < self.max_retry_attempts:
            # ä¸´æ—¶å¼‚å¸¸ï¼Œå°è¯•é‡è¯•
            logger.info(f"ğŸ”„ ä¸´æ—¶å¼‚å¸¸ï¼Œå°è¯•é‡è¯• ({context.get('retry_count', 0) + 1}/{self.max_retry_attempts})")
            context['retry_count'] = context.get('retry_count', 0) + 1
            
            # å»¶è¿Ÿé‡è¯•
            await asyncio.sleep(2 ** context['retry_count'])  # æŒ‡æ•°é€€é¿
            
            try:
                return await self.execute(
                    context['task_id'],
                    context['user_id'], 
                    context['template_id'],
                    context['data_source_id'],
                    db,
                    context['execution_context']
                )
            except Exception as retry_error:
                logger.error(f"é‡è¯•å¤±è´¥: {retry_error}")
        
        # æ‰§è¡Œæ¢å¤æ¨¡å¼
        try:
            return await self._execute_recovery_mode(context, db)
        except Exception as recovery_error:
            logger.error(f"æ¢å¤æ¨¡å¼ä¹Ÿå¤±è´¥: {recovery_error}")
            
            return {
                'success': False,
                'error': str(exception),
                'recovery_error': str(recovery_error),
                'severity': severity.value,
                'requires_manual_intervention': True
            }
    
    # ä¸ºäº†ç¼–è¯‘é€šè¿‡ï¼Œæ·»åŠ å…¶ä»–å¿…è¦çš„æ–¹æ³•ï¼ˆå®ç°å¯ä»¥æ˜¯å ä½ç¬¦ï¼‰
    
    async def _execute_full_pipeline(self, context, db):
        """æ‰§è¡Œå®Œæ•´æµæ°´çº¿ - é›†æˆTaskSQLExecutionAgent"""
        logger.info("ğŸ”§ æ‰§è¡Œå®Œæ•´æµæ°´çº¿æ¨¡å¼")
        
        try:
            # é˜¶æ®µ1ï¼šå ä½ç¬¦åˆ†æ (ä¿æŒç°æœ‰é€»è¾‘)
            analysis_result = await self._execute_phase_1_analysis(context, db)
            
            # é˜¶æ®µ2ï¼šä½¿ç”¨TaskSQLExecutionAgentæ‰§è¡ŒSQLä»»åŠ¡
            if analysis_result.get('success', False):
                execution_result = await self._execute_phase_2_with_agent(context, db, analysis_result)
                
                return {
                    'success': execution_result.get('success', False),
                    'mode': 'full_pipeline',
                    'phase_1_result': analysis_result,
                    'phase_2_result': execution_result,
                    'agent_integrated': True
                }
            else:
                return {
                    'success': False,
                    'mode': 'full_pipeline',
                    'phase_1_result': analysis_result,
                    'error': 'Phase 1 analysis failed'
                }
                
        except Exception as e:
            logger.error(f"å®Œæ•´æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}")
            return {
                'success': False,
                'mode': 'full_pipeline',
                'error': str(e)
            }
    
    async def _execute_phase_1_only(self, context, db):
        """æ‰§è¡Œé˜¶æ®µ1 - å ä½ç¬¦åˆ†æ"""
        return await self._execute_phase_1_analysis(context, db)
    
    async def _execute_phase_2_only(self, context, db):
        """æ‰§è¡Œé˜¶æ®µ2 - ä½¿ç”¨TaskSQLExecutionAgentæ‰§è¡ŒSQL"""
        return await self._execute_phase_2_with_agent(context, db)
    
    async def _enhance_result(self, result, execution_mode, context):
        result['enhancement_applied'] = True
        result['context_info'] = {
            'pipeline_id': context['pipeline_id'],
            'execution_mode': execution_mode.value
        }
        return result
    
    async def _get_placeholder_quality_info(self, template_id, db):
        return {'avg_confidence': 0.8, 'min_confidence': 0.6, 'max_confidence': 0.9, 'syntax_issues': 0, 'performance_issues': 1, 'business_logic_issues': 0}
    
    async def _get_last_analysis_time(self, template_id, db):
        return datetime.now() - timedelta(hours=2)
    
    async def _analyze_critical_placeholders(self, placeholders, context, db):
        return [{'placeholder_id': p['id'], 'success': True} for p in placeholders]
    
    async def _assess_execution_feasibility(self, template_id, db):
        return {'feasible': True, 'confidence': 0.85}
    
    async def _execute_phase_2_with_partial_data(self, context, db):
        return {'success': True, 'partial_data_used': True}
    
    async def _identify_update_candidates(self, template_id, db):
        return [{'id': 'ph_update_1', 'reason': 'low_confidence'}]
    
    async def _perform_incremental_analysis(self, candidates, context, db):
        return [{'placeholder_id': c['id'], 'updated': True} for c in candidates]
    
    async def _validate_incremental_updates(self, results, db):
        return {'valid': True, 'issues': []}
    
    async def _create_intelligent_execution_plan(self, context: Dict[str, Any], db: Session) -> TaskExecutionPlan:
        """åˆ›å»ºæ™ºèƒ½æ‰§è¡Œè®¡åˆ’"""
        try:
            # æ„å»ºä»»åŠ¡ä¸Šä¸‹æ–‡ç”¨äºè°ƒåº¦åˆ†æ
            task_context = await self._build_task_context_for_scheduling(context, db)
            
            # ä½¿ç”¨æ™ºèƒ½è°ƒåº¦å™¨åˆ›å»ºæ‰§è¡Œè®¡åˆ’
            execution_plan = await self.intelligent_scheduler.schedule_task(
                task_id=context['task_id'],
                user_id=context['user_id'],
                task_context=task_context
            )
            
            return execution_plan
            
        except Exception as e:
            logger.error(f"åˆ›å»ºæ™ºèƒ½æ‰§è¡Œè®¡åˆ’å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤æ‰§è¡Œè®¡åˆ’
            return await self._create_fallback_execution_plan(context)
    
    async def _build_task_context_for_scheduling(self, context: Dict[str, Any], db: Session) -> Dict[str, Any]:
        """ä¸ºè°ƒåº¦å™¨æ„å»ºä»»åŠ¡ä¸Šä¸‹æ–‡"""
        try:
            from app import crud
            
            template_id = context.get('template_id')
            if not template_id:
                return {"placeholders": [], "template_content": "", "data_source": {}}
            
            # è·å–æ¨¡æ¿å’Œå ä½ç¬¦ä¿¡æ¯
            template = crud.template.get(db, id=template_id)
            placeholders = crud.template_placeholder.get_by_template(db, template_id=template_id)
            
            task_context = {
                "placeholders": [
                    {
                        "name": p.name,
                        "agent_analyzed": p.agent_analyzed,
                        "generated_sql": p.generated_sql,
                        "confidence_score": p.confidence_score,
                        "data_volume_hint": getattr(p, 'data_volume_hint', 1000)
                    }
                    for p in placeholders
                ],
                "template_content": template.content if template else "",
                "data_source": {
                    "id": context.get('data_source_id'),
                    "type": "doris"  # é»˜è®¤ç±»å‹
                }
            }
            
            return task_context
            
        except Exception as e:
            logger.error(f"æ„å»ºè°ƒåº¦ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return {"placeholders": [], "template_content": "", "data_source": {}}
    
    async def _create_fallback_execution_plan(self, context: Dict[str, Any]) -> TaskExecutionPlan:
        """åˆ›å»ºé™çº§æ‰§è¡Œè®¡åˆ’"""
        from .intelligent_task_scheduler import ExecutionStrategy, ExecutionPriority, TaskExecutionPlan
        
        fallback_strategy = ExecutionStrategy(
            execution_mode="SMART_EXECUTION",
            parallel_degree=2,
            cache_strategy="BALANCED",
            priority_level=ExecutionPriority.NORMAL,
            estimated_duration=120,
            max_retries=2,
            timeout=1800
        )
        
        return TaskExecutionPlan(
            task_id=context['task_id'],
            strategy=fallback_strategy,
            scheduled_time=datetime.now(),
            resource_allocation={"mode": "fallback"},
            dependencies=[]
        )
    
    async def _decompose_task_for_balancing(self, context: Dict[str, Any], execution_plan: TaskExecutionPlan) -> List[Dict[str, Any]]:
        """ä¸ºè´Ÿè½½å‡è¡¡åˆ†è§£ä»»åŠ¡"""
        subtasks = []
        
        # åŸºäºæ‰§è¡Œè®¡åˆ’çš„å¹¶å‘åº¦åˆ›å»ºå­ä»»åŠ¡
        parallel_degree = execution_plan.strategy.parallel_degree
        
        # åˆ›å»ºå ä½ç¬¦åˆ†æå­ä»»åŠ¡
        for i in range(min(parallel_degree, 2)):  # é™åˆ¶å ä½ç¬¦åˆ†æä»»åŠ¡æ•°é‡
            subtasks.append({
                "subtask_id": f"placeholder_analysis_{context['task_id']}_{i}",
                "type": TaskType.PLACEHOLDER_ANALYSIS.value,
                "priority": 7,  # é«˜ä¼˜å…ˆçº§
                "estimated_duration": 45,
                "resource_requirements": {"cpu": 1, "memory": "512MB"}
            })
        
        # åˆ›å»ºSQLæ‰§è¡Œå­ä»»åŠ¡
        for i in range(min(parallel_degree, 4)):  # SQLæ‰§è¡Œä»»åŠ¡æ•°é‡é™åˆ¶
            subtasks.append({
                "subtask_id": f"sql_execution_{context['task_id']}_{i}",
                "type": TaskType.SQL_QUERY.value,
                "priority": 8,  # æ›´é«˜ä¼˜å…ˆçº§
                "estimated_duration": 30,
                "resource_requirements": {"cpu": 1, "memory": "256MB"}
            })
        
        # åˆ›å»ºæŠ¥å‘Šç”Ÿæˆå­ä»»åŠ¡
        subtasks.append({
            "subtask_id": f"report_generation_{context['task_id']}",
            "type": TaskType.REPORT_COMPILE.value,
            "priority": 5,  # ä¸­ç­‰ä¼˜å…ˆçº§
            "estimated_duration": 90,
            "resource_requirements": {"cpu": 2, "memory": "1GB"}
        })
        
        return subtasks
    
    async def _attempt_cache_recovery(self, context, db):
        return {'success': True, 'source': 'cache', 'data': 'cached_data'}
    
    async def _attempt_history_recovery(self, context, db):
        return {'success': True, 'source': 'history', 'data': 'historical_data'}
    
    async def _attempt_minimal_execution(self, context, db):
        return {'success': True, 'source': 'minimal', 'data': 'minimal_result'}
    
    async def _inventory_available_cache(self, context, db):
        return {'total_items': 10, 'completeness_score': 0.85}
    
    async def _execute_with_cache_data(self, cache_inventory, context, db):
        return {'success': True, 'source': 'cache_data', 'completeness': cache_inventory['completeness_score']}
    
    async def _execute_hybrid_cache_mode(self, cache_inventory, context, db):
        return {'success': True, 'source': 'hybrid_cache', 'cache_used': 0.6, 'computed': 0.4}
    
    async def _execute_phase_1_analysis(self, context, db):
        """æ‰§è¡Œé˜¶æ®µ1å ä½ç¬¦åˆ†æ"""
        try:
            template_id = context.get('template_id')
            if not template_id:
                return {'success': False, 'error': 'Template ID missing'}
            
            # ä½¿ç”¨ç°æœ‰çš„å ä½ç¬¦åˆ†æé€»è¾‘
            # template_parser = EnhancedTemplateParser(db)  # Disabled
            # In DAG architecture, use IntelligentPlaceholderService instead
            from app.services.domain.placeholder import IntelligentPlaceholderService
            template_parser = IntelligentPlaceholderService()
            analysis_result = await template_parser.analyze_template_placeholders(template_id)
            
            return {
                'success': True,
                'analysis_result': analysis_result,
                'analyzed_placeholders': analysis_result.get('placeholders', []),
                'phase': 'phase_1'
            }
            
        except Exception as e:
            logger.error(f"é˜¶æ®µ1åˆ†æå¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'phase': 'phase_1'
            }
    
    async def _execute_phase_2_with_agent(self, context, db, analysis_result=None):
        """æ‰§è¡Œé˜¶æ®µ2 - ä½¿ç”¨TaskSQLExecutionAgentæ‰§è¡ŒSQL"""
        try:
            # ç›´æ¥ä½¿ç”¨IAOPä¸“ä¸šåŒ–ä»£ç†
            # REMOVED: IAOP specialized agents - Use MCP servers instead
            
            logger.info("ğŸ¤– å¼€å§‹ä½¿ç”¨TaskSQLExecutionAgentæ‰§è¡Œé˜¶æ®µ2")
            
            # åˆ›å»ºAgentå®ä¾‹ï¼Œé›†æˆæ™ºèƒ½è°ƒåº¦é…ç½®
            agent_config = {
                'enable_intelligent_analysis': True,
                'enable_cache_optimization': True,
                'enable_recovery_mode': True,
                'default_timeout': 180
            }
            
            # ä»æ‰§è¡Œè®¡åˆ’ä¸­è·å–ä¼˜åŒ–é…ç½®
            execution_plan = context.get('execution_plan')
            if execution_plan:
                agent_config.update({
                    'parallel_degree': execution_plan.strategy.parallel_degree,
                    'cache_strategy': execution_plan.strategy.cache_strategy,
                    'timeout': execution_plan.strategy.timeout,
                    'priority_level': execution_plan.strategy.priority_level.value
                })
            
            sql_agent = TaskSQLExecutionAgent(db, agent_config)
            
            # å‡†å¤‡æ‰§è¡Œä»»åŠ¡åˆ—è¡¨
            execution_results = []
            
            # å¦‚æœæœ‰åˆ†æç»“æœï¼Œå¤„ç†æ¯ä¸ªå ä½ç¬¦çš„SQL
            if analysis_result and analysis_result.get('analyzed_placeholders'):
                placeholders = analysis_result['analyzed_placeholders']
                
                for placeholder in placeholders:
                    try:
                        # æ„å»ºETLæŒ‡ä»¤
                        etl_instruction = {
                            'query_type': 'sql',
                            'sql_query': placeholder.get('generated_sql', ''),
                            'placeholder_id': placeholder.get('id'),
                            'placeholder_name': placeholder.get('placeholder_name', ''),
                            'placeholder_type': placeholder.get('placeholder_type', 'text')
                        }
                        
                        if not etl_instruction['sql_query']:
                            logger.warning(f"å ä½ç¬¦ {placeholder.get('placeholder_name')} æ²¡æœ‰SQLæŸ¥è¯¢ï¼Œè·³è¿‡")
                            continue
                        
                        # æ„å»ºAgentæ‰§è¡Œä¸Šä¸‹æ–‡ï¼Œåº”ç”¨æ™ºèƒ½è°ƒåº¦ç­–ç•¥
                        timeout_seconds = 120
                        priority = ExecutionPriority.MEDIUM
                        
                        # ä»æ‰§è¡Œè®¡åˆ’è°ƒæ•´å‚æ•°
                        execution_plan = context.get('execution_plan')
                        if execution_plan:
                            timeout_seconds = min(execution_plan.strategy.timeout, 300)  # æœ€å¤§5åˆ†é’Ÿ
                            if execution_plan.strategy.priority_level.value == 'high':
                                priority = ExecutionPriority.HIGH
                            elif execution_plan.strategy.priority_level.value == 'low':
                                priority = ExecutionPriority.LOW
                        
                        agent_context = TaskExecutionContext(
                            task_id=context.get('task_id', 0),
                            etl_instruction=etl_instruction,
                            data_source_id=context.get('data_source_id', ''),
                            execution_mode=TaskExecutionMode.INTELLIGENT_ANALYSIS,
                            priority=priority,
                            timeout_seconds=timeout_seconds,
                            enable_cache=True,
                            enable_recovery=True
                        )
                        
                        # æ‰§è¡ŒAgentä»»åŠ¡
                        agent_result = await sql_agent.execute_task(agent_context)
                        
                        execution_results.append({
                            'placeholder_id': placeholder.get('id'),
                            'placeholder_name': placeholder.get('placeholder_name'),
                            'success': agent_result.success,
                            'data': agent_result.data,
                            'execution_time': agent_result.execution_time,
                            'cache_hit': agent_result.cache_hit,
                            'execution_mode': agent_result.mode_used.value if agent_result.mode_used else None,
                            'error': agent_result.error_message
                        })
                        
                        if agent_result.success:
                            logger.info(f"âœ… å ä½ç¬¦ {placeholder.get('placeholder_name')} Agentæ‰§è¡ŒæˆåŠŸ")
                        else:
                            logger.error(f"âŒ å ä½ç¬¦ {placeholder.get('placeholder_name')} Agentæ‰§è¡Œå¤±è´¥: {agent_result.error_message}")
                    
                    except Exception as placeholder_error:
                        logger.error(f"å ä½ç¬¦å¤„ç†å¤±è´¥ {placeholder.get('placeholder_name')}: {placeholder_error}")
                        execution_results.append({
                            'placeholder_id': placeholder.get('id'),
                            'placeholder_name': placeholder.get('placeholder_name'),
                            'success': False,
                            'error': str(placeholder_error)
                        })
                        
            else:
                # æ²¡æœ‰å…·ä½“å ä½ç¬¦ï¼Œæ‰§è¡Œé€šç”¨SQLä»»åŠ¡
                logger.info("ğŸ“ æ²¡æœ‰åˆ†æç»“æœï¼Œæ‰§è¡Œé€šç”¨SQLä»»åŠ¡")
                
                # æ„å»ºé€šç”¨ETLæŒ‡ä»¤ï¼ˆç¤ºä¾‹ï¼‰
                etl_instruction = {
                    'query_type': 'sql',
                    'sql_query': context.get('sql_query', 'SELECT 1 as test'),
                    'task_type': 'generic'
                }
                
                agent_context = TaskExecutionContext(
                    task_id=context.get('task_id', 0),
                    etl_instruction=etl_instruction,
                    data_source_id=context.get('data_source_id', ''),
                    execution_mode=TaskExecutionMode.DIRECT_SQL,
                    priority=ExecutionPriority.MEDIUM,
                    timeout_seconds=120,
                    enable_cache=True,
                    enable_recovery=True
                )
                
                agent_result = await sql_agent.execute_task(agent_context)
                
                execution_results.append({
                    'task_type': 'generic',
                    'success': agent_result.success,
                    'data': agent_result.data,
                    'execution_time': agent_result.execution_time,
                    'cache_hit': agent_result.cache_hit,
                    'execution_mode': agent_result.mode_used.value if agent_result.mode_used else None,
                    'error': agent_result.error_message
                })
            
            # è®¡ç®—æ€»ä½“æˆåŠŸç‡
            successful_tasks = sum(1 for result in execution_results if result.get('success', False))
            total_tasks = len(execution_results)
            success_rate = successful_tasks / total_tasks if total_tasks > 0 else 0
            
            overall_success = success_rate >= 0.5  # è‡³å°‘50%æˆåŠŸæ‰ç®—æ•´ä½“æˆåŠŸ
            
            logger.info(f"ğŸ¯ é˜¶æ®µ2æ‰§è¡Œå®Œæˆ: æˆåŠŸç‡ {success_rate:.1%} ({successful_tasks}/{total_tasks})")
            
            # æ›´æ–°è´Ÿè½½å‡è¡¡å™¨ç»Ÿè®¡
            load_balancing_result = context.get('load_balancing_result')
            if load_balancing_result and self.enable_load_balancing:
                try:
                    # é€šçŸ¥è´Ÿè½½å‡è¡¡å™¨ä»»åŠ¡å®Œæˆ
                    total_execution_time = sum(r.get('execution_time', 0) for r in execution_results)
                    avg_execution_time = total_execution_time / len(execution_results) if execution_results else 0
                    
                    for allocation in load_balancing_result.allocations:
                        await self.load_balancer.complete_task(
                            allocation=allocation,
                            execution_time=avg_execution_time,
                            success=overall_success
                        )
                except Exception as lb_error:
                    logger.warning(f"æ›´æ–°è´Ÿè½½å‡è¡¡ç»Ÿè®¡å¤±è´¥: {lb_error}")
            
            return {
                'success': overall_success,
                'execution_results': execution_results,
                'success_rate': success_rate,
                'total_tasks': total_tasks,
                'successful_tasks': successful_tasks,
                'phase': 'phase_2',
                'agent_used': True,
                'intelligent_scheduling_applied': context.get('execution_plan') is not None,
                'load_balancing_applied': context.get('load_balancing_result') is not None
            }
            
        except Exception as e:
            logger.error(f"é˜¶æ®µ2æ‰§è¡Œå¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'phase': 'phase_2',
                'agent_used': True
            }


# ä¾¿æ·å‡½æ•°
def create_enhanced_pipeline(config: Dict[str, Any] = None) -> EnhancedTwoPhasePipeline:
    """åˆ›å»ºå¢å¼ºçš„ä¸¤é˜¶æ®µæµæ°´çº¿"""
    return EnhancedTwoPhasePipeline(config)


async def execute_enhanced_pipeline(task_id: int,
                                  user_id: str,
                                  template_id: str = None,
                                  data_source_id: str = None,
                                  config: Dict[str, Any] = None,
                                  execution_context: Dict[str, Any] = None) -> Dict[str, Any]:
    """æ‰§è¡Œå¢å¼ºæµæ°´çº¿çš„ä¾¿æ·å‡½æ•° - é›†æˆæ™ºèƒ½è°ƒåº¦å’Œè´Ÿè½½å‡è¡¡"""
    
    # é»˜è®¤å¯ç”¨æ™ºèƒ½è°ƒåº¦å’Œè´Ÿè½½å‡è¡¡
    default_config = {
        'enable_intelligent_scheduling': True,
        'enable_load_balancing': True,
        'enable_recovery_mode': True,
        'enable_partial_analysis': True,
        'max_retry_attempts': 3
    }
    
    if config:
        default_config.update(config)
    
    pipeline = create_enhanced_pipeline(default_config)
    
    db = SessionLocal()
    try:
        result = await pipeline.execute(
            task_id=task_id,
            user_id=user_id,
            template_id=template_id,
            data_source_id=data_source_id,
            db=db,
            execution_context=execution_context
        )
        return result
    finally:
        db.close()


async def get_pipeline_optimization_report(pipeline: EnhancedTwoPhasePipeline) -> Dict[str, Any]:
    """è·å–æµæ°´çº¿ä¼˜åŒ–æŠ¥å‘Š"""
    try:
        # è·å–è°ƒåº¦å™¨ç»Ÿè®¡
        scheduling_stats = await pipeline.intelligent_scheduler.get_scheduling_stats()
        
        # è·å–è´Ÿè½½å‡è¡¡ç»Ÿè®¡  
        load_balancing_stats = await pipeline.load_balancer.get_load_statistics()
        
        return {
            "pipeline_id": pipeline.pipeline_id,
            "scheduling_statistics": scheduling_stats,
            "load_balancing_statistics": load_balancing_stats,
            "optimization_features": {
                "intelligent_scheduling": pipeline.enable_intelligent_scheduling,
                "load_balancing": pipeline.enable_load_balancing,
                "recovery_mode": pipeline.enable_recovery_mode,
                "partial_analysis": pipeline.enable_partial_analysis,
                "incremental_update": pipeline.enable_incremental_update
            },
            "configuration": {
                "max_retry_attempts": pipeline.max_retry_attempts,
                "cache_preference_threshold": pipeline.cache_preference_threshold
            },
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"è·å–ä¼˜åŒ–æŠ¥å‘Šå¤±è´¥: {e}")
        return {
            "error": str(e),
            "pipeline_id": pipeline.pipeline_id if pipeline else None,
            "timestamp": datetime.now().isoformat()
        }


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    import asyncio
    
    async def test_enhanced_pipeline():
        """æµ‹è¯•å¢å¼ºæµæ°´çº¿ - åŒ…å«æ™ºèƒ½è°ƒåº¦å’Œè´Ÿè½½å‡è¡¡"""
        result = await execute_enhanced_pipeline(
            task_id=123,
            user_id="test_user",
            template_id="test_template",
            data_source_id="test_datasource",
            config={
                'enable_intelligent_scheduling': True,
                'enable_load_balancing': True,
                'enable_partial_analysis': True,
                'enable_recovery_mode': True,
                'max_retry_attempts': 2
            }
        )
        print(f"Enhanced Pipeline result: {result}")
        
        # æ‰“å°ä¼˜åŒ–ä¿¡æ¯
        if result.get('scheduling_info'):
            print(f"Scheduling applied: {result['scheduling_info']}")
        if result.get('load_balancing_info'):
            print(f"Load balancing applied: {result['load_balancing_info']}")
    
    async def test_optimization_report():
        """æµ‹è¯•ä¼˜åŒ–æŠ¥å‘Šç”Ÿæˆ"""
        pipeline = create_enhanced_pipeline({
            'enable_intelligent_scheduling': True,
            'enable_load_balancing': True
        })
        
        report = await get_pipeline_optimization_report(pipeline)
        print(f"Optimization report: {report}")
    
    # asyncio.run(test_enhanced_pipeline())
    # asyncio.run(test_optimization_report())