"""
Applicationå±‚ - ä»»åŠ¡åº”ç”¨æœåŠ¡ - DDDæ¶æ„v2.0

åŸºäºæ–°DDDæ¶æ„çš„ä»»åŠ¡åº”ç”¨æœåŠ¡ï¼Œé›†æˆTaskExecutionServiceå’ŒAgentsç³»ç»Ÿ
"""

import logging
from typing import List, Optional, Dict, Any
from datetime import datetime
from sqlalchemy.orm import Session
from celery.result import AsyncResult

from app.models.task import Task, TaskExecution, TaskStatus, ReportPeriod, ProcessingMode, AgentWorkflowType
from app.models.user import User
from app.models.data_source import DataSource  
from app.models.template import Template
from app.services.application.base_application_service import (
    TransactionalApplicationService, 
    ApplicationResult, 
    PaginationRequest, 
    PaginationResult
)
from app.services.application.tasks.task_execution_service import TaskExecutionService
# Use lazy loading for domain services to avoid circular imports
# Temporarily removed to fix circular imports
# from app.services.infrastructure.task_queue.tasks import validate_placeholders_task, scheduled_task_runner
from app.services.infrastructure.task_queue.celery_config import celery_app
from app.services.infrastructure.task_queue.tasks import execute_report_task
from app.core.exceptions import ValidationError, NotFoundError
from app.utils.time_context import TimeContextManager

logger = logging.getLogger(__name__)

class TaskApplicationService(TransactionalApplicationService):
    """ä»»åŠ¡åº”ç”¨æœåŠ¡ - DDDæ¶æ„v2.0ç‰ˆæœ¬ï¼Œé›†æˆæ–°çš„æ‰§è¡Œèƒ½åŠ›"""
    
    def __init__(self):
        super().__init__("TaskApplicationService")
        self.task_execution_service = TaskExecutionService()
        self.time_context_manager = TimeContextManager()
        
        # Lazy-loaded domain services to avoid circular imports
        self._task_execution_domain_service = None
        self._placeholder_analysis_domain_service = None
    
    @property
    def task_execution_domain_service(self):
        """Lazy load task execution domain service"""
        if self._task_execution_domain_service is None:
            from app.services.domain.tasks.services.task_execution_domain_service import TaskExecutionDomainService
            self._task_execution_domain_service = TaskExecutionDomainService()
        return self._task_execution_domain_service
    
    @property
    def placeholder_analysis_domain_service(self):
        """Lazy load placeholder analysis domain service"""
        if self._placeholder_analysis_domain_service is None:
            from app.services.domain.placeholder.services.placeholder_analysis_domain_service import PlaceholderAnalysisDomainService
            self._placeholder_analysis_domain_service = PlaceholderAnalysisDomainService()
        return self._placeholder_analysis_domain_service
    
    def create_task(
        self,
        db: Session,
        user_id: str,
        name: str,
        template_id: str,
        data_source_id: str,
        description: Optional[str] = None,
        schedule: Optional[str] = None,
        recipients: Optional[List[str]] = None,
        is_active: bool = True,
        processing_mode: ProcessingMode = ProcessingMode.INTELLIGENT,
        workflow_type: AgentWorkflowType = AgentWorkflowType.SIMPLE_REPORT,
        max_context_tokens: int = 32000,
        enable_compression: bool = True
    ) -> ApplicationResult[Task]:
        """
        åˆ›å»ºæ–°ä»»åŠ¡

        Args:
            db: æ•°æ®åº“ä¼šè¯
            user_id: ç”¨æˆ·ID
            name: ä»»åŠ¡åç§°
            template_id: æ¨¡æ¿ID
            data_source_id: æ•°æ®æºID
            description: ä»»åŠ¡æè¿°
            schedule: è°ƒåº¦è¡¨è¾¾å¼
            recipients: é€šçŸ¥é‚®ç®±åˆ—è¡¨
            is_active: æ˜¯å¦å¯ç”¨
            processing_mode: å¤„ç†æ¨¡å¼
            workflow_type: Agentå·¥ä½œæµç±»å‹
            max_context_tokens: æœ€å¤§ä¸Šä¸‹æ–‡ä»¤ç‰Œæ•°
            enable_compression: æ˜¯å¦å¯ç”¨å‹ç¼©

        Returns:
            ApplicationResult[Task]: åˆ›å»ºç»“æœ
        """
        # éªŒè¯å¿…éœ€å‚æ•°
        validation_result = self.validate_required_params(
            user_id=user_id,
            name=name,
            template_id=template_id,
            data_source_id=data_source_id
        )
        if not validation_result.success:
            return validation_result
        
        def _create_task_internal():
            # éªŒè¯ç”¨æˆ·å­˜åœ¨
            user = db.query(User).filter(User.id == user_id).first()
            if not user:
                return ApplicationResult.not_found_result(f"ç”¨æˆ· {user_id} ä¸å­˜åœ¨")
            
            # éªŒè¯æ¨¡æ¿å­˜åœ¨
            template = db.query(Template).filter(Template.id == template_id).first()
            if not template:
                return ApplicationResult.not_found_result(f"æ¨¡æ¿ {template_id} ä¸å­˜åœ¨")
                
            # éªŒè¯æ•°æ®æºå­˜åœ¨
            data_source = db.query(DataSource).filter(DataSource.id == data_source_id).first()
            if not data_source:
                return ApplicationResult.not_found_result(f"æ•°æ®æº {data_source_id} ä¸å­˜åœ¨")
            
            # åŸºäºè°ƒåº¦è¡¨è¾¾å¼æ¨æ–­æŠ¥å‘Šå‘¨æœŸ
            final_report_period = self._infer_report_period_from_cron(schedule)

            # éªŒè¯è°ƒåº¦è¡¨è¾¾å¼ï¼ˆå¦‚æœæä¾›ï¼‰
            if schedule:
                try:
                    from croniter import croniter
                    if not croniter.is_valid(schedule):
                        return ApplicationResult.validation_error_result(
                            f"æ— æ•ˆçš„Cronè¡¨è¾¾å¼: {schedule}",
                            ["è°ƒåº¦è¡¨è¾¾å¼æ ¼å¼ä¸æ­£ç¡®"]
                        )
                except ImportError:
                    self.logger.warning("croniteræ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡cronéªŒè¯")
            
            # åˆ›å»ºä»»åŠ¡ - ç¡®ä¿ UUID å­—æ®µä½¿ç”¨æ­£ç¡®çš„ç±»å‹
            from uuid import UUID as UUIDType

            # è½¬æ¢UUIDå¹¶è®°å½•è°ƒè¯•ä¿¡æ¯
            owner_uuid = UUIDType(user_id) if isinstance(user_id, str) else user_id
            template_uuid = UUIDType(template_id) if isinstance(template_id, str) else template_id
            data_source_uuid = UUIDType(data_source_id) if isinstance(data_source_id, str) else data_source_id

            self.logger.info(f"åˆ›å»ºä»»åŠ¡ï¼ŒUUIDè½¬æ¢: owner_id={owner_uuid}, template_id={template_uuid}, data_source_id={data_source_uuid}")

            task = Task(
                name=name,
                description=description,
                owner_id=owner_uuid,
                template_id=template_uuid,
                data_source_id=data_source_uuid,
                report_period=final_report_period,
                schedule=schedule,
                recipients=recipients or [],
                is_active=is_active,
                status=TaskStatus.PENDING,
                processing_mode=processing_mode,
                workflow_type=workflow_type,
                max_context_tokens=max_context_tokens,
                enable_compression=enable_compression
            )
            
            db.add(task)
            db.flush()  # å°†æŒ‚èµ·çš„æ›´æ”¹åˆ·æ–°åˆ°æ•°æ®åº“ï¼Œä½†ä¸æäº¤äº‹åŠ¡
            db.refresh(task)  # ç°åœ¨å¯ä»¥å®‰å…¨åœ°åˆ·æ–°å¯¹è±¡
            
            # å¼‚æ­¥éªŒè¯æ¨¡æ¿å ä½ç¬¦ï¼ˆå¯é€‰ï¼‰
            if is_active:
                try:
                    # å»¶è¿Ÿå¯¼å…¥ä»¥é¿å…å¾ªç¯å¯¼å…¥
                    from app.services.infrastructure.task_queue.tasks import validate_placeholders_task

                    validate_placeholders_task.delay(
                        template_id=template_id,
                        data_source_id=data_source_id,
                        user_id=user_id
                    )
                    self.logger.info(f"å ä½ç¬¦éªŒè¯ä»»åŠ¡å·²æ’é˜Ÿï¼Œä»»åŠ¡ID: {task.id}")
                except ImportError as e:
                    self.logger.warning(f"æ— æ³•å¯¼å…¥å ä½ç¬¦éªŒè¯ä»»åŠ¡ï¼Œè·³è¿‡å¼‚æ­¥éªŒè¯ï¼Œä»»åŠ¡ID: {task.id}, é”™è¯¯: {e}")
                except Exception as e:
                    self.logger.warning(f"æ’é˜Ÿå ä½ç¬¦éªŒè¯å¤±è´¥ï¼Œä»»åŠ¡ID: {task.id}, é”™è¯¯: {e}")
            
            return ApplicationResult.success_result(
                data=task,
                message=f"ä»»åŠ¡ '{name}' åˆ›å»ºæˆåŠŸ"
            )
        
        return self.execute_in_transaction(db, "create_task", _create_task_internal)
    
    async def analyze_task_with_domain_services(
        self,
        db: Session,
        task_id: int,
        user_id: str
    ) -> ApplicationResult[Dict[str, Any]]:
        """
        ä½¿ç”¨é¢†åŸŸæœåŠ¡åˆ†æä»»åŠ¡ - å±•ç¤ºæ­£ç¡®çš„DDDæ¶æ„ä½¿ç”¨agentsçš„æ–¹å¼
        
        ä¸šåŠ¡é€»è¾‘æµç¨‹ï¼š
        1. åº”ç”¨æœåŠ¡ç¼–æ’ä¸šåŠ¡æµç¨‹
        2. é¢†åŸŸæœåŠ¡æ‰§è¡Œçº¯ä¸šåŠ¡é€»è¾‘
        3. åŸºç¡€è®¾æ–½å±‚agentsæ‰§è¡ŒæŠ€æœ¯å®ç°
        
        Args:
            db: æ•°æ®åº“ä¼šè¯
            task_id: ä»»åŠ¡ID
            user_id: ç”¨æˆ·ID
            
        Returns:
            ApplicationResult[Dict]: åˆ†æç»“æœ
        """
        async def _analyze_task_internal():
            # 1. è·å–ä»»åŠ¡ä¿¡æ¯
            task = db.query(Task).filter(Task.id == task_id, Task.owner_id == user_id).first()
            if not task:
                return ApplicationResult.not_found_result(f"ä»»åŠ¡ {task_id} ä¸å­˜åœ¨æˆ–æ— æƒé™")
            
            # 2. æ„å»ºä»»åŠ¡å®šä¹‰ - åº”ç”¨å±‚ç»„è£…æ•°æ®
            task_definition = {
                "task_id": task.id,
                "name": task.name,
                "description": task.description,
                "template_id": str(task.template_id),
                "data_source_ids": [str(task.data_source_id)],
                "processing_mode": task.processing_mode.value if task.processing_mode else "simple",
                "workflow_type": task.workflow_type.value if task.workflow_type else "simple_report",
                "template_info": self._get_template_info(db, task.template_id),
                "data_source_info": self._get_data_source_info(db, task.data_source_id)
            }
            
            # 3. ä½¿ç”¨é¢†åŸŸæœåŠ¡åˆ†æä»»åŠ¡æ‰§è¡Œéœ€æ±‚ - çº¯ä¸šåŠ¡é€»è¾‘
            execution_context = {
                "user_id": user_id,
                "current_time": datetime.now(),
                "environment": "development"  # è¿™é‡Œå¯ä»¥ä»é…ç½®è·å–
            }
            
            execution_requirements = self.task_execution_domain_service.analyze_task_execution_requirements(
                task_definition, execution_context
            )
            
            # 4. éªŒè¯æ‰§è¡Œå¯è¡Œæ€§ - çº¯ä¸šåŠ¡é€»è¾‘
            available_resources = {
                "cpu_cores": 4,
                "memory_mb": 8192,
                "storage_mb": 10240,
                "network_bandwidth": "high"
            }
            
            feasibility_check = self.task_execution_domain_service.validate_task_execution_feasibility(
                task_definition, available_resources, execution_context
            )
            
            # 5. åˆ†æå ä½ç¬¦éœ€æ±‚ - çº¯ä¸šåŠ¡é€»è¾‘
            placeholder_analysis = None
            if task_definition["processing_mode"] == "intelligent":
                business_context = {
                    "template_type": task_definition.get("template_info", {}).get("type", "general"),
                    "data_scope": task_definition.get("data_source_info", {}).get("scope", "full"),
                    "execution_urgency": "normal"
                }
                
                placeholder_analysis = await self.placeholder_analysis_domain_service.analyze_placeholder_business_requirements(
                    f"åˆ†æä»»åŠ¡ {task.name} çš„å ä½ç¬¦éœ€æ±‚",
                    business_context
                )
            
            # 6. æ„å»ºåˆ†æç»“æœ
            analysis_result = {
                "task_info": {
                    "id": task.id,
                    "name": task.name,
                    "status": task.status.value if task.status else "unknown"
                },
                "execution_requirements": execution_requirements,
                "feasibility_check": feasibility_check,
                "placeholder_analysis": placeholder_analysis,
                "recommendations": self._generate_task_recommendations(
                    execution_requirements, feasibility_check, placeholder_analysis
                ),
                "estimated_agents_needed": self._estimate_agents_needed(task_definition, execution_requirements)
            }
            
            return ApplicationResult.success_result(
                data=analysis_result,
                message=f"ä»»åŠ¡ {task.name} åˆ†æå®Œæˆ"
            )
        
        return await self.handle_domain_exceptions_async("analyze_task_with_domain_services", _analyze_task_internal)
    
    def execute_task_through_agents(
        self,
        db: Session,
        task_id: int,
        user_id: str,
        execution_plan: Optional[Dict[str, Any]] = None
    ) -> ApplicationResult[Dict[str, Any]]:
        """
        é€šè¿‡agentsæ‰§è¡Œä»»åŠ¡ - å±•ç¤ºæ­£ç¡®çš„åŸºç¡€è®¾æ–½å±‚agentsä½¿ç”¨æ–¹å¼
        
        æ¶æ„å±‚æ¬¡ï¼š
        1. åº”ç”¨æœåŠ¡(æ­¤æ–¹æ³•) - ç¼–æ’ä¸šåŠ¡æµç¨‹
        2. é¢†åŸŸæœåŠ¡ - æ‰§è¡Œä¸šåŠ¡é€»è¾‘å’Œè§„åˆ™
        3. åŸºç¡€è®¾æ–½å±‚agents - æ‰§è¡ŒæŠ€æœ¯å®ç°
        
        Args:
            db: æ•°æ®åº“ä¼šè¯
            task_id: ä»»åŠ¡ID
            user_id: ç”¨æˆ·ID
            execution_plan: æ‰§è¡Œè®¡åˆ’(å¯é€‰)
            
        Returns:
            ApplicationResult[Dict]: æ‰§è¡Œç»“æœ
        """
        def _execute_task_internal():
            # 1. è·å–ä»»åŠ¡å¹¶éªŒè¯æƒé™
            task = db.query(Task).filter(Task.id == task_id, Task.owner_id == user_id).first()
            if not task:
                return ApplicationResult.not_found_result(f"ä»»åŠ¡ {task_id} ä¸å­˜åœ¨æˆ–æ— æƒé™")
            
            # 2. ä½¿ç”¨é¢†åŸŸæœåŠ¡åˆ›å»ºæ‰§è¡Œè®¡åˆ’
            if not execution_plan:
                task_definition = self._build_task_definition(db, task)
                execution_requirements = self.task_execution_domain_service.analyze_task_execution_requirements(
                    task_definition, {"user_id": user_id}
                )
                
                execution_plan = self.task_execution_domain_service.create_task_execution_plan(
                    task_definition, execution_requirements, {"timeout": 300}
                )
            
            # 3. è°ƒç”¨åŸºç¡€è®¾æ–½å±‚agentsæ‰§è¡ŒæŠ€æœ¯å®ç°
            # æ³¨æ„ï¼šè¿™é‡Œåº”è¯¥é€šè¿‡åŸºç¡€è®¾æ–½å±‚çš„agent provideræ¥è°ƒç”¨agents
            try:
                from app.services.infrastructure.agents.agent_provider import get_agent_provider
                
                agent_provider = get_agent_provider()
                
                # æ‰§è¡Œå„ä¸ªæ­¥éª¤ï¼Œæ¯ä¸ªæ­¥éª¤ä½¿ç”¨å¯¹åº”çš„agent
                execution_results = []
                for step in execution_plan.get("execution_steps", []):
                    step_result = self._execute_step_with_agents(
                        agent_provider, step, task, execution_plan
                    )
                    execution_results.append(step_result)
                    
                    # å¦‚æœæ­¥éª¤å¤±è´¥ï¼Œæ ¹æ®å›æ»šç­–ç•¥å¤„ç†
                    if not step_result.get("success", False):
                        rollback_result = self._handle_step_failure(
                            agent_provider, step, execution_plan, execution_results
                        )
                        if rollback_result.get("should_abort", False):
                            break
                
                # 4. æ±‡æ€»æ‰§è¡Œç»“æœ
                overall_success = all(result.get("success", False) for result in execution_results)
                
                final_result = {
                    "task_id": task_id,
                    "execution_plan_id": execution_plan.get("plan_id"),
                    "overall_success": overall_success,
                    "step_results": execution_results,
                    "execution_summary": {
                        "total_steps": len(execution_results),
                        "successful_steps": sum(1 for r in execution_results if r.get("success", False)),
                        "failed_steps": sum(1 for r in execution_results if not r.get("success", False))
                    },
                    "agents_used": list(set(
                        agent for result in execution_results 
                        for agent in result.get("agents_used", [])
                    ))
                }
                
                return ApplicationResult.success_result(
                    data=final_result,
                    message=f"ä»»åŠ¡æ‰§è¡Œå®Œæˆï¼ŒæˆåŠŸç‡: {final_result['execution_summary']['successful_steps']}/{final_result['execution_summary']['total_steps']}"
                )
                
            except ImportError:
                # å¦‚æœagentsä¸å¯ç”¨ï¼Œé™çº§åˆ°ä¼ ç»Ÿæ‰§è¡Œæ–¹å¼
                self.logger.warning("åŸºç¡€è®¾æ–½å±‚agentsä¸å¯ç”¨ï¼Œä½¿ç”¨ä¼ ç»Ÿæ‰§è¡Œæ–¹å¼")
                return self._execute_task_traditional_way(db, task, user_id)
        
        return self.handle_domain_exceptions("execute_task_through_agents", _execute_task_internal)
    
    def execute_task_immediately(
        self, 
        db: Session, 
        task_id: int,
        user_id: str,
        execution_context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ç«‹å³æ‰§è¡Œä»»åŠ¡
        
        Args:
            db: æ•°æ®åº“ä¼šè¯
            task_id: ä»»åŠ¡ID
            user_id: æ‰§è¡Œç”¨æˆ·ID
            execution_context: æ‰§è¡Œä¸Šä¸‹æ–‡
            
        Returns:
            Dict: æ‰§è¡Œç»“æœä¿¡æ¯
        """
        try:
            # éªŒè¯ä»»åŠ¡å­˜åœ¨å’Œæƒé™
            task = db.query(Task).filter(Task.id == task_id, Task.owner_id == user_id).first()
            if not task:
                raise NotFoundError(f"Task {task_id} not found or access denied")
            
            if not task.is_active:
                raise ValidationError(f"Task {task_id} is not active")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ­£åœ¨è¿›è¡Œçš„æ‰§è¡Œ
            ongoing_execution = db.query(TaskExecution).filter(
                TaskExecution.task_id == task_id,
                TaskExecution.execution_status.in_([TaskStatus.PENDING, TaskStatus.PROCESSING])
            ).first()
            
            if ongoing_execution:
                return {
                    "status": "already_running",
                    "message": f"Task {task_id} is already being executed",
                    "execution_id": str(ongoing_execution.execution_id)
                }
            
            # æ„å»ºæ‰§è¡Œä¸Šä¸‹æ–‡
            context = execution_context or {}
            context.update({
                "trigger": "manual",
                "triggered_by": user_id,
                "triggered_at": datetime.utcnow().isoformat()
            })
            
            # æäº¤Celeryä»»åŠ¡
            celery_result = execute_report_task.delay(task_id, context)
            
            logger.info(f"Task {task_id} execution queued with Celery task ID: {celery_result.id}")
            
            return {
                "status": "queued",
                "message": f"Task {task_id} has been queued for execution",
                "celery_task_id": celery_result.id,
                "task_id": task_id
            }
            
        except Exception as e:
            logger.error(f"Failed to execute task {task_id}: {str(e)}")
            raise
    
    def get_task_status(
        self, 
        db: Session, 
        task_id: int, 
        user_id: str
    ) -> Dict[str, Any]:
        """
        è·å–ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€
        
        Args:
            db: æ•°æ®åº“ä¼šè¯
            task_id: ä»»åŠ¡ID
            user_id: ç”¨æˆ·ID
            
        Returns:
            Dict: ä»»åŠ¡çŠ¶æ€ä¿¡æ¯
        """
        try:
            # éªŒè¯ä»»åŠ¡å­˜åœ¨å’Œæƒé™
            task = db.query(Task).filter(Task.id == task_id, Task.owner_id == user_id).first()
            if not task:
                raise NotFoundError(f"Task {task_id} not found or access denied")
            
            # è·å–æœ€æ–°çš„æ‰§è¡Œè®°å½•
            latest_execution = db.query(TaskExecution).filter(
                TaskExecution.task_id == task_id
            ).order_by(TaskExecution.created_at.desc()).first()
            
            if not latest_execution:
                return {
                    "task_id": task_id,
                    "status": task.status.value,
                    "message": "No executions found",
                    "progress": 0
                }
            
            # è·å–Celeryä»»åŠ¡çŠ¶æ€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            celery_status = None
            celery_info = {}
            if latest_execution.celery_task_id:
                try:
                    celery_result = AsyncResult(latest_execution.celery_task_id, app=celery_app)
                    celery_status = celery_result.status
                    if celery_result.info:
                        celery_info = celery_result.info
                except Exception as e:
                    logger.warning(f"Failed to get Celery status for task {latest_execution.celery_task_id}: {e}")
            
            # ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½å¯ä»¥åºåˆ—åŒ–
            def safe_serialize(value):
                """å®‰å…¨åºåˆ—åŒ–ï¼Œé¿å…TypeError"""
                if value is None:
                    return None
                if isinstance(value, (str, int, float, bool)):
                    return value
                if isinstance(value, dict):
                    return {k: safe_serialize(v) for k, v in value.items()}
                if isinstance(value, list):
                    return [safe_serialize(item) for item in value]
                try:
                    return str(value)
                except Exception:
                    return None

            return {
                "task_id": task_id,
                "execution_id": str(latest_execution.execution_id) if latest_execution.execution_id else None,
                "status": latest_execution.execution_status.value if latest_execution.execution_status else "unknown",
                "progress": latest_execution.progress_percentage or 0,
                "current_step": latest_execution.current_step or "",
                "started_at": latest_execution.started_at.isoformat() if latest_execution.started_at else None,
                "completed_at": latest_execution.completed_at.isoformat() if latest_execution.completed_at else None,
                "duration": latest_execution.total_duration or 0,
                "error_details": safe_serialize(latest_execution.error_details),
                "celery_status": celery_status,
                "celery_info": safe_serialize(celery_info),
                "execution_result": safe_serialize(latest_execution.execution_result)
            }
            
        except Exception as e:
            logger.error(f"Failed to get task status for task {task_id}: {str(e)}")
            raise
    
    def get_task_executions(
        self,
        db: Session,
        task_id: int,
        user_id: str,
        limit: int = 50
    ) -> List[Dict[str, Any]]:
        """
        è·å–ä»»åŠ¡æ‰§è¡Œå†å²
        
        Args:
            db: æ•°æ®åº“ä¼šè¯
            task_id: ä»»åŠ¡ID
            user_id: ç”¨æˆ·ID
            limit: è¿”å›è®°å½•æ•°é‡é™åˆ¶
            
        Returns:
            List[Dict]: æ‰§è¡Œå†å²åˆ—è¡¨
        """
        try:
            # éªŒè¯ä»»åŠ¡å­˜åœ¨å’Œæƒé™
            task = db.query(Task).filter(Task.id == task_id, Task.owner_id == user_id).first()
            if not task:
                raise NotFoundError(f"Task {task_id} not found or access denied")
            
            # è·å–æ‰§è¡Œå†å²
            executions = db.query(TaskExecution).filter(
                TaskExecution.task_id == task_id
            ).order_by(TaskExecution.created_at.desc()).limit(limit).all()
            
            return [
                {
                    "execution_id": str(execution.execution_id),
                    "status": execution.execution_status.value,
                    "progress": execution.progress_percentage,
                    "started_at": execution.started_at.isoformat() if execution.started_at else None,
                    "completed_at": execution.completed_at.isoformat() if execution.completed_at else None,
                    "duration": execution.total_duration,
                    "error_details": execution.error_details,
                    "current_step": execution.current_step,
                    "workflow_type": execution.workflow_type.value if execution.workflow_type else None,
                    "created_at": execution.created_at.isoformat()
                }
                for execution in executions
            ]
            
        except Exception as e:
            logger.error(f"Failed to get task executions for task {task_id}: {str(e)}")
            raise
    
    def update_task(
        self,
        db: Session,
        task_id: int,
        user_id: str,
        **update_data
    ) -> Task:
        """
        æ›´æ–°ä»»åŠ¡ï¼ˆå¸¦å®Œæ•´éªŒè¯ï¼‰

        Args:
            db: æ•°æ®åº“ä¼šè¯
            task_id: ä»»åŠ¡ID
            user_id: ç”¨æˆ·ID
            **update_data: æ›´æ–°æ•°æ®

        Returns:
            Task: æ›´æ–°åçš„ä»»åŠ¡å¯¹è±¡
        """
        try:
            # 1. éªŒè¯ä»»åŠ¡å­˜åœ¨å’Œæƒé™
            task = db.query(Task).filter(Task.id == task_id, Task.owner_id == user_id).first()
            if not task:
                raise NotFoundError(f"Task {task_id} not found or access denied")

            # 2. å­—æ®µç™½åå•
            allowed_fields = {
                'name', 'description', 'schedule', 'report_period',
                'recipients', 'is_active', 'processing_mode', 'workflow_type',
                'max_context_tokens', 'enable_compression'
            }

            # 3. éªŒè¯å’Œåº”ç”¨æ›´æ–°
            validated_updates = {}
            scheduler_needs_update = False

            for field, value in update_data.items():
                if field not in allowed_fields:
                    logger.warning(f"âŒ å­—æ®µ {field} ä¸å…è®¸æ›´æ–°")
                    continue

                # === å­—æ®µç‰¹å®šéªŒè¯ ===

                # 3.1 éªŒè¯ä»»åŠ¡åç§°
                if field == 'name':
                    if not value or not isinstance(value, str):
                        raise ValidationError("ä»»åŠ¡åç§°ä¸èƒ½ä¸ºç©º")
                    if len(value) > 200:
                        raise ValidationError("ä»»åŠ¡åç§°ä¸èƒ½è¶…è¿‡200ä¸ªå­—ç¬¦")
                    # æ£€æŸ¥åŒåä»»åŠ¡
                    existing = db.query(Task).filter(
                        Task.owner_id == user_id,
                        Task.name == value,
                        Task.id != task_id
                    ).first()
                    if existing:
                        raise ValidationError(f"ä»»åŠ¡åç§° '{value}' å·²å­˜åœ¨")
                    validated_updates[field] = value

                # 3.2 éªŒè¯ Cron è¡¨è¾¾å¼
                elif field == 'schedule':
                    if value is not None:
                        try:
                            from croniter import croniter
                            if not croniter.is_valid(value):
                                raise ValidationError(f"æ— æ•ˆçš„Cronè¡¨è¾¾å¼: {value}")
                            # éªŒè¯è¡¨è¾¾å¼æ ¼å¼
                            if len(value.split()) != 5:
                                raise ValidationError("Cronè¡¨è¾¾å¼å¿…é¡»åŒ…å«5ä¸ªå­—æ®µ (åˆ† æ—¶ æ—¥ æœˆ å‘¨)")
                        except ImportError:
                            logger.warning("croniter ä¸å¯ç”¨ï¼Œè·³è¿‡ cron éªŒè¯")

                        # æ¨æ–­æŠ¥å‘Šå‘¨æœŸ
                        inferred_period = self._infer_report_period_from_cron(value)
                        validated_updates['report_period'] = inferred_period
                        scheduler_needs_update = True
                        logger.info(f"ğŸ”„ æ ¹æ® cron è¡¨è¾¾å¼æ¨æ–­æŠ¥å‘Šå‘¨æœŸ: {inferred_period.value}")

                    validated_updates[field] = value

                # 3.3 éªŒè¯æ”¶ä»¶äººåˆ—è¡¨
                elif field == 'recipients':
                    if value is not None:
                        if not isinstance(value, list):
                            raise ValidationError("recipients å¿…é¡»æ˜¯åˆ—è¡¨")
                        # éªŒè¯é‚®ç®±æ ¼å¼
                        import re
                        email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
                        for email in value:
                            if not re.match(email_pattern, email):
                                raise ValidationError(f"æ— æ•ˆçš„é‚®ç®±åœ°å€: {email}")
                    validated_updates[field] = value

                # 3.4 éªŒè¯å¤„ç†æ¨¡å¼
                elif field == 'processing_mode':
                    if value not in [mode.value for mode in ProcessingMode]:
                        raise ValidationError(f"æ— æ•ˆçš„å¤„ç†æ¨¡å¼: {value}ï¼Œå¯é€‰å€¼: {[m.value for m in ProcessingMode]}")
                    # è½¬æ¢ä¸ºæšä¸¾
                    validated_updates[field] = ProcessingMode(value)

                # 3.5 éªŒè¯å·¥ä½œæµç±»å‹
                elif field == 'workflow_type':
                    if value not in [wf.value for wf in AgentWorkflowType]:
                        raise ValidationError(f"æ— æ•ˆçš„å·¥ä½œæµç±»å‹: {value}ï¼Œå¯é€‰å€¼: {[w.value for w in AgentWorkflowType]}")
                    validated_updates[field] = AgentWorkflowType(value)

                # 3.6 éªŒè¯æŠ¥å‘Šå‘¨æœŸ
                elif field == 'report_period':
                    if value not in [period.value for period in ReportPeriod]:
                        raise ValidationError(f"æ— æ•ˆçš„æŠ¥å‘Šå‘¨æœŸ: {value}ï¼Œå¯é€‰å€¼: {[p.value for p in ReportPeriod]}")
                    validated_updates[field] = ReportPeriod(value)

                # 3.7 éªŒè¯ä¸Šä¸‹æ–‡ä»¤ç‰Œæ•°
                elif field == 'max_context_tokens':
                    if not isinstance(value, int) or value < 1000 or value > 128000:
                        raise ValidationError("max_context_tokens å¿…é¡»åœ¨ 1000-128000 ä¹‹é—´")
                    validated_updates[field] = value

                # 3.8 éªŒè¯å¸ƒå°”å€¼å­—æ®µ
                elif field in ('is_active', 'enable_compression'):
                    if not isinstance(value, bool):
                        raise ValidationError(f"{field} å¿…é¡»æ˜¯å¸ƒå°”å€¼")
                    validated_updates[field] = value

                    # is_active å˜æ›´éœ€è¦æ›´æ–°è°ƒåº¦å™¨
                    if field == 'is_active':
                        scheduler_needs_update = True

                # å…¶ä»–å­—æ®µç›´æ¥åº”ç”¨
                else:
                    validated_updates[field] = value

            # 4. åº”ç”¨éªŒè¯åçš„æ›´æ–°
            for field, value in validated_updates.items():
                setattr(task, field, value)

            task.updated_at = datetime.utcnow()
            db.commit()
            db.refresh(task)

            # 5. åŒæ­¥è°ƒåº¦å™¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if scheduler_needs_update:
                import asyncio
                try:
                    loop = asyncio.get_event_loop()
                    if loop.is_running():
                        asyncio.create_task(self._sync_task_to_scheduler(task))
                    else:
                        loop.run_until_complete(self._sync_task_to_scheduler(task))
                except RuntimeError:
                    # å¦‚æœæ²¡æœ‰äº‹ä»¶å¾ªç¯ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„
                    asyncio.run(self._sync_task_to_scheduler(task))

            logger.info(f"âœ… ä»»åŠ¡ {task_id} æ›´æ–°æˆåŠŸï¼Œæ›´æ–°äº† {len(validated_updates)} ä¸ªå­—æ®µ")
            return task

        except (ValidationError, NotFoundError):
            raise
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°ä»»åŠ¡ {task_id} å¤±è´¥: {str(e)}")
            db.rollback()
            raise

    async def _sync_task_to_scheduler(self, task: Task):
        """åŒæ­¥ä»»åŠ¡åˆ°è°ƒåº¦å™¨"""
        try:
            from app.core.unified_scheduler import get_scheduler
            scheduler = await get_scheduler()

            if task.is_active and task.schedule:
                # æ·»åŠ æˆ–æ›´æ–°è°ƒåº¦
                await scheduler.add_or_update_task(task.id, task.schedule)
                logger.info(f"âœ… ä»»åŠ¡ {task.id} å·²åŒæ­¥åˆ°è°ƒåº¦å™¨")
            elif not task.is_active:
                # ä»è°ƒåº¦å™¨ç§»é™¤
                await scheduler.remove_task(task.id)
                logger.info(f"âœ… ä»»åŠ¡ {task.id} å·²ä»è°ƒåº¦å™¨ç§»é™¤")
        except Exception as e:
            logger.error(f"âš ï¸ åŒæ­¥ä»»åŠ¡ {task.id} åˆ°è°ƒåº¦å™¨å¤±è´¥: {e}")
    
    def delete_task(
        self,
        db: Session,
        task_id: int,
        user_id: str
    ) -> bool:
        """
        åˆ é™¤ä»»åŠ¡
        
        Args:
            db: æ•°æ®åº“ä¼šè¯
            task_id: ä»»åŠ¡ID
            user_id: ç”¨æˆ·ID
            
        Returns:
            bool: æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        try:
            # éªŒè¯ä»»åŠ¡å­˜åœ¨å’Œæƒé™
            task = db.query(Task).filter(Task.id == task_id, Task.owner_id == user_id).first()
            if not task:
                raise NotFoundError(f"Task {task_id} not found or access denied")
            
            # å–æ¶ˆæ­£åœ¨è¿›è¡Œçš„æ‰§è¡Œ
            ongoing_executions = db.query(TaskExecution).filter(
                TaskExecution.task_id == task_id,
                TaskExecution.execution_status.in_([TaskStatus.PENDING, TaskStatus.PROCESSING])
            ).all()
            
            for execution in ongoing_executions:
                if execution.celery_task_id:
                    try:
                        celery_app.control.revoke(execution.celery_task_id, terminate=True)
                        logger.info(f"Revoked Celery task {execution.celery_task_id}")
                    except Exception as e:
                        logger.warning(f"Failed to revoke Celery task {execution.celery_task_id}: {e}")
                
                execution.execution_status = TaskStatus.CANCELLED
                execution.completed_at = datetime.utcnow()
            
            # åˆ é™¤ä»»åŠ¡ï¼ˆçº§è”åˆ é™¤æ‰§è¡Œè®°å½•ï¼‰
            db.delete(task)
            db.commit()
            
            logger.info(f"Task {task_id} deleted successfully")
            return True
            
        except Exception as e:
            logger.error(f"Failed to delete task {task_id}: {str(e)}")
            db.rollback()
            raise
    
    def schedule_task(
        self,
        db: Session,
        task_id: int,
        schedule: str,
        user_id: str
    ) -> Dict[str, Any]:
        """
        è®¾ç½®ä»»åŠ¡è°ƒåº¦
        
        Args:
            db: æ•°æ®åº“ä¼šè¯
            task_id: ä»»åŠ¡ID
            schedule: Cronè¡¨è¾¾å¼
            user_id: ç”¨æˆ·ID
            
        Returns:
            Dict: è°ƒåº¦ç»“æœ
        """
        try:
            # éªŒè¯ä»»åŠ¡å­˜åœ¨å’Œæƒé™
            task = db.query(Task).filter(Task.id == task_id, Task.owner_id == user_id).first()
            if not task:
                raise NotFoundError(f"Task {task_id} not found or access denied")
            
            # éªŒè¯Cronè¡¨è¾¾å¼
            try:
                from croniter import croniter
                if not croniter.is_valid(schedule):
                    raise ValidationError(f"Invalid cron expression: {schedule}")
            except ImportError:
                logger.warning("croniter not available, skipping cron validation")
            
            # æ›´æ–°è°ƒåº¦
            task.schedule = schedule
            task.updated_at = datetime.utcnow()
            db.commit()
            
            logger.info(f"Task {task_id} schedule updated to: {schedule}")
            
            return {
                "task_id": task_id,
                "schedule": schedule,
                "status": "scheduled",
                "message": "Task schedule updated successfully"
            }
            
        except Exception as e:
            logger.error(f"Failed to schedule task {task_id}: {str(e)}")
            db.rollback()
            raise
    
    def validate_task_configuration(
        self,
        db: Session,
        task_id: int,
        user_id: str
    ) -> Dict[str, Any]:
        """
        éªŒè¯ä»»åŠ¡é…ç½®ï¼ˆåŒ…æ‹¬å ä½ç¬¦ï¼‰
        
        Args:
            db: æ•°æ®åº“ä¼šè¯
            task_id: ä»»åŠ¡ID
            user_id: ç”¨æˆ·ID
            
        Returns:
            Dict: éªŒè¯ç»“æœ
        """
        try:
            # éªŒè¯ä»»åŠ¡å­˜åœ¨å’Œæƒé™
            task = db.query(Task).filter(Task.id == task_id, Task.owner_id == user_id).first()
            if not task:
                raise NotFoundError(f"Task {task_id} not found or access denied")
            
            # å¼‚æ­¥éªŒè¯å ä½ç¬¦
            try:
                from app.services.infrastructure.task_queue.tasks import validate_placeholders_task
                validation_result = validate_placeholders_task.delay(
                    template_id=str(task.template_id),
                    data_source_id=str(task.data_source_id),
                    user_id=user_id
                )
            except ImportError as e:
                logger.warning(f"Cannot import validate_placeholders_task: {e}")
                return {
                    "task_id": task_id,
                    "status": "validation_unavailable",
                    "message": "Validation service is temporarily unavailable"
                }
            
            logger.info(f"Validation task queued for task {task_id}: {validation_result.id}")
            
            return {
                "task_id": task_id,
                "validation_task_id": validation_result.id,
                "status": "validation_queued",
                "message": "Task validation has been queued"
            }
            
        except Exception as e:
            logger.error(f"Failed to validate task {task_id}: {str(e)}")
            raise

    async def execute_task_with_claude_code(
        self,
        db: Session,
        task_id: int,
        user_id: str,
        execution_context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨æ–°çš„Claude Codeæ¶æ„æ‰§è¡Œä»»åŠ¡
        
        Args:
            db: æ•°æ®åº“ä¼šè¯
            task_id: ä»»åŠ¡ID
            user_id: ç”¨æˆ·ID
            execution_context: æ‰§è¡Œä¸Šä¸‹æ–‡
            
        Returns:
            Dict: æ‰§è¡Œç»“æœ
        """
        try:
            # éªŒè¯ä»»åŠ¡å­˜åœ¨å’Œæƒé™
            task = db.query(Task).filter(Task.id == task_id, Task.owner_id == user_id).first()
            if not task:
                raise NotFoundError(f"Task {task_id} not found or access denied")
            
            # è·å–ä»»åŠ¡ç›¸å…³ä¿¡æ¯
            template = db.query(Template).filter(Template.id == task.template_id).first()
            data_source = db.query(DataSource).filter(DataSource.id == task.data_source_id).first()
            
            if not template or not data_source:
                raise NotFoundError("Template or DataSource not found")
            
            # ä½¿ç”¨æ–°çš„agentsç³»ç»Ÿ
            from app.api.utils.agent_context_helpers import create_task_execution_context
            
            # å‡†å¤‡ä»»åŠ¡æ•°æ®
            task_data = {
                "task_id": task_id,
                "task_name": task.name,
                "template_id": str(task.template_id),
                "data_source_id": str(task.data_source_id),
                "report_period": task.report_period.value,
                "processing_mode": task.processing_mode.value,
                "workflow_type": task.workflow_type.value,
                "template_info": {
                    "name": template.name if hasattr(template, 'name') else 'Unknown Template',
                    "content": template.content if hasattr(template, 'content') else "",
                    "type": getattr(template, 'template_type', 'report')
                },
                "data_source_config": {
                    "name": data_source.name,
                    "source_type": str(data_source.source_type),
                    "database": getattr(data_source, 'doris_database', 'unknown')
                }
            }
            
            execution_options = {
                "processing_mode": task.processing_mode.value,
                "max_tokens": getattr(task, 'max_context_tokens', 32000),
                "enable_compression": getattr(task, 'enable_compression', True)
            }
            
            # åˆ›å»ºä»»åŠ¡æ‰§è¡Œä¸Šä¸‹æ–‡
            context = create_task_execution_context(
                task_name=task.name,
                task_description=f"æ‰§è¡Œä»»åŠ¡: {task.name} - {task.workflow_type.value}æŠ¥å‘Šç”Ÿæˆ",
                task_data=task_data,
                execution_options=execution_options
            )
            
            # æ‰§è¡Œagentsä»»åŠ¡
            try:
                from app.services.infrastructure.agents import execute_agent_task
                agent_result = await execute_agent_task(
                    task_name="task_execution",
                    task_description=f"æ‰§è¡Œä»»åŠ¡: {task.name} - {task.workflow_type.value}æŠ¥å‘Šç”Ÿæˆ",
                    context_data=context,
                    additional_data={
                        "task_id": task_id,
                        "task_name": task.name,
                        "template_id": str(task.template_id),
                        "data_source_id": str(task.data_source_id),
                        "data_source_config": {
                            "name": data_source.name,
                            "source_type": str(data_source.source_type)
                        },
                        "report_period": task.report_period.value,
                        "processing_mode": task.processing_mode.value,
                        "workflow_type": task.workflow_type.value,
                        "execution_context": execution_context or {}
                    }
                )
            except ImportError as e:
                logger.warning(f"Cannot import execute_agent_task: {e}")
                agent_result = {
                    "status": "fallback_execution",
                    "message": "Agents system unavailable, using fallback execution",
                    "success": True
                }
            
            logger.info(f"Agents system task execution completed for task {task_id}")
            
            return {
                "task_id": task_id,
                "execution_id": f"task_{task_id}_{datetime.now().timestamp()}",
                "status": "completed",
                "results": agent_result,
                "architecture": "agents_v2"
            }
            
        except Exception as e:
            logger.error(f"Claude Code task execution failed for task {task_id}: {str(e)}")
            raise
    
    async def generate_sql_with_claude_code(
        self,
        user_id: str,
        query_description: str,
        table_info: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨Claude Codeæ¶æ„ç”ŸæˆSQL
        
        Args:
            user_id: ç”¨æˆ·ID
            query_description: æŸ¥è¯¢æè¿°
            table_info: è¡¨ä¿¡æ¯
            
        Returns:
            Dict: ç”Ÿæˆç»“æœ
        """
        try:
            # ä½¿ç”¨æ–°çš„agentsç³»ç»Ÿ
            from app.api.utils.agent_context_helpers import create_sql_generation_context
            
            # å‡†å¤‡è¡¨ç»“æ„ä¿¡æ¯
            table_schemas = []
            if table_info:
                for table_name, table_data in table_info.items():
                    schema_dict = {
                        "table_name": table_name,
                        "columns": table_data.get("columns", []),
                        "relationships": table_data.get("relationships", []),
                        "indexes": table_data.get("indexes", []),
                        "constraints": table_data.get("constraints", []),
                        "statistics": table_data.get("statistics", {}),
                        "sample_data": table_data.get("sample_data", [])
                    }
                    table_schemas.append(schema_dict)
            
            # åˆ›å»ºSQLç”Ÿæˆä¸Šä¸‹æ–‡
            context = create_sql_generation_context(
                query_description=query_description,
                table_schemas=table_schemas,
                query_parameters={"user_id": user_id}
            )
            
            # æ‰§è¡ŒSQLç”Ÿæˆä»»åŠ¡
            try:
                from app.services.infrastructure.agents import execute_agent_task
                agent_result = await execute_agent_task(
                    task_name="sql_generation",
                    task_description=f"ç”ŸæˆSQLæŸ¥è¯¢: {query_description}",
                    context_data=context,
                    additional_data={
                        "table_info": table_info,
                        "user_id": user_id
                    }
                )
            except ImportError as e:
                logger.warning(f"Cannot import execute_agent_task for SQL generation: {e}")
                agent_result = {
                    "status": "fallback_sql_generation",
                    "message": "Agents system unavailable for SQL generation",
                    "success": False
                }
            
            return {
                "query_description": query_description,
                "results": agent_result,
                "architecture": "agents_v2"
            }
            
        except Exception as e:
            logger.error(f"Claude Code SQL generation failed: {str(e)}")
            raise
    
    async def analyze_data_with_claude_code(
        self,
        user_id: str,
        dataset: Dict[str, Any],
        analysis_type: str = "exploratory"
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨æ–°çš„LLMç¼–æ’æœåŠ¡åˆ†ææ•°æ®
        
        Args:
            user_id: ç”¨æˆ·ID
            dataset: æ•°æ®é›†
            analysis_type: åˆ†æç±»å‹
            
        Returns:
            Dict: åˆ†æç»“æœ
        """
        try:
            # ä½¿ç”¨æ–°çš„LLMç¼–æ’æœåŠ¡
            from app.services.application.llm import get_llm_orchestration_service
            
            # å‡†å¤‡æ•°æ®ä¿¡æ¯æè¿°
            columns_info = dataset.get("columns", [])
            sample_data = dataset.get("sample_data", [])
            statistics = dataset.get("statistics", {})
            
            # æ„å»ºä¸šåŠ¡é—®é¢˜æè¿°
            if analysis_type == "exploratory":
                business_question = "å¯¹è¿™ä¸ªæ•°æ®é›†è¿›è¡Œæ¢ç´¢æ€§æ•°æ®åˆ†æï¼Œè¯†åˆ«ä¸»è¦æ¨¡å¼ã€è¶‹åŠ¿å’Œå¼‚å¸¸å€¼"
            elif analysis_type == "correlation":
                business_question = "åˆ†ææ•°æ®é›†ä¸­å„å˜é‡ä¹‹é—´çš„ç›¸å…³å…³ç³»å’Œä¾èµ–æ¨¡å¼"
            elif analysis_type == "summary":
                business_question = "æä¾›æ•°æ®é›†çš„ç»Ÿè®¡æ‘˜è¦å’Œå…³é”®æ´å¯Ÿ"
            else:
                business_question = f"æ‰§è¡Œ{analysis_type}ç±»å‹çš„æ•°æ®åˆ†æ"
            
            # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
            context_info = {
                "dataset_info": {
                    "columns": columns_info,
                    "sample_data": sample_data,
                    "statistics": statistics,
                    "size": len(dataset.get("data", [])) if "data" in dataset else 0
                },
                "analysis_type": analysis_type,
                "requested_insights": [
                    "æ•°æ®è´¨é‡è¯„ä¼°",
                    "ä¸»è¦è¶‹åŠ¿è¯†åˆ«",
                    "å¼‚å¸¸å€¼æ£€æµ‹",
                    "å…³é”®ç»Ÿè®¡ç‰¹å¾",
                    "ä¸šåŠ¡æ´å¯Ÿå»ºè®®"
                ]
            }
            
            # è·å–ç¼–æ’æœåŠ¡å¹¶æ‰§è¡Œåˆ†æ
            service = get_llm_orchestration_service()
            result = await service.analyze_data_requirements(
                user_id=user_id,
                business_question=business_question,
                context_info=context_info
            )
            
            if not result.get('success'):
                raise Exception(f"æ•°æ®åˆ†æå¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            
            # æ ¼å¼åŒ–è¿”å›ç»“æœ
            return {
                "success": True,
                "dataset_info": {
                    "columns": columns_info,
                    "size": len(dataset.get("data", [])) if "data" in dataset else 0,
                    "sample_data_count": len(sample_data)
                },
                "analysis_type": analysis_type,
                "analysis_results": {
                    "analysis": result.get('analysis', ''),
                    "recommended_approach": result.get('recommended_approach', ''),
                    "confidence": result.get('confidence', 0.8)
                },
                "insights": {
                    "llm_participated": True,
                    "analysis_method": "six_stage_orchestration",
                    "timestamp": datetime.now().isoformat()
                },
                "architecture": "llm_orchestration_v2"
            }
            
        except Exception as e:
            logger.error(f"Claude Code data analysis failed: {str(e)}")
            raise


    # =================================================================
    # DDDæ¶æ„v2.0 - è¾…åŠ©æ–¹æ³•
    # =================================================================

    def _infer_report_period_from_cron(self, cron_expression: Optional[str]) -> ReportPeriod:
        """åŸºäºcronè¡¨è¾¾å¼æ¨æ–­æŠ¥å‘Šå‘¨æœŸ"""
        if not cron_expression or not isinstance(cron_expression, str):
            return ReportPeriod.MONTHLY

        try:
            # è§£æcronè¡¨è¾¾å¼çš„5ä¸ªå­—æ®µ: m h dom mon dow
            parts = cron_expression.strip().split()
            if len(parts) < 5:
                return ReportPeriod.MONTHLY

            minute, hour, day_of_month, month, day_of_week = parts[:5]

            # å¦‚æœæŒ‡å®šäº†æ˜ŸæœŸå‡ ï¼ˆé *ï¼‰ï¼Œåˆ¤å®šä¸ºæ¯å‘¨
            if day_of_week and day_of_week != '*':
                return ReportPeriod.WEEKLY

            # å¦‚æœæŒ‡å®šäº†æœˆä»½ï¼ˆé *ï¼‰ï¼Œé€šå¸¸ä¸ºæ¯å¹´
            if month and month != '*':
                return ReportPeriod.YEARLY

            # å¦‚æœæŒ‡å®šäº†æŸä¸€å¤©ï¼ˆé *ï¼‰ï¼Œé€šå¸¸ä¸ºæ¯æœˆ
            if day_of_month and day_of_month != '*':
                return ReportPeriod.MONTHLY

            # å…¶ä½™é»˜è®¤æŒ‰æ¯æ—¥
            return ReportPeriod.DAILY

        except Exception as e:
            self.logger.warning(f"è§£æcronè¡¨è¾¾å¼å¤±è´¥: {e}, ä½¿ç”¨é»˜è®¤å‘¨æœŸ: monthly")
            return ReportPeriod.MONTHLY
    
    def _get_template_info(self, db: Session, template_id: str) -> Dict[str, Any]:
        """è·å–æ¨¡æ¿ä¿¡æ¯"""
        template = db.query(Template).filter(Template.id == template_id).first()
        if not template:
            return {}
        
        return {
            "id": template.id,
            "name": template.name,
            "type": template.type if hasattr(template, 'type') else "general",
            "content": template.content if hasattr(template, 'content') else "",
            "size": len(template.content) if hasattr(template, 'content') and template.content else 0
        }
    
    def _get_data_source_info(self, db: Session, data_source_id: str) -> Dict[str, Any]:
        """è·å–æ•°æ®æºä¿¡æ¯"""
        data_source = db.query(DataSource).filter(DataSource.id == data_source_id).first()
        if not data_source:
            return {}
        
        return {
            "id": data_source.id,
            "name": data_source.name,
            "type": data_source.source_type.value if data_source.source_type else "unknown",
            "scope": "full",  # è¿™é‡Œå¯ä»¥åŸºäºå®é™…æ•°æ®æºé…ç½®
            "capabilities": {
                "supports_aggregation": True,
                "supports_time_queries": True,
                "supports_joins": True
            }
        }
    
    def _generate_task_recommendations(
        self, 
        execution_requirements: Dict[str, Any], 
        feasibility_check: Dict[str, Any], 
        placeholder_analysis: Optional[Dict[str, Any]]
    ) -> List[str]:
        """ç”Ÿæˆä»»åŠ¡å»ºè®®"""
        recommendations = []
        
        # åŸºäºå¯è¡Œæ€§æ£€æŸ¥
        if not feasibility_check.get("is_feasible", True):
            recommendations.append("ä»»åŠ¡å½“å‰ä¸å¯æ‰§è¡Œï¼Œéœ€è¦è§£å†³é˜»å¡é—®é¢˜")
        
        # åŸºäºå¤æ‚åº¦
        complexity = execution_requirements.get("complexity", "simple")
        if complexity in ["complex", "highly_complex"]:
            recommendations.append("å»ºè®®åˆ†æ‰¹æ‰§è¡Œæˆ–å¢åŠ ç›‘æ§ç‚¹")
        
        # åŸºäºå ä½ç¬¦åˆ†æ
        if placeholder_analysis:
            priority = placeholder_analysis.get("priority", "normal")
            if priority == "high":
                recommendations.append("å»ºè®®ä¼˜å…ˆå¤„ç†æ­¤ä»»åŠ¡çš„å ä½ç¬¦")
        
        # åŸºäºèµ„æºéœ€æ±‚
        resource_req = execution_requirements.get("resource_requirements", {})
        if resource_req.get("memory_mb", 0) > 4096:
            recommendations.append("ä»»åŠ¡éœ€è¦å¤§é‡å†…å­˜ï¼Œå»ºè®®åœ¨èµ„æºå……è¶³æ—¶æ‰§è¡Œ")
        
        return recommendations
    
    def _estimate_agents_needed(
        self, 
        task_definition: Dict[str, Any], 
        execution_requirements: Dict[str, Any]
    ) -> List[str]:
        """ä¼°ç®—éœ€è¦çš„agents"""
        agents_needed = ["task_coordination_agent"]
        
        # åŸºäºå¤„ç†æ¨¡å¼
        if task_definition.get("processing_mode") == "intelligent":
            agents_needed.append("placeholder_analysis_agent")
        
        # åŸºäºå·¥ä½œæµç±»å‹
        workflow_type = task_definition.get("workflow_type", "simple_report")
        if workflow_type == "complex_analysis":
            agents_needed.extend(["data_analysis_agent", "report_generation_agent"])
        elif workflow_type == "multi_step_report":
            agents_needed.append("report_generation_agent")
        
        # åŸºäºæ•°æ®æº
        if len(task_definition.get("data_source_ids", [])) > 1:
            agents_needed.append("data_integration_agent")
        
        return agents_needed
    
    def _build_task_definition(self, db: Session, task: Task) -> Dict[str, Any]:
        """æ„å»ºä»»åŠ¡å®šä¹‰"""
        return {
            "task_id": task.id,
            "name": task.name,
            "description": task.description,
            "template_id": str(task.template_id),
            "data_source_ids": [str(task.data_source_id)],
            "processing_mode": task.processing_mode.value if task.processing_mode else "simple",
            "workflow_type": task.workflow_type.value if task.workflow_type else "simple_report",
            "template_info": self._get_template_info(db, task.template_id),
            "data_source_info": self._get_data_source_info(db, task.data_source_id)
        }
    
    def _execute_step_with_agents(
        self, 
        agent_provider, 
        step: Dict[str, Any], 
        task: Task, 
        execution_plan: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ä½¿ç”¨agentsæ‰§è¡Œå•ä¸ªæ­¥éª¤"""
        step_id = step.get("step_id")
        agents_required = step.get("agents_required", [])
        
        self.logger.info(f"æ‰§è¡Œæ­¥éª¤ {step_id}ï¼Œéœ€è¦agents: {agents_required}")
        
        step_result = {
            "step_id": step_id,
            "success": False,
            "agents_used": [],
            "execution_time": 0,
            "output": {},
            "errors": []
        }
        
        try:
            start_time = datetime.now()
            
            # æ¨¡æ‹Ÿè°ƒç”¨åŸºç¡€è®¾æ–½å±‚agents
            for agent_name in agents_required:
                if hasattr(agent_provider, f"get_{agent_name}"):
                    agent = getattr(agent_provider, f"get_{agent_name}")()
                    
                    # è°ƒç”¨agentæ‰§è¡Œå…·ä½“ä»»åŠ¡
                    agent_result = self._call_agent_for_step(agent, step, task, execution_plan)
                    step_result["agents_used"].append(agent_name)
                    step_result["output"][agent_name] = agent_result
                else:
                    # å¦‚æœagentä¸å­˜åœ¨ï¼Œè®°å½•ä½†ä¸é˜»æ­¢æ‰§è¡Œ
                    self.logger.warning(f"Agent {agent_name} ä¸å¯ç”¨ï¼Œè·³è¿‡")
                    step_result["errors"].append(f"Agent {agent_name} ä¸å¯ç”¨")
            
            end_time = datetime.now()
            step_result["execution_time"] = (end_time - start_time).total_seconds()
            step_result["success"] = len(step_result["errors"]) == 0
            
        except Exception as e:
            step_result["errors"].append(str(e))
            self.logger.error(f"æ­¥éª¤ {step_id} æ‰§è¡Œå¤±è´¥: {e}")
        
        return step_result
    
    def _call_agent_for_step(
        self, 
        agent, 
        step: Dict[str, Any], 
        task: Task, 
        execution_plan: Dict[str, Any]
    ) -> Dict[str, Any]:
        """è°ƒç”¨agentæ‰§è¡Œå…·ä½“æ­¥éª¤"""
        # è¿™é‡Œæ˜¯ä¸åŸºç¡€è®¾æ–½å±‚agentsçš„æ¥å£
        # å®é™…å®ç°æ—¶éœ€è¦æ ¹æ®å…·ä½“çš„agentæ¥å£è¿›è¡Œè°ƒç”¨
        
        step_type = step.get("type", "unknown")
        
        if step_type == "preparation":
            return {"status": "prepared", "message": "ä»»åŠ¡å‡†å¤‡å®Œæˆ"}
        elif step_type == "data_processing":
            return {"status": "data_acquired", "records": 100, "message": "æ•°æ®è·å–å®Œæˆ"}
        elif step_type == "business_logic":
            return {"status": "processed", "placeholders": 5, "message": "å ä½ç¬¦å¤„ç†å®Œæˆ"}
        elif step_type == "output_generation":
            return {"status": "generated", "file_path": "/tmp/report.html", "message": "æŠ¥å‘Šç”Ÿæˆå®Œæˆ"}
        else:
            return {"status": "completed", "message": f"æ­¥éª¤ {step_type} æ‰§è¡Œå®Œæˆ"}
    
    def _handle_step_failure(
        self, 
        agent_provider, 
        failed_step: Dict[str, Any], 
        execution_plan: Dict[str, Any], 
        previous_results: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """å¤„ç†æ­¥éª¤å¤±è´¥"""
        rollback_strategy = execution_plan.get("rollback_strategy", {})
        
        # ç®€åŒ–çš„å›æ»šé€»è¾‘
        if rollback_strategy.get("fallback_options"):
            return {"should_abort": False, "action": "retry_with_fallback"}
        else:
            return {"should_abort": True, "action": "abort_execution"}
    
    def _execute_task_traditional_way(
        self, 
        db: Session, 
        task: Task, 
        user_id: str
    ) -> ApplicationResult[Dict[str, Any]]:
        """ä¼ ç»Ÿæ–¹å¼æ‰§è¡Œä»»åŠ¡ï¼ˆé™çº§æ–¹æ¡ˆï¼‰"""
        self.logger.info(f"ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼æ‰§è¡Œä»»åŠ¡ {task.id}")
        
        # è°ƒç”¨ç°æœ‰çš„ä»»åŠ¡æ‰§è¡ŒæœåŠ¡
        result = self.task_execution_service.execute_task(
            db, task.id, user_id, {}
        )
        
        return ApplicationResult.success_result(
            data={"traditional_execution": True, "result": result},
            message="ä»»åŠ¡é€šè¿‡ä¼ ç»Ÿæ–¹å¼æ‰§è¡Œå®Œæˆ"
        )


logger.info("âœ… Task Application Service DDDæ¶æ„v2.0 loaded with proper domain/infrastructure separation")