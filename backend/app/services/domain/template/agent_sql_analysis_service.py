"""
Agent SQL Analysis Service

ä½¿ç”¨ç°æœ‰Agentç³»ç»Ÿåˆ†æå ä½ç¬¦ï¼Œç”Ÿæˆå¯¹åº”çš„æ•°æ®åº“æŸ¥è¯¢SQLå¹¶æŒä¹…åŒ–å­˜å‚¨
"""

import logging
import json
import re
from datetime import datetime
from typing import Dict, Any, List, Optional, Tuple
from sqlalchemy.orm import Session

from app.models.template_placeholder import TemplatePlaceholder
from app.models.data_source import DataSource
from app.services.data.connectors.connector_factory import create_connector

logger = logging.getLogger(__name__)


class AgentSQLAnalysisService:
    """Agent SQLåˆ†ææœåŠ¡"""
    
    def __init__(self, db: Session, user_id: str, multi_db_agent=None):
        self.db = db
        if not user_id:
            raise ValueError("user_id is required for Agent SQL Analysis Service")
        self.user_id = user_id
        # ä½¿ç”¨ä¾èµ–æ³¨å…¥ï¼Œé¿å…å¾ªç¯ä¾èµ–
        self._multi_db_agent = multi_db_agent
    
    @property
    def multi_db_agent(self):
        """å»¶è¿Ÿåˆ›å»ºå¤šæ•°æ®åº“ä»£ç†ï¼Œé¿å…å¾ªç¯ä¾èµ–"""
        if self._multi_db_agent is None:
            # é€šè¿‡å·¥å‚æ–¹æ³•åˆ›å»ºï¼Œé¿å…ç›´æ¥å¯¼å…¥
            from app.services.application.factories import create_multi_database_agent
            if not self.user_id:
                raise ValueError("user_id is required to create multi-database agent")
            self._multi_db_agent = create_multi_database_agent(self.db, self.user_id)
        return self._multi_db_agent

    def _extract_context_from_template(self, template_id: str, placeholder_text: str) -> str:
        """ä»æ¨¡æ¿å†…å®¹ä¸­æå–ä¸å ä½ç¬¦æœ€è¿‘çš„ä¸‰ä¸ªå¥å­ä½œä¸ºä¸Šä¸‹æ–‡ã€‚
        ä¼˜å…ˆä½¿ç”¨ä¸­æ–‡æ ‡ç‚¹è¿›è¡Œåˆ†å¥ï¼Œå…¶æ¬¡è‹±æ–‡æ ‡ç‚¹å’Œæ¢è¡Œã€‚
        """
        try:
            from app.models.template import Template
            tpl = self.db.query(Template).filter(Template.id == template_id).first()
            if not tpl or not tpl.content:
                return ""
            content = tpl.content
            # ç®€åŒ–ï¼šè‹¥æ˜¯hexæˆ–äºŒè¿›åˆ¶ï¼Œåˆ™æ— æ³•æŠ½å–è¯­ä¹‰ä¸Šä¸‹æ–‡
            # ä»…å¤„ç†æ–‡æœ¬æ¨¡æ¿
            if isinstance(content, str) and len(content) < 200000:
                # åˆ†å¥ï¼šä¸­æ–‡å¥å·ã€é—®å·ã€æ„Ÿå¹å·ï¼›è‹±æ–‡ .!?ï¼›ä¿ç•™åˆ†éš”ç¬¦
                sentences = re.split(r"(?<=[ã€‚ï¼ï¼Ÿ!?\.])\s+|\n+", content)
                # å»é™¤ç©ºç™½
                sentences = [s.strip() for s in sentences if s and s.strip()]
                if not sentences:
                    return ""
                # åœ¨contentä¸­å®šä½å ä½ç¬¦å‡ºç°çš„å­—ç¬¦ä½ç½®
                idx = content.find(placeholder_text) if placeholder_text else -1
                if idx == -1 and placeholder_text:
                    # å°è¯•å»æ‰èŠ±æ‹¬å·åŒ¹é…
                    stripped = placeholder_text.replace("{{", "").replace("}}", "").strip()
                    idx = content.find(stripped)
                if idx == -1:
                    # Fallback: è¿”å›å‰ä¸¤å¥
                    join_ctx = "\n".join(sentences[:3])
                    return join_ctx[:600]
                # æ‰¾åˆ°è¯¥å­—ç¬¦ä½ç½®æ‰€å±å¥å­çš„ç´¢å¼•
                # é€šè¿‡ç´¯ç§¯é•¿åº¦å®šä½
                cumulative = 0
                target_i = 0
                for i, s in enumerate(sentences):
                    cumulative += len(s) + 1  # ç²—ç•¥åŠ åˆ†éš”
                    if cumulative >= idx:
                        target_i = i
                        break
                start = max(0, target_i - 1)
                end = min(len(sentences), target_i + 2)
                context_block = "\n".join(sentences[start:end])
                return context_block[:600]
            return ""
        except Exception as e:
            logger.debug(f"æå–æ¨¡æ¿ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return ""
    
    async def analyze_placeholder_with_agent(
        self,
        placeholder_id: str,
        data_source_id: str,
        force_reanalyze: bool = False,
        execution_context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨Agentåˆ†æå•ä¸ªå ä½ç¬¦ï¼Œç”ŸæˆSQLæŸ¥è¯¢
        """
        try:
            # 1. è·å–å ä½ç¬¦ä¿¡æ¯
            placeholder = self.db.query(TemplatePlaceholder).filter(
                TemplatePlaceholder.id == placeholder_id
            ).first()
            
            if not placeholder:
                raise ValueError(f"å ä½ç¬¦ä¸å­˜åœ¨: {placeholder_id}")
            
            # 2. æ£€æŸ¥æ˜¯å¦å·²åˆ†æï¼ˆé™¤éå¼ºåˆ¶é‡æ–°åˆ†æï¼‰
            if placeholder.agent_analyzed and not force_reanalyze:
                logger.info(f"å ä½ç¬¦å·²åˆ†æï¼Œè·³è¿‡: {placeholder.placeholder_name}")
                return await self._get_existing_analysis_result(placeholder)
            
            # 3. è·å–æ•°æ®æºä¿¡æ¯
            data_source = self.db.query(DataSource).filter(
                DataSource.id == data_source_id
            ).first()
            
            if not data_source:
                raise ValueError(f"æ•°æ®æºä¸å­˜åœ¨: {data_source_id}")
            
            logger.info(f"ğŸ” å¼€å§‹Agentåˆ†æå ä½ç¬¦: ã€{placeholder.placeholder_name}ã€‘")
            
            # 4. ç»„è£…å ä½ç¬¦ä¸Šä¸‹æ–‡ï¼ˆæ¥è‡ªæ¨¡æ¿æœ€è¿‘ä¸‰å¥ï¼‰
            context_text = self._extract_context_from_template(placeholder.template_id, placeholder.placeholder_text or placeholder.placeholder_name)
            
            # 5. ä½¿ç”¨Multi-Database Agentè¿›è¡Œåˆ†æ
            analysis_result = await self._perform_agent_analysis(
                placeholder,
                data_source,
                {**(execution_context or {}), "context_text": context_text}
            )
            
            # 6. éªŒè¯ç”Ÿæˆçš„SQL
            validation_result = await self._validate_generated_sql(
                analysis_result["generated_sql"], 
                data_source
            )
            
            # 7. æŒä¹…åŒ–åˆ†æç»“æœ
            await self._save_analysis_result(
                placeholder, 
                analysis_result, 
                validation_result
            )
            
            logger.info(f"âœ… Agentåˆ†æå®Œæˆ: ã€{placeholder.placeholder_name}ã€‘")
            
            return {
                "success": True,
                "placeholder_id": placeholder_id,
                "placeholder_name": placeholder.placeholder_name,
                "analysis_result": analysis_result,
                "validation_result": validation_result,
                "confidence_score": analysis_result.get("confidence_score", 0.0)
            }
            
        except Exception as e:
            placeholder_name = placeholder.placeholder_name if 'placeholder' in locals() else "unknown"
            logger.error(f"Agentåˆ†æå¤±è´¥: ã€{placeholder_name}ã€‘, é”™è¯¯: {str(e)}")
            
            # è®°å½•å¤±è´¥çŠ¶æ€
            if 'placeholder' in locals():
                placeholder.confidence_score = 0.0
                placeholder.analyzed_at = datetime.now()
                self.db.commit()
            
            return {
                "success": False,
                "placeholder_id": placeholder_id,
                "error": str(e),
                "confidence_score": 0.0
            }
    
    async def batch_analyze_template_placeholders(
        self,
        template_id: str,
        data_source_id: str,
        force_reanalyze: bool = False,
        execution_context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        æ‰¹é‡åˆ†ææ¨¡æ¿çš„æ‰€æœ‰å ä½ç¬¦
        """
        try:
            # 1. è·å–éœ€è¦åˆ†æçš„å ä½ç¬¦
            query = self.db.query(TemplatePlaceholder).filter(
                TemplatePlaceholder.template_id == template_id,
                TemplatePlaceholder.is_active == True
            )
            
            if not force_reanalyze:
                query = query.filter(TemplatePlaceholder.agent_analyzed == False)
            
            placeholders = query.order_by(TemplatePlaceholder.execution_order).all()
            
            if not placeholders:
                return {
                    "success": True,
                    "template_id": template_id,
                    "total_placeholders": 0,
                    "analyzed_placeholders": 0,
                    "message": "æ²¡æœ‰éœ€è¦åˆ†æçš„å ä½ç¬¦"
                }
            
            logger.info(f"å¼€å§‹æ‰¹é‡åˆ†ææ¨¡æ¿å ä½ç¬¦: {template_id}, æ•°é‡: {len(placeholders)}")
            
            # 2. é€ä¸ªåˆ†æå ä½ç¬¦
            analysis_results = []
            successful_count = 0
            
            for placeholder in placeholders:
                result = await self.analyze_placeholder_with_agent(
                    str(placeholder.id),
                    data_source_id,
                    force_reanalyze,
                    execution_context
                )
                
                analysis_results.append(result)
                
                if result["success"]:
                    successful_count += 1
            
            # 3. æ±‡æ€»ç»“æœ
            total_count = len(placeholders)
            success_rate = (successful_count / total_count) * 100 if total_count > 0 else 0
            
            logger.info(f"æ‰¹é‡åˆ†æå®Œæˆ: {template_id}, æˆåŠŸç‡: {success_rate:.1f}%")
            
            return {
                "success": True,
                "template_id": template_id,
                "total_placeholders": total_count,
                "analyzed_placeholders": successful_count,
                "success_rate": success_rate,
                "analysis_results": analysis_results,
                "summary": {
                    "successful": successful_count,
                    "failed": total_count - successful_count,
                    "avg_confidence": sum(r.get("confidence_score", 0) for r in analysis_results) / total_count
                }
            }
            
        except Exception as e:
            logger.error(f"æ‰¹é‡åˆ†æå¤±è´¥: {template_id}, é”™è¯¯: {str(e)}")
            return {
                "success": False,
                "template_id": template_id,
                "error": str(e),
                "total_placeholders": 0,
                "analyzed_placeholders": 0
            }
    
    async def _perform_agent_analysis(
        self,
        placeholder: TemplatePlaceholder,
        data_source: DataSource,
        execution_context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """æ‰§è¡ŒAgentåˆ†æ"""
        execution_context = execution_context or {}
        
        # 1. å‡†å¤‡Agentåˆ†æçš„è¾“å…¥æ•°æ®
        agent_input = {
            "placeholder_id": str(placeholder.id),
            "placeholder_name": placeholder.placeholder_name,
            "placeholder_text": placeholder.placeholder_text,  # ä¼ é€’å ä½ç¬¦åŸå§‹æ–‡æœ¬
            "placeholder_type": placeholder.placeholder_type,
            "content_type": placeholder.content_type,
            "description": placeholder.description,
            "context_text": execution_context.get("context_text", ""),  # æ¨¡æ¿æœ€è¿‘ä¸‰å¥ä¸Šä¸‹æ–‡
            "intent_analysis": placeholder.agent_config.get("intent_analysis", {}),
            "context_keywords": placeholder.agent_config.get("context_keywords", []),
            "data_source": {
                "id": str(data_source.id),
                "name": data_source.name,
                "source_type": data_source.source_type,
                "connection_config": self._get_safe_connection_config(data_source)
            }
        }
        
        # 2. ä½¿ç”¨Multi-Database Agentè¿›è¡Œåˆ†æ
        try:
            # è·å–æ•°æ®åº“schemaä¿¡æ¯
            schema_info = await self._get_database_schema(data_source)
            agent_input["schema_info"] = schema_info
            
            # è°ƒç”¨Agentåˆ†æï¼Œä¼ é€’æ‰§è¡Œä¸Šä¸‹æ–‡
            agent_result = await self.multi_db_agent.analyze_placeholder_requirements(agent_input, execution_context)
            
            # 3. å¤„ç†Agentè¿”å›ç»“æœ
            if agent_result.get("success", False):
                return {
                    "target_database": agent_result.get("target_database", ""),
                    "target_table": agent_result.get("target_table", ""),
                    "required_fields": agent_result.get("required_fields", []),
                    "generated_sql": agent_result.get("generated_sql", ""),
                    "confidence_score": agent_result.get("confidence_score", 0.0),
                    "analysis_reasoning": agent_result.get("reasoning", ""),
                    "suggested_optimizations": agent_result.get("optimizations", []),
                    "estimated_execution_time": agent_result.get("estimated_time_ms", 0)
                }
            else:
                raise Exception(f"Agentåˆ†æå¤±è´¥: {agent_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                
        except Exception as e:
            logger.error(f"Agentåˆ†ææ‰§è¡Œå¤±è´¥: {str(e)}")
            
            # é™çº§åˆ°åŸºäºè§„åˆ™çš„SQLç”Ÿæˆ
            return await self._fallback_sql_generation(placeholder, data_source)
    
    async def _fallback_sql_generation(
        self,
        placeholder: TemplatePlaceholder,
        data_source: DataSource
    ) -> Dict[str, Any]:
        """é™çº§åˆ°åŸºäºè§„åˆ™çš„SQLç”Ÿæˆ"""
        logger.warning(f"ä½¿ç”¨é™çº§SQLç”Ÿæˆ: {placeholder.placeholder_name}")
        
        # åŸºäºå ä½ç¬¦åç§°å’Œç±»å‹ç”Ÿæˆç®€å•SQL
        intent = placeholder.agent_config.get("intent_analysis", {})
        placeholder_name = placeholder.placeholder_name.lower()
        
        # å°è¯•æ¨æ–­è¡¨åï¼ˆåŸºäºå…³é”®è¯ï¼‰
        schema_info = await self._get_database_schema(data_source)
        target_table = self._guess_target_table(placeholder_name, schema_info)
        
        # ç”ŸæˆåŸºç¡€SQL
        if intent.get("data_operation") == "count":
            sql = f"SELECT COUNT(*) as count FROM {target_table}"
        elif intent.get("data_operation") == "sum":
            sql = f"SELECT SUM(amount) as sum_value FROM {target_table}"
        elif intent.get("data_operation") == "average":
            sql = f"SELECT AVG(amount) as avg_value FROM {target_table}"
        elif intent.get("data_operation") == "list":
            sql = f"SELECT * FROM {target_table} LIMIT 100"
        else:
            sql = f"SELECT COUNT(*) as count FROM {target_table}"
        
        return {
            "target_database": data_source.doris_database or "default",
            "target_table": target_table,
            "required_fields": ["*"],
            "generated_sql": sql,
            "confidence_score": 0.3,  # ä½ç½®ä¿¡åº¦
            "analysis_reasoning": "åŸºäºè§„åˆ™çš„é™çº§ç”Ÿæˆ",
            "suggested_optimizations": [],
            "estimated_execution_time": 1000
        }
    
    async def _validate_generated_sql(
        self,
        sql: str,
        data_source: DataSource
    ) -> Dict[str, Any]:
        """éªŒè¯ç”Ÿæˆçš„SQLå¹¶æä¾›ä¿®æ­£å»ºè®®"""
        try:
            # 1. åŸºç¡€è¯­æ³•æ£€æŸ¥
            syntax_check = self._check_sql_syntax(sql)
            if not syntax_check["valid"]:
                # å°è¯•è‡ªåŠ¨ä¿®æ­£è¯­æ³•é”™è¯¯
                corrected_sql = await self._attempt_sql_correction(sql, syntax_check["error"])
                return {
                    "valid": False,
                    "error_type": "syntax_error",
                    "original_sql": sql,
                    "corrected_sql": corrected_sql,
                    "error_message": syntax_check["error"],
                    "validated_at": datetime.now().isoformat()
                }
            
            # 2. å°è¯•æ‰§è¡ŒEXPLAINï¼ˆä¸å®é™…æŸ¥è¯¢æ•°æ®ï¼‰
            connector = None
            try:
                connector = create_connector(data_source)
                await connector.connect()
                
                # å…ˆå°è¯•ç®€å•çš„è¡¨å­˜åœ¨æ€§æ£€æŸ¥
                table_check = await self._validate_table_existence(sql, connector)
                if not table_check["valid"]:
                    # è¡¨ä¸å­˜åœ¨ï¼Œå°è¯•ä¿®æ­£è¡¨å
                    corrected_sql = await self._fix_table_names(sql, connector)
                    return {
                        "valid": False,
                        "error_type": "table_not_found",
                        "original_sql": sql,
                        "corrected_sql": corrected_sql,
                        "error_message": table_check["error"],
                        "validated_at": datetime.now().isoformat()
                    }
                
                # æ‰§è¡ŒEXPLAINæŸ¥è¯¢éªŒè¯
                explain_sql = f"EXPLAIN {sql}"
                explain_result = await connector.execute_query(explain_sql)
                
                # ç¡®ä¿ç»“æœå¯ä»¥JSONåºåˆ—åŒ–
                serializable_plan = str(explain_result) if explain_result else ""
                
                return {
                    "valid": True,
                    "execution_plan": serializable_plan,
                    "estimated_cost": self._extract_cost_from_plan(explain_result),
                    "validated_at": datetime.now().isoformat()
                }
                
            except Exception as e:
                logger.warning(f"SQLéªŒè¯æ‰§è¡Œå¤±è´¥: {str(e)}")
                
                # å°è¯•ä¿®æ­£SQLé”™è¯¯
                corrected_sql = await self._attempt_runtime_correction(sql, str(e), data_source)
                
                return {
                    "valid": False,
                    "error_type": "execution_error",
                    "original_sql": sql,
                    "corrected_sql": corrected_sql,
                    "error_message": str(e),
                    "validated_at": datetime.now().isoformat()
                }
            
            finally:
                if connector:
                    try:
                        await connector.disconnect()
                    except Exception as e:
                        logger.warning(f"æ–­å¼€è¿æ¥å¤±è´¥: {str(e)}")
                
        except Exception as e:
            logger.error(f"SQLéªŒè¯å¤±è´¥: {str(e)}")
            return {
                "valid": False,
                "error_type": "validation_error",
                "error_message": str(e),
                "validated_at": datetime.now().isoformat()
            }
    
    async def _save_analysis_result(
        self,
        placeholder: TemplatePlaceholder,
        analysis_result: Dict[str, Any],
        validation_result: Dict[str, Any]
    ):
        """ä¿å­˜åˆ†æç»“æœåˆ°æ•°æ®åº“"""
        
        # æ›´æ–°å ä½ç¬¦è®°å½•
        placeholder.agent_analyzed = True
        placeholder.target_database = analysis_result.get("target_database", "")
        placeholder.target_table = analysis_result.get("target_table", "")
        placeholder.required_fields = analysis_result.get("required_fields", [])
        placeholder.generated_sql = analysis_result.get("generated_sql", "")
        placeholder.sql_validated = validation_result.get("valid", False)
        placeholder.confidence_score = analysis_result.get("confidence_score", 0.0)
        placeholder.analyzed_at = datetime.now()
        
        # æ›´æ–°agent_configï¼Œä¿å­˜åˆ†æè¯¦æƒ…
        if not placeholder.agent_config:
            placeholder.agent_config = {}
            
        placeholder.agent_config.update({
            "analysis_result": {
                "reasoning": analysis_result.get("analysis_reasoning", ""),
                "optimizations": analysis_result.get("suggested_optimizations", []),
                "estimated_time_ms": analysis_result.get("estimated_execution_time", 0)
            },
            "validation_result": validation_result,
            "last_analysis_at": datetime.now().isoformat()
        })
        
        self.db.commit()
        logger.info(f"ğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜: ã€{placeholder.placeholder_name}ã€‘")
        logger.info(f"ğŸ“ å­˜å‚¨SQL: {analysis_result.get('generated_sql', '')[:100]}{'...' if len(analysis_result.get('generated_sql', '')) > 100 else ''}")
    
    def _get_safe_connection_config(self, data_source: DataSource) -> Dict[str, Any]:
        """è·å–å®‰å…¨çš„è¿æ¥é…ç½®ï¼ˆä¸åŒ…å«å¯†ç ï¼‰"""
        config = {
            "host": getattr(data_source, 'doris_fe_hosts', []),
            "port": getattr(data_source, 'doris_query_port', 9030),
            "database": getattr(data_source, 'doris_database', 'default'),
            "username": getattr(data_source, 'doris_username', '')
        }
        return config
    
    async def _get_database_schema(self, data_source: DataSource) -> Dict[str, Any]:
        """è·å–æ•°æ®åº“schemaä¿¡æ¯ - ä½¿ç”¨å¢å¼ºçš„connector APIæ–¹æ³•"""
        try:
            # é¦–å…ˆå°è¯•ä»ç¼“å­˜è·å–è¡¨ç»“æ„
            cached_schema = self._get_cached_schema(data_source.id)
            if cached_schema and cached_schema.get("tables"):
                logger.info(f"ä½¿ç”¨ç¼“å­˜çš„è¡¨ç»“æ„ä¿¡æ¯: {data_source.name}, å‘ç° {len(cached_schema.get('tables', []))} ä¸ªè¡¨")
                return cached_schema
            
            # å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰ï¼Œåˆ™ç›´æ¥æŸ¥è¯¢æ•°æ®æº
            logger.info(f"ç¼“å­˜ä¸­æ— è¡¨ç»“æ„ï¼Œä½¿ç”¨å¢å¼ºçš„APIæŸ¥è¯¢æ•°æ®æº: {data_source.name}")
            connector = None
            try:
                connector = create_connector(data_source)
                await connector.connect()
                
                # è·å–æ•°æ®åº“åˆ—è¡¨ - ä½¿ç”¨å¢å¼ºçš„API
                databases = await connector.get_databases()
                logger.info(f"è·å–åˆ°æ•°æ®åº“åˆ—è¡¨: {databases}")
                
                # è·å–è¡¨åˆ—è¡¨ - ä½¿ç”¨å¢å¼ºçš„APIï¼Œæ”¯æŒå¤šå±‚å›é€€æœºåˆ¶
                tables = await connector.get_tables()
                logger.info(f"è·å–åˆ°è¡¨åˆ—è¡¨: {tables}")
                
                # è·å–è¡¨ç»“æ„ä¿¡æ¯ï¼ˆå‰å‡ ä¸ªè¡¨ä½œä¸ºç¤ºä¾‹ï¼‰- ä½¿ç”¨å¢å¼ºçš„API
                table_schemas = {}
                for table_name in tables[:15]:  # å¢åŠ åˆ°15ä¸ªè¡¨ï¼Œæå‡Agentåˆ†æè´¨é‡
                    try:
                        # ä½¿ç”¨å¢å¼ºçš„get_table_schemaæ–¹æ³•ï¼ŒåŒ…å«æ›´è¯¦ç»†çš„å­—æ®µä¿¡æ¯
                        schema_info = await connector.get_table_schema(table_name)
                        table_schemas[table_name] = schema_info
                        
                        # è®°å½•æ›´è¯¦ç»†çš„ä¿¡æ¯ç”¨äºAgentåˆ†æ
                        columns_count = len(schema_info.get('columns', []))
                        logger.info(f"è·å–è¡¨ç»“æ„æˆåŠŸ: {table_name}, å­—æ®µæ•°é‡: {columns_count}")
                        
                        # ä¸ºAgentåˆ†ææ·»åŠ é¢å¤–çš„è¡¨å…ƒæ•°æ®
                        if 'metadata' not in table_schemas[table_name]:
                            table_schemas[table_name]['metadata'] = {}
                        table_schemas[table_name]['metadata'].update({
                            'columns_count': columns_count,
                            'table_type': schema_info.get('table_type', 'table'),
                            'business_relevance': self._assess_business_relevance(table_name, schema_info)
                        })
                        
                    except Exception as e:
                        logger.warning(f"è·å–è¡¨ç»“æ„å¤±è´¥: {table_name}, {str(e)}")
                
                # ç¼“å­˜è¡¨ç»“æ„ä¿¡æ¯åˆ°æ•°æ®åº“
                try:
                    await self._cache_schema_info(data_source.id, databases, tables, table_schemas)
                    logger.info(f"è¡¨ç»“æ„ä¿¡æ¯å·²ç¼“å­˜åˆ°æ•°æ®åº“: {len(tables)} ä¸ªè¡¨")
                except Exception as e:
                    logger.warning(f"ç¼“å­˜è¡¨ç»“æ„ä¿¡æ¯å¤±è´¥: {e}")
                
                # ä¸ºAgentåˆ†ææ„å»ºæ›´å®Œæ•´çš„schemaä¿¡æ¯
                enhanced_schema = {
                    "databases": databases,
                    "tables": tables,
                    "table_schemas": table_schemas,
                    "schema_retrieved_at": datetime.now().isoformat(),
                    "source": "enhanced_api_query",
                    "quality_metrics": {
                        "total_tables": len(tables),
                        "tables_with_schema": len(table_schemas),
                        "schema_completion_rate": (len(table_schemas) / len(tables)) * 100 if tables else 0
                    }
                }
                
                logger.info(f"å¢å¼ºschemaä¿¡æ¯æ„å»ºå®Œæˆï¼Œè¡¨ç»“æ„å®Œæ•´ç‡: {enhanced_schema['quality_metrics']['schema_completion_rate']:.1f}%")
                return enhanced_schema
                
            finally:
                if connector:
                    try:
                        await connector.disconnect()
                    except Exception as e:
                        logger.warning(f"æ–­å¼€è¿æ¥å¤±è´¥: {str(e)}")
            
        except Exception as e:
            logger.error(f"è·å–æ•°æ®åº“schemaå¤±è´¥: {str(e)}")
            return {
                "databases": [],
                "tables": [],
                "table_schemas": {},
                "error": str(e)
            }
    
    def _get_cached_schema(self, data_source_id: str) -> Optional[Dict[str, Any]]:
        """ä»ç¼“å­˜è·å–è¡¨ç»“æ„ä¿¡æ¯"""
        try:
            from app.models.table_schema import TableSchema
            
            # æŸ¥è¯¢ç¼“å­˜çš„è¡¨ç»“æ„
            cached_tables = self.db.query(TableSchema).filter(
                TableSchema.data_source_id == data_source_id,
                TableSchema.is_active == True
            ).all()
            
            if not cached_tables:
                return None
            
            tables = []
            table_schemas = {}
            
            for table_schema in cached_tables:
                table_name = table_schema.table_name
                tables.append(table_name)
                
                # ä»JSONå­—æ®µè·å–åˆ—ä¿¡æ¯
                columns_info = table_schema.columns_info or []
                
                # ç¡®ä¿åˆ—ä¿¡æ¯æ ¼å¼ä¸€è‡´
                formatted_columns = []
                for col_info in columns_info:
                    if isinstance(col_info, dict):
                        formatted_columns.append({
                            "name": col_info.get("name", ""),
                            "type": col_info.get("type", ""),
                            "nullable": col_info.get("nullable", True),
                            "key": col_info.get("key", ""),
                            "default": col_info.get("default", ""),
                            "extra": col_info.get("extra", "")
                        })
                
                table_schemas[table_name] = {
                    "table_name": table_name,
                    "columns": formatted_columns,
                    "total_columns": len(formatted_columns),
                    "estimated_rows": table_schema.estimated_row_count or 0,
                    "table_size": table_schema.table_size_bytes or 0,
                    "last_analyzed": table_schema.last_analyzed.isoformat() if table_schema.last_analyzed else None
                }
            
            return {
                "databases": ["default"],  # é»˜è®¤æ•°æ®åº“
                "tables": tables,
                "table_schemas": table_schemas,
                "schema_retrieved_at": datetime.now().isoformat(),
                "source": "cached"
            }
            
        except Exception as e:
            logger.error(f"ä»ç¼“å­˜è·å–è¡¨ç»“æ„å¤±è´¥: {e}")
            return None
    
    async def _cache_schema_info(self, data_source_id: str, databases: List[str], tables: List[str], table_schemas: Dict[str, Any]):
        """ç¼“å­˜è¡¨ç»“æ„ä¿¡æ¯åˆ°æ•°æ®åº“"""
        try:
            from app.models.table_schema import TableSchema, ColumnSchema
            
            # é¦–å…ˆæ¸…ç†æ—§çš„ç¼“å­˜æ•°æ®
            self.db.query(TableSchema).filter(
                TableSchema.data_source_id == data_source_id
            ).update({"is_active": False})
            
            # ä¿å­˜æ–°çš„è¡¨ç»“æ„ä¿¡æ¯
            for table_name, schema_info in table_schemas.items():
                if "error" in schema_info:
                    continue
                    
                # åˆ›å»ºè¡¨ç»“æ„è®°å½•
                columns_info = schema_info.get("columns", [])
                
                table_schema = TableSchema(
                    data_source_id=data_source_id,
                    table_name=table_name,
                    table_schema=None,  # æ•°æ®åº“schemaå
                    table_catalog=None,  # æ•°æ®åº“catalogå
                    columns_info=columns_info,
                    primary_keys=[col.get("name") for col in columns_info if col.get("key") == "PRI"],
                    indexes=[],  # å¯ä»¥åç»­æ›´æ–°
                    constraints=[],  # å¯ä»¥åç»­æ›´æ–°
                    estimated_row_count=0,
                    table_size_bytes=0,
                    last_analyzed=datetime.now(),
                    is_active=True
                )
                
                self.db.add(table_schema)
                self.db.flush()  # è·å–ID
            
            self.db.commit()
            logger.info(f"æˆåŠŸç¼“å­˜ {len(table_schemas)} ä¸ªè¡¨çš„ç»“æ„ä¿¡æ¯")
            
        except Exception as e:
            self.db.rollback()
            logger.error(f"ç¼“å­˜è¡¨ç»“æ„ä¿¡æ¯å¤±è´¥: {e}")
            raise
    
    def _normalize_column_type(self, column_type: str) -> str:
        """æ ‡å‡†åŒ–åˆ—ç±»å‹"""
        if not column_type:
            return "UNKNOWN"
        
        column_type = column_type.upper()
        
        if "INT" in column_type or "BIGINT" in column_type:
            return "INTEGER"
        elif "VARCHAR" in column_type or "TEXT" in column_type or "STRING" in column_type:
            return "STRING"
        elif "DECIMAL" in column_type or "DOUBLE" in column_type or "FLOAT" in column_type:
            return "NUMERIC"
        elif "DATE" in column_type:
            return "DATE"
        elif "DATETIME" in column_type or "TIMESTAMP" in column_type:
            return "DATETIME"
        else:
            return column_type
    
    def _check_sql_syntax(self, sql: str) -> Dict[str, Any]:
        """åŸºç¡€SQLè¯­æ³•æ£€æŸ¥"""
        try:
            sql = sql.strip().upper()
            
            # åŸºç¡€è¯­æ³•æ£€æŸ¥
            if not sql.startswith('SELECT'):
                return {"valid": False, "error": "SQLå¿…é¡»ä»¥SELECTå¼€å§‹"}
            
            if 'FROM' not in sql:
                return {"valid": False, "error": "ç¼ºå°‘FROMå­å¥"}
            
            # æ£€æŸ¥å±é™©æ“ä½œ
            dangerous_keywords = ['DROP', 'DELETE', 'INSERT', 'UPDATE', 'CREATE', 'ALTER']
            for keyword in dangerous_keywords:
                if keyword in sql:
                    return {"valid": False, "error": f"åŒ…å«å±é™©å…³é”®è¯: {keyword}"}
            
            return {"valid": True}
            
        except Exception as e:
            return {"valid": False, "error": str(e)}
    
    def _extract_cost_from_plan(self, explain_result: Any) -> float:
        """ä»æ‰§è¡Œè®¡åˆ’ä¸­æå–æˆæœ¬ä¼°ç®—"""
        # è¿™é‡Œæ˜¯ç®€åŒ–å®ç°ï¼Œå®é™…éœ€è¦æ ¹æ®å…·ä½“æ•°æ®åº“çš„EXPLAINæ ¼å¼è§£æ
        return 1.0
    
    def _guess_target_table(self, placeholder_name: str, schema_info: Dict) -> str:
        """åŸºäºå ä½ç¬¦åç§°çŒœæµ‹ç›®æ ‡è¡¨"""
        tables = schema_info.get("tables", [])
        
        if not tables:
            logger.warning(f"æ²¡æœ‰æ‰¾åˆ°å¯ç”¨è¡¨ï¼Œå ä½ç¬¦: {placeholder_name}")
            # è¿”å›ä¸€ä¸ªé€šç”¨çš„è™šæ‹Ÿè¡¨åï¼Œä½†å®é™…æ‰§è¡Œæ—¶ä¼šè¢«Dorisè¿æ¥å™¨å¤„ç†
            return "default_table"
        
        # åŸºäºå…³é”®è¯åŒ¹é…
        name_lower = placeholder_name.lower()
        
        # æŠ•è¯‰ç›¸å…³
        if any(keyword in name_lower for keyword in ['æŠ•è¯‰', 'complaint', 'ä¸¾æŠ¥']):
            for table in tables:
                if any(keyword in table.lower() for keyword in ['complain', 'complaint', 'report', 'feedback']):
                    return table
        
        # èº«ä»½è¯ç›¸å…³
        if any(keyword in name_lower for keyword in ['èº«ä»½è¯', 'id_card', 'identity']):
            for table in tables:
                if any(keyword in table.lower() for keyword in ['identity', 'id_card', 'person']):
                    return table
        
        # æ‰‹æœºå·ç›¸å…³
        if any(keyword in name_lower for keyword in ['æ‰‹æœºå·', 'phone', 'mobile']):
            for table in tables:
                if any(keyword in table.lower() for keyword in ['phone', 'mobile', 'contact']):
                    return table
        
        # å¯¼æ¸¸ç›¸å…³
        if any(keyword in name_lower for keyword in ['å¯¼æ¸¸', 'guide', 'å‘å¯¼']):
            for table in tables:
                if 'guide' in table.lower():
                    return table
        
        # ä¼ä¸šç›¸å…³
        if any(keyword in name_lower for keyword in ['ä¼ä¸š', 'enterprise', 'å…¬å¸']):
            for table in tables:
                if any(keyword in table.lower() for keyword in ['enterprise', 'company', 'biz']):
                    return table
        
        # è¡Œç¨‹ç›¸å…³
        if any(keyword in name_lower for keyword in ['è¡Œç¨‹', 'itinerary', 'æ—…ç¨‹']):
            for table in tables:
                if 'itinerary' in table.lower():
                    return table
        
        # æ—…è¡Œç¤¾ç›¸å…³
        if any(keyword in name_lower for keyword in ['æ—…è¡Œç¤¾', 'travel', 'agency']):
            for table in tables:
                if any(keyword in table.lower() for keyword in ['travel', 'agency', 'tour']):
                    return table
        
        # ç”¨æˆ·ç›¸å…³
        if any(keyword in name_lower for keyword in ['ç”¨æˆ·', 'user', 'customer']):
            for table in tables:
                if any(keyword in table.lower() for keyword in ['user', 'customer', 'member']):
                    return table
        
        # è®¢å•ç›¸å…³
        if any(keyword in name_lower for keyword in ['è®¢å•', 'order', 'booking']):
            for table in tables:
                if any(keyword in table.lower() for keyword in ['order', 'booking', 'reservation']):
                    return table
        
        # ç»Ÿè®¡ç›¸å…³ - å°è¯•æ‰¾ä¸»è¦ä¸šåŠ¡è¡¨
        if any(keyword in name_lower for keyword in ['ç»Ÿè®¡', 'count', 'total']):
            # ä¼˜å…ˆé€‰æ‹©çœ‹èµ·æ¥åƒä¸»ä¸šåŠ¡è¡¨çš„è¡¨å
            for table in tables:
                if any(keyword in table.lower() for keyword in ['main', 'primary', 'data', 'info']):
                    return table
        
        # é»˜è®¤è¿”å›ç¬¬ä¸€ä¸ªè¡¨
        logger.info(f"ä½¿ç”¨é»˜è®¤è¡¨ {tables[0]} ä½œä¸ºå ä½ç¬¦ {placeholder_name} çš„ç›®æ ‡è¡¨")
        return tables[0]
    
    def _assess_business_relevance(self, table_name: str, schema_info: Dict) -> float:
        """è¯„ä¼°è¡¨çš„ä¸šåŠ¡ç›¸å…³æ€§åˆ†æ•°"""
        relevance_score = 0.0
        table_lower = table_name.lower()
        
        # åŸºäºè¡¨åçš„ä¸šåŠ¡ç›¸å…³æ€§
        business_keywords = {
            'complaint': 1.0, 'report': 0.9, 'feedback': 0.8,
            'user': 0.9, 'customer': 0.9, 'member': 0.8,
            'order': 0.9, 'booking': 0.8, 'reservation': 0.7,
            'travel': 0.8, 'agency': 0.7, 'tour': 0.7,
            'phone': 0.6, 'contact': 0.6, 'identity': 0.6,
            'data': 0.5, 'info': 0.5, 'main': 0.7
        }
        
        for keyword, score in business_keywords.items():
            if keyword in table_lower:
                relevance_score = max(relevance_score, score)
        
        # åŸºäºå­—æ®µæ•°é‡è°ƒæ•´åˆ†æ•°
        columns_count = len(schema_info.get('columns', []))
        if columns_count > 10:
            relevance_score += 0.1
        elif columns_count > 5:
            relevance_score += 0.05
            
        return min(1.0, relevance_score)
    
    async def _attempt_sql_correction(self, sql: str, error_message: str) -> str:
        """å°è¯•ä¿®æ­£SQLè¯­æ³•é”™è¯¯"""
        corrected_sql = sql
        
        # å¸¸è§è¯­æ³•é”™è¯¯ä¿®æ­£
        if "missing FROM" in error_message.lower():
            # æ·»åŠ FROMå­å¥
            if "SELECT" in sql and "FROM" not in sql:
                corrected_sql = sql.replace("SELECT", "SELECT * FROM default_table WHERE")
        
        elif "semicolon" in error_message.lower():
            # ç§»é™¤å¤šä½™çš„åˆ†å·
            corrected_sql = sql.rstrip(';')
        
        elif "quote" in error_message.lower():
            # ä¿®æ­£å¼•å·é—®é¢˜
            corrected_sql = sql.replace("'", "\"")
        
        logger.info(f"SQLè¯­æ³•ä¿®æ­£: {sql} -> {corrected_sql}")
        return corrected_sql
    
    async def _validate_table_existence(self, sql: str, connector) -> Dict[str, Any]:
        """éªŒè¯SQLä¸­çš„è¡¨æ˜¯å¦å­˜åœ¨"""
        try:
            import re
            # ç®€å•çš„è¡¨åæå–ï¼ˆå¯ä»¥æ”¹è¿›ï¼‰
            table_pattern = r'FROM\s+([\w_]+)'
            matches = re.findall(table_pattern, sql, re.IGNORECASE)
            
            if not matches:
                return {"valid": True}  # æ²¡æœ‰æ‰¾åˆ°è¡¨åï¼Œè·³è¿‡æ£€æŸ¥
            
            tables = await connector.get_tables()
            for table_name in matches:
                if table_name not in tables:
                    return {
                        "valid": False,
                        "error": f"è¡¨ {table_name} ä¸å­˜åœ¨",
                        "available_tables": tables[:10]  # è¿”å›å‰10ä¸ªå¯ç”¨è¡¨
                    }
            
            return {"valid": True}
            
        except Exception as e:
            logger.warning(f"è¡¨å­˜åœ¨æ€§æ£€æŸ¥å¤±è´¥: {e}")
            return {"valid": True}  # æ£€æŸ¥å¤±è´¥æ—¶å‡è®¾å­˜åœ¨
    
    async def _fix_table_names(self, sql: str, connector) -> str:
        """ä¿®æ­£SQLä¸­çš„è¡¨å"""
        try:
            import re
            tables = await connector.get_tables()
            
            if not tables:
                return sql
            
            # æå–å¹¶æ›¿æ¢è¡¨å
            def replace_table(match):
                original_table = match.group(1)
                # æŸ¥æ‰¾æœ€ç›¸ä¼¼çš„è¡¨å
                best_match = self._find_similar_table(original_table, tables)
                logger.info(f"è¡¨åä¿®æ­£: {original_table} -> {best_match}")
                return f"FROM {best_match}"
            
            corrected_sql = re.sub(r'FROM\s+([\w_]+)', replace_table, sql, flags=re.IGNORECASE)
            return corrected_sql
            
        except Exception as e:
            logger.error(f"è¡¨åä¿®æ­£å¤±è´¥: {e}")
            return sql
    
    def _find_similar_table(self, target_table: str, available_tables: List[str]) -> str:
        """æŸ¥æ‰¾æœ€ç›¸ä¼¼çš„è¡¨å"""
        target_lower = target_table.lower()
        
        # ç²¾ç¡®åŒ¹é…
        for table in available_tables:
            if table.lower() == target_lower:
                return table
        
        # åŒ…å«åŒ¹é…
        for table in available_tables:
            if target_lower in table.lower() or table.lower() in target_lower:
                return table
        
        # è¿”å›ç¬¬ä¸€ä¸ªå¯ç”¨è¡¨ä½œä¸ºé»˜è®¤å€¼
        return available_tables[0] if available_tables else "default_table"
    
    async def _attempt_runtime_correction(self, sql: str, error_message: str, data_source: DataSource) -> str:
        """è¿è¡Œæ—¶é”™è¯¯ä¿®æ­£"""
        corrected_sql = sql
        error_lower = error_message.lower()
        
        # å­—æ®µä¸å­˜åœ¨é”™è¯¯
        if "column" in error_lower and "not found" in error_lower:
            # æ›¿æ¢ä¸ºé€šé…ç¬¦æŸ¥è¯¢
            corrected_sql = re.sub(r'SELECT\s+[^\s]+', 'SELECT *', sql, flags=re.IGNORECASE)
            logger.info(f"å­—æ®µä¿®æ­£ä¸ºé€šé…ç¬¦æŸ¥è¯¢: {corrected_sql}")
        
        # æƒé™é”™è¯¯
        elif "access denied" in error_lower or "permission" in error_lower:
            # ç®€åŒ–æŸ¥è¯¢ï¼ŒåªæŸ¥è¯¢åŸºç¡€ä¿¡æ¯
            corrected_sql = re.sub(r'SELECT\s+.*?FROM', 'SELECT COUNT(*) FROM', sql, flags=re.IGNORECASE)
            logger.info(f"æƒé™é—®é¢˜ä¿®æ­£ä¸ºCOUNTæŸ¥è¯¢: {corrected_sql}")
        
        return corrected_sql
    
    async def _get_existing_analysis_result(self, placeholder: TemplatePlaceholder) -> Dict[str, Any]:
        """è·å–å·²å­˜åœ¨çš„åˆ†æç»“æœ"""
        return {
            "success": True,
            "placeholder_id": str(placeholder.id),
            "placeholder_name": placeholder.placeholder_name,
            "analysis_result": {
                "target_database": placeholder.target_database,
                "target_table": placeholder.target_table,
                "required_fields": placeholder.required_fields,
                "generated_sql": placeholder.generated_sql,
                "confidence_score": placeholder.confidence_score,
                "analysis_reasoning": placeholder.agent_config.get("analysis_result", {}).get("reasoning", ""),
                "suggested_optimizations": placeholder.agent_config.get("analysis_result", {}).get("optimizations", []),
                "estimated_execution_time": placeholder.agent_config.get("analysis_result", {}).get("estimated_time_ms", 0)
            },
            "validation_result": placeholder.agent_config.get("validation_result", {}),
            "confidence_score": placeholder.confidence_score,
            "from_cache": True
        }