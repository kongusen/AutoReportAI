"""
æ™ºèƒ½ä¸Šä¸‹æ–‡æ£€ç´¢å™¨

åŸºäº Loom 0.0.3 çš„ ContextRetriever æœºåˆ¶ï¼Œä¸º Agent æä¾›åŠ¨æ€è¡¨ç»“æ„ä¸Šä¸‹æ–‡æ³¨å…¥
å®ç°é›¶å·¥å…·è°ƒç”¨çš„ Schema æ³¨å…¥ï¼Œæå‡ Agent çš„å‡†ç¡®æ€§å’Œæ€§èƒ½
"""

from __future__ import annotations

import logging
from typing import List, Dict, Any, Optional, Set
from loom.interfaces.retriever import BaseRetriever, Document

from .types import BaseContextRetriever, ContextInfo, ExecutionStage
from .intelligent_retriever import (
    IntelligentSchemaRetriever, RetrievalConfig,
    create_intelligent_retriever
)

logger = logging.getLogger(__name__)


class SchemaContextRetriever(BaseRetriever):
    """
    è¡¨ç»“æ„ä¸Šä¸‹æ–‡æ£€ç´¢å™¨
    
    åŠŸèƒ½ï¼š
    1. åˆå§‹åŒ–æ—¶è·å–æ•°æ®æºçš„æ‰€æœ‰è¡¨ç»“æ„ä¿¡æ¯
    2. æ ¹æ®ç”¨æˆ·æŸ¥è¯¢æ£€ç´¢ç›¸å…³çš„è¡¨å’Œåˆ—ä¿¡æ¯
    3. æ ¼å¼åŒ–ä¸º Document ä¾› Agent ä½¿ç”¨
    4. æ”¯æŒé˜¶æ®µæ„ŸçŸ¥çš„ä¸Šä¸‹æ–‡ç®¡ç†
    """

    def __init__(
        self,
        data_source_id: str,
        connection_config: Dict[str, Any],
        container: Any,
        top_k: int = 5,
        enable_stage_aware: bool = True,
        use_intelligent_retrieval: bool = True,
        enable_lazy_loading: bool = True
    ):
        """
        Args:
            data_source_id: æ•°æ®æºID
            connection_config: æ•°æ®æºè¿æ¥é…ç½®
            container: æœåŠ¡å®¹å™¨ï¼Œç”¨äºè·å–æ•°æ®æºæœåŠ¡
            top_k: é»˜è®¤è¿”å›çš„è¡¨æ•°é‡
            enable_stage_aware: æ˜¯å¦å¯ç”¨é˜¶æ®µæ„ŸçŸ¥
            use_intelligent_retrieval: æ˜¯å¦ä½¿ç”¨æ™ºèƒ½æ£€ç´¢ï¼ˆTF-IDFï¼‰
            enable_lazy_loading: æ˜¯å¦å¯ç”¨æ‡’åŠ è½½ï¼ˆå¯åŠ¨æ—¶åªè·å–è¡¨åï¼‰
        """
        self.data_source_id = data_source_id
        self.connection_config = connection_config
        self.container = container
        self.top_k = top_k
        self.enable_stage_aware = enable_stage_aware
        self.use_intelligent_retrieval = use_intelligent_retrieval
        self.enable_lazy_loading = enable_lazy_loading

        # Schema ç¼“å­˜
        self.schema_cache: Dict[str, Dict[str, Any]] = {}
        self._initialized = False

        # æ‡’åŠ è½½ç›¸å…³å±æ€§
        self.table_names: List[str] = []
        self.loaded_tables: Set[str] = set()

        # é˜¶æ®µæ„ŸçŸ¥çŠ¶æ€
        self.current_stage = ExecutionStage.INITIALIZATION
        self.stage_context_cache: Dict[str, List[Document]] = {}
        self.recent_selected_tables = None  # ä¿å­˜æœ€è¿‘é€‰æ‹©çš„è¡¨

        # æ™ºèƒ½æ£€ç´¢å™¨ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self._intelligent_retriever: Optional[IntelligentSchemaRetriever] = None

    async def initialize(self):
        """åˆå§‹åŒ–ï¼šæ‡’åŠ è½½æ¨¡å¼ä¸‹åªè·å–è¡¨åï¼Œä¼ ç»Ÿæ¨¡å¼ä¸‹è·å–æ‰€æœ‰è¡¨ç»“æ„"""
        if self._initialized:
            return

        try:
            if self.enable_lazy_loading:
                logger.info(f"ğŸ” å¼€å§‹åˆå§‹åŒ–æ•°æ®æº {self.data_source_id} çš„è¡¨åç¼“å­˜ï¼ˆæ‡’åŠ è½½æ¨¡å¼ï¼‰")
            else:
                logger.info(f"ğŸ” å¼€å§‹åˆå§‹åŒ–æ•°æ®æº {self.data_source_id} çš„ schema ç¼“å­˜ï¼ˆä¼ ç»Ÿæ¨¡å¼ï¼‰")

            # è·å–æ•°æ®æºæœåŠ¡
            data_source_service = getattr(self.container, 'data_source', None) or \
                                 getattr(self.container, 'data_source_service', None)

            if not data_source_service:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°æ•°æ®æºæœåŠ¡ï¼Œæ— æ³•åˆå§‹åŒ– schema ç¼“å­˜")
                self._initialized = True
                return

            # 1. è·å–æ‰€æœ‰è¡¨å
            tables_sql = "SHOW TABLES"
            tables_result = await data_source_service.run_query(
                connection_config=self.connection_config,
                sql=tables_sql,
                limit=1000
            )

            if not isinstance(tables_result, dict) or not tables_result.get('success'):
                error_info = tables_result.get('error', 'Unknown error') if isinstance(tables_result, dict) else str(tables_result)
                logger.warning(f"âš ï¸ è·å–è¡¨åˆ—è¡¨å¤±è´¥: {error_info}")
                self._initialized = True
                return

            # è§£æè¡¨å
            tables = []
            for row in tables_result.get('rows', []) or tables_result.get('data', []):
                if isinstance(row, dict):
                    table_name = next(iter(row.values())) if row else None
                elif isinstance(row, (list, tuple)) and row:
                    table_name = row[0]
                elif isinstance(row, str):
                    table_name = row
                else:
                    table_name = None

                if table_name:
                    tables.append(str(table_name))

            self.table_names = tables
            logger.info(f"âœ… å‘ç° {len(tables)} ä¸ªè¡¨")

            if self.enable_lazy_loading:
                # æ‡’åŠ è½½æ¨¡å¼ï¼šåªç¼“å­˜è¡¨åï¼Œä¸è·å–åˆ—ä¿¡æ¯
                logger.info(f"âœ… è¡¨åç¼“å­˜åˆå§‹åŒ–å®Œæˆï¼ˆæ‡’åŠ è½½æ¨¡å¼ï¼‰")
                logger.info(f"   è¡¨å: {tables[:10]}{'...' if len(tables) > 10 else ''}")
                self._initialized = True
                return

            # ä¼ ç»Ÿæ¨¡å¼ï¼šå¹¶è¡Œè·å–æ¯ä¸ªè¡¨çš„åˆ—ä¿¡æ¯
            async def fetch_table_columns(table_name: str):
                """è·å–å•ä¸ªè¡¨çš„åˆ—ä¿¡æ¯"""
                try:
                    columns_sql = f"SHOW FULL COLUMNS FROM `{table_name}`"
                    columns_result = await data_source_service.run_query(
                        connection_config=self.connection_config,
                        sql=columns_sql,
                        limit=1000
                    )

                    if isinstance(columns_result, dict) and columns_result.get('success'):
                        rows = columns_result.get('rows', []) or columns_result.get('data', [])
                        columns = []

                        for row in rows:
                            if isinstance(row, dict):
                                columns.append({
                                    'name': row.get('Field') or row.get('column_name') or row.get('COLUMN_NAME') or '',
                                    'type': row.get('Type') or row.get('column_type') or row.get('DATA_TYPE') or '',
                                    'nullable': row.get('Null') or row.get('IS_NULLABLE'),
                                    'key': row.get('Key') or row.get('COLUMN_KEY'),
                                    'default': row.get('Default'),
                                    'comment': row.get('Comment') or row.get('COLUMN_COMMENT') or '',
                                })

                        return table_name, {
                            'table_name': table_name,
                            'columns': [col for col in columns if col.get('name')],
                            'table_comment': '',
                            'table_type': 'TABLE',
                        }
                    else:
                        logger.warning(f"âš ï¸ è·å–è¡¨ {table_name} åˆ—ä¿¡æ¯å¤±è´¥")
                        return table_name, None
                except Exception as e:
                    logger.warning(f"âš ï¸ è·å–è¡¨ {table_name} çš„åˆ—ä¿¡æ¯å¤±è´¥: {e}")
                    return table_name, None

            # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰è¡¨çš„åˆ—ä¿¡æ¯æŸ¥è¯¢
            import asyncio
            tasks = [fetch_table_columns(table_name) for table_name in tables]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # å¤„ç†ç»“æœ
            for result in results:
                if isinstance(result, Exception):
                    logger.error(f"âŒ å¹¶è¡ŒæŸ¥è¯¢å‡ºé”™: {result}")
                    continue
                    
                table_name, table_info = result
                if table_info:
                    self.schema_cache[table_name] = table_info
                    self.loaded_tables.add(table_name)
                    logger.info(f"  ğŸ“‹ è¡¨ {table_name}: {len(table_info['columns'])} åˆ—")

            logger.info(f"âœ… Schema ç¼“å­˜åˆå§‹åŒ–å®Œæˆï¼Œå…± {len(self.schema_cache)} ä¸ªè¡¨")

            # åˆå§‹åŒ–æ™ºèƒ½æ£€ç´¢å™¨
            if self.use_intelligent_retrieval and self.schema_cache:
                logger.info("ğŸ”§ åˆå§‹åŒ–æ™ºèƒ½æ£€ç´¢å™¨ï¼ˆTF-IDFï¼‰...")
                self._intelligent_retriever = create_intelligent_retriever(
                    schema_cache=self.schema_cache,
                    config=RetrievalConfig(
                        use_tfidf=True,
                        enable_synonyms=True,
                        enable_caching=True
                    )
                )
                await self._intelligent_retriever.initialize()
                logger.info("âœ… æ™ºèƒ½æ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆ")

            self._initialized = True

        except Exception as e:
            logger.error(f"âŒ Schema ç¼“å­˜åˆå§‹åŒ–å¤±è´¥: {e}", exc_info=True)

    async def _load_tables_on_demand(self, table_names: List[str]):
        """æŒ‰éœ€åŠ è½½è¡¨çš„åˆ—ä¿¡æ¯"""
        # æ‰¾å‡ºéœ€è¦åŠ è½½çš„è¡¨ï¼ˆæœªç¼“å­˜çš„ï¼‰
        tables_to_load = [name for name in table_names if name not in self.loaded_tables]
        
        if not tables_to_load:
            logger.info(f"âœ… æ‰€æœ‰è¡¨å·²åŠ è½½ï¼Œæ— éœ€é‡å¤æŸ¥è¯¢")
            return
        
        logger.info(f"ğŸ”„ æŒ‰éœ€åŠ è½½ {len(tables_to_load)} ä¸ªè¡¨çš„åˆ—ä¿¡æ¯: {tables_to_load}")
        
        # è·å–æ•°æ®æºæœåŠ¡
        data_source_service = getattr(self.container, 'data_source', None) or \
                             getattr(self.container, 'data_source_service', None)
        
        if not data_source_service:
            logger.warning("âš ï¸ æ•°æ®æºæœåŠ¡ä¸å¯ç”¨ï¼Œæ— æ³•åŠ è½½è¡¨ç»“æ„")
            return

        # å¹¶è¡ŒåŠ è½½è¡¨ç»“æ„
        async def load_table_columns(table_name: str):
            """åŠ è½½å•ä¸ªè¡¨çš„åˆ—ä¿¡æ¯"""
            try:
                columns_sql = f"SHOW FULL COLUMNS FROM `{table_name}`"
                columns_result = await data_source_service.run_query(
                    connection_config=self.connection_config,
                    sql=columns_sql,
                    limit=1000
                )

                if isinstance(columns_result, dict) and columns_result.get('success'):
                    rows = columns_result.get('rows', []) or columns_result.get('data', [])
                    columns = []

                    for row in rows:
                        if isinstance(row, dict):
                            columns.append({
                                'name': row.get('Field') or row.get('column_name') or row.get('COLUMN_NAME') or '',
                                'type': row.get('Type') or row.get('column_type') or row.get('DATA_TYPE') or '',
                                'nullable': row.get('Null') or row.get('IS_NULLABLE'),
                                'key': row.get('Key') or row.get('COLUMN_KEY'),
                                'default': row.get('Default'),
                                'comment': row.get('Comment') or row.get('COLUMN_COMMENT') or '',
                            })

                    return table_name, {
                        'table_name': table_name,
                        'columns': [col for col in columns if col.get('name')],
                        'table_comment': '',
                        'table_type': 'TABLE',
                    }
                else:
                    logger.warning(f"âš ï¸ è·å–è¡¨ {table_name} åˆ—ä¿¡æ¯å¤±è´¥")
                    return table_name, None
            except Exception as e:
                logger.warning(f"âš ï¸ è·å–è¡¨ {table_name} çš„åˆ—ä¿¡æ¯å¤±è´¥: {e}")
                return table_name, None

        # å¹¶è¡Œæ‰§è¡Œ
        import asyncio
        tasks = [load_table_columns(table_name) for table_name in tables_to_load]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†ç»“æœ
        loaded_count = 0
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"âŒ å¹¶è¡ŒæŸ¥è¯¢å‡ºé”™: {result}")
                continue
                
            table_name, table_info = result
            if table_info:
                self.schema_cache[table_name] = table_info
                self.loaded_tables.add(table_name)
                loaded_count += 1
                logger.info(f"  ğŸ“‹ è¡¨ {table_name}: {len(table_info['columns'])} åˆ—")

        logger.info(f"âœ… æŒ‰éœ€åŠ è½½å®Œæˆï¼Œæ–°å¢ {loaded_count} ä¸ªè¡¨ç»“æ„")
        
        # å¦‚æœè¿™æ˜¯ç¬¬ä¸€æ¬¡åŠ è½½è¡¨ç»“æ„ï¼Œåˆå§‹åŒ–æ™ºèƒ½æ£€ç´¢å™¨
        if self.use_intelligent_retrieval and self.schema_cache and self._intelligent_retriever is None:
            logger.info("ğŸ”§ åˆå§‹åŒ–æ™ºèƒ½æ£€ç´¢å™¨ï¼ˆLLMï¼‰...")
            self._intelligent_retriever = create_intelligent_retriever(
                schema_cache=self.schema_cache,
                config=RetrievalConfig(
                    use_llm_judgment=True,
                    llm_model="gpt-4o-mini",
                    enable_caching=True
                ),
                container=self.container
            )
            await self._intelligent_retriever.initialize()
            logger.info("âœ… æ™ºèƒ½æ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆ")

    async def retrieve_for_query(
        self,
        query: str,
        top_k: Optional[int] = None,
        filters: Optional[Dict[str, Any]] = None
    ) -> List[Document]:
        """
        ä¸ºæŸ¥è¯¢æ£€ç´¢ç›¸å…³æ–‡æ¡£ - Loom ContextRetriever æ ‡å‡†æ¥å£

        è¿™æ˜¯ Loom TT é€’å½’æ¨¡å¼è¦æ±‚çš„æ ‡å‡†æ–¹æ³•å

        Args:
            query: ç”¨æˆ·çš„ä¸šåŠ¡éœ€æ±‚æè¿°
            top_k: è¿”å›æœ€ç›¸å…³çš„ top_k ä¸ªè¡¨
            filters: å¯é€‰çš„è¿‡æ»¤æ¡ä»¶

        Returns:
            Document åˆ—è¡¨ï¼Œæ¯ä¸ª Document åŒ…å«ä¸€ä¸ªè¡¨çš„å®Œæ•´ç»“æ„ä¿¡æ¯
        """
        return await self.retrieve(query, top_k, filters)

    async def retrieve(
        self,
        query: str,
        top_k: Optional[int] = None,
        filters: Optional[Dict[str, Any]] = None
    ) -> List[Document]:
        """
        æ ¹æ®æŸ¥è¯¢æ£€ç´¢ç›¸å…³çš„è¡¨ç»“æ„æ–‡æ¡£ - æ”¯æŒæ‡’åŠ è½½

        Args:
            query: ç”¨æˆ·çš„ä¸šåŠ¡éœ€æ±‚æè¿°
            top_k: è¿”å›æœ€ç›¸å…³çš„ top_k ä¸ªè¡¨
            filters: å¯é€‰çš„è¿‡æ»¤æ¡ä»¶

        Returns:
            Document åˆ—è¡¨ï¼Œæ¯ä¸ª Document åŒ…å«ä¸€ä¸ªè¡¨çš„å®Œæ•´ç»“æ„ä¿¡æ¯
        """
        # ğŸ”¥ å‡€åŒ–æŸ¥è¯¢ï¼šåªä¿ç•™ä¸šåŠ¡éœ€æ±‚ï¼Œå»é™¤ç³»ç»ŸæŒ‡ä»¤å’Œé€’å½’å†…å®¹
        query = self._sanitize_query(query)
        
        logger.info(f"ğŸ” [SchemaContextRetriever.retrieve] è¢«è°ƒç”¨")
        logger.info(f"   æŸ¥è¯¢å†…å®¹ï¼ˆå‰200å­—ç¬¦ï¼‰: {query[:200]}")
        logger.info(f"   æ‡’åŠ è½½æ¨¡å¼: {'å¯ç”¨' if self.enable_lazy_loading else 'ç¦ç”¨'}")

        top_k = top_k or self.top_k
        logger.info(f"   è¯·æ±‚è¿”å› top_k={top_k} ä¸ªè¡¨")

        if not self._initialized:
            logger.info("   Schema ç¼“å­˜æœªåˆå§‹åŒ–ï¼Œæ­£åœ¨åˆå§‹åŒ–...")
            await self.initialize()

        # æ‡’åŠ è½½æ¨¡å¼ï¼šåŸºäºå¤§æ¨¡å‹çš„æ™ºèƒ½è¡¨ååŒ¹é…å’Œè¿­ä»£ä¼˜åŒ–
        if self.enable_lazy_loading:
            if not self.table_names:
                logger.warning("âš ï¸ è¡¨ååˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•æä¾›ä¸Šä¸‹æ–‡")
                return []
            
            # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨å¤§æ¨¡å‹è¿›è¡Œè¡¨ååŒ¹é…
            candidate_tables = await self._filter_tables_by_name(query, self.table_names)
            
            # ç¬¬äºŒæ­¥ï¼šæŒ‰éœ€åŠ è½½ç›¸å…³è¡¨çš„åˆ—ä¿¡æ¯
            await self._load_tables_on_demand(candidate_tables[:top_k * 2])
            
            # ç¬¬ä¸‰æ­¥ï¼šåŸºäºåˆ—ä¿¡æ¯è¿›è¡ŒéªŒè¯å’Œä¼˜åŒ–
            if not self.schema_cache:
                raise Exception("Schemaç¼“å­˜ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œåˆ—ä¿¡æ¯éªŒè¯")
            
            # æå–åˆ—ä¿¡æ¯
            table_columns = {}
            for table_name in candidate_tables:
                if table_name in self.schema_cache:
                    table_columns[table_name] = self.schema_cache[table_name].get('columns', [])
            
            # ä½¿ç”¨å¤§æ¨¡å‹éªŒè¯å’Œä¼˜åŒ–è¡¨é€‰æ‹©
            refined_tables = await self._validate_and_refine_tables(query, candidate_tables, table_columns)
            
            # ä¿å­˜æœ€è¿‘é€‰æ‹©çš„è¡¨åˆ°ä¸Šä¸‹æ–‡
            self.recent_selected_tables = refined_tables
            logger.info(f"ğŸ’¾ [SchemaContextRetriever] ä¿å­˜æœ€è¿‘é€‰æ‹©çš„è¡¨: {refined_tables}")
            
            # ä½¿ç”¨ä¼˜åŒ–åçš„è¡¨è¿›è¡Œæ£€ç´¢
            documents = await self._intelligent_retrieve(query, top_k, filters, refined_tables)
        else:
            # ä¼ ç»Ÿæ¨¡å¼
            if not self.schema_cache:
                logger.warning("âš ï¸ Schema ç¼“å­˜ä¸ºç©ºï¼Œæ— æ³•æä¾›ä¸Šä¸‹æ–‡")
                return []
            
            # æ™ºèƒ½æ£€ç´¢ç­–ç•¥
            documents = await self._intelligent_retrieve(query, top_k, filters)

        # æ£€æŸ¥é˜¶æ®µæ„ŸçŸ¥ç¼“å­˜
        if self.enable_stage_aware:
            cache_key = f"{query[:100]}_{top_k}"
            if cache_key in self.stage_context_cache:
                cached_docs = self.stage_context_cache[cache_key]
                # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä¸è¦è¿”å›ç©ºç¼“å­˜ï¼Œé‡æ–°æ£€ç´¢
                if len(cached_docs) > 0:
                    logger.info(f"âœ… ä½¿ç”¨é˜¶æ®µæ„ŸçŸ¥ç¼“å­˜ï¼Œè¿”å› {len(cached_docs)} ä¸ªè¡¨")
                    return cached_docs
                else:
                    logger.warning(f"âš ï¸ ç¼“å­˜ä¸ºç©ºï¼Œé‡æ–°æ£€ç´¢")
                    # æ¸…é™¤è¿™ä¸ªç©ºç¼“å­˜
                    del self.stage_context_cache[cache_key]

            # æ›´æ–°é˜¶æ®µæ„ŸçŸ¥ç¼“å­˜ï¼ˆä»…å½“ç»“æœéç©ºæ—¶ï¼‰
            if len(documents) > 0:
                self.stage_context_cache[cache_key] = documents
                # é™åˆ¶ç¼“å­˜å¤§å°
                if len(self.stage_context_cache) > 50:
                    # åˆ é™¤æœ€æ—§çš„ç¼“å­˜
                    oldest_key = next(iter(self.stage_context_cache))
                    del self.stage_context_cache[oldest_key]
            else:
                logger.warning(f"âš ï¸ æ£€ç´¢ç»“æœä¸ºç©ºï¼Œä¸ç¼“å­˜æ­¤ç»“æœ")

        logger.info(f"âœ… [SchemaContextRetriever] æ£€ç´¢åˆ° {len(documents)} ä¸ªç›¸å…³è¡¨")
        logger.info(f"   è¿”å›çš„è¡¨: {[d.metadata['table_name'] for d in documents]}")

        return documents

    async def _intelligent_retrieve(
        self,
        query: str,
        top_k: int,
        filters: Optional[Dict[str, Any]] = None,
        refined_tables: Optional[List[str]] = None
    ) -> List[Document]:
        """
        æ™ºèƒ½æ£€ç´¢ç­–ç•¥

        ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½è¡¨ååŒ¹é…ï¼Œä¸å†ä½¿ç”¨ç®—æ³•å›é€€
        æ”¯æŒåŸºäºå¤§æ¨¡å‹ä¼˜åŒ–åçš„è¡¨ååˆ—è¡¨
        """
        # ä½¿ç”¨LLMæ™ºèƒ½æ£€ç´¢å™¨
        if self._intelligent_retriever is not None:
            # ä½¿ç”¨LLMæ£€ç´¢
            stage_str = self.current_stage.value if self.enable_stage_aware else None
            scored_tables = await self._intelligent_retriever.retrieve(
                query=query,
                top_k=top_k,
                stage=stage_str
            )

            # å¦‚æœæä¾›äº†ä¼˜åŒ–åçš„è¡¨åï¼Œä¼˜å…ˆä½¿ç”¨è¿™äº›è¡¨
            if refined_tables:
                # è¿‡æ»¤å‡ºä¼˜åŒ–åçš„è¡¨
                filtered_scored_tables = []
                for table_name, score in scored_tables:
                    if table_name in refined_tables:
                        filtered_scored_tables.append((table_name, score))
                
                # å¦‚æœè¿‡æ»¤åçš„ç»“æœä¸ºç©ºï¼Œä½¿ç”¨åŸå§‹ç»“æœ
                if filtered_scored_tables:
                    scored_tables = filtered_scored_tables
                    logger.info(f"ğŸ¯ [LLMæ£€ç´¢] ä½¿ç”¨ä¼˜åŒ–åçš„è¡¨å: {refined_tables}")

            # è½¬æ¢ä¸º Document æ ¼å¼
            documents = []
            for table_name, score in scored_tables:
                if table_name not in self.schema_cache:
                    continue

                table_info = self.schema_cache[table_name]
                content = self._format_table_info(table_name, table_info)

                doc = Document(
                    content=content,
                    metadata={
                        "source": "schema",
                        "table_name": table_name,
                        "data_source_id": self.data_source_id,
                        "relevance_score": score,
                        "stage": self.current_stage.value,
                        "retrieval_method": "llm_intelligent",
                    },
                    score=score
                )
                documents.append(doc)

            if documents:
                logger.info(f"âœ… [LLMæ£€ç´¢] è¿”å› {len(documents)} ä¸ªè¡¨")
                return documents
            else:
                logger.warning(f"âš ï¸ [LLMæ£€ç´¢] æœªæ‰¾åˆ°ç›¸å…³è¡¨")
                raise Exception(f"LLMæ£€ç´¢æœªæ‰¾åˆ°ç›¸å…³è¡¨: {query}")
        else:
            logger.error("âŒ [LLMæ£€ç´¢] æ™ºèƒ½æ£€ç´¢å™¨æœªåˆå§‹åŒ–")
            raise Exception("æ™ºèƒ½æ£€ç´¢å™¨æœªåˆå§‹åŒ–ï¼Œæ— æ³•è¿›è¡Œæ£€ç´¢")

    async def _keyword_retrieve(
        self,
        query: str,
        top_k: int,
        filters: Optional[Dict[str, Any]] = None,
        refined_tables: Optional[List[str]] = None
    ) -> List[Document]:
        """åŸºç¡€å…³é”®è¯æ£€ç´¢ï¼ˆé™çº§æ–¹æ¡ˆï¼‰"""
        query_lower = query.lower()
        scored_tables = []

        # å¦‚æœæä¾›äº†ä¼˜åŒ–åçš„è¡¨åï¼Œä¼˜å…ˆä½¿ç”¨è¿™äº›è¡¨
        target_tables = refined_tables if refined_tables else list(self.schema_cache.keys())

        # 1. åŸºç¡€å…³é”®è¯åŒ¹é…
        for table_name in target_tables:
            if table_name not in self.schema_cache:
                continue
            table_info = self.schema_cache[table_name]
            score = 0.0

            # è¡¨ååŒ¹é…
            if table_name.lower() in query_lower:
                score += 10.0

            # è¡¨æ³¨é‡ŠåŒ¹é…
            comment = table_info.get('table_comment') or ''
            if comment and any(keyword in comment.lower() for keyword in query_lower.split()):
                score += 5.0

            # åˆ—ååŒ¹é…
            for column in table_info.get('columns', []):
                col_name = (column.get('name') or '').lower()
                col_comment = column.get('comment') or ''

                if col_name in query_lower:
                    score += 3.0
                if col_comment and any(keyword in col_comment.lower() for keyword in query_lower.split()):
                    score += 1.0

            if score > 0:
                scored_tables.append((table_name, table_info, score))

        # 2. é˜¶æ®µæ„ŸçŸ¥ä¼˜åŒ–
        if self.enable_stage_aware:
            scored_tables = self._apply_stage_aware_scoring(scored_tables, query)

        # 3. æŒ‰åˆ†æ•°æ’åºï¼Œå– top_k
        scored_tables.sort(key=lambda x: x[2], reverse=True)
        top_tables = scored_tables[:top_k]

        if not top_tables:
            raise Exception(f"æ²¡æœ‰è¡¨åŒ¹é…æŸ¥è¯¢å…³é”®è¯: {query}")

        # è½¬æ¢ä¸º Document æ ¼å¼
        documents = []
        for table_name, table_info, score in top_tables:
            content = self._format_table_info(table_name, table_info)

            doc = Document(
                content=content,
                metadata={
                    "source": "schema",
                    "table_name": table_name,
                    "data_source_id": self.data_source_id,
                    "relevance_score": score,
                    "stage": self.current_stage.value,
                    "retrieval_method": "keyword",
                },
                score=score
            )
            documents.append(doc)

        logger.info(f"âœ… ä½¿ç”¨å…³é”®è¯æ£€ç´¢ï¼Œè¿”å› {len(documents)} ä¸ªè¡¨")
        return documents

    def _apply_stage_aware_scoring(
        self, 
        scored_tables: List[tuple], 
        query: str
    ) -> List[tuple]:
        """åº”ç”¨é˜¶æ®µæ„ŸçŸ¥è¯„åˆ†"""
        # æ ¹æ®å½“å‰é˜¶æ®µè°ƒæ•´è¯„åˆ†
        stage_multipliers = {
            ExecutionStage.SCHEMA_DISCOVERY: 1.2,  # è¡¨å‘ç°é˜¶æ®µï¼Œæé«˜æ‰€æœ‰è¡¨çš„ç›¸å…³æ€§
            ExecutionStage.SQL_GENERATION: 1.0,    # SQLç”Ÿæˆé˜¶æ®µï¼Œä¿æŒåŸå§‹è¯„åˆ†
            ExecutionStage.SQL_VALIDATION: 0.8,     # SQLéªŒè¯é˜¶æ®µï¼Œé™ä½è¯„åˆ†
            ExecutionStage.DATA_EXTRACTION: 1.1,   # æ•°æ®æå–é˜¶æ®µï¼Œç•¥å¾®æé«˜è¯„åˆ†
        }
        
        multiplier = stage_multipliers.get(self.current_stage, 1.0)
        
        # åº”ç”¨ä¹˜æ•°
        enhanced_tables = []
        for table_name, table_info, score in scored_tables:
            enhanced_score = score * multiplier
            enhanced_tables.append((table_name, table_info, enhanced_score))
        
        return enhanced_tables

    def _format_table_info(self, table_name: str, table_info: Dict[str, Any]) -> str:
        """
        æ ¼å¼åŒ–è¡¨ç»“æ„ä¿¡æ¯ä¸ºæ˜“è¯»çš„æ–‡æœ¬
        
        è¿”å›æ ¼å¼ï¼š
        ```
        ### è¡¨: orders
        **è¯´æ˜**: è®¢å•è¡¨
        **åˆ—ä¿¡æ¯**:
        - **order_id** (BIGINT) [NOT NULL]: è®¢å•ID
        - **customer_id** (BIGINT): å®¢æˆ·ID
        - **order_date** (DATE): è®¢å•æ—¥æœŸ
        - **total_amount** (DECIMAL(10,2)): è®¢å•æ€»é‡‘é¢
        ```
        """
        lines = [
            f"### è¡¨: {table_name}",
        ]

        # æ·»åŠ è¡¨æ³¨é‡Š
        if table_info.get('table_comment'):
            lines.append(f"**è¯´æ˜**: {table_info['table_comment']}")

        # æ·»åŠ åˆ—ä¿¡æ¯
        columns = table_info.get('columns', [])
        if columns:
            lines.append("\n**åˆ—ä¿¡æ¯**:")
            for col in columns:
                col_name = col.get('name') or ''
                col_type = col.get('type') or ''
                col_comment = col.get('comment') or ''
                is_nullable = col.get('nullable', True)

                # æ„å»ºåˆ—æè¿°
                col_desc = f"- **{col_name}** ({col_type})"

                if not is_nullable:
                    col_desc += " [NOT NULL]"

                if col_comment:
                    col_desc += f": {col_comment}"

                lines.append(col_desc)

        return "\n".join(lines)

    async def add_documents(self, documents: List[Document]) -> None:
        """æ·»åŠ æ–‡æ¡£ï¼ˆæœ¬å®ç°ä¸éœ€è¦ï¼Œå› ä¸ºæˆ‘ä»¬ç›´æ¥ä»æ•°æ®æºè·å– schemaï¼‰"""
        logger.warning("SchemaContextRetriever ä¸æ”¯æŒæ·»åŠ æ–‡æ¡£ï¼Œschema ä¿¡æ¯æ¥è‡ªæ•°æ®æº")
        pass

    def format_documents(
        self,
        documents: List[Document],
        max_length: int = 2000
    ) -> str:
        """
        æ ¼å¼åŒ–æ–‡æ¡£ä¸ºå­—ç¬¦ä¸²ï¼ˆç”¨äºä¸Šä¸‹æ–‡æ³¨å…¥ï¼‰

        è¿™æ˜¯ Loom ContextRetriever çš„æ ‡å‡†æ–¹æ³•ï¼Œç”¨äºå°†æ£€ç´¢åˆ°çš„æ–‡æ¡£æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²æ³¨å…¥åˆ° LLM ä¸Šä¸‹æ–‡ä¸­

        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            max_length: å•ä¸ªæ–‡æ¡£æœ€å¤§é•¿åº¦

        Returns:
            æ ¼å¼åŒ–çš„æ–‡æ¡£å­—ç¬¦ä¸²
        """
        if not documents:
            return ""

        lines = ["## Retrieved Schema Context\n"]
        lines.append(f"Found {len(documents)} relevant table(s):\n")

        for i, doc in enumerate(documents, 1):
            lines.append(f"### Document {i}")

            # å…ƒæ•°æ®
            if doc.metadata:
                table_name = doc.metadata.get("table_name", "Unknown")
                lines.append(f"**Table**: {table_name}")

                source = doc.metadata.get("source", "schema")
                if source:
                    lines.append(f"**Source**: {source}")

            # ç›¸å…³æ€§åˆ†æ•°
            if doc.score is not None:
                lines.append(f"**Relevance**: {doc.score:.2%}")

            # å†…å®¹ï¼ˆæˆªæ–­ï¼‰
            content = doc.content
            if len(content) > max_length:
                content = content[:max_length] + "...\n[truncated]"

            lines.append(f"\n{content}\n")

        lines.append("---\n")
        lines.append("Please use the above schema information to answer the question.\n")

        return "\n".join(lines)

    def set_stage(self, stage: ExecutionStage):
        """è®¾ç½®å½“å‰æ‰§è¡Œé˜¶æ®µ"""
        self.current_stage = stage
        logger.info(f"ğŸ”„ [SchemaContextRetriever] åˆ‡æ¢åˆ°é˜¶æ®µ: {stage.value}")

    def clear_stage_cache(self):
        """æ¸…é™¤é˜¶æ®µæ„ŸçŸ¥ç¼“å­˜"""
        self.stage_context_cache.clear()
        logger.info("ğŸ§¹ [SchemaContextRetriever] æ¸…é™¤é˜¶æ®µæ„ŸçŸ¥ç¼“å­˜")
    
    def _sanitize_query(self, query: str) -> str:
        """
        å‡€åŒ–æŸ¥è¯¢æ–‡æœ¬ï¼Œåªä¿ç•™ä¸šåŠ¡éœ€æ±‚
        
        é—®é¢˜ï¼šLoomæ¡†æ¶åœ¨é€’å½’è°ƒç”¨æ—¶ä¼šå°†æ•´ä¸ªpromptï¼ˆåŒ…æ‹¬ç³»ç»ŸæŒ‡ä»¤ï¼‰ä¼ é€’ç»™context_retriever
        è§£å†³ï¼šæå–ä¸šåŠ¡éœ€æ±‚éƒ¨åˆ†ï¼Œå»é™¤ç³»ç»ŸæŒ‡ä»¤å’Œé€’å½’æ ‡è®°
        
        Args:
            query: åŸå§‹æŸ¥è¯¢æ–‡æœ¬ï¼ˆå¯èƒ½åŒ…å«ç³»ç»ŸæŒ‡ä»¤ï¼‰
            
        Returns:
            å‡€åŒ–åçš„ä¸šåŠ¡éœ€æ±‚æ–‡æœ¬
        """
        # å®‰å…¨çš„å ä½ç¬¦æ–‡æœ¬ï¼ˆå¦‚æœæ— æ³•æå–ä¸šåŠ¡éœ€æ±‚æ—¶ä½¿ç”¨ï¼‰
        DEFAULT_PLACEHOLDER_TEXT = "è¯·åŸºäºæœ€è¿‘çš„ä¸šåŠ¡éœ€æ±‚æ£€ç´¢åŸºç¡€è¡¨ç»“æ„"
        
        # å¦‚æœæŸ¥è¯¢æ–‡æœ¬è¾ƒçŸ­ï¼ˆ<200å­—ç¬¦ï¼‰ï¼Œå¯èƒ½æ˜¯çº¯ä¸šåŠ¡éœ€æ±‚ï¼Œç›´æ¥è¿”å›
        if len(query) < 200:
            logger.debug(f"âœ… [Query Sanitization] æŸ¥è¯¢è¾ƒçŸ­ï¼Œç›´æ¥ä½¿ç”¨: {query[:50]}...")
            return query
        
        # æ ‡è®°éœ€è¦è¿‡æ»¤çš„ç³»ç»Ÿå…³é”®è¯
        system_keywords = [
            "# ç³»ç»ŸæŒ‡ä»¤",
            "# SYSTEM INSTRUCTIONS", 
            "TTé€’å½’",
            "ç»§ç»­å¤„ç†ä»»åŠ¡ï¼š",
            "ä½ æ˜¯ä¸€ä¸ª",
            "## å…³é”®è¦æ±‚",
            "## è´¨é‡æ ‡å‡†",
            "## Doris SQLç¤ºä¾‹",
            "## é‡è¦åŸåˆ™"
        ]
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç³»ç»ŸæŒ‡ä»¤æ ‡è®°
        contains_system_instructions = any(keyword in query for keyword in system_keywords)
        
        if not contains_system_instructions:
            # ä¸åŒ…å«ç³»ç»ŸæŒ‡ä»¤ï¼Œè¿”å›å‰512å­—ç¬¦ï¼ˆé™åˆ¶é•¿åº¦ï¼‰
            logger.debug(f"âœ… [Query Sanitization] ä¸åŒ…å«ç³»ç»ŸæŒ‡ä»¤ï¼Œæˆªå–å‰512å­—ç¬¦")
            return query[:512]
        
        # åŒ…å«ç³»ç»ŸæŒ‡ä»¤ï¼Œå°è¯•æå–ä¸šåŠ¡éœ€æ±‚
        # ç­–ç•¥1ï¼šæå–"# ä»»åŠ¡æè¿°"æˆ–"## ä»»åŠ¡æè¿°"ä¹‹åçš„å†…å®¹
        task_markers = ["# ä»»åŠ¡æè¿°", "## ä»»åŠ¡æè¿°", "# USER", "ç”¨æˆ·æŸ¥è¯¢", "ä¸šåŠ¡éœ€æ±‚", "å ä½ç¬¦"]
        for marker in task_markers:
            if marker in query:
                parts = query.split(marker, 1)
                if len(parts) > 1:
                    extracted = parts[1].strip()
                    # åªä¿ç•™å‰512å­—ç¬¦çš„ä¸šåŠ¡éœ€æ±‚
                    if len(extracted) > 512:
                        extracted = extracted[:512]
                    logger.info(f"âœ… [Query Sanitization] æå–ä»»åŠ¡æè¿°: {extracted[:100]}...")
                    return extracted
        
        # ç­–ç•¥2ï¼šå¦‚æœæŸ¥è¯¢ä¸­æœ‰å¤šä¸ª"ç»§ç»­å¤„ç†ä»»åŠ¡ï¼š"ï¼Œæå–æœ€åä¸€ä¸ª
        if "ç»§ç»­å¤„ç†ä»»åŠ¡ï¼š" in query:
            parts = query.split("ç»§ç»­å¤„ç†ä»»åŠ¡ï¼š")
            if len(parts) > 1:
                # è·å–æœ€åä¸€ä¸ªéƒ¨åˆ†ï¼ˆæœ€å¯èƒ½æ˜¯ä¸šåŠ¡éœ€æ±‚ï¼‰
                last_part = parts[-1].strip()
                # æ¸…ç†ç³»ç»ŸæŒ‡ä»¤å…³é”®è¯
                for keyword in system_keywords:
                    if keyword in last_part:
                        last_part = last_part.split(keyword)[0].strip()
                # é™åˆ¶é•¿åº¦
                if len(last_part) > 512:
                    last_part = last_part[:512]
                logger.info(f"âœ… [Query Sanitization] æå–æœ€åä¸€ä¸ªä»»åŠ¡: {last_part[:100]}...")
                return last_part
        
        # ç­–ç•¥3ï¼šå¦‚æœéƒ½æ— æ³•æå–ï¼Œè®°å½•è­¦å‘Šå¹¶ä½¿ç”¨é»˜è®¤å ä½ç¬¦
        logger.warning(f"âš ï¸ [Query Sanitization] æ— æ³•æå–ä¸šåŠ¡éœ€æ±‚ï¼Œä½¿ç”¨é»˜è®¤å ä½ç¬¦")
        logger.debug(f"âš ï¸ [Query Sanitization] åŸå§‹æŸ¥è¯¢å‰500å­—ç¬¦: {query[:500]}")
        return DEFAULT_PLACEHOLDER_TEXT

    async def _filter_tables_by_name(self, query: str, table_names: List[str]) -> List[str]:
        """
        åŸºäºå¤§æ¨¡å‹çš„æ™ºèƒ½è¡¨ååŒ¹é…

        å°†å ä½ç¬¦å’Œè¡¨ååˆ—è¡¨äº¤ç»™å¤§æ¨¡å‹ï¼Œè®©å®ƒæ™ºèƒ½é€‰æ‹©æœ€ç›¸å…³çš„è¡¨
        å¦‚æœå¤§æ¨¡å‹å¤±è´¥ï¼Œç›´æ¥æŠ›å‡ºå¼‚å¸¸
        """
        if not table_names:
            logger.warning("âš ï¸ è¡¨ååˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•è¿›è¡ŒåŒ¹é…")
            return []
        
        logger.info(f"ğŸ” [æ™ºèƒ½è¡¨ååŒ¹é…] æŸ¥è¯¢: {query[:50]}...")
        logger.info(f"ğŸ” [æ™ºèƒ½è¡¨ååŒ¹é…] å¯ç”¨è¡¨å: {table_names}")
        
        # ä½¿ç”¨å¤§æ¨¡å‹è¿›è¡Œè¡¨ååŒ¹é…
        matched_tables = await self._llm_table_matching(query, table_names)
        
        if not matched_tables:
            raise Exception(f"å¤§æ¨¡å‹è¡¨ååŒ¹é…å¤±è´¥ï¼šæ— æ³•ä¸ºæŸ¥è¯¢ '{query}' æ‰¾åˆ°ç›¸å…³è¡¨")
        
        logger.info(f"âœ… [æ™ºèƒ½è¡¨ååŒ¹é…] åŒ¹é…ç»“æœ: {matched_tables}")
        return matched_tables

    async def _llm_table_matching(self, query: str, table_names: List[str]) -> List[str]:
        """
        ä½¿ç”¨å¤§æ¨¡å‹è¿›è¡Œè¡¨ååŒ¹é…
        
        Args:
            query: å ä½ç¬¦æŸ¥è¯¢
            table_names: å¯ç”¨è¡¨ååˆ—è¡¨
            
        Returns:
            åŒ¹é…çš„è¡¨ååˆ—è¡¨
        """
        # æ„å»ºæç¤ºè¯
        prompt = self._build_table_matching_prompt(query, table_names)
        
        # è·å–LLMé€‚é…å™¨
        from .llm_adapter import create_llm_adapter
        llm_adapter = create_llm_adapter(self.container)
        
        # è°ƒç”¨å¤§æ¨¡å‹
        response = await llm_adapter.generate_response(
            messages=[{"role": "user", "content": prompt}],
            max_tokens=500,
            temperature=0.1  # ä½æ¸©åº¦ç¡®ä¿ä¸€è‡´æ€§
        )
        
        # è§£æå“åº”
        matched_tables = self._parse_table_matching_response(response, table_names)
        
        return matched_tables

    def _build_table_matching_prompt(self, query: str, table_names: List[str]) -> str:
        """æ„å»ºè¡¨ååŒ¹é…çš„æç¤ºè¯"""
        table_list = "\n".join([f"- {name}" for name in table_names])
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ•°æ®åº“ä¸“å®¶ï¼Œéœ€è¦æ ¹æ®ç”¨æˆ·çš„æŸ¥è¯¢éœ€æ±‚ï¼Œä»ç»™å®šçš„è¡¨ååˆ—è¡¨ä¸­é€‰æ‹©æœ€ç›¸å…³çš„è¡¨ã€‚

ç”¨æˆ·æŸ¥è¯¢ï¼š{query}

å¯ç”¨è¡¨ååˆ—è¡¨ï¼š
{table_list}

è¯·åˆ†æç”¨æˆ·æŸ¥è¯¢çš„ä¸šåŠ¡éœ€æ±‚ï¼Œé€‰æ‹©æœ€ç›¸å…³çš„è¡¨åã€‚è€ƒè™‘ä»¥ä¸‹å› ç´ ï¼š
1. æŸ¥è¯¢çš„ä¸šåŠ¡é¢†åŸŸï¼ˆç”¨æˆ·ã€è®¢å•ã€äº§å“ã€é”€å”®ç­‰ï¼‰
2. è¡¨åçš„è¯­ä¹‰å«ä¹‰
3. æŸ¥è¯¢å¯èƒ½æ¶‰åŠçš„æ•°æ®ç±»å‹

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼š
{{
    "matched_tables": ["table1", "table2", "table3"],
    "reasoning": "é€‰æ‹©è¿™äº›è¡¨çš„åŸå› è¯´æ˜"
}}

æ³¨æ„ï¼š
- æœ€å¤šé€‰æ‹©5ä¸ªè¡¨
- è¡¨åå¿…é¡»å®Œå…¨åŒ¹é…åˆ—è¡¨ä¸­çš„åç§°
- å¦‚æœæŸ¥è¯¢ä¸æ˜ç¡®ï¼Œé€‰æ‹©æœ€å¯èƒ½ç›¸å…³çš„è¡¨
- ä¼˜å…ˆé€‰æ‹©æ ¸å¿ƒä¸šåŠ¡è¡¨"""
        
        return prompt

    def _parse_table_matching_response(self, response: str, available_tables: List[str]) -> List[str]:
        """è§£æå¤§æ¨¡å‹çš„è¡¨ååŒ¹é…å“åº”"""
        import json
        import re
        
        # å°è¯•æå–JSON
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            json_str = json_match.group()
            result = json.loads(json_str)
            
            matched_tables = result.get("matched_tables", [])
            reasoning = result.get("reasoning", "")
            
            logger.info(f"ğŸ§  [LLMæ¨ç†] {reasoning}")
            
            # éªŒè¯è¡¨åæ˜¯å¦åœ¨å¯ç”¨åˆ—è¡¨ä¸­
            valid_tables = []
            for table in matched_tables:
                if table in available_tables:
                    valid_tables.append(table)
                else:
                    logger.warning(f"âš ï¸ å¤§æ¨¡å‹è¿”å›äº†æ— æ•ˆè¡¨å: {table}")
            
            return valid_tables
        else:
            # å¦‚æœæ²¡æœ‰JSONæ ¼å¼ï¼Œå°è¯•ç›´æ¥æå–è¡¨å
            matched_tables = []
            for table in available_tables:
                if table.lower() in response.lower():
                    matched_tables.append(table)
            
            return matched_tables

    async def _validate_and_refine_tables(self, query: str, matched_tables: List[str], table_columns: Dict[str, List[Dict]]) -> List[str]:
        """
        éªŒè¯è¡¨ååŒ¹é…ç»“æœï¼ŒåŸºäºåˆ—ä¿¡æ¯è¿›è¡Œè¿­ä»£ä¼˜åŒ–
        
        Args:
            query: åŸå§‹æŸ¥è¯¢
            matched_tables: åˆæ­¥åŒ¹é…çš„è¡¨å
            table_columns: è¡¨çš„åˆ—ä¿¡æ¯
            
        Returns:
            ä¼˜åŒ–åçš„è¡¨ååˆ—è¡¨
        """
        if not matched_tables or not table_columns:
            return matched_tables
        
        logger.info(f"ğŸ” [è¡¨åéªŒè¯] éªŒè¯è¡¨: {matched_tables}")
        
        # æ„å»ºåˆ—ä¿¡æ¯éªŒè¯æç¤ºè¯
        prompt = self._build_column_validation_prompt(query, matched_tables, table_columns)
        
        # è·å–LLMé€‚é…å™¨
        from .llm_adapter import create_llm_adapter
        llm_adapter = create_llm_adapter(self.container)
        
        # è°ƒç”¨å¤§æ¨¡å‹è¿›è¡ŒéªŒè¯
        response = await llm_adapter.generate_response(
            messages=[{"role": "user", "content": prompt}],
            max_tokens=800,
            temperature=0.1
        )
        
        # è§£æéªŒè¯ç»“æœ
        refined_tables = self._parse_column_validation_response(response, matched_tables)
        
        if refined_tables != matched_tables:
            logger.info(f"ğŸ”„ [è¡¨åä¼˜åŒ–] åŸå§‹: {matched_tables} -> ä¼˜åŒ–å: {refined_tables}")
        
        return refined_tables

    def _build_column_validation_prompt(self, query: str, matched_tables: List[str], table_columns: Dict[str, List[Dict]]) -> str:
        """æ„å»ºåˆ—ä¿¡æ¯éªŒè¯çš„æç¤ºè¯"""
        table_info = ""
        for table_name in matched_tables:
            if table_name in table_columns:
                columns = table_columns[table_name]
                column_list = "\n".join([f"  - {col.get('name', '')} ({col.get('data_type', '')})" for col in columns[:10]])  # é™åˆ¶æ˜¾ç¤ºå‰10åˆ—
                table_info += f"\nè¡¨ {table_name}:\n{column_list}\n"
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ•°æ®åº“ä¸“å®¶ï¼Œéœ€è¦éªŒè¯è¡¨ååŒ¹é…çš„å‡†ç¡®æ€§ã€‚

ç”¨æˆ·æŸ¥è¯¢ï¼š{query}

åˆæ­¥åŒ¹é…çš„è¡¨åï¼š{matched_tables}

è¿™äº›è¡¨çš„åˆ—ä¿¡æ¯ï¼š
{table_info}

è¯·åˆ†æï¼š
1. è¿™äº›è¡¨æ˜¯å¦çœŸçš„ä¸ç”¨æˆ·æŸ¥è¯¢ç›¸å…³ï¼Ÿ
2. åˆ—ä¿¡æ¯æ˜¯å¦æ”¯æŒç”¨æˆ·æŸ¥è¯¢çš„éœ€æ±‚ï¼Ÿ
3. æ˜¯å¦éœ€è¦è°ƒæ•´è¡¨çš„é€‰æ‹©ï¼Ÿ

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼š
{{
    "is_accurate": true/false,
    "refined_tables": ["table1", "table2"],
    "reasoning": "éªŒè¯å’Œè°ƒæ•´çš„åŸå› è¯´æ˜",
    "confidence": 0.8
}}

æ³¨æ„ï¼š
- å¦‚æœè¡¨é€‰æ‹©å‡†ç¡®ï¼Œis_accurateè®¾ä¸ºtrue
- å¦‚æœéœ€è¦è°ƒæ•´ï¼Œrefined_tablesåŒ…å«ä¼˜åŒ–åçš„è¡¨å
- confidenceè¡¨ç¤ºåŒ¹é…çš„ç½®ä¿¡åº¦ï¼ˆ0-1ï¼‰"""
        
        return prompt

    def _parse_column_validation_response(self, response: str, original_tables: List[str]) -> List[str]:
        """è§£æåˆ—ä¿¡æ¯éªŒè¯çš„å“åº”"""
        import json
        import re
        
        # å°è¯•æå–JSON
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            json_str = json_match.group()
            result = json.loads(json_str)
            
            is_accurate = result.get("is_accurate", True)
            refined_tables = result.get("refined_tables", original_tables)
            reasoning = result.get("reasoning", "")
            confidence = result.get("confidence", 0.5)
            
            logger.info(f"ğŸ§  [åˆ—ä¿¡æ¯éªŒè¯] å‡†ç¡®æ€§: {is_accurate}, ç½®ä¿¡åº¦: {confidence}")
            logger.info(f"ğŸ§  [åˆ—ä¿¡æ¯éªŒè¯] æ¨ç†: {reasoning}")
            
            return refined_tables if refined_tables else original_tables
        else:
            # å¦‚æœæ²¡æœ‰JSONæ ¼å¼ï¼Œè¿”å›åŸå§‹è¡¨å
            logger.warning("âš ï¸ æ— æ³•è§£æéªŒè¯å“åº”ï¼Œä¿æŒåŸå§‹è¡¨å")
            return original_tables

    async def _basic_retrieve(
        self,
        query: str,
        top_k: int,
        candidate_tables: List[str]
    ) -> List[Document]:
        """
        åŸºç¡€æ£€ç´¢ç­–ç•¥

        å½“æ™ºèƒ½æ£€ç´¢ä¸å¯ç”¨æ—¶ï¼Œä½¿ç”¨ç®€å•çš„å…³é”®è¯åŒ¹é…
        """
        documents = []
        query_lower = query.lower()

        for table_name in candidate_tables[:top_k]:
            if table_name not in self.schema_cache:
                # å¦‚æœè¡¨ç»“æ„æœªåŠ è½½ï¼Œåˆ›å»ºä¸€ä¸ªåŸºç¡€æ–‡æ¡£
                content = f"è¡¨å: {table_name}\nè¯´æ˜: è¡¨ç»“æ„ä¿¡æ¯å°†åœ¨éœ€è¦æ—¶åŠ¨æ€åŠ è½½"
                doc = Document(
                    content=content,
                    metadata={
                        "source": "schema",
                        "table_name": table_name,
                        "data_source_id": self.data_source_id,
                        "relevance_score": 0.5,
                        "stage": self.current_stage.value,
                        "retrieval_method": "basic",
                        "lazy_loaded": True
                    },
                    score=0.5
                )
            else:
                # ä½¿ç”¨å·²åŠ è½½çš„è¡¨ç»“æ„
                table_info = self.schema_cache[table_name]
                content = self._format_table_info(table_name, table_info)
                doc = Document(
                    content=content,
                    metadata={
                        "source": "schema",
                        "table_name": table_name,
                        "data_source_id": self.data_source_id,
                        "relevance_score": 0.7,
                        "stage": self.current_stage.value,
                        "retrieval_method": "basic",
                        "lazy_loaded": False
                    },
                    score=0.7
                )

            documents.append(doc)

        return documents

    def _format_table_info(self, table_name: str, table_info: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–è¡¨ä¿¡æ¯ä¸ºæ–‡æœ¬"""
        columns = table_info.get('columns', [])

        # æ„å»ºåˆ—ä¿¡æ¯
        column_lines = []
        for col in columns:
            col_name = col.get('name', '')
            col_type = col.get('type', '')
            col_comment = col.get('comment', '')

            if col_comment:
                column_lines.append(f"  - {col_name} ({col_type}): {col_comment}")
            else:
                column_lines.append(f"  - {col_name} ({col_type})")

        # æ„å»ºå®Œæ•´è¡¨ä¿¡æ¯
        content = f"è¡¨å: {table_name}\n"
        content += f"ç±»å‹: {table_info.get('table_type', 'TABLE')}\n"
        content += f"åˆ—æ•°: {len(columns)}\n"

        if column_lines:
            content += "åˆ—ä¿¡æ¯:\n" + "\n".join(column_lines)

        return content

    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "lazy_loading_enabled": self.enable_lazy_loading,
            "total_tables": len(self.table_names),
            "loaded_tables": len(self.loaded_tables),
            "cache_size": len(self.schema_cache),
            "stage_cache_size": len(self.stage_context_cache),
            "intelligent_retrieval_enabled": self._intelligent_retriever is not None
        }


class IntelligentContextRetriever(BaseContextRetriever):
    """
    æ™ºèƒ½ä¸Šä¸‹æ–‡æ£€ç´¢å™¨
    
    åŸºäº SchemaContextRetriever çš„é«˜çº§å°è£…ï¼Œæä¾›æ›´æ™ºèƒ½çš„ä¸Šä¸‹æ–‡ç®¡ç†
    """

    def __init__(
        self,
        schema_retriever: SchemaContextRetriever,
        enable_context_caching: bool = True,
        max_cache_size: int = 100
    ):
        """
        Args:
            schema_retriever: Schema æ£€ç´¢å™¨å®ä¾‹
            enable_context_caching: æ˜¯å¦å¯ç”¨ä¸Šä¸‹æ–‡ç¼“å­˜
            max_cache_size: æœ€å¤§ç¼“å­˜å¤§å°
        """
        self.schema_retriever = schema_retriever
        self.enable_context_caching = enable_context_caching
        self.max_cache_size = max_cache_size
        
        # ä¸Šä¸‹æ–‡ç¼“å­˜
        self.context_cache: Dict[str, ContextInfo] = {}
        self.query_cache: Dict[str, List[str]] = {}

    async def retrieve(
        self, 
        query: str, 
        context_type: str = "schema",
        top_k: Optional[int] = None
    ) -> List[str]:
        """
        æ£€ç´¢ä¸Šä¸‹æ–‡ä¿¡æ¯
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            context_type: ä¸Šä¸‹æ–‡ç±»å‹ï¼ˆschema, data, businessç­‰ï¼‰
            top_k: è¿”å›æ•°é‡
            
        Returns:
            ä¸Šä¸‹æ–‡ä¿¡æ¯å­—ç¬¦ä¸²åˆ—è¡¨
        """
        logger.info(f"ğŸ” [IntelligentContextRetriever] æ£€ç´¢ä¸Šä¸‹æ–‡: {context_type}")
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"{query}_{context_type}_{top_k}"
        if self.enable_context_caching and cache_key in self.query_cache:
            logger.info("âœ… ä½¿ç”¨æŸ¥è¯¢ç¼“å­˜")
            return self.query_cache[cache_key]

        # æ ¹æ®ä¸Šä¸‹æ–‡ç±»å‹é€‰æ‹©æ£€ç´¢ç­–ç•¥
        if context_type == "schema":
            documents = await self.schema_retriever.retrieve(query, top_k)
            context_strings = [doc.content for doc in documents]
        else:
            # å…¶ä»–ç±»å‹çš„ä¸Šä¸‹æ–‡æ£€ç´¢ï¼ˆå¯æ‰©å±•ï¼‰
            context_strings = []

        # æ›´æ–°ç¼“å­˜
        if self.enable_context_caching:
            self.query_cache[cache_key] = context_strings
            # é™åˆ¶ç¼“å­˜å¤§å°
            if len(self.query_cache) > self.max_cache_size:
                oldest_key = next(iter(self.query_cache))
                del self.query_cache[oldest_key]

        return context_strings

    async def update_context(self, context: ContextInfo) -> None:
        """
        æ›´æ–°ä¸Šä¸‹æ–‡ç¼“å­˜
        
        Args:
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
        """
        logger.info("ğŸ”„ [IntelligentContextRetriever] æ›´æ–°ä¸Šä¸‹æ–‡ç¼“å­˜")

        # æ›´æ–° Schema ä¿¡æ¯
        if context.tables:
            for table in context.tables:
                table_name = table.get('table_name', '')
                if table_name:
                    self.schema_retriever.schema_cache[table_name] = table

        # æ›´æ–°å…¶ä»–ä¸Šä¸‹æ–‡ä¿¡æ¯
        # TODO: å®ç°å…¶ä»–ç±»å‹çš„ä¸Šä¸‹æ–‡æ›´æ–°


def create_schema_context_retriever(
    data_source_id: str,
    connection_config: Dict[str, Any],
    container: Any,
    top_k: int = 5,
    # å…¼å®¹è°ƒç”¨æ–¹ä¼ å…¥ï¼Œä½†æœ¬å®ç°ä¸ç›´æ¥ä½¿ç”¨
    inject_as: Optional[str] = None,
    enable_stage_aware: bool = True,
    enable_lazy_loading: bool = True
) -> SchemaContextRetriever:
    """
    åˆ›å»º Schema ä¸Šä¸‹æ–‡æ£€ç´¢å™¨
    
    Args:
        data_source_id: æ•°æ®æºID
        connection_config: è¿æ¥é…ç½®
        container: æœåŠ¡å®¹å™¨
        top_k: é»˜è®¤è¿”å›è¡¨æ•°é‡
        enable_stage_aware: æ˜¯å¦å¯ç”¨é˜¶æ®µæ„ŸçŸ¥
        enable_lazy_loading: æ˜¯å¦å¯ç”¨æ‡’åŠ è½½ä¼˜åŒ–
        
    Returns:
        SchemaContextRetriever å®ä¾‹
    """
    return SchemaContextRetriever(
        data_source_id=data_source_id,
        connection_config=connection_config,
        container=container,
        top_k=top_k,
        enable_stage_aware=enable_stage_aware,
        enable_lazy_loading=enable_lazy_loading
    )


def create_intelligent_context_retriever(
    schema_retriever: SchemaContextRetriever,
    enable_context_caching: bool = True
) -> IntelligentContextRetriever:
    """
    åˆ›å»ºæ™ºèƒ½ä¸Šä¸‹æ–‡æ£€ç´¢å™¨
    
    Args:
        schema_retriever: Schema æ£€ç´¢å™¨
        enable_context_caching: æ˜¯å¦å¯ç”¨ç¼“å­˜
        
    Returns:
        IntelligentContextRetriever å®ä¾‹
    """
    return IntelligentContextRetriever(
        schema_retriever=schema_retriever,
        enable_context_caching=enable_context_caching
    )


# å¯¼å‡º
__all__ = [
    "SchemaContextRetriever",
    "IntelligentContextRetriever", 
    "create_schema_context_retriever",
    "create_intelligent_context_retriever",
]