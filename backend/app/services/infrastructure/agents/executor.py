"""
æ­¥éª¤æ‰§è¡Œå™¨

æ‰§è¡Œè®¡åˆ’ä¸­çš„å·¥å…·æ­¥éª¤åºåˆ—
ç»´æŠ¤æ‰§è¡Œä¸Šä¸‹æ–‡å¹¶äº§ç”Ÿè§‚å¯Ÿè®°å½•
æ”¯æŒç®€å•çš„é‡è¯•å’Œé”™è¯¯å¤„ç†
"""

import time
import logging
from typing import Any, Dict, List, Optional
import structlog

from .types import AgentInput
from .tools.registry import ToolRegistry
from .auth_context import auth_manager
from .config_context import config_manager
from .sql_generation import SQLGenerationCoordinator, SQLGenerationConfig


class StepExecutor:
    """æ­¥éª¤æ‰§è¡Œå™¨"""

    def __init__(self, container) -> None:
        """
        åˆå§‹åŒ–æ‰§è¡Œå™¨

        Args:
            container: backupç³»ç»Ÿçš„æœåŠ¡å®¹å™¨
        """
        self.container = container
        self._logger = logging.getLogger(self.__class__.__name__)
        # ç»“æ„åŒ–æ—¥å¿—è®°å½•å™¨
        self._struct_logger = structlog.get_logger(self.__class__.__name__)
        self.registry = ToolRegistry()
        self._setup_tools()
        self._sql_generation_config = SQLGenerationConfig()
        self._sql_coordinator: Optional[SQLGenerationCoordinator] = None
        # é«˜å¯ç”¨ï¼šå·¥å…·è°ƒç”¨é‡è¯•é…ç½®
        self.max_tool_retries = 2
        self.retry_backoff_base = 0.5  # seconds
        # æ€§èƒ½ç»Ÿè®¡
        self._execution_stats = {
            "total_executions": 0,
            "successful_executions": 0,
            "failed_executions": 0,
            "total_execution_time_ms": 0
        }

    def _setup_tools(self) -> None:
        """è®¾ç½®å’Œæ³¨å†Œå·¥å…·"""
        # å¯¼å…¥æ ¸å¿ƒå·¥å…· (ç¨ååˆ›å»º) - SQLDraftToolå·²åˆ é™¤
        from .tools.sql_tools import SQLValidateTool, SQLExecuteTool, SQLRefineTool, SQLPolicyTool
        from .tools.schema_tools import SchemaListColumnsTool, SchemaListTablesTool, SchemaGetColumnsTool
        from .tools.chart_tools import ChartSpecTool, WordChartGeneratorTool
        from .tools.time_tools import TimeWindowTool
        from .tools.data_quality_tools import DataQualityTool
        from .tools.workflow_tools import StatBasicWorkflowTool, StatRatioWorkflowTool, StatCategoryMixWorkflowTool

        # æ³¨å†ŒåŸºç¡€å·¥å…· - ç§»é™¤SQLDraftToolæ³¨å†Œ
        self.registry.register(SchemaListTablesTool(self.container))
        self.registry.register(SchemaListColumnsTool(self.container))
        self.registry.register(SchemaGetColumnsTool(self.container))
        # ä¸å†æ³¨å†Œç»Ÿä¸€æŸ¥è¯¢å·¥å…·ï¼Œé‡‡ç”¨ä¸¤æ­¥Schemaï¼ˆlist_tables â†’ get_columnsï¼‰
        self.registry.register(SQLValidateTool(self.container))
        self.registry.register(SQLRefineTool(self.container))
        self.registry.register(SQLExecuteTool(self.container))
        self.registry.register(SQLPolicyTool(self.container))
        self.registry.register(ChartSpecTool(self.container))
        self.registry.register(WordChartGeneratorTool(self.container))
        self.registry.register(TimeWindowTool(self.container))
        self.registry.register(DataQualityTool(self.container))
        # å·¥ä½œæµå·¥å…·ï¼ˆPTOF å¤åˆå·¥å…·ï¼‰
        self.registry.register(StatBasicWorkflowTool(self.container))
        self.registry.register(StatRatioWorkflowTool(self.container))
        self.registry.register(StatCategoryMixWorkflowTool(self.container))

        self._logger.info(f"å·²æ³¨å†Œ {len(self.registry._tools)} ä¸ªå·¥å…·")

    def _get_sql_coordinator(self) -> Optional[SQLGenerationCoordinator]:
        """Lazily instantiate the SQL generation coordinator."""
        if self._sql_coordinator is not None:
            return self._sql_coordinator

        llm_client = getattr(self.container, "llm_service", None) or getattr(self.container, "llm", None)
        if not llm_client:
            self._logger.warning("â³ [SQLCoordinator] ç¼ºå°‘LLMæœåŠ¡ï¼Œæ— æ³•å¯ç”¨æ–°SQLç”Ÿæˆç®¡çº¿")
            self._sql_coordinator = None
            return None

        db_connector = getattr(self.container, "data_source", None) or getattr(self.container, "db_connector", None)
        config = self._sql_generation_config

        try:
            self._sql_coordinator = SQLGenerationCoordinator(
                self.container,
                llm_client=llm_client,
                db_connector=db_connector,
                config=config,
            )
        except Exception as exc:
            self._logger.error(f"âŒ [SQLCoordinator] åˆå§‹åŒ–å¤±è´¥: {exc}")
            self._sql_coordinator = None
        return self._sql_coordinator

    def _should_use_sql_coordinator(self, ai: AgentInput, context: Dict[str, Any]) -> bool:
        """Determine whether the new SQL coordinator should handle the request."""
        try:
            tdc = getattr(ai, "task_driven_context", {}) or {}
            if isinstance(tdc, dict) and tdc.get("force_sql_generation_coordinator"):
                return True

            user_id = ai.user_id or auth_manager.get_current_user_id()
            config = config_manager.get_config(user_id)
            custom_settings = getattr(config, "custom_settings", {}) or {}
            flag_key = self._sql_generation_config.feature_flag_key
            return bool(custom_settings.get(flag_key))
        except Exception:
            return False

    async def _generate_sql_with_coordinator(
        self,
        ai: AgentInput,
        context: Dict[str, Any],
        user_id: str,
        observations: List[str],
    ) -> Optional[Dict[str, Any]]:
        """Run the coordinator-based SQL generation pipeline."""
        coordinator = self._get_sql_coordinator()
        if not coordinator:
            return None

        query_text = (
            context.get("placeholder_description")
            or getattr(getattr(ai, "placeholder", None), "description", "")
            or ai.user_prompt
        )
        if not query_text:
            query_text = "ç”Ÿæˆç»Ÿè®¡åˆ†æSQL"

        snapshot = {
            "time_context": context.get("time_context"),
            "window": context.get("window"),
            "start_date": context.get("start_date"),
            "end_date": context.get("end_date"),
            "column_details": context.get("column_details"),
            "columns": context.get("columns"),
            "selected_tables": context.get("selected_tables"),
            "tables": context.get("tables"),
            "data_source": context.get("data_source"),
            "user_id": user_id,
            "semantic_type": context.get("semantic_type"),
        }

        try:
            coordinator_result = await coordinator.generate(query_text, snapshot)
        except Exception as exc:
            self._logger.error(f"âŒ [SQLCoordinator] æ‰§è¡Œå¼‚å¸¸: {exc}", exc_info=True)
            return {
                "success": False,
                "action": "sql_generation",
                "error": f"sql_coordinator_exception: {exc}",
            }

        if coordinator_result.success:
            context["current_sql"] = coordinator_result.sql
            context["sql"] = coordinator_result.sql
            context["coordinator_metadata"] = coordinator_result.metadata
            observations.append("âœ… SQLGenerationCoordinator ç”Ÿæˆæœ‰æ•ˆSQL")
            return {
                "success": True,
                "action": "sql_generation",
                "current_sql": coordinator_result.sql,
                "sql": coordinator_result.sql,
                "message": "SQLå·²ç”Ÿæˆ",
                "next_step_hint": "è°ƒç”¨sql.validateéªŒè¯SQL",
                "coordinator_metadata": coordinator_result.metadata,
            }

        if coordinator_result.needs_user_input:
            observations.append("âš ï¸ SQLGenerationCoordinator éœ€è¦é¢å¤–è¾“å…¥")
            return {
                "success": False,
                "action": "sql_generation",
                "error": coordinator_result.error or "user_input_required",
                "message": coordinator_result.error,
                "suggestions": coordinator_result.suggestions,
            }

        observations.append("âŒ SQLGenerationCoordinator ç”Ÿæˆå¤±è´¥")
        return {
            "success": False,
            "action": "sql_generation",
            "error": coordinator_result.error or "generation_failed",
            "debug_info": coordinator_result.debug_info,
        }

    async def _load_data_source_config(
        self,
        ai: AgentInput,
        user_id: str,
        initial_ds: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """
        ç»Ÿä¸€çš„æ•°æ®æºé…ç½®åŠ è½½é€»è¾‘

        Args:
            ai: Agentè¾“å…¥
            user_id: ç”¨æˆ·ID
            initial_ds: åˆå§‹æ•°æ®æºå­—å…¸

        Returns:
            Dict: æ•°æ®æºè¿æ¥é…ç½®
        """
        ds = initial_ds or {}

        # è§„èŒƒåŒ–æ•°æ®æºï¼ˆå…¼å®¹ data_source_id â†’ idï¼‰
        try:
            if ds and isinstance(ds, dict) and ("data_source_id" in ds) and ("id" not in ds):
                ds = {**ds, "id": ds.get("data_source_id")}
        except Exception:
            pass

        # å¦‚æœæœªæä¾›è¿æ¥é…ç½®ï¼Œå°è¯•åŸºäº user_id + data_source_id è‡ªåŠ¨åŠ è½½
        try:
            if (not isinstance(ds, dict)) or (not ds) or ("source_type" not in ds and "connection_string" not in ds and "fe_hosts" not in ds):
                # ğŸ” ä¼˜å…ˆä»ä¼ å…¥çš„ ds ä¸­æå– data_source_id
                ds_id = None
                if isinstance(ds, dict):
                    ds_id = ds.get("data_source_id") or ds.get("id")

                # å¦‚æœä¼ å…¥çš„ ds ä¸­æ²¡æœ‰ï¼Œå†ä» task_driven_context ä¸­æå–
                if not ds_id:
                    tdc = getattr(ai, 'task_driven_context', None)
                    if isinstance(tdc, dict):
                        # æ”¯æŒå¤šç§ä½ç½®ï¼šé¡¶å±‚ data_source_id æˆ– data_source_info å†…
                        ds_id = tdc.get("data_source_id")
                        if not ds_id:
                            dsi = tdc.get("data_source_info") or tdc.get("data_source") or {}
                            if isinstance(dsi, dict):
                                ds_id = dsi.get("id") or dsi.get("data_source_id")

                if ds_id:
                    self._logger.info(f"ğŸ” [Executor] å°è¯•åŠ è½½ data_source_id={ds_id}")
                    try:
                        # ğŸ”§ ä½¿ç”¨containerçš„get_user_data_sourceæ–¹æ³•ï¼Œç¡®ä¿å¯†ç è§£å¯†å’Œé…ç½®å®Œæ•´æ€§
                        has_container_method = hasattr(self.container, 'get_user_data_source')
                        self._logger.info(f"ğŸ” [Executor] Containeræœ‰get_user_data_sourceæ–¹æ³•: {has_container_method}")

                        if has_container_method:
                            # ä½¿ç”¨Containeræä¾›çš„æ–¹æ³•ï¼ˆåŒ…å«å¯†ç è§£å¯†ï¼‰
                            ds_obj = await self.container.get_user_data_source(str(user_id), str(ds_id))
                            if ds_obj and hasattr(ds_obj, 'connection_config'):
                                ds = ds_obj.connection_config
                                self._logger.info(f"ğŸ”Œ [Executor] å·²æ ¹æ® data_source_id={ds_id} åŠ è½½è¿æ¥é…ç½® (via Container, å¯†ç å·²è§£å¯†)")
                                self._logger.info(f"ğŸ” [Executor] åŠ è½½çš„é…ç½®é”®: {list(ds.keys()) if isinstance(ds, dict) else 'Not dict'}")
                                self._logger.info(f"ğŸ” [Executor] source_type={ds.get('source_type') if isinstance(ds, dict) else 'N/A'}")
                        elif hasattr(self.container, 'user_data_source_service'):
                            # å›é€€åˆ°ç›´æ¥ä½¿ç”¨serviceï¼ˆä½†æ³¨æ„å¯†ç å¯èƒ½æœªè§£å¯†ï¼‰
                            self._logger.info(f"ğŸ” [Executor] ä½¿ç”¨user_data_source_serviceå›é€€æ–¹å¼")
                            uds = await self.container.user_data_source_service.get_user_data_source(str(user_id), str(ds_id))
                            if uds and getattr(uds, 'connection_config', None):
                                ds = uds.connection_config
                                self._logger.warning(f"âš ï¸ [Executor] ä½¿ç”¨æ¨¡å‹å±æ€§åŠ è½½é…ç½®ï¼Œå¯†ç å¯èƒ½æœªè§£å¯†")
                    except Exception as e:
                        self._logger.warning(f"âš ï¸ [Executor] è‡ªåŠ¨åŠ è½½æ•°æ®æºé…ç½®å¤±è´¥: {e}")
                        import traceback
                        self._logger.warning(f"âš ï¸ [Executor] å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
        except Exception as e2:
            self._logger.warning(f"âš ï¸ [Executor] å¤–å±‚å¼‚å¸¸: {e2}")
            import traceback
            self._logger.warning(f"âš ï¸ [Executor] å¤–å±‚å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")

        # æœ€ç»ˆæ£€æŸ¥è¿”å›å€¼
        if isinstance(ds, dict):
            self._logger.info(f"ğŸ” [Executor] æœ€ç»ˆè¿”å›é…ç½®é”®: {list(ds.keys())}")
            has_source_type = "source_type" in ds or "connection_string" in ds or "fe_hosts" in ds
            self._logger.info(f"ğŸ” [Executor] åŒ…å«è¿æ¥ä¿¡æ¯: {has_source_type}")
        else:
            self._logger.warning(f"âš ï¸ [Executor] è¿”å›å€¼ä¸æ˜¯å­—å…¸: {type(ds)}")

        return ds

    async def _build_execution_context(
        self,
        ai: AgentInput,
        user_id: str,
        ds: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        æ„å»ºæ‰§è¡Œä¸Šä¸‹æ–‡

        Args:
            ai: Agentè¾“å…¥
            user_id: ç”¨æˆ·ID
            ds: æ•°æ®æºé…ç½®

        Returns:
            Dict: æ‰§è¡Œä¸Šä¸‹æ–‡
        """
        # ğŸ” è°ƒè¯•ï¼šæ£€æŸ¥ä¼ å…¥çš„æ•°æ®æºé…ç½®
        self._logger.info(f"ğŸ” [BuildContext] æ¥æ”¶åˆ°çš„dsç±»å‹: {type(ds)}")
        if isinstance(ds, dict):
            self._logger.info(f"ğŸ” [BuildContext] dsé”®: {list(ds.keys())}")
            self._logger.info(f"ğŸ” [BuildContext] source_typeå­˜åœ¨: {'source_type' in ds}")

        # ä»ä»»åŠ¡ä¸Šä¸‹æ–‡ä¸­æå–å¯é€‰çš„è¯­ä¹‰ä¸å‚æ•°
        semantic_info = self._extract_semantic_info(ai)

        # å°†çº¦æŸä¼ å…¥ä¸Šä¸‹æ–‡
        constraints_dict = None
        try:
            c = ai.constraints
            constraints_dict = {
                "sql_only": c.sql_only,
                "output_kind": c.output_kind,
                "max_attempts": c.max_attempts,
                "policy_row_limit": c.policy_row_limit,
                "quality_min_rows": c.quality_min_rows,
            }
        except Exception:
            constraints_dict = None

        # æ„å»ºåŸºç¡€ä¸Šä¸‹æ–‡
        context = {
            "user_prompt": ai.user_prompt,
            "placeholder_description": ai.placeholder.description,
            "tables": ai.schema.tables,
            "columns": ai.schema.columns,
            "window": ai.context.window,
            "data_source": ds,
            "user_id": user_id,
            "constraints": constraints_dict,
            "semantic_type": semantic_info.get("semantic_type"),
            "top_n": semantic_info.get("top_n"),
            "template_context": None,
        }

        # ğŸ—„ï¸ [ResourcePoolæ¨¡å¼] ä»ContextMemoryè¯»å–çŠ¶æ€ï¼Œè€Œä¸æ˜¯ç›´æ¥è¯»å–å®Œæ•´æ•°æ®
        try:
            tdc = ai.task_driven_context or {}
            if isinstance(tdc, dict):
                self._logger.debug(f"ğŸ“‹ [æ„å»ºä¸Šä¸‹æ–‡] task_driven_contextåŒ…å«çš„é”®: {list(tdc.keys())}")

                # template_context: ç›´æ¥ä¼ é€’ï¼ˆè½»é‡çº§ï¼‰
                if tdc.get("template_context_snippet"):
                    context["template_context"] = tdc.get("template_context_snippet")
                elif tdc.get("template_context"):
                    context["template_context"] = tdc.get("template_context")

                # ğŸ—„ï¸ [ResourcePoolæ¨¡å¼] ä»ContextMemoryè¯»å–çŠ¶æ€
                context_memory_dict = tdc.get("context_memory")
                if context_memory_dict and isinstance(context_memory_dict, dict):
                    from .resource_pool import ContextMemory
                    context_memory = ContextMemory.from_dict(context_memory_dict)

                    # å­˜å‚¨ContextMemoryåˆ°contextï¼ˆç”¨äºåç»­åˆ¤æ–­ï¼‰
                    context["context_memory"] = context_memory

                    # è®°å½•çŠ¶æ€æ—¥å¿—
                    self._logger.info(
                        f"ğŸ—„ï¸ [Executor] ContextMemoryçŠ¶æ€: "
                        f"has_sql={context_memory.has_sql}, "
                        f"schema_available={context_memory.schema_available}, "
                        f"tables={len(context_memory.available_tables)}"
                    )

                    # æ³¨æ„ï¼šä¸å†ä»tdcç›´æ¥è·å–column_details
                    # è¯¦ç»†ä¿¡æ¯å­˜å‚¨åœ¨ResourcePoolä¸­ï¼Œéœ€è¦æ—¶é€šè¿‡_extract_from_resource_poolæŒ‰éœ€æå–
                else:
                    self._logger.warning("âš ï¸ [Executor] task_driven_contextä¸­æ²¡æœ‰context_memory")

        except Exception as e:
            self._logger.error(f"âŒ [Executor] å¤„ç†task_driven_contextå¤±è´¥: {e}")

        return context

    async def execute(self, plan: Dict[str, Any], ai: AgentInput) -> Dict[str, Any]:
        """
        æ‰§è¡Œå•æ­¥éª¤è®¡åˆ’ - Plan-Tool-Active-Validateå¾ªç¯

        Args:
            plan: å•æ­¥éª¤æ‰§è¡Œè®¡åˆ’
            ai: Agentè¾“å…¥ä¸Šä¸‹æ–‡

        Returns:
            Dict: æ‰§è¡Œç»“æœï¼Œæ”¯æŒAgentç»§ç»­å†³ç­–
        """
        steps = plan.get("steps", [])
        if not steps:
            return {"success": False, "error": "no_steps", "context": {}}

        # åªæ‰§è¡Œç¬¬ä¸€ä¸ªæ­¥éª¤ - å•æ­¥éª¤å¾ªç¯åŸåˆ™
        step = steps[0]

        # ğŸš¨ é˜²å¾¡æ€§æ£€æŸ¥ï¼šç¡®ä¿stepæ˜¯å­—å…¸
        if not isinstance(step, dict):
            self._logger.error(f"ğŸš¨ [Executor] stepä¸æ˜¯å­—å…¸ç±»å‹: {type(step)}, å†…å®¹: {step}")
            return {"success": False, "error": "invalid_step_type", "context": {}, "observations": ["âŒ Stepæ ¼å¼é”™è¯¯ï¼šä¸æ˜¯å­—å…¸ç±»å‹"]}

        observations = []

        # è·å– user_id
        user_id = ai.user_id or auth_manager.get_current_user_id()
        if not user_id:
            self._logger.warning("âš ï¸ [Executor] æœªæä¾›user_idï¼Œå°†ä½¿ç”¨å…¨å±€æ¨¡å‹é…ç½®")

        # åŠ è½½æ•°æ®æºé…ç½®ï¼ˆç»Ÿä¸€æ–¹æ³•ï¼‰
        initial_ds = ai.data_source if isinstance(ai.data_source, dict) else (ai.data_source or {})
        ds = await self._load_data_source_config(ai, user_id, initial_ds)

        # æ„å»ºæ‰§è¡Œä¸Šä¸‹æ–‡ï¼ˆç»Ÿä¸€æ–¹æ³•ï¼‰
        context = await self._build_execution_context(ai, user_id, ds)

        # ğŸ—„ï¸ [ResourcePoolæ¨¡å¼] å°†ResourcePoolå¼•ç”¨å­˜å‚¨åˆ°contextä¸­ï¼ˆç”¨äº_update_context_stateå’Œ_reduce_contextï¼‰
        try:
            if hasattr(ai, 'task_driven_context') and isinstance(ai.task_driven_context, dict):
                resource_pool = ai.task_driven_context.get("resource_pool")
                if resource_pool:
                    context["_resource_pool"] = resource_pool
                    self._logger.info("ğŸ—„ï¸ [Executor] ResourcePoolå·²åŠ è½½åˆ°contextä¸­")
        except Exception as e:
            self._logger.error(f"âŒ [Executor] åŠ è½½ResourcePoolå¤±è´¥: {e}")

        self._logger.info(f"ğŸ”„ [å•æ­¥éª¤æ‰§è¡Œ] å¼€å§‹æ‰§è¡Œ: {step.get('action', 'tool_call')}")

        try:
            step_start = time.time()
            step_action = step.get("action", "tool_call")

            # SQLç”ŸæˆåŠ¨ä½œ - ç›´æ¥è°ƒç”¨LLMç”ŸæˆSQLï¼ˆä¸é€šè¿‡å·¥å…·ï¼‰
            if step_action == "sql_generation":
                reason = step.get("reason", "Agentç”ŸæˆSQL")
                self._logger.info(f"ğŸ§  [Agentæ€è€ƒ] {reason}")
                self._logger.info("=== ğŸ†•ğŸ†•ğŸ†• ç«‹å³æµ‹è¯•æ—¥å¿—ï¼šè¿›å…¥sql_generationåˆ†æ”¯ï¼ ===")

                # ğŸ—„ï¸ [ResourcePoolæ¨¡å¼] **ä¼˜å…ˆ**ä»ResourcePoolæå–è¯¦ç»†ä¿¡æ¯ï¼ˆå¿…é¡»åœ¨æ‰€æœ‰æ£€æŸ¥ä¹‹å‰ï¼‰
                # è°ƒè¯•ï¼šæ£€æŸ¥task_driven_context
                self._logger.info(f"ğŸ” [Debug] hasattr task_driven_context: {hasattr(ai, 'task_driven_context')}")
                if hasattr(ai, 'task_driven_context'):
                    tdc = ai.task_driven_context
                    self._logger.info(f"ğŸ” [Debug] task_driven_context type: {type(tdc)}")
                    self._logger.info(f"ğŸ” [Debug] task_driven_context keys: {list(tdc.keys()) if isinstance(tdc, dict) else 'Not a dict'}")
                    if isinstance(tdc, dict) and "resource_pool" in tdc:
                        rp = tdc["resource_pool"]
                        self._logger.info(f"ğŸ” [Debug] resource_pool type: {type(rp)}")
                        self._logger.info(f"ğŸ” [Debug] resource_pool has extract_for_step: {hasattr(rp, 'extract_for_step')}")

                resource_pool = ai.task_driven_context.get("resource_pool") if hasattr(ai, 'task_driven_context') and isinstance(ai.task_driven_context, dict) else None
                if resource_pool:
                    # ä½¿ç”¨ResourcePoolçš„extract_for_stepæ–¹æ³•æå–SQLç”Ÿæˆæ‰€éœ€çš„æ•°æ®
                    extracted = resource_pool.extract_for_step("sql_generation", context)
                    context.update(extracted)

                    if extracted.get("column_details"):
                        self._logger.info(
                            f"ğŸ—„ï¸ [SQLç”Ÿæˆå‰æå–] ä»ResourcePoolæå–column_details: "
                            f"{len(extracted['column_details'])}å¼ è¡¨"
                        )
                else:
                    self._logger.warning("âš ï¸ [SQLç”Ÿæˆ] ResourcePoolä¸å¯ç”¨ï¼Œæ— æ³•æå–è¯¦ç»†ä¿¡æ¯")

                use_legacy_sql_generation = True
                result: Optional[Dict[str, Any]] = None

                if self._should_use_sql_coordinator(ai, context):
                    coordinator_payload = await self._generate_sql_with_coordinator(ai, context, user_id, observations)
                    if coordinator_payload:
                        use_legacy_sql_generation = False
                        result = coordinator_payload
                        context["sql_generation_strategy"] = "coordinator"

                if use_legacy_sql_generation:
                    context["sql_generation_strategy"] = "legacy"

                # ğŸ—„ï¸ [ResourcePoolæ¨¡å¼] ä»ContextMemoryåˆ¤æ–­schemaçŠ¶æ€
                if use_legacy_sql_generation:
                    context_memory = context.get("context_memory")
                    missing_time = not (context.get("start_date") or (isinstance(context.get("window"), dict) and context.get("window", {}).get("start_date")))

                    # ä»ContextMemoryåˆ¤æ–­schemaå¯ç”¨æ€§
                    if context_memory:
                        missing_schema = not context_memory.schema_available
                        self._logger.info(
                            f"ğŸ—„ï¸ [Gatingæ£€æŸ¥] ContextMemory: "
                            f"schema_available={context_memory.schema_available}"
                        )
                    else:
                        # å›é€€ï¼šå¦‚æœæ²¡æœ‰ContextMemoryï¼Œä½¿ç”¨ä¼ ç»Ÿæ£€æŸ¥
                        missing_schema = (
                            not context.get("schema_summary") and
                            not (context.get("columns") and len(context.get("columns")) > 0)
                        )
                        self._logger.warning("âš ï¸ [Gatingæ£€æŸ¥] æ²¡æœ‰ContextMemoryï¼Œä½¿ç”¨ä¼ ç»Ÿæ£€æŸ¥")
                    if missing_time:
                        observations.append("âš ï¸ ç¼ºå°‘æ—¶é—´èŒƒå›´ï¼Œå»ºè®®å…ˆè®¡ç®—æ—¶é—´çª—å£ time.window")
                        decision = {
                            "success": True,
                            "action": "gating",
                            "gating_redirect": "time.window",
                            "message": "ç¼ºå°‘æ—¶é—´èŒƒå›´ï¼Œå·²å»ºè®®å…ˆæ‰§è¡Œ time.window",
                            "next_step_hint": "è¯·å…ˆè°ƒç”¨ time.window è®¡ç®—ç»Ÿè®¡æ—¶é—´èŒƒå›´"
                        }
                        # è¿”å›è®©ä¸‹ä¸€è½®æŒ‰å»ºè®®æ‰§è¡Œ
                        return {
                            "success": True,
                            "step_result": decision,
                            "context": context,
                            "observations": observations,
                            "decision_info": {
                                "step_completed": "gating",
                                "step_reason": "ç¼ºå°‘æ—¶é—´èŒƒå›´",
                                "next_recommendations": ["è°ƒç”¨ time.window è®¡ç®—æ—¶é—´çª—å£"]
                            },
                            "execution_time": int((time.time() - step_start) * 1000)
                        }

                    if missing_schema:
                        observations.append("âš ï¸ ç¼ºå°‘è¡¨åˆ—ä¿¡æ¯ï¼Œä¸»åŠ¨è·å–schemaä¿¡æ¯")
                        self._logger.info("ğŸ” [Gating] æ£€æµ‹åˆ°ç¼ºå°‘å­—æ®µè¯¦æƒ…ï¼Œä¸»åŠ¨è°ƒç”¨schema.get_columnsè·å–")

                        # é¢„é€‰ä¸€æ‰¹æœ€ç›¸å…³çš„è¡¨
                        try:
                            suggested_tables = self._suggest_tables_from_names(
                                context.get("tables") or [],
                                context.get("placeholder_description") or ""
                            )
                            if not suggested_tables and context.get("tables"):
                                # å¦‚æœæ™ºèƒ½é€‰æ‹©å¤±è´¥ï¼Œå–å‰5å¼ è¡¨ä½œä¸ºå…œåº•
                                suggested_tables = context.get("tables")[:5]
                        except Exception:
                            suggested_tables = context.get("tables", [])[:5] if context.get("tables") else []

                        if not suggested_tables:
                            # å¦‚æœè¿è¡¨åˆ—è¡¨éƒ½æ²¡æœ‰ï¼Œéœ€è¦å…ˆlist_tables
                            self._logger.warning("ğŸ” [Gating] è¿è¡¨åˆ—è¡¨éƒ½æ²¡æœ‰ï¼Œéœ€è¦å…ˆè°ƒç”¨schema.list_tables")
                            decision = {
                                "success": True,
                                "action": "gating",
                                "gating_redirect": "schema.list_tables",
                                "message": "ç¼ºå°‘è¡¨åˆ—è¡¨ï¼Œéœ€è¦å…ˆæ‰§è¡Œ schema.list_tables",
                                "next_step_hint": "è¯·å…ˆè°ƒç”¨ schema.list_tables è·å–æ‰€æœ‰è¡¨"
                            }
                            return {
                                "success": True,
                                "step_result": decision,
                                "context": context,
                                "observations": observations,
                                "decision_info": {
                                    "step_completed": "gating",
                                    "step_reason": "ç¼ºå°‘è¡¨åˆ—è¡¨",
                                    "next_recommendations": ["è°ƒç”¨ schema.list_tables è·å–è¡¨åˆ—è¡¨"]
                                },
                                "execution_time": int((time.time() - step_start) * 1000)
                            }

                        # ä¸»åŠ¨è°ƒç”¨schema.get_columnsè·å–å­—æ®µä¿¡æ¯
                        try:
                            self._logger.info(f"ğŸ”§ [Gatingä¸»åŠ¨è·å–] è°ƒç”¨schema.get_columnsè·å–è¡¨å­—æ®µ: {suggested_tables}")

                            # è·å–schema.get_columnså·¥å…·
                            schema_tool = self.registry.get("schema.get_columns")
                            if not schema_tool:
                                raise ValueError("schema.get_columnså·¥å…·æœªæ‰¾åˆ°")

                            # å‡†å¤‡å·¥å…·è¾“å…¥
                            schema_input = {
                                "tables": suggested_tables,
                                "data_source": ds,
                                "connection_config": ds,
                                "user_id": user_id
                            }

                            # æ‰§è¡Œå·¥å…·
                            schema_result = await self._execute_tool_with_retry("schema.get_columns", schema_tool, schema_input)

                            if schema_result.get("success") and schema_result.get("column_details"):
                                # æˆåŠŸè·å–å­—æ®µä¿¡æ¯ï¼Œæ›´æ–°ä¸Šä¸‹æ–‡
                                self._update_context_state(context, schema_result, "schema.get_columns")
                                observations.append(f"âœ… å·²ä¸»åŠ¨è·å–{len(schema_result.get('column_details', {}))}å¼ è¡¨çš„å­—æ®µä¿¡æ¯")
                                self._logger.info(f"âœ… [Gatingä¸»åŠ¨è·å–] æˆåŠŸè·å–{len(schema_result.get('column_details', {}))}å¼ è¡¨çš„å­—æ®µä¿¡æ¯")

                                # ç»§ç»­æ‰§è¡ŒSQLç”Ÿæˆï¼ˆä¸è¿”å›ï¼Œç»§ç»­å¾€ä¸‹èµ°ï¼‰
                            else:
                                # è·å–å¤±è´¥ï¼Œè¿”å›é”™è¯¯
                                error_msg = schema_result.get("error", "æœªçŸ¥é”™è¯¯")
                                self._logger.error(f"âŒ [Gatingä¸»åŠ¨è·å–] schema.get_columnså¤±è´¥: {error_msg}")
                                return {
                                    "success": False,
                                    "error": f"auto_schema_fetch_failed: {error_msg}",
                                    "context": context,
                                    "observations": observations + [f"âŒ ä¸»åŠ¨è·å–schemaå¤±è´¥: {error_msg}"],
                                    "execution_time": int((time.time() - step_start) * 1000)
                                }
                        except Exception as e:
                            self._logger.error(f"âŒ [Gatingä¸»åŠ¨è·å–] å¼‚å¸¸: {str(e)}")
                            return {
                                "success": False,
                                "error": f"auto_schema_fetch_exception: {str(e)}",
                                "context": context,
                                "observations": observations + [f"âŒ ä¸»åŠ¨è·å–schemaå¼‚å¸¸: {str(e)}"],
                                "execution_time": int((time.time() - step_start) * 1000)
                            }

                # å‰ç½®ç»¼åˆåˆ†æï¼ˆè¡¨/åˆ—/æ¨¡æ¿/æ—¶é—´/å ä½ç¬¦ï¼‰ï¼ŒæŒ‡å¯¼SQLç”Ÿæˆ
                if use_legacy_sql_generation and not context.get("pre_sql_analysis"):
                    try:
                        pre = await self._run_pre_sql_analysis(context, user_id)
                        if pre:
                            context["pre_sql_analysis"] = pre
                            observations.append("âœ… å·²å®Œæˆå‰ç½®åˆ†æï¼ŒæŒ‡å¯¼SQLç”Ÿæˆ")
                    except Exception:
                        pass

                if use_legacy_sql_generation:
                    sql_prompt = self._build_sql_generation_prompt(context, step.get("input", {}))

                    # é€‰æ‹©LLMç­–ç•¥
                    try:
                        llm_service = getattr(self.container, 'llm_service', None) or getattr(self.container, 'llm', None)
                        if not llm_service:
                            raise ValueError("LLM service not found in container")

                        # æ ¹æ®ä¸Šä¸‹æ–‡æ„å»ºpolicy
                        from .llm_strategy_manager import llm_strategy_manager
                        llm_policy = llm_strategy_manager.build_llm_policy(
                            user_id=user_id,
                            stage="tool",
                            complexity="high",
                            tool_name="sql.draft",
                            output_kind="sql",
                            context=context
                        )

                        # è°ƒç”¨LLMç”Ÿæˆç»“æ„åŒ–JSONï¼Œä¼˜å…ˆä» {"sql": "..."} è·å–SQL
                        llm_text = await self._call_llm(llm_service, sql_prompt, user_id, llm_policy)
                        self._logger.info("ğŸ§¾ [sql_generation] LLMåŸå§‹è¾“å‡ºé¢„è§ˆ: %s", llm_text[:500])

                        extracted_sql = ""
                        gen_struct = None
                        try:
                            from .utils.json_utils import parse_json_safely
                            gen_struct = parse_json_safely(llm_text)
                            if isinstance(gen_struct, dict):
                                extracted_sql = self._extract_sql_from_struct(gen_struct)
                        except Exception:
                            gen_struct = None

                        if not extracted_sql and isinstance(gen_struct, dict):
                            self._logger.warning(
                                "âš ï¸ [sql_generation] JSONç»“æ„ä¸­æœªæ‰¾åˆ°SQLå­—æ®µ, keys=%s",
                                list(gen_struct.keys())
                            )

                        # å…¼å®¹å›é€€ï¼šä»æ–‡æœ¬ä¸­æå–SQL
                        if not extracted_sql:
                            extracted_sql = self._extract_sql(llm_text)

                        if not extracted_sql:
                            self._logger.error(
                                "âŒ [sql_generation] æœªæå–åˆ°SQL, stage=%s, tool=%s",
                                llm_policy.get("stage"),
                                llm_policy.get("step")
                            )
                            result = {
                                "success": False,
                                "action": "sql_generation",
                                "sql_generation_prompt": sql_prompt,
                                "error": "æœªèƒ½ä»LLMè¾“å‡ºä¸­æå–SQL",
                                "llm_raw": llm_text,
                            }
                        else:
                            result = {
                                "success": True,
                                "action": "sql_generation",
                                "sql_generation_prompt": sql_prompt,
                                "current_sql": extracted_sql,
                                "sql": extracted_sql,
                                "generation_struct": gen_struct,
                                "message": "SQLå·²ç”Ÿæˆ",
                                "next_step_hint": "è°ƒç”¨sql.validateéªŒè¯SQL"
                            }
                    except Exception as e:
                        result = {
                            "success": False,
                            "action": "sql_generation",
                            "sql_generation_prompt": sql_prompt,
                            "error": f"llm_generation_failed: {str(e)}"
                        }

            # å·¥å…·è°ƒç”¨åŠ¨ä½œ
            else:
                tool_name = step.get("tool")
                tool_input = step.get("input", {})
                reason = step.get("reason", f"æ‰§è¡Œ{tool_name}")

                tool = self.registry.get(tool_name)
                if not tool:
                    return {
                        "success": False,
                        "error": f"tool_not_found: {tool_name}",
                        "context": context,
                        "observations": [f"å·¥å…· {tool_name} æœªæ‰¾åˆ°"]
                    }

                # åˆå¹¶ä¸Šä¸‹æ–‡åˆ°å·¥å…·è¾“å…¥
                enriched_input = {**tool_input, **context}

                # ğŸ”§ ä¸ºå¯èƒ½è®¿é—®æ•°æ®åº“çš„å·¥å…·æ·»åŠ æ•°æ®æºè¿æ¥é…ç½®
                if tool_name in ("schema.list_tables", "schema.list_columns", "schema.get_columns", "sql.validate", "sql.execute",
                                 "workflow.stat_basic", "workflow.stat_ratio", "workflow.stat_category_mix"):
                    # ä½¿ç”¨ç»Ÿä¸€çš„æ•°æ®æºåŠ è½½æ–¹æ³•
                    tool_ds = await self._load_data_source_config(ai, user_id, enriched_input.get("data_source"))
                    if tool_ds:
                        enriched_input["data_source"] = tool_ds
                        enriched_input["connection_config"] = tool_ds
                        self._logger.info(f"ğŸ“‹ [Executor] ä¸º{tool_name}æ·»åŠ æ•°æ®æºè¿æ¥é…ç½®")

                # ğŸš¨ é¢å¤–ä¿æŠ¤ï¼šæ£€æŸ¥å·¥å…·è¾“å…¥ä¸­çš„SQLå­—æ®µæ˜¯å¦ä¸ºæè¿°æ€§æ–‡æœ¬
                for sql_field in ["current_sql", "sql"]:
                    if sql_field in enriched_input:
                        sql_value = enriched_input[sql_field]
                        if sql_value and self._is_description_text(sql_value):
                            self._logger.error(f"ğŸš¨ [è¾“å…¥ä¿æŠ¤] {tool_name}.{sql_field} åŒ…å«æè¿°æ€§æ–‡æœ¬: '{sql_value[:50]}'")
                            # å°è¯•ä»contextä¸­è·å–æ­£ç¡®çš„SQL
                            if sql_field != "current_sql" and context.get("current_sql"):
                                corrected_sql = context["current_sql"]
                                if not self._is_description_text(corrected_sql):
                                    self._logger.info(f"âœ… [è¾“å…¥ä¿®å¤] ä½¿ç”¨context.current_sqlä¿®å¤{sql_field}: '{corrected_sql[:50]}...'")
                                    enriched_input[sql_field] = corrected_sql
                                else:
                                    self._logger.error(f"âŒ [è¾“å…¥ä¿æŠ¤] context.current_sqlä¹Ÿæ˜¯æè¿°æ€§æ–‡æœ¬ï¼Œæ— æ³•ä¿®å¤")
                                    return {
                                        "success": False,
                                        "error": "invalid_sql_description",
                                        "message": f"å·¥å…· {tool_name} çš„ {sql_field} å‚æ•°åŒ…å«æè¿°æ€§æ–‡æœ¬ï¼Œæ— æ³•ä¿®å¤",
                                        "context": context,
                                        "observations": observations + [f"âŒ {tool_name}.{sql_field} å‚æ•°é”™è¯¯"]
                                    }

                # âš ï¸ è¡¨é€‰æ‹©é€»è¾‘ï¼ˆéµå¾ªPTAVåŸåˆ™ï¼‰ï¼š
                # - ä¼˜å…ˆä½¿ç”¨Plané˜¶æ®µAgentæ˜ç¡®æŒ‡å®šçš„tableså‚æ•°
                # - åªæœ‰å½“PlanæœªæŒ‡å®štablesæ—¶ï¼Œæ‰ä½¿ç”¨å…œåº•ç­–ç•¥æ™ºèƒ½é€‰æ‹©
                # - è¡¨é€‰æ‹©åº”è¯¥æ˜¯Plané˜¶æ®µçš„å†³ç­–è´£ä»»ï¼ŒToolåªè´Ÿè´£æ‰§è¡Œ
                if tool_name in ("schema.list_columns", "schema.get_columns"):
                    try:
                        tables_input = enriched_input.get("tables") or []
                        if tables_input:
                            # Plané˜¶æ®µå·²æ˜ç¡®æŒ‡å®štables - è¿™æ˜¯æ­£ç¡®çš„PTAVæ¶æ„
                            self._logger.info(f"âœ… [PTAV-Tool] ä½¿ç”¨PlanæŒ‡å®šçš„tables: {tables_input}")
                        if not tables_input:
                            # âš ï¸ å…œåº•ç­–ç•¥ï¼šPlané˜¶æ®µæœªæŒ‡å®štablesï¼ŒToolé˜¶æ®µè¢«è¿«æ™ºèƒ½é€‰æ‹©
                            # è¿™ä¸æ˜¯æœ€ä½³å®è·µï¼Œåº”è¯¥åœ¨Planæç¤ºè¯ä¸­å¼ºè°ƒAgentå¿…é¡»æ˜ç¡®æŒ‡å®štables
                            self._logger.warning(f"âš ï¸ [PTAV-è¿è§„] PlanæœªæŒ‡å®štablesï¼ŒToolé˜¶æ®µè¢«è¿«ä½¿ç”¨å…œåº•ç­–ç•¥æ™ºèƒ½é€‰æ‹©")

                            candidates = []
                            # å·²åœ¨ä¸Šä¸‹æ–‡ä¸­çš„è¡¨åï¼ˆç¬¬ä¸€æ­¥å‘ç°ï¼‰
                            if isinstance(context.get("tables"), list):
                                candidates = context.get("tables")
                            # å…¼å®¹ AgentInput ä¸­çš„ schema è¡¨
                            if not candidates:
                                try:
                                    if ai.schema and isinstance(ai.schema.tables, list):
                                        candidates = ai.schema.tables
                                except Exception:
                                    pass

                            # ä»å ä½ç¬¦æè¿°æ¨æ–­å…³é”®è¯
                            keywords = self._infer_table_keywords(getattr(ai.placeholder, 'description', ''))

                            selected: List[str] = []
                            # åŸºäºå…³é”®è¯ä»å€™é€‰è¡¨ç­›é€‰
                            if candidates and keywords:
                                lowered = [k for k in keywords]
                                selected = [t for t in candidates if any(k in str(t).lower() for k in lowered)]

                            # è‹¥æ— å…³é”®è¯æˆ–æœªå‘½ä¸­ï¼Œé‡‡ç”¨æ‰¹æ¬¡æ‰«æç­–ç•¥
                            if candidates and not selected:
                                batch_size = int(enriched_input.get("batch_size") or 5)
                                offset = int(context.get("schema_scan_offset") or 0)
                                # ç®€å•ç›¸ä¼¼åº¦ï¼šæ ¹æ®æè¿°ä¸­çš„çŸ­è¯å¯¹è¡¨åè¿›è¡ŒåŒ…å«åŒ¹é…ï¼Œä¼˜å…ˆåŒ¹é…åˆ°çš„å‰è‹¥å¹²ä¸ª
                                tokens = self._extract_tokens(getattr(ai.placeholder, 'description', ''))
                                ranked = []
                                for t in candidates:
                                    name = str(t).lower()
                                    score = sum(1 for tok in tokens if tok and tok in name)
                                    ranked.append((score, t))
                                ranked.sort(key=lambda x: (-x[0], str(x[1])))
                                # å¦‚æœæœ‰å¾—åˆ†>0çš„ï¼Œå–å‰ batch_sizeï¼›å¦åˆ™æŒ‰offsetåˆ†æ‰¹
                                positives = [t for s, t in ranked if s > 0]
                                if positives:
                                    selected = positives[:batch_size]
                                else:
                                    selected = candidates[offset:offset + batch_size]
                                    context["schema_scan_offset"] = offset + batch_size

                            # è‹¥ä»æœªé€‰ä¸­ï¼Œå°è¯•ä» reason / tool_input ä¸­è§£ææ˜¾å¼è¡¨å
                            if not selected:
                                explicit = self._extract_explicit_tables(reason, tool_input, candidates)
                                if explicit:
                                    selected = explicit

                            if selected:
                                enriched_input["tables"] = selected
                                self._logger.info(f"ğŸ”§ [å…œåº•ç­–ç•¥] è‡ªåŠ¨é€‰æ‹©è¡¨: {selected} (å»ºè®®åœ¨Plané˜¶æ®µæ˜ç¡®æŒ‡å®š)")
                    except Exception:
                        pass

                # å‰ç½®ä¿éšœï¼šæ ¡éªŒç±»/ç­–ç•¥ç±»/æ‰§è¡Œç±»å·¥å…·éœ€è¦SQL
                if tool_name in ("sql.validate", "sql.policy", "sql.execute"):
                    sql_in = (enriched_input.get("current_sql") or enriched_input.get("sql") or "").strip()

                    # ğŸš¨ æ–°å¢ï¼šæ£€æŸ¥SQLæ˜¯å¦ä¸ºæè¿°æ–‡æœ¬
                    if sql_in and self._is_description_text(sql_in):
                        self._logger.error(f"ğŸš¨ [å·¥å…·ä¿æŠ¤] {tool_name} æ”¶åˆ°æè¿°æ–‡æœ¬è€ŒéSQL: '{sql_in[:50]}'")
                        return {
                            "success": False,
                            "error": "invalid_sql_input",
                            "message": f"å·¥å…· {tool_name} æ”¶åˆ°æè¿°æ–‡æœ¬è€ŒéSQLè¯­å¥",
                            "context": context,
                            "observations": observations + [f"âŒ {tool_name} æ”¶åˆ°æ— æ•ˆSQLè¾“å…¥"]
                        }

                    if not sql_in:
                        # è‹¥æœ‰ç”Ÿæˆæç¤ºï¼Œå°è¯•å³æ—¶ç”Ÿæˆä¸€æ¬¡ï¼Œé¿å…ç©ºSQLéªŒè¯
                        try:
                            gen_prompt = context.get("sql_generation_prompt")
                            if gen_prompt:
                                self._logger.info("ğŸ§© [é¢„ç”ŸæˆSQL] ç¼ºå°‘current_sqlï¼Œä½¿ç”¨å·²å­˜åœ¨çš„ç”Ÿæˆæç¤ºå³æ—¶äº§å‡ºä¸€ç‰ˆ")
                                llm_service = getattr(self.container, 'llm_service', None) or getattr(self.container, 'llm', None)
                                llm_policy = None
                                llm_text = await self._call_llm(llm_service, gen_prompt, user_id)
                                extracted = ""
                                try:
                                    from .utils.json_utils import parse_json_safely
                                    gen_struct = parse_json_safely(llm_text)
                                    if isinstance(gen_struct, dict) and isinstance(gen_struct.get("sql"), str):
                                        extracted = gen_struct["sql"].strip()
                                except Exception:
                                    pass
                                if not extracted:
                                    extracted = self._extract_sql(llm_text)
                                if extracted:
                                    enriched_input["current_sql"] = extracted
                                    context["current_sql"] = extracted
                                else:
                                    return {
                                        "success": False,
                                        "error": "missing_current_sql",
                                        "message": "éªŒè¯/æ‰§è¡Œéœ€è¦SQLï¼Œä½†å½“å‰ä¸ºç©ºã€‚è¯·å…ˆæ‰§è¡Œsql_generationã€‚",
                                        "observations": observations + ["âš ï¸ ç¼ºå°‘current_sqlï¼Œä¸”å³æ—¶ç”Ÿæˆå¤±è´¥"],
                                        "context": context
                                    }
                            else:
                                return {
                                    "success": False,
                                    "error": "missing_current_sql",
                                    "message": "éªŒè¯/æ‰§è¡Œéœ€è¦SQLï¼Œä½†å½“å‰ä¸ºç©ºã€‚è¯·å…ˆæ‰§è¡Œsql_generationã€‚",
                                    "observations": observations + ["âš ï¸ ç¼ºå°‘current_sqlï¼Œå»ºè®®å…ˆç”ŸæˆSQL"],
                                    "context": context
                                }
                        except Exception as _:
                            return {
                                "success": False,
                                "error": "missing_current_sql",
                                "message": "éªŒè¯/æ‰§è¡Œéœ€è¦SQLï¼Œä½†å½“å‰ä¸ºç©ºã€‚è¯·å…ˆæ‰§è¡Œsql_generationã€‚",
                                "observations": observations + ["âš ï¸ ç¼ºå°‘current_sqlï¼Œä¸”å³æ—¶ç”Ÿæˆå‡ºé”™"],
                                "context": context
                            }

                self._logger.info(f"ğŸ”§ [å·¥å…·æ‰§è¡Œ] {tool_name} - {reason}")
                result = await self._execute_tool_with_retry(tool_name, tool, enriched_input)

            step_duration = int((time.time() - step_start) * 1000)

            # å¤„ç†æ‰§è¡Œç»“æœ
            if result.get("success"):
                observations.append(f"âœ… {reason} - æˆåŠŸ ({step_duration}ms)")

                # æ›´æ–°ä¸Šä¸‹æ–‡çŠ¶æ€
                self._update_context_state(context, result, step.get("tool"))
                # è£å‰ªä¸Šä¸‹æ–‡ï¼Œä¿ç•™å¯¹ä¸‹ä¸€è½®å†³ç­–æœ€æœ‰ç”¨çš„å…³é”®ä¿¡æ¯
                try:
                    self._reduce_context(context, step.get("tool"), result)
                except Exception:
                    pass

                # ä¸ºAgentæä¾›å†³ç­–æ”¯æŒä¿¡æ¯
                decision_info = self._build_decision_info(result, step)

                return {
                    "success": True,
                    "step_result": result,
                    "context": context,
                    "observations": observations,
                    "decision_info": decision_info,
                    "execution_time": step_duration
                }
            else:
                error_msg = result.get("error", "æœªçŸ¥é”™è¯¯")
                observations.append(f"âŒ {reason} - å¤±è´¥: {error_msg}")

                return {
                    "success": False,
                    "error": error_msg,
                    "step_result": result,
                    "context": context,
                    "observations": observations,
                    "execution_time": step_duration
                }

        except Exception as e:
            step_duration = int((time.time() - step_start) * 1000)
            error_msg = f"æ‰§è¡Œå¼‚å¸¸: {str(e)}"

            # æ·»åŠ è¯¦ç»†çš„é”™è¯¯å †æ ˆä¿¡æ¯
            import traceback
            error_traceback = traceback.format_exc()
            self._logger.error(f"ğŸš¨ [æ‰§è¡Œå¼‚å¸¸] {error_msg}")
            self._logger.error(f"ğŸ” [é”™è¯¯å †æ ˆ]\n{error_traceback}")

            return {
                "success": False,
                "error": f"execution_exception: {str(e)}",
                "error_traceback": error_traceback,
                "context": context,
                "observations": [f"âŒ æ‰§è¡Œå¼‚å¸¸: {error_msg} ({step_duration}ms)"],
                "execution_time": step_duration
            }

    def _build_sql_generation_prompt(self, context: Dict[str, Any], step_input: Dict[str, Any]) -> str:
        """æ„å»ºSQLç”Ÿæˆçš„å®Œæ•´æç¤ºè¯ï¼ˆJSONè¾“å‡ºï¼‰"""

        # æå–ä¸Šä¸‹æ–‡ä¿¡æ¯
        # ğŸ”§ ä¿®å¤ï¼šplaceholder å¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–å­—å…¸
        placeholder_val = step_input.get("placeholder", {})
        if isinstance(placeholder_val, dict):
            placeholder_desc = placeholder_val.get("description", "")
        elif isinstance(placeholder_val, str):
            placeholder_desc = placeholder_val
        else:
            placeholder_desc = ""

        if not placeholder_desc:
            placeholder_desc = context.get("placeholder_description", "")

        schema_summary = context.get("schema_summary", "")
        # ä¼˜å…ˆä»æ‰å¹³å­—æ®µå–æ—¶é—´ï¼Œå…¶æ¬¡ä» window å–
        start_date = context.get("start_date", "")
        end_date = context.get("end_date", "")
        if (not start_date or not end_date) and isinstance(context.get("window"), dict):
            w = context.get("window") or {}
            start_date = start_date or w.get("start_date") or w.get("data_start_time") or ""
            end_date = end_date or w.get("end_date") or w.get("data_end_time") or ""
        semantic_type = context.get("semantic_type", "")
        top_n = context.get("top_n", "")

        # æ·»åŠ è°ƒè¯•æ—¥å¿—
        self._logger.info(f"ğŸ” [SQLç”Ÿæˆæç¤º] placeholder_desc: {placeholder_desc}")
        self._logger.info(f"ğŸ” [SQLç”Ÿæˆæç¤º] schema_summary: {schema_summary[:200] if schema_summary else 'æ— schemaæ‘˜è¦'}...")
        self._logger.info(f"ğŸ” [SQLç”Ÿæˆæç¤º] æ—¶é—´èŒƒå›´: {start_date} ~ {end_date}")
        self._logger.info(f"ğŸ” [SQLç”Ÿæˆæç¤º] semantic_type: {semantic_type}, top_n: {top_n}")

        # ä¸å†ç¡¬ç¼–ç æ¨èæ—¶é—´åˆ—ï¼Œè®©Agentä»å®é™…è¡¨ç»“æ„ä¸­æ™ºèƒ½é€‰æ‹©
        rec_time_col = None  # ä¸ç»™é»˜è®¤å€¼ï¼Œè®©Agentè‡ªå·±åˆ¤æ–­

        # æ„å»ºæ—¶é—´æç¤º - å¼ºåˆ¶ä½¿ç”¨å ä½ç¬¦æ ¼å¼(å…³é”®ä¿®å¤)
        time_hint = ""
        if start_date and end_date:
            if start_date == end_date:
                time_hint = f"âš ï¸ å‚è€ƒæ—¶é—´èŒƒå›´: {start_date}ï¼ˆå•æ—¥ï¼Œä»…ç”¨äºç†è§£ä¸šåŠ¡éœ€æ±‚ï¼‰\n\nğŸš¨ **å¼ºåˆ¶è¦æ±‚**: SQLä¸­å¿…é¡»ä½¿ç”¨å ä½ç¬¦æ ¼å¼ {{{{start_date}}}} å’Œ {{{{end_date}}}}ï¼Œç»å¯¹ç¦æ­¢ä½¿ç”¨å…·ä½“æ—¥æœŸå¦‚ '{start_date}'ï¼\nç¤ºä¾‹: WHERE dt BETWEEN {{{{start_date}}}} AND {{{{end_date}}}}"
            else:
                time_hint = f"âš ï¸ å‚è€ƒæ—¶é—´èŒƒå›´: {start_date} åˆ° {end_date}ï¼ˆä»…ç”¨äºç†è§£ä¸šåŠ¡éœ€æ±‚ï¼‰\n\nğŸš¨ **å¼ºåˆ¶è¦æ±‚**: SQLä¸­å¿…é¡»ä½¿ç”¨å ä½ç¬¦æ ¼å¼ {{{{start_date}}}} å’Œ {{{{end_date}}}}ï¼Œç»å¯¹ç¦æ­¢ä½¿ç”¨å…·ä½“æ—¥æœŸå¦‚ '{start_date}' æˆ– '{end_date}'ï¼\nç¤ºä¾‹: WHERE dt BETWEEN {{{{start_date}}}} AND {{{{end_date}}}}"
        else:
            time_hint = "âš ï¸ æ—¶é—´èŒƒå›´: æœªæŒ‡å®šå…·ä½“æ—¥æœŸ\n\nğŸš¨ **å¼ºåˆ¶è¦æ±‚**: SQLä¸­å¿…é¡»ä½¿ç”¨å ä½ç¬¦æ ¼å¼ {{{{start_date}}}} å’Œ {{{{end_date}}}} è¿›è¡Œæ—¶é—´è¿‡æ»¤ï¼"

        # æ„å»ºè¯­ä¹‰ç±»å‹æŒ‡å¯¼
        type_guidance = ""
        if semantic_type == "ranking" and top_n:
            type_guidance = f"è¿™æ˜¯æ’åç±»æŸ¥è¯¢ï¼Œéœ€è¦æŒ‰åº¦é‡é™åºæ’åºå¹¶å–å‰{top_n}å"
        elif semantic_type == "compare":
            type_guidance = "è¿™æ˜¯å¯¹æ¯”ç±»æŸ¥è¯¢ï¼Œéœ€è¦è¾“å‡ºåŸºå‡†å€¼ã€å¯¹æ¯”å€¼ã€å·®å€¼å’Œç™¾åˆ†æ¯”å˜åŒ–"
        elif semantic_type == "statistical":
            type_guidance = "è¿™æ˜¯ç»Ÿè®¡ç±»æŸ¥è¯¢ï¼Œéœ€è¦è®¡ç®—æ€»è®¡ã€å¹³å‡å€¼æˆ–è®¡æ•°"

        # è‹¥æ²¡æœ‰schemaæ‘˜è¦ï¼Œè‡³å°‘æä¾›è¡¨ååˆ—è¡¨ï¼Œå¸®åŠ©LLMé¿å…å¹»è§‰
        if not schema_summary:
            tables = context.get("tables") or []
            columns = context.get("columns") or {}
            column_details = context.get("column_details") or {}

            if tables:
                preview = ", ".join(tables[:15]) + ("..." if len(tables) > 15 else "")
                schema_summary = f"å¯ç”¨æ•°æ®è¡¨(éƒ¨åˆ†): {preview}"

                # ğŸ”§ å…³é”®ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨column_detailsæä¾›å®Œæ•´å­—æ®µä¿¡æ¯
                if isinstance(column_details, dict) and column_details:
                    schema_details = []
                    for table, cols_data in column_details.items():
                        if isinstance(cols_data, dict):
                            # æ˜¾ç¤ºæ‰€æœ‰å­—æ®µï¼Œå¸¦ç±»å‹å’Œæ³¨é‡Š
                            field_descs = []
                            for field_name, field_info in cols_data.items():
                                desc = field_name
                                if field_info.get("type"):
                                    desc += f"({field_info['type']})"
                                if field_info.get("comment"):
                                    desc += f" - {field_info['comment']}"
                                field_descs.append(desc)
                            fields_text = "\n    ".join(field_descs)
                            schema_details.append(f"**{table}** ({len(cols_data)}åˆ—):\n    {fields_text}")
                    if schema_details:
                        schema_summary += "\n\nè¯¦ç»†è¡¨ç»“æ„ï¼ˆæ‰€æœ‰å­—æ®µï¼‰:\n" + "\n".join(schema_details)
                # å›é€€ï¼šå¦‚æœæ²¡æœ‰column_detailsï¼Œä½¿ç”¨columnsä½†æ˜¾ç¤ºæ‰€æœ‰å­—æ®µ
                elif isinstance(columns, dict) and columns:
                    schema_details = []
                    for table, cols in columns.items():
                        if isinstance(cols, list) and cols:
                            # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ˜¾ç¤ºæ‰€æœ‰åˆ—ï¼Œä¸å†é™åˆ¶ä¸º10ä¸ª
                            cols_all = ", ".join(cols)
                            schema_details.append(f"**{table}** ({len(cols)}åˆ—): {cols_all}")
                    if schema_details:
                        schema_summary += "\n\nè¯¦ç»†è¡¨ç»“æ„:\n" + "\n".join(schema_details)

        # è‹¥æœ‰å·²ç­›é€‰çš„ç›®æ ‡è¡¨ï¼Œæ˜ç¡®å‘ŠçŸ¥åªèƒ½ä½¿ç”¨è¿™äº›è¡¨
        selected_tables = context.get("selected_tables") or []
        allowed_tables_note = ""
        if selected_tables:
            allowed_tables_note = f"\n**ä¸¥æ ¼é™åˆ¶å¯ç”¨è¡¨**: ä½ åªèƒ½ä½¿ç”¨ä»¥ä¸‹è¡¨åä¹‹ä¸€: {', '.join(selected_tables)}\n"
        prompt = f"""
# SQLæŸ¥è¯¢ç”Ÿæˆä»»åŠ¡

## ä¸šåŠ¡éœ€æ±‚
**ç”¨æˆ·éœ€æ±‚**: {placeholder_desc}
**æŸ¥è¯¢ç±»å‹**: {semantic_type if semantic_type else "ç»Ÿè®¡æŸ¥è¯¢"}
**æ—¶é—´ä¸Šä¸‹æ–‡**: {time_hint if time_hint else "æ— ç‰¹å®šæ—¶é—´èŒƒå›´"}

## æ•°æ®åº“æ¶æ„
{schema_summary}
{allowed_tables_note}

## æŸ¥è¯¢æŒ‡å¯¼
{type_guidance if type_guidance else "ç”Ÿæˆç¬¦åˆä¸šåŠ¡éœ€æ±‚çš„ç»Ÿè®¡æŸ¥è¯¢"}

## å‰ç½®åˆ†æï¼ˆç»“æ„åŒ–ï¼‰
{context.get('pre_sql_analysis') or 'ï¼ˆæœ¬æ¬¡æ— å‰ç½®åˆ†æï¼‰'}

## è¾“å‡ºè¦æ±‚ï¼ˆä»…è¿”å›ä¸€ä¸ªJSONå¯¹è±¡ï¼Œä¸è¦å…¶ä»–æ–‡æœ¬ï¼‰
{{
  "sql": "ä»¥å•è¡Œå­—ç¬¦ä¸²è¿”å›å®Œæ•´SELECTè¯­å¥ï¼Œæ—¶é—´è¿‡æ»¤å¿…é¡»ä½¿ç”¨å ä½ç¬¦æ ¼å¼",
  "time": {{
    "column": "å®é™…ä½¿ç”¨çš„æ—¶é—´åˆ—å",
    "range": {{"start_date": "{{{{start_date}}}}", "end_date": "{{{{end_date}}}}"}}
  }},
  "tables": ["æ¶‰åŠåˆ°çš„çœŸå®è¡¨ååˆ—è¡¨"],
  "measures": ["COUNT(*) as cnt", "å¯é€‰å…¶ä»–åº¦é‡"],
  "dimensions": ["å¯é€‰ç»´åº¦åˆ—å"],
  "filters": [{{"field":"åˆ—å","op":"=|IN|BETWEEN|LIKE","value":"å€¼æˆ–æ•°ç»„"}}],
  "assumptions": ["å¯é€‰ï¼šå¯¹ä¸ç¡®å®šä¿¡æ¯çš„å‡è®¾"],
  "notes": ["å¯é€‰ï¼šä»»ä½•æ³¨æ„äº‹é¡¹"]
}}

## âš ï¸ å¼ºåˆ¶è§„åˆ™ï¼ˆè¿åå°†å¯¼è‡´SQLæ— æ•ˆï¼‰

### 1. è¡¨ååˆ—åè§„åˆ™
- ä¸¥æ ¼ä½¿ç”¨ä¸Šè¿°æ•°æ®åº“æ¶æ„ä¸­çš„çœŸå®è¡¨åå’Œåˆ—åï¼Œç¦æ­¢è™šæ„
- é€šè¿‡å­—æ®µåç§°ã€ç±»å‹ã€æ³¨é‡Šæ¥æ™ºèƒ½åˆ¤æ–­å“ªä¸ªæ˜¯æ—¶é—´å­—æ®µ

### 2. æ—¶é—´å ä½ç¬¦è§„åˆ™ï¼ˆæœ€é‡è¦ï¼ï¼‰
**ğŸš¨ ç»å¯¹ç¦æ­¢ä½¿ç”¨å…·ä½“æ—¥æœŸï¼Œå¿…é¡»ä½¿ç”¨å ä½ç¬¦ï¼**

âœ… **æ­£ç¡®ç¤ºä¾‹**:
```sql
WHERE dt BETWEEN {{{{start_date}}}} AND {{{{end_date}}}}
WHERE DATE(create_time) >= {{{{start_date}}}} AND DATE(create_time) <= {{{{end_date}}}}
WHERE update_time BETWEEN {{{{start_date}}}} AND {{{{end_date}}}}
```

âŒ **é”™è¯¯ç¤ºä¾‹ï¼ˆä¸¥æ ¼ç¦æ­¢ï¼‰**:
```sql
WHERE dt >= '2025-09-27' AND dt <= '2025-09-27'  -- ç¦æ­¢ï¼
WHERE DATE(æ—¶é—´åˆ—) = '2025-09-27'  -- ç¦æ­¢ï¼
WHERE dt BETWEEN '2025-10-09' AND '2025-10-09'  -- ç¦æ­¢ï¼
```

### 3. å ä½ç¬¦æ ¼å¼è¯´æ˜
- ä½¿ç”¨åŒå¤§æ‹¬å·æ ¼å¼: {{{{start_date}}}} å’Œ {{{{end_date}}}}
- è¿™æ˜¯SQLæ¨¡æ¿ï¼Œåç»­ä¼šæ›¿æ¢ä¸ºå®é™…æ—¥æœŸ
- ä¿æŒå ä½ç¬¦æ ¼å¼å¯ä»¥è®©SQLåœ¨ä¸åŒæ—¶é—´èŒƒå›´ä¸‹å¤ç”¨

### 4. ä¸ºä»€ä¹ˆå¿…é¡»ä½¿ç”¨å ä½ç¬¦ï¼Ÿ
- æ¨¡æ¿é˜¶æ®µç”Ÿæˆçš„SQLéœ€è¦åœ¨ä»»åŠ¡æ‰§è¡Œæ—¶åŠ¨æ€æ›¿æ¢æ—¥æœŸ
- ç¡¬ç¼–ç æ—¥æœŸä¼šå¯¼è‡´SQLæ— æ³•é€‚é…ä¸åŒçš„æ‰§è¡Œæ—¶é—´
- å ä½ç¬¦æ˜¯æ¨¡æ¿åŒ–SQLçš„æ ¸å¿ƒæœºåˆ¶

ä»…è¿”å›çº¯JSONï¼Œä¸è¦ä½¿ç”¨Markdownä»£ç å—ã€‚
        """
        return prompt.strip()

    async def _run_pre_sql_analysis(self, context: Dict[str, Any], user_id: str) -> str:
        """ä½¿ç”¨LLMå¯¹è¡¨/åˆ—/æ¨¡æ¿/æ—¶é—´/å ä½ç¬¦è¿›è¡Œä¸€æ¬¡ç»“æ„åŒ–åˆ†æï¼Œè¾“å‡ºJSONæŒ‡å¯¼ç‚¹ã€‚

        ç›®æ ‡ï¼šé€‰å®šç›®æ ‡è¡¨/æ—¶é—´åˆ—/å…³é”®è¿‡æ»¤/åº¦é‡ä¸ç»´åº¦å»ºè®®ï¼Œé¿å…åç»­SQLç”Ÿæˆèµ°åã€‚
        """
        llm_service = getattr(self.container, 'llm_service', None) or getattr(self.container, 'llm', None)
        if not llm_service:
            return ""

        # ç»„è£…ç®€è¦åˆ—ä¿¡æ¯ï¼ˆä»…é€‰ä¸­è¡¨æˆ–æœ€å¤š3å¼ è¡¨ï¼Œæ¯è¡¨æœ€å¤š20åˆ—ï¼‰
        selected = context.get("selected_tables") or context.get("tables") or []
        selected = selected[:3] if isinstance(selected, list) else []
        details = context.get("column_details") or {}
        self._logger.info(f"ğŸ“‹ [Executor] SQLç”Ÿæˆæç¤ºä¸­ä½¿ç”¨è¯¦ç»†å­—æ®µä¿¡æ¯: {len(details)}å¼ è¡¨, é€‰ä¸­è¡¨: {selected}")
        cols_preview = {}
        for t in selected:
            tmap = details.get(t) or {}
            cols_preview[t] = [{"name": k, "type": (v.get("type") if isinstance(v, dict) else None), "comment": (v.get("comment") if isinstance(v, dict) else None)} for i, (k, v) in enumerate(tmap.items()) if i < 20]

        prompt = f"""
ä½ æ˜¯æ•°æ®å»ºæ¨¡ä¸SQLè§„åˆ’ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡ç”Ÿæˆä¸€ä»½â€œSQLç”Ÿæˆå‰çš„ç»“æ„åŒ–åˆ†æâ€ï¼ˆJSONå¯¹è±¡ï¼‰ï¼ŒæŒ‡å¯¼ä¸‹ä¸€æ­¥SQLç¼–å†™ã€‚

ä¸Šä¸‹æ–‡ï¼š
- å ä½ç¬¦æè¿°: {context.get('placeholder_description')}
- å¯é€‰è¡¨: {', '.join(selected) if selected else '(æœªæä¾›)'}
- æ¨èæ—¶é—´åˆ—: {context.get('recommended_time_column')}
- æ—¶é—´èŒƒå›´: {context.get('start_date') or context.get('window', {}).get('start_date')} ~ {context.get('end_date') or context.get('window', {}).get('end_date')}
- æ¨¡æ¿ä¸Šä¸‹æ–‡: {str(context.get('template_context'))[:300] if context.get('template_context') else '(æ— )'}
- é€‰è¡¨åˆ—é¢„è§ˆ: {cols_preview}

è¯·è¾“å‡ºä¸€ä¸ªJSONå¯¹è±¡ï¼š
{{
  "target_table": "å»ºè®®ä½¿ç”¨çš„è¡¨åï¼ˆå¿…é¡»åœ¨å¯é€‰è¡¨ä¸­ï¼‰",
  "time_column": "å»ºè®®ä½¿ç”¨çš„æ—¶é—´åˆ—",
  "measures": ["COUNT(*) as ..." æˆ–å…¶ä»–åº¦é‡è¡¨è¾¾å¼],
  "filters": [{{"field":"åˆ—å","op":"=|IN|BETWEEN|LIKE","value":"å€¼æˆ–æ•°ç»„"}}],
  "dimensions": ["ç»´åº¦åˆ—å"],
  "sql_skeleton": "å¯é€‰ï¼Œç»™å‡ºä¸€ä¸ªSQLéª¨æ¶ï¼ˆFROM/WHERE/æ—¶é—´è¿‡æ»¤/åˆ†ç»„/èšåˆï¼‰",
  "notes": ["ä»»ä½•æ³¨æ„äº‹é¡¹"]
}}
åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–è¯´æ˜ã€‚
"""

        try:
            # å°½é‡è¿”å›JSON
            if hasattr(llm_service, 'ask'):
                res = await llm_service.ask(user_id=user_id, prompt=prompt, response_format={"type": "json_object"})
                text = res.get("response", "") if isinstance(res, dict) else str(res)
            elif hasattr(llm_service, 'generate_response'):
                res = await llm_service.generate_response(prompt=prompt, user_id=user_id, response_format={"type": "json_object"})
                text = res.get("response", "") if isinstance(res, dict) else str(res)
            else:
                text = await llm_service(prompt)
            return text.strip()
        except Exception:
            return ""

    async def _call_llm(self, llm_service, prompt: str, user_id: str = "system", llm_policy: Dict[str, Any] | None = None) -> str:
        """ç»Ÿä¸€è°ƒç”¨LLMï¼Œå°½é‡ä¸Planner/Orchestratorä¿æŒä¸€è‡´æ¥å£ã€‚"""
        try:
            if hasattr(llm_service, 'ask'):
                result = await llm_service.ask(
                    user_id=user_id,
                    prompt=prompt,
                    response_format={"type": "json_object"},
                    llm_policy=llm_policy or {"stage": "tool", "output_kind": "sql"}
                )
                return result.get("response", "") if isinstance(result, dict) else str(result)
            elif hasattr(llm_service, 'generate_response'):
                result = await llm_service.generate_response(
                    prompt=prompt,
                    user_id=user_id,
                    response_format={"type": "json_object"},
                    llm_policy=llm_policy or {"stage": "tool", "output_kind": "sql"}
                )
                return result.get("response", "") if isinstance(result, dict) else str(result)
            elif callable(llm_service):
                return await llm_service(prompt)
            else:
                raise ValueError("Unsupported LLM service interface")
        except Exception as e:
            self._logger.error(f"LLMè°ƒç”¨å¤±è´¥: {str(e)}")
            raise

    def _extract_sql_from_struct(self, struct: Dict[str, Any]) -> str:
        """ä»LLMè¿”å›çš„ç»“æ„åŒ–ç»“æœä¸­æå–SQLã€‚"""
        if isinstance(struct, list):
            for item in struct:
                if isinstance(item, dict):
                    nested = self._extract_sql_from_struct(item)
                    if nested:
                        return nested
                elif isinstance(item, str):
                    text = item.strip()
                    if text and text.lower().startswith(("select", "with")):
                        return text
            return ""

        if not isinstance(struct, dict):
            return ""

        candidate_keys = [
            "sql",
            "result",
            "final_sql",
            "generated_sql",
            "sql_template",
            "query",
        ]

        for key in candidate_keys:
            value = struct.get(key)
            if isinstance(value, str):
                candidate = value.strip()
                if candidate and candidate.lower().startswith(("select", "with")):
                    return candidate
            elif isinstance(value, dict):
                nested = self._extract_sql_from_struct(value)
                if nested:
                    return nested
            elif isinstance(value, list):
                nested = self._extract_sql_from_struct(value)
                if nested:
                    return nested

        sql_lines = struct.get("sql_lines") or struct.get("lines")
        if isinstance(sql_lines, list):
            joined = "\n".join(line for line in sql_lines if isinstance(line, str)).strip()
            if joined and joined.lower().startswith(("select", "with")):
                return joined

        nested_keys = ["data", "payload", "output", "response", "details"]
        for key in nested_keys:
            nested = struct.get(key)
            if isinstance(nested, dict):
                nested_sql = self._extract_sql_from_struct(nested)
                if nested_sql:
                    return nested_sql
            elif isinstance(nested, list):
                nested_sql = self._extract_sql_from_struct(nested)
                if nested_sql:
                    return nested_sql

        return ""

    def _extract_sql(self, text: str) -> str:
        """ä»LLMæ–‡æœ¬ä¸­æå–SQLï¼Œæ”¯æŒ```sql```ä»£ç å—æˆ–çº¯æ–‡æœ¬ã€‚"""
        try:
            t = (text or "").strip()
            if not t:
                return ""
            if t.startswith("{") or t.startswith("["):
                import json
                try:
                    parsed = json.loads(t)
                    sql = self._extract_sql_from_struct(parsed)
                    if sql:
                        return sql
                except Exception:
                    pass
            # ä¼˜å…ˆæå–```sql```ä»£ç å—
            import re
            code_fence = re.search(r"```sql\s*([\s\S]*?)```", t, re.IGNORECASE)
            if code_fence:
                candidate = code_fence.group(1).strip()
            else:
                # å»æ‰é€šç”¨ä»£ç å—
                generic_fence = re.search(r"```\s*([\s\S]*?)```", t)
                candidate = (generic_fence.group(1).strip() if generic_fence else t)

            # å»æ‰å¼€å¤´çš„æ³¨é‡Šè¡Œï¼Œä»…ä¿ç•™ä»¥SELECT/WITHå¼€å¤´çš„ä¸»ä½“
            lines = [ln for ln in candidate.splitlines() if ln.strip()]
            body = []
            started = False
            for ln in lines:
                ln_strip = ln.strip()
                up = ln_strip.upper()
                if not started and (up.startswith("SELECT") or up.startswith("WITH")):
                    started = True
                if started:
                    body.append(ln)
            sql = "\n".join(body).strip()
            # æ¸…ç†å°¾éšåå¼•å·/å¤šä½™å†…å®¹
            sql = sql.strip().strip('`').strip()
            return sql
        except Exception:
            return (text or "").strip()

    async def _execute_tool_with_retry(self, tool_name: str, tool, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå·¥å…·ï¼Œå¸¦åŸºæœ¬é‡è¯•å’Œç»“æœæ ‡å‡†åŒ–ã€‚"""
        import asyncio
        from .utils.json_utils import normalize_tool_result, is_transient_error

        last_result: Dict[str, Any] | None = None
        for attempt in range(self.max_tool_retries + 1):
            if attempt > 0:
                await asyncio.sleep(self.retry_backoff_base * (2 ** (attempt - 1)))
            try:
                raw = await tool.execute(input_data)
            except Exception as e:
                raw = {"success": False, "error": str(e)}

            result = normalize_tool_result(tool_name, raw)
            last_result = result

            # æˆåŠŸæˆ–éç¬æ—¶é”™è¯¯åˆ™åœæ­¢é‡è¯•
            if result.get("success") and not result.get("error"):
                break
            if not is_transient_error(result.get("error")):
                break

        return last_result or {"success": False, "error": "unknown_error"}

    def _extract_tables_from_sql(self, sql: str) -> List[str]:
        """ä½¿ç”¨sqlparseä»SQLè¯­å¥ä¸­æå–è¡¨åï¼ˆæ›´å‡†ç¡®ï¼Œæ”¯æŒå¤æ‚SQLï¼‰

        Args:
            sql: SQLè¯­å¥

        Returns:
            List[str]: æå–åˆ°çš„è¡¨ååˆ—è¡¨
        """
        try:
            import sqlparse
            from sqlparse.sql import IdentifierList, Identifier
            from sqlparse.tokens import Keyword, DML

            if not sql or not isinstance(sql, str):
                return []

            tables = []

            # è§£æSQL
            parsed = sqlparse.parse(sql)
            if not parsed:
                return []

            stmt = parsed[0]

            from_seen = False
            for token in stmt.tokens:
                # è·³è¿‡æ³¨é‡Šå’Œç©ºç™½
                if token.is_whitespace:
                    continue

                # æ‰¾åˆ°FROM/JOINå…³é”®å­—
                if token.ttype is Keyword and token.value.upper() in ('FROM', 'JOIN', 'INNER JOIN', 'LEFT JOIN', 'RIGHT JOIN', 'FULL JOIN'):
                    from_seen = True
                    continue

                # FROMä¹‹åçš„æ ‡è¯†ç¬¦å°±æ˜¯è¡¨å
                if from_seen:
                    if isinstance(token, IdentifierList):
                        # å¤šä¸ªè¡¨åï¼ˆé€—å·åˆ†éš”ï¼‰
                        for identifier in token.get_identifiers():
                            table_name = self._get_real_name(identifier)
                            if table_name:
                                tables.append(table_name)
                        from_seen = False
                    elif isinstance(token, Identifier):
                        # å•ä¸ªè¡¨å
                        table_name = self._get_real_name(token)
                        if table_name:
                            tables.append(table_name)
                        from_seen = False
                    elif token.ttype is Keyword:
                        # é‡åˆ°ä¸‹ä¸€ä¸ªå…³é”®å­—ï¼Œåœæ­¢
                        from_seen = False

            # å»é‡å¹¶ä¿æŒé¡ºåº
            seen = set()
            unique_tables = []
            for t in tables:
                if t and t not in seen:
                    unique_tables.append(t)
                    seen.add(t)

            return unique_tables

        except Exception as e:
            self._logger.debug(f"sqlparseæå–è¡¨åå¤±è´¥: {e}ï¼Œå›é€€åˆ°æ­£åˆ™è¡¨è¾¾å¼")
            # å›é€€åˆ°ç®€å•æ­£åˆ™è¡¨è¾¾å¼
            try:
                import re
                matches = re.findall(r'\bFROM\s+(\w+)|\bJOIN\s+(\w+)', sql, re.IGNORECASE)
                tables = []
                for match in matches:
                    for t in match:
                        if t:
                            tables.append(t)
                return tables
            except Exception:
                return []

    def _get_real_name(self, identifier) -> str:
        """ä»sqlparseçš„Identifierä¸­æå–çœŸå®è¡¨åï¼ˆå»æ‰åˆ«åï¼‰

        Args:
            identifier: sqlparseçš„Identifierå¯¹è±¡

        Returns:
            str: çœŸå®è¡¨å
        """
        try:
            # è·å–çœŸå®åç§°ï¼ˆå»æ‰åˆ«åï¼‰
            name = identifier.get_real_name()
            if name:
                return name
            # å¦‚æœæ²¡æœ‰real_nameï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªtoken
            return identifier.get_name()
        except Exception:
            return str(identifier).strip().split()[0] if identifier else ""

    def _infer_table_keywords(self, description: str) -> List[str]:
        """ä»å ä½ç¬¦æè¿°ä¸­æ¨æ–­ç”¨äºåŒ¹é…è¡¨åçš„å…³é”®è¯ã€‚"""
        try:
            text = (description or "").lower()
            keywords: List[str] = []
            # å¸¸è§é€€è´§/é€€æ¬¾åœºæ™¯å…³é”®è¯
            if any(k in text for k in ["é€€è´§", "é€€æ¬¾", "return", "refund"]):
                keywords.extend(["refund", "return", "é€€è´§", "é€€æ¬¾"])
            # å¸¸è§ODS/DWå‰ç¼€ä¸å¼ºåˆ¶æ·»åŠ ï¼Œç”±å€™é€‰è¡¨åŒ¹é…
            # å»é‡å¹¶ä¿æŒé¡ºåº
            seen = set()
            ordered = []
            for k in keywords:
                if k not in seen:
                    ordered.append(k)
                    seen.add(k)
            return ordered
        except Exception:
            return []

    def _extract_tokens(self, text: str) -> List[str]:
        """æå–ç”¨äºç›¸ä¼¼åº¦åŒ¹é…çš„çŸ­è¯/ç¼©å†™ï¼ˆç®€å•åˆ†è¯ä¸é™å™ªï¼‰ã€‚"""
        try:
            t = (text or "").lower()
            # åŸºç¡€åˆ†è¯ï¼šéå­—æ¯æ•°å­—æ‹†åˆ† + å»åœç”¨è¯ + é•¿åº¦è¿‡æ»¤
            import re
            raw = re.split(r"[^a-z0-9\u4e00-\u9fa5]+", t)
            stop = {"çš„", "å’Œ", "ä¸", "æ€»æ•°", "ç»Ÿè®¡", "æ•°é‡", "ä¸ªæ•°", "ä¿¡æ¯", "æ•°æ®", "è¡¨", "ç”³è¯·"}
            tokens = [w for w in raw if w and w not in stop and len(w) >= 2]
            # æ·»åŠ å¸¸è§ç¼©å†™æ˜ å°„ï¼ˆå¯æ‰©å±•ï¼‰
            mapped: List[str] = []
            alias = {
                "refund": ["rf", "rfd"],
                "return": ["ret", "rtn"],
            }
            for w in tokens:
                mapped.append(w)
                for key, al in alias.items():
                    if w.startswith(key):
                        mapped.extend(al)
            # å»é‡
            seen: set[str] = set()
            ordered: List[str] = []
            for w in mapped:
                if w not in seen:
                    ordered.append(w)
                    seen.add(w)
            return ordered
        except Exception:
            return []

    def _extract_explicit_tables(self, reason: str, tool_input: Dict[str, Any], candidates: List[str]) -> List[str]:
        """ä»reasonæˆ–è¾“å…¥ä¸­è§£ææ˜¾å¼è¡¨åï¼ˆä¾‹å¦‚åŒ…å« `è·å– ods_refund è¡¨`ï¼‰ã€‚"""
        try:
            explicit: List[str] = []
            # 1) ç›´æ¥ä» tool_input.tables è¯»å–ï¼ˆå­—ç¬¦ä¸²æˆ–æ•°ç»„ï¼‰
            if isinstance(tool_input.get("tables"), list) and tool_input["tables"]:
                return tool_input["tables"]
            if isinstance(tool_input.get("tables"), str) and tool_input["tables"].strip():
                return [tool_input["tables"].strip()]

            # 2) ä» reason ä¸­æ­£åˆ™æå–å•è¯ï¼ŒåŒ¹é…å€™é€‰è¡¨
            import re
            text = f"{reason or ''}"
            words = re.findall(r"[A-Za-z0-9_\.]+", text)
            lowered = {c.lower(): c for c in candidates}
            for w in words:
                lw = w.lower()
                if lw in lowered:
                    explicit.append(lowered[lw])
            # å»é‡ä¿æŒé¡ºåº
            seen: set[str] = set()
            ordered: List[str] = []
            for t in explicit:
                if t not in seen:
                    ordered.append(t)
                    seen.add(t)
            return ordered
        except Exception:
            return []

    async def execute_single_tool(self, tool_name: str, tool_input: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ‰§è¡Œå•ä¸ªå·¥å…· (ç”¨äºè°ƒè¯•å’Œæµ‹è¯•)

        Args:
            tool_name: å·¥å…·åç§°
            tool_input: å·¥å…·è¾“å…¥

        Returns:
            Dict: å·¥å…·æ‰§è¡Œç»“æœ
        """
        tool = self.registry.get(tool_name)
        if not tool:
            return {"success": False, "error": f"å·¥å…· {tool_name} æœªæ‰¾åˆ°"}

        try:
            result = await tool.execute(tool_input)
            return result
        except Exception as e:
            return {"success": False, "error": f"å·¥å…·æ‰§è¡Œå¼‚å¸¸: {str(e)}"}

    def _is_description_text(self, sql: str) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸ºæè¿°æ–‡æœ¬è€ŒéSQL - ä¸SQLValidateToolä¿æŒä¸€è‡´"""
        if not sql or not isinstance(sql, str):
            return False

        sql_lower = sql.lower().strip()

        # æ˜æ˜¾çš„æè¿°æ€§å…³é”®è¯
        description_keywords = [
            "å½“å‰å€™é€‰", "å€™é€‰sql", "sqlå†…å®¹", "å·²æœ‰sql", "ç°æœ‰sql",
            "å¾…éªŒè¯", "ç­‰å¾…", "è¯·", "éœ€è¦", "å»ºè®®", "åº”è¯¥",
            "å€™é€‰çš„", "å½“å‰çš„", "ç”Ÿæˆçš„", "æä¾›çš„", "è¿”å›çš„"
        ]

        # å¦‚æœåŒ…å«æè¿°æ€§å…³é”®è¯ä½†ä¸åŒ…å«SQLå…³é”®è¯ï¼Œå¯èƒ½æ˜¯æè¿°æ–‡æœ¬
        has_description = any(keyword in sql_lower for keyword in description_keywords)
        has_sql_keywords = any(keyword in sql_lower for keyword in ["select", "from", "where", "insert", "update", "delete"])

        if has_description and not has_sql_keywords:
            return True

        # å¦‚æœæ˜¯å¾ˆçŸ­çš„æ–‡æœ¬ä¸”ä¸åŒ…å«SQLå…³é”®è¯ï¼Œå¯èƒ½æ˜¯æè¿°
        if len(sql.strip()) < 50 and not has_sql_keywords:
            return True

        # å¦‚æœåŒ…å«ä¸­æ–‡æè¿°æ€§è¯è¯­
        chinese_description = ["å½“å‰", "å€™é€‰", "å†…å®¹", "æè¿°", "ä¿¡æ¯", "æ•°æ®", "ç»“æœ"]
        has_chinese_desc = any(keyword in sql for keyword in chinese_description)
        if has_chinese_desc and not has_sql_keywords:
            return True

        return False

    def list_available_tools(self) -> List[str]:
        """åˆ—å‡ºå¯ç”¨å·¥å…·"""
        return list(self.registry._tools.keys())

    def get_tool_info(self, tool_name: str) -> Dict[str, Any]:
        """è·å–å·¥å…·ä¿¡æ¯"""
        tool = self.registry.get(tool_name)
        if not tool:
            return {"exists": False}

        return {
            "exists": True,
            "name": tool.name,
            "description": getattr(tool, 'description', 'No description available'),
            "type": tool.__class__.__name__
        }

    def _extract_semantic_info(self, ai: AgentInput) -> Dict[str, Any]:
        """ä»AgentInputçš„task_driven_contextä¸­æå–å ä½ç¬¦è¯­ä¹‰ä¿¡æ¯ï¼ˆsemantic_typeã€top_nï¼‰ã€‚"""
        info: Dict[str, Any] = {}
        try:
            tdc = ai.task_driven_context or {}
            # ä¼˜å…ˆä½¿ç”¨å ä½ç¬¦IDã€å…¶æ¬¡æè¿°ï¼Œå°½é‡åŒ¹é…æ¨¡æ¿ä¸Šä¸‹æ–‡ä¸­çš„placeholder_name
            ph_candidates = []
            try:
                if ai.placeholder.id:
                    ph_candidates.append(str(ai.placeholder.id))
            except Exception:
                pass
            try:
                if ai.placeholder.description:
                    ph_candidates.append(str(ai.placeholder.description))
            except Exception:
                pass
            # å»é‡
            ph_candidates = list(dict.fromkeys(ph_candidates))
            contexts = tdc.get("placeholder_contexts") or []
            match = None
            for c in contexts:
                pname = c.get("placeholder_name")
                if not pname:
                    continue
                if any(pname == cand or pname in cand or cand in pname for cand in ph_candidates):
                    match = c
                    break
            if match:
                info["semantic_type"] = match.get("semantic_type")
                params = match.get("parsed_params") or {}
                if isinstance(params, dict):
                    info["top_n"] = params.get("top_n")
        except Exception:
            pass
        return info

    def _update_context_state(self, context: Dict[str, Any], result: Dict[str, Any], tool_name: str) -> None:
        """æ›´æ–°æ‰§è¡Œä¸Šä¸‹æ–‡çŠ¶æ€"""
        # å°†å·¥å…·æ‰§è¡Œç»“æœåˆå¹¶åˆ°ä¸Šä¸‹æ–‡
        if isinstance(result, dict):
            for key, value in result.items():
                if key not in ["success", "error", "action", "message"]:
                    context[key] = value

        # ç‰¹æ®Šå¤„ç†å…³é”®å·¥å…·çš„ç»“æœ
        if tool_name == "sql.validate":
            if result.get("issues"):
                context["validation_issues"] = result["issues"]
            if result.get("warnings"):
                context["validation_warnings"] = result["warnings"]
            if result.get("corrected_sql"):
                context["corrected_sql"] = result["corrected_sql"]
                # è‡ªåŠ¨é‡‡ç”¨ä¿®æ­£åçš„SQLä½œä¸ºå½“å‰SQLï¼Œä¾¿äºåç»­ç­–ç•¥ä¸æ‰§è¡Œ
                try:
                    if result["corrected_sql"]:
                        context["current_sql"] = result["corrected_sql"]
                except Exception:
                    pass
            if result.get("agent_analysis"):
                context["agent_analysis"] = result["agent_analysis"]

        elif tool_name == "sql.execute":
            if result.get("rows"):
                context["execution_result"] = {
                    "rows": result["rows"],
                    "columns": result.get("columns", []),
                    "row_count": len(result["rows"])
                }
                context["sql_executed_successfully"] = True

        # é€šç”¨ï¼šè‹¥å·¥å…·è¿”å›äº†sqlå­—æ®µï¼Œåˆ™å°†å…¶è®¾ç½®ä¸ºå½“å‰SQLï¼ˆå¦‚sql.policyã€workflow.* ç­‰ï¼‰
        try:
            if isinstance(result.get("sql"), str) and result.get("sql").strip():
                context["current_sql"] = result["sql"].strip()
        except Exception:
            pass

        # é€šç”¨ï¼šè‹¥å·¥å…·è¿”å›äº†æ‰§è¡Œç»“æœæ•°æ®ï¼ˆrows/columnsï¼‰ï¼Œä¹Ÿè§†ä¸ºä¸€æ¬¡æ‰§è¡ŒæˆåŠŸï¼ˆå¦‚workflow.*å†…éƒ¨å·²æ‰§è¡ŒSQLï¼‰
        try:
            if isinstance(result.get("rows"), list):
                context["execution_result"] = {
                    "rows": result.get("rows", []),
                    "columns": result.get("columns", []),
                    "row_count": len(result.get("rows", []))
                }
                context["sql_executed_successfully"] = True
        except Exception:
            pass

        if tool_name == "schema.list_columns":
            if result.get("schema_summary"):
                context["schema_summary"] = result["schema_summary"]
            if result.get("columns"):
                context["columns"] = result["columns"]
            if result.get("column_details"):
                context["column_details"] = result["column_details"]
                self._logger.info(f"ğŸ“‹ [Executor] å­˜å‚¨schema.list_columnsè¯¦ç»†å­—æ®µä¿¡æ¯: {len(result['column_details'])}å¼ è¡¨")

        elif tool_name == "schema.get_columns":
            # ğŸ” [è°ƒè¯•1] schema.get_columnså·¥å…·è¿”å›å€¼æ£€æŸ¥
            self._logger.info(f"ğŸ“‹ [Executor] å¤„ç†schema.get_columnsç»“æœ: success={result.get('success')}")
            self._logger.info(f"ğŸ“‹ [Executor] ç»“æœåŒ…å«çš„é”®: {list(result.keys()) if isinstance(result, dict) else 'Not dict'}")
            self._logger.info(f"ğŸ“‹ [è°ƒè¯•1] column_detailsæ˜¯å¦å­˜åœ¨: {bool(result.get('column_details'))}")
            if result.get('column_details'):
                self._logger.info(f"ğŸ“‹ [è°ƒè¯•1] column_detailsç±»å‹: {type(result['column_details'])}")
                self._logger.info(f"ğŸ“‹ [è°ƒè¯•1] column_detailsè¡¨æ•°é‡: {len(result['column_details'])}")
                self._logger.info(f"ğŸ“‹ [è°ƒè¯•1] column_detailsè¡¨å: {list(result['column_details'].keys())}")

            if result.get("schema_summary"):
                context["schema_summary"] = result["schema_summary"]
                self._logger.info(f"ğŸ“‹ [Executor] å­˜å‚¨schema_summary: {len(result['schema_summary'])}å­—ç¬¦")
            if result.get("columns"):
                context["columns"] = result["columns"]
                self._logger.info(f"ğŸ“‹ [Executor] å­˜å‚¨columns: {len(result['columns'])}å¼ è¡¨")
            if result.get("column_details"):
                context["column_details"] = result["column_details"]
                self._logger.info(f"ğŸ“‹ [Executor] å­˜å‚¨schema.get_columnsè¯¦ç»†å­—æ®µä¿¡æ¯: {len(result['column_details'])}å¼ è¡¨")
                # æ˜¾ç¤ºç¬¬ä¸€ä¸ªè¡¨çš„è¯¦ç»†ä¿¡æ¯ä½œä¸ºæ ·ä¾‹
                if result['column_details']:
                    first_table = list(result['column_details'].keys())[0]
                    first_columns = result['column_details'][first_table]
                    self._logger.info(f"ğŸ“‹ [Executor] æ ·ä¾‹è¡¨{first_table}çš„å­—æ®µ: {list(first_columns.keys())}")
                # ğŸ” [è°ƒè¯•1] ç¡®è®¤column_detailså·²å­˜å…¥context
                self._logger.info(f"ğŸ“‹ [è°ƒè¯•1] ç¡®è®¤context.column_detailså·²è®¾ç½®: {bool(context.get('column_details'))}")
                if context.get('column_details'):
                    self._logger.info(f"ğŸ“‹ [è°ƒè¯•1] context.column_detailsè¡¨å: {list(context['column_details'].keys())}")
            # ä¸å†ç¡¬ç¼–ç æ¨èæ—¶é—´åˆ—ï¼Œè®©Agenté€šè¿‡æŸ¥çœ‹å®é™…æ•°æ®æ¥æ™ºèƒ½åˆ¤æ–­
            # Agentæ¯”æˆ‘ä»¬çš„ç®—æ³•èªæ˜ï¼Œç›´æ¥æŸ¥5è¡Œæ•°æ®å°±çŸ¥é“å“ªä¸ªæ˜¯æ—¶é—´å­—æ®µäº†
            try:
                self._logger.info("ğŸ“‹ [Executor] è·³è¿‡ç¡¬ç¼–ç æ—¶é—´åˆ—æ¨èï¼Œè®©Agenté€šè¿‡æ•°æ®æŸ¥è¯¢æ™ºèƒ½åˆ¤æ–­")
                context["use_agent_time_column_detection"] = True
            except Exception:
                pass

            # è¡¨å†ç­›é€‰ï¼šåŸºäºæ—¶é—´åˆ—ã€è¡¨åå…³é”®è¯ã€ç»´åº¦åˆ—å‘½ä¸­å¯¹è¡¨è¿›è¡Œæ’åºä¸é€‰æ‹©
            try:
                selected = self._rank_and_select_tables(context, result)
                if selected:
                    context["selected_tables"] = selected
                    # å°† tables é¡ºåºè°ƒæ•´ä¸ºæ‰€é€‰è¡¨ä¼˜å…ˆ
                    tlist = context.get("tables") or []
                    rest = [t for t in tlist if t not in selected]
                    context["tables"] = selected + rest

                    # è°ƒæ•´æ¨èæ—¶é—´åˆ—ä»¥é€‚é…é¦–é€‰ç›®æ ‡è¡¨ï¼ˆä¼˜å…ˆä¿è¯å¯ç”¨æ€§ï¼‰
                    try:
                        top_table = selected[0]
                        # é¦–å…ˆå°è¯•ä»è¯¦ç»†å­—æ®µä¸­è·å–è¯¥è¡¨åˆ—é›†åˆï¼Œå¦åˆ™é€€å› columns ç®€è¡¨
                        t_details = (result.get("column_details") or {}).get(top_table) or {}
                        if isinstance(t_details, dict) and t_details:
                            table_cols = {c.lower() for c in t_details.keys()}
                        else:
                            table_cols = {c.lower() for c in (result.get("columns") or {}).get(top_table, [])}

                        rec = (context.get("recommended_time_column") or "").lower()

                        # ä¸å†ç¡¬ç¼–ç è°ƒæ•´æ—¶é—´åˆ—ï¼Œè®©Agentä»å®é™…æ•°æ®ä¸­åˆ¤æ–­
                        self._logger.info(f"ğŸ“‹ [Executor] è·³è¿‡ç¡¬ç¼–ç æ—¶é—´åˆ—è°ƒæ•´ï¼Œè®©Agentæ™ºèƒ½é€‰æ‹© (è¡¨: {top_table})")
                    except Exception:
                        pass
            except Exception:
                pass

        elif tool_name == "time.window":
            if result.get("start_date"):
                context["start_date"] = result["start_date"]
            if result.get("end_date"):
                context["end_date"] = result["end_date"]

        elif tool_name == "chart.spec":
            if result.get("chart_spec"):
                context["chart_spec"] = result["chart_spec"]

        elif tool_name == "word_chart_generator":
            if result.get("chart_image_path"):
                context["chart_image_path"] = result["chart_image_path"]

        # ğŸ—„ï¸ [ResourcePoolæ¨¡å¼] åŒæ­¥é‡è¦çŠ¶æ€åˆ°ResourcePool
        # å¦‚æœå¯ç”¨äº†ResourcePoolï¼ŒåŒæ­¥å…³é”®çŠ¶æ€å˜æ›´
        try:
            resource_pool = context.get("_resource_pool")
            if resource_pool:
                # æ”¶é›†éœ€è¦åŒæ­¥çš„çŠ¶æ€
                updates_to_sync = {}

                # 1. åŒæ­¥ column_details
                if result.get("column_details"):
                    updates_to_sync["column_details"] = result["column_details"]

                # 2. åŒæ­¥ current_sqlï¼ˆå¦‚æœcontextä¸­æœ‰ï¼‰
                if context.get("current_sql"):
                    updates_to_sync["current_sql"] = context["current_sql"]

                # 3. åŒæ­¥æ‰§è¡ŒçŠ¶æ€
                if context.get("sql_executed_successfully"):
                    updates_to_sync["sql_executed_successfully"] = True

                # æ‰§è¡ŒåŒæ­¥
                if updates_to_sync:
                    resource_pool.update(updates_to_sync)
                    self._logger.info(
                        f"ğŸ—„ï¸ [_update_context_state] å·²åŒæ­¥åˆ°ResourcePool: "
                        f"{', '.join(updates_to_sync.keys())}"
                    )

                    # ğŸ”„ åŒæ­¥æ›´æ–°ContextMemoryï¼ˆä¿æŒçŠ¶æ€ä¸€è‡´ï¼‰
                    context_memory = resource_pool.build_context_memory()
                    context["context_memory"] = context_memory
                    self._logger.info(
                        f"ğŸ”„ [_update_context_state] å·²æ›´æ–°ContextMemory: "
                        f"has_sql={context_memory.has_sql}, "
                        f"schema_available={context_memory.schema_available}, "
                        f"tables={len(context_memory.available_tables)}"
                    )
        except Exception as e:
            self._logger.error(f"âŒ [_update_context_state] åŒæ­¥åˆ°ResourcePoolå¤±è´¥: {e}")

    def _build_decision_info(self, result: Dict[str, Any], step: Dict[str, Any]) -> Dict[str, Any]:
        """æ„å»ºAgentå†³ç­–æ”¯æŒä¿¡æ¯"""
        decision_info = {
            "step_completed": step.get("action", "tool_call"),
            "step_reason": step.get("reason", ""),
            "next_recommendations": []
        }

        # åŸºäºæ‰§è¡Œç»“æœæä¾›ä¸‹ä¸€æ­¥å»ºè®®
        if result.get("success"):
            action = step.get("action")
            tool_name = step.get("tool")

            # SQLç”Ÿæˆå®Œæˆå»ºè®®
            if action == "sql_generation":
                decision_info["next_recommendations"] = [
                    "Agentåº”ç”ŸæˆSQLè¯­å¥",
                    "ç„¶åè°ƒç”¨sql.validateéªŒè¯SQLæ­£ç¡®æ€§",
                    "å¦‚æœ‰é—®é¢˜åˆ™ä¿®æ­£SQL"
                ]

            # Schemaä¿¡æ¯è·å–å®Œæˆå»ºè®®
            elif tool_name == "schema.list_columns":
                decision_info["next_recommendations"] = [
                    "Schemaä¿¡æ¯å·²è·å–ï¼Œå¯ä»¥å¼€å§‹ç”ŸæˆSQL",
                    "å»ºè®®ä½¿ç”¨sql_generationåŠ¨ä½œç”ŸæˆSQL",
                    "ç¡®ä¿ä½¿ç”¨çœŸå®çš„è¡¨åå’Œåˆ—å"
                ]

            elif tool_name == "schema.list_tables":
                decision_info["next_recommendations"] = [
                    "å·²åˆ—å‡ºå¯ç”¨è¡¨å",
                    "è¯·é€‰æ‹©ä¸éœ€æ±‚ç›¸å…³çš„è¡¨å¹¶è°ƒç”¨schema.get_columnsè·å–åˆ—ä¿¡æ¯",
                    "éšåä½¿ç”¨sql_generationç”ŸæˆSQL"
                ]

            elif tool_name == "schema.get_columns":
                decision_info["next_recommendations"] = [
                    "å·²è·å–ç›®æ ‡è¡¨çš„åˆ—ä¿¡æ¯",
                    "å»ºè®®ä½¿ç”¨sql_generationç”ŸæˆSQL",
                    "ç”Ÿæˆåè°ƒç”¨sql.validateéªŒè¯"
                ]

            # æ—¶é—´çª—å£è®¡ç®—å®Œæˆå»ºè®®
            elif tool_name == "time.window":
                decision_info["next_recommendations"] = [
                    "æ—¶é—´çª—å£å·²è®¡ç®—å®Œæˆ",
                    "å»ºè®®è·å–Schemaä¿¡æ¯æˆ–ç›´æ¥ç”ŸæˆSQL",
                    "ç¡®ä¿åœ¨SQLä¸­æ·»åŠ æ—¶é—´è¿‡æ»¤æ¡ä»¶"
                ]

            # SQLéªŒè¯å®Œæˆå»ºè®®
            elif tool_name == "sql.validate":
                if result.get("issues"):
                    decision_info["next_recommendations"] = [
                        "SQLéªŒè¯å‘ç°é—®é¢˜ï¼Œéœ€è¦ä¿®æ­£",
                        "å»ºè®®é‡æ–°ç”ŸæˆSQLè§£å†³éªŒè¯é—®é¢˜",
                        "æˆ–è°ƒç”¨sql.refineå·¥å…·ä¿®æ­£SQL"
                    ]
                else:
                    decision_info["next_recommendations"] = [
                        "SQLéªŒè¯é€šè¿‡ï¼Œå¯ä»¥æ‰§è¡Œ",
                        "å»ºè®®è°ƒç”¨sql.executeè·å–æ•°æ®",
                        "ç„¶åæ£€æŸ¥æ•°æ®è´¨é‡"
                    ]

            # SQLæ‰§è¡Œå®Œæˆå»ºè®®
            elif tool_name == "sql.execute":
                if result.get("rows"):
                    decision_info["next_recommendations"] = [
                        "SQLæ‰§è¡ŒæˆåŠŸï¼Œæ•°æ®å·²è·å–",
                        "å»ºè®®è°ƒç”¨data.qualityæ£€æŸ¥æ•°æ®è´¨é‡",
                        "å¦‚éœ€å›¾è¡¨åˆ™è°ƒç”¨chart.specç”Ÿæˆé…ç½®"
                    ]
                else:
                    decision_info["next_recommendations"] = [
                        "SQLæ‰§è¡Œæ— ç»“æœï¼Œæ£€æŸ¥SQLé€»è¾‘",
                        "å¯èƒ½éœ€è¦è°ƒæ•´æ—¶é—´èŒƒå›´æˆ–è¿‡æ»¤æ¡ä»¶"
                    ]

            # å·¥ä½œæµå·¥å…·å®Œæˆå»ºè®®
            elif tool_name in ("workflow.stat_basic", "workflow.stat_ratio", "workflow.stat_category_mix"):
                if result.get("rows"):
                    decision_info["next_recommendations"] = [
                        "å·¥ä½œæµå·²è¿”å›ç»Ÿè®¡ç»“æœ",
                        "å¦‚éœ€ç»§ç»­å¯è¿›è¡Œdata.qualityæˆ–æ¸²æŸ“å›¾è¡¨",
                        "å¦åˆ™å¯ç»“æŸæœ¬è½®PTAV"
                    ]
                else:
                    decision_info["next_recommendations"] = [
                        "å·¥ä½œæµæœªè¿”å›æ•°æ®ï¼Œæ£€æŸ¥ç”Ÿæˆçš„SQLä¸è¿‡æ»¤æ¡ä»¶",
                        "å¿…è¦æ—¶é‡æ–°ç”Ÿæˆæˆ–æ”¾å®½æ¡ä»¶"
                    ]

        else:
            # æ‰§è¡Œå¤±è´¥çš„é€šç”¨å»ºè®®
            decision_info["next_recommendations"] = [
                "ä¸Šä¸€æ­¥æ‰§è¡Œå¤±è´¥ï¼Œåˆ†æé”™è¯¯åŸå› ",
                "å¯èƒ½éœ€è¦é‡æ–°è§„åˆ’æˆ–è°ƒæ•´ç­–ç•¥",
                "æ£€æŸ¥è¾“å…¥å‚æ•°å’Œä¸Šä¸‹æ–‡æ˜¯å¦æ­£ç¡®"
            ]

        return decision_info

    def _rank_and_select_tables(self, context: Dict[str, Any], result: Dict[str, Any]) -> List[str]:
        """æ ¹æ®åˆ—è¯¦æƒ…ä¸å ä½ç¬¦æè¿°ä¸ºè¡¨æ‰“åˆ†é€‰æ‹©æœ€ç›¸å…³çš„è‹¥å¹²è¡¨ã€‚

        æ‰“åˆ†è¦ç´ ï¼š
        - è‹¥è¡¨åŒ…å«æ¨èæ—¶é—´åˆ— +5
        - è¡¨åå‘½ä¸­å ä½ç¬¦å…³é”®è¯ï¼ˆrefund/return/é€€è´§/é€€æ¬¾ç­‰ï¼‰æ¯å‘½ä¸­ +3
        - è¡¨å†…åˆ—å/æ³¨é‡Šå‘½ä¸­ç»´åº¦å…³é”®è¯ï¼ˆtype/category/ç±»åˆ«/å•†å“/äº§å“ï¼‰æ¯å‘½ä¸­ +2ï¼ˆæœ€å¤šåŠ 6ï¼‰
        - tokens ç›¸ä¼¼åŒ…å«æ¯å‘½ä¸­ +1ï¼ˆæœ€å¤šåŠ 5ï¼‰
        """
        from collections import defaultdict

        details = result.get("column_details") or {}
        columns_map = result.get("columns") or {}
        tables = list(columns_map.keys())
        if not tables:
            return []

        placeholder_desc = context.get("placeholder_description", "")
        # å…³é”®è¯ä¸token
        kw = set(self._infer_table_keywords(placeholder_desc))
        toks = set(self._extract_tokens(placeholder_desc))
        time_col = (context.get("recommended_time_column") or "").lower()

        def score_table(tname: str) -> int:
            s = 0
            lower_name = str(tname).lower()
            # è¡¨åå…³é”®è¯
            for k in kw:
                if k in lower_name:
                    s += 3
            # tokens
            tok_hits = sum(1 for tok in toks if tok and tok in lower_name)
            s += min(tok_hits, 5)  # cap

            # æ—¶é—´åˆ—å‘½ä¸­
            tdetails = details.get(tname) or {}
            if time_col and time_col in {c.lower() for c in tdetails.keys()}:
                s += 5

            # ç»´åº¦å…³é”®è¯å‘½ä¸­
            dim_kws = ["type", "category", "kind", "class", "å•†å“", "å“ç±»", "ç±»åˆ«", "äº§å“"]
            dim_hits = 0
            for col, meta in tdetails.items():
                name_l = col.lower()
                if any(dk in name_l for dk in dim_kws):
                    dim_hits += 1
                cmt = meta.get("comment")
                if isinstance(cmt, str) and any(dk in cmt for dk in dim_kws):
                    dim_hits += 1
                if dim_hits >= 3:  # cap
                    break
            s += min(dim_hits * 2, 6)
            return s

        ranked = sorted(tables, key=lambda t: (-score_table(t), t))
        top_k = min(3, len(ranked))
        return ranked[:top_k]

    def _suggest_tables_from_names(self, candidates: List[str], description: str, top_k: int = 3) -> List[str]:
        """åœ¨åªæœ‰è¡¨åæ—¶çš„è½»é‡æ¨èï¼šåŸºäºå…³é”®è¯ä¸tokenså‘½ä¸­è¿›è¡Œæ’åºã€‚"""
        try:
            kw = set(self._infer_table_keywords(description))
            toks = set(self._extract_tokens(description))
            def score(name: str) -> int:
                n = (name or "").lower()
                s = 0
                for k in kw:
                    if k in n:
                        s += 3
                s += min(sum(1 for t in toks if t and t in n), 5)
                return s
            ranked = sorted(candidates or [], key=lambda n: (-score(n), n))
            return ranked[:min(top_k, len(ranked))]
        except Exception:
            return (candidates or [])[:min(top_k, len(candidates or []))]

    def _reduce_context(self, context: Dict[str, Any], tool_name: str, result: Dict[str, Any] | None = None) -> None:
        """è£å‰ªä¸Šä¸‹æ–‡ï¼Œåˆ é™¤æ— å…³æˆ–è¿‡å¤§çš„å­—æ®µï¼Œä¿ç•™å¯¹ä¸‹ä¸€æ­¥å†³ç­–æœ‰ä»·å€¼çš„å…³é”®ä¿¡æ¯ã€‚

        ç­–ç•¥ï¼š
        - æ°¸ä¹…ä¿ç•™ï¼šcurrent_sql, sql_executed_successfully, execution_result(rows/columns/row_count),
          start_date, end_date, timezone, window(ä»…ä¿ç•™è½»é‡é”®), tables(æœ€å¤šå‰50ä¸ª), schema_summary,
          recommended_time_column, validation_issues/warningsï¼ˆçŸ­æ–‡æœ¬ï¼‰ã€‚
        - column_detailsï¼šä»…ä¿ç•™å½“å‰å‘½ä¸­çš„è¡¨ï¼ˆå¦‚æœ‰ï¼‰ä¸”æ¯è¡¨æœ€å¤šå‰20åˆ—ï¼›å¦åˆ™åˆ é™¤ä»¥å‡å°ä½“ç§¯ã€‚
        - åˆ é™¤ï¼šå¤§å‹ä¸´æ—¶æ–‡æœ¬ï¼ˆagent_analysis, llm_raw ç­‰ï¼‰ã€schema_scan_offset ç­‰å†…éƒ¨å…‰æ ‡ã€‚
        """
        if not isinstance(context, dict):
            return

        keep_keys = {
            "current_sql", "sql_executed_successfully", "execution_result",
            "start_date", "end_date", "timezone", "window", "tables",
            "schema_summary", "recommended_time_column",
            "validation_issues", "validation_warnings",
        }

        # è½»é‡åŒ– window
        if isinstance(context.get("window"), dict):
            w = context["window"]
            light_w = {
                k: w.get(k) for k in ["start_date", "end_date", "time_column", "timezone", "cron_expression"] if k in w
            }
            context["window"] = light_w

        # tables é™åˆ¶é•¿åº¦
        if isinstance(context.get("tables"), list) and len(context["tables"]) > 50:
            context["tables"] = context["tables"][:50]

        # execution_result é™åˆ¶æ ·æœ¬è¡Œæ•°
        if isinstance(context.get("execution_result"), dict):
            er = context["execution_result"]
            rows = er.get("rows")
            if isinstance(rows, list) and len(rows) > 5:
                er["rows"] = rows[:5]
                er["row_count"] = er.get("row_count", len(rows))
            context["execution_result"] = er

        # ğŸ—„ï¸ [ResourcePoolæ¨¡å¼] ä¸è¦åœ¨contextä¸­ä¿ç•™column_details
        # column_detailsåº”è¯¥åªå­˜åœ¨äºResourcePoolä¸­ï¼Œä»é‚£é‡ŒæŒ‰éœ€æå–
        # ä¿æŒcontextè½»é‡ï¼Œé¿å…tokenæ¶ˆè€—
        # æ³¨æ„ï¼šè¿™é‡Œæ£€æŸ¥æ˜¯å¦å­˜åœ¨ _resource_pool æˆ– context_memory æ¥åˆ¤æ–­æ˜¯å¦å¯ç”¨ResourcePoolæ¨¡å¼
        resource_pool_enabled = bool(context.get("_resource_pool") or context.get("context_memory"))

        if resource_pool_enabled and "column_details" in context:
            # ResourcePoolæ¨¡å¼ï¼šåˆ é™¤column_detailsï¼Œè®©å®ƒåªå­˜åœ¨äºResourcePoolä¸­
            column_details_count = len(context.get("column_details", {}))
            context.pop("column_details", None)
            self._logger.info(
                f"ğŸ—„ï¸ [_reduce_context] ResourcePoolæ¨¡å¼ï¼šç§»é™¤contextä¸­çš„column_details "
                f"({column_details_count}å¼ è¡¨ï¼Œå·²å­˜å‚¨åœ¨ResourcePoolä¸­ï¼‰"
            )
        elif isinstance(context.get("column_details"), dict):
            # ä¼ ç»Ÿæ¨¡å¼ï¼šä¿ç•™å¹¶è£å‰ªcolumn_details
            details = context["column_details"]
            # ğŸ” [è°ƒè¯•2] _reduce_contextå¼€å§‹å¤„ç†column_details
            self._logger.info(f"ğŸ” [_reduce_contextå¼€å§‹] å½“å‰column_details: {len(details)}å¼ è¡¨ - {list(details.keys())}")
            selected_tables = set()

            # ä»å¤šä¸ªæ¥æºæ”¶é›†éœ€è¦ä¿ç•™çš„è¡¨
            # 1. ä» result ä¸­è·å–å½“å‰å·¥å…·è¿”å›çš„è¡¨
            if isinstance(result, dict):
                if isinstance(result.get("tables"), list):
                    selected_tables.update(result.get("tables"))

            # 2. ä» context.selected_tables è·å–å·²é€‰ä¸­çš„è¡¨
            if isinstance(context.get("selected_tables"), list):
                selected_tables.update(context.get("selected_tables"))

            # 3. ä» context.tables è·å–å‰5ä¸ªè¡¨ä½œä¸ºå¤‡é€‰
            if not selected_tables and isinstance(context.get("tables"), list):
                selected_tables.update(context.get("tables")[:5])

            # 4. å¦‚æœå½“å‰SQLä¸­å¼•ç”¨äº†æŸäº›è¡¨ï¼Œä¹Ÿä¿ç•™è¿™äº›è¡¨
            try:
                current_sql = context.get("current_sql", "")
                if current_sql:
                    # ä½¿ç”¨sqlparseä»SQLä¸­æå–è¡¨åï¼ˆæ›´å‡†ç¡®ï¼‰
                    sql_tables = self._extract_tables_from_sql(current_sql)
                    for t in sql_tables:
                        if t in details:
                            selected_tables.add(t)
            except Exception:
                pass

            # ğŸ”§ ä¿®å¤ï¼šå¦‚æœä»ç„¶æ²¡æœ‰é€‰ä¸­è¡¨ï¼Œä¿ç•™ column_details ä¸­å·²æœ‰çš„æ‰€æœ‰è¡¨
            # é¿å…è¯¯åˆ å·²è·å–çš„å­—æ®µä¿¡æ¯ï¼ˆç‰¹åˆ«æ˜¯åœ¨ sql_generation ç­‰ä¸è¿”å› tables çš„åŠ¨ä½œåï¼‰
            if not selected_tables:
                selected_tables.update(details.keys())
                self._logger.debug(f"ğŸ” [_reduce_context] æœªæ‰¾åˆ°æŒ‡å®šè¡¨ï¼Œä¿ç•™column_detailsä¸­çš„æ‰€æœ‰è¡¨: {list(selected_tables)}")

            new_details = {}
            for t in selected_tables:
                cols = details.get(t)
                if isinstance(cols, dict):
                    # æ¯è¡¨æœ€å¤šä¿ç•™100åˆ—çš„å…ƒä¿¡æ¯
                    limited = {}
                    for i, (col, meta) in enumerate(cols.items()):
                        if i >= 100:
                            break
                        limited[col] = meta
                    new_details[t] = limited

            # ğŸ”§ å…³é”®ä¿®å¤ï¼šåªè¦æœ‰column_detailsï¼Œå°±ä¿ç•™å®ƒ
            # PTAVå¾ªç¯éœ€è¦åœ¨å¤šè½®è¿­ä»£ä¸­æŒç»­è®¿é—®column_details
            # ä¸èƒ½å› ä¸ºæŸä¸€è½®ï¼ˆå¦‚sql_generationï¼‰ä¸è¿”å›tableså°±åˆ é™¤å®ƒ
            if new_details:
                context["column_details"] = new_details
                self._logger.debug(f"ğŸ” [_reduce_context] ä¿ç•™column_details: {len(new_details)}å¼ è¡¨ - {list(new_details.keys())}")
                # ğŸ” [è°ƒè¯•2] _reduce_contextç»“æŸ - ä¿ç•™new_details
                self._logger.info(f"ğŸ” [_reduce_contextç»“æŸ] ä¿ç•™new_details: {len(new_details)}å¼ è¡¨ - {list(new_details.keys())}")
            elif details:
                # å³ä½¿new_detailsä¸ºç©ºï¼Œå¦‚æœåŸå§‹detailså­˜åœ¨ï¼Œä¹Ÿä¿ç•™å®ƒ
                # è¿™ç¡®ä¿column_detailsåœ¨æ•´ä¸ªPTAVå¾ªç¯ä¸­æŒä¹…å­˜åœ¨
                context["column_details"] = details
                self._logger.debug(f"ğŸ” [_reduce_context] ä¿ç•™åŸå§‹column_detailsï¼ˆæœªè£å‰ªï¼‰: {len(details)}å¼ è¡¨")
                # ğŸ” [è°ƒè¯•2] _reduce_contextç»“æŸ - ä¿ç•™åŸå§‹details
                self._logger.info(f"ğŸ” [_reduce_contextç»“æŸ] ä¿ç•™åŸå§‹details: {len(details)}å¼ è¡¨ - {list(details.keys())}")
            else:
                # ğŸ” [è°ƒè¯•2] _reduce_contextç»“æŸ - column_detailsè¢«æ¸…ç©º
                self._logger.warning(f"âŒ [_reduce_contextç»“æŸ] column_detailsè¢«æ¸…ç©ºï¼")

        # åˆ é™¤ä¸å¿…è¦çš„ä¸´æ—¶/å¤§å‹é”®
        for k in ["agent_analysis", "llm_raw", "schema_scan_offset", "sql_generation_candidates"]:
            if k in context:
                context.pop(k, None)

        # ğŸ—„ï¸ [ResourcePoolæ¨¡å¼] å°† _resource_pool é‡å‘½åä¸º resource_poolï¼ˆä¾› Orchestrator ä½¿ç”¨ï¼‰
        if "_resource_pool" in context:
            context["resource_pool"] = context.pop("_resource_pool")
            self._logger.debug("ğŸ—„ï¸ [_reduce_context] å·²æ¢å¤resource_poolå¼•ç”¨ä¾›Orchestratorä½¿ç”¨")

        # ä¸¥æ ¼ä¿ç•™ç™½åå•ï¼ˆé¿å…è¯¯åˆ å·²æœ‰å…³é”®é”®ï¼‰
        keys = list(context.keys())
        for k in keys:
            if k not in keep_keys and k not in {"column_details"}:
                # ä¸åˆ é™¤ç”¨äºå†…éƒ¨ç»§ç»­ä½¿ç”¨çš„è‹¥å¹²é”®ï¼ˆå¦‚ constraints, data_source ç­‰ï¼‰
                if k in {"constraints", "data_source"}:
                    continue
                # å…¶ä»–é”®è‹¥ä¸æ˜¯å¿…é¡»ä¿ç•™çš„ï¼Œä¿ç•™ç°çŠ¶ï¼ˆé¿å…æ‰“ç ´å…¼å®¹ï¼‰ã€‚ä»…åœ¨ä¸Šé¢é’ˆå¯¹å¤§å¯¹è±¡åšè£å‰ªã€‚
                pass
