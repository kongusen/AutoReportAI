"""
æ™ºèƒ½æ£€ç´¢å™¨ - LLM å¢å¼ºç‰ˆ

åŸºäº LLM çš„æ™ºèƒ½ Schema æ£€ç´¢
ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œè¯­ä¹‰ç†è§£å’Œè¡¨ååŒ¹é…
"""

from __future__ import annotations

import logging
import hashlib
import json
from typing import Any, Dict, List, Optional, Tuple, Set
from dataclasses import dataclass, field

logger = logging.getLogger(__name__)


@dataclass
class RetrievalConfig:
    """æ£€ç´¢é…ç½®"""
    # LLM é…ç½®
    use_llm_judgment: bool = True
    llm_model: str = "gpt-4o-mini"
    
    # ç¼“å­˜é…ç½®
    enable_caching: bool = True
    cache_ttl: int = 3600  # 1å°æ—¶
    
    # è¯„åˆ†æƒé‡
    llm_weight: float = 0.8
    keyword_weight: float = 0.2


class IntelligentSchemaRetriever:
    """
    æ™ºèƒ½ Schema æ£€ç´¢å™¨

    ä½¿ç”¨ LLM è¿›è¡Œè¯­ä¹‰ç†è§£å’Œè¡¨ååŒ¹é…
    ä¸å†ä¾èµ–ç®—æ³•ï¼Œå®Œå…¨åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„åˆ¤æ–­èƒ½åŠ›
    """

    def __init__(
        self,
        schema_cache: Dict[str, Dict[str, Any]],
        config: Optional[RetrievalConfig] = None,
        container: Optional[Any] = None
    ):
        """
        Args:
            schema_cache: Schema ç¼“å­˜å­—å…¸
            config: æ£€ç´¢é…ç½®
            container: æœåŠ¡å®¹å™¨ï¼Œç”¨äºè·å–LLMæœåŠ¡
        """
        self.schema_cache = schema_cache
        self.config = config or RetrievalConfig()
        self.container = container

        # ç¼“å­˜
        self._retrieval_cache: Dict[str, List[Tuple[str, float]]] = {}

        # åˆå§‹åŒ–
        self._initialized = False

        logger.info("ğŸ” [IntelligentSchemaRetriever] åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   LLMåˆ¤æ–­: {'å¯ç”¨' if self.config.use_llm_judgment else 'ç¦ç”¨'}")
        logger.info(f"   æ¨¡å‹: {self.config.llm_model}")

    async def initialize(self):
        """åˆå§‹åŒ–æ£€ç´¢å™¨"""
        if self._initialized:
            return

        logger.info("ğŸ”§ [IntelligentSchemaRetriever] åˆå§‹åŒ–LLMæ£€ç´¢å™¨")
        
        # éªŒè¯å®¹å™¨å’ŒLLMæœåŠ¡
        if self.container is None:
            logger.error("âŒ [IntelligentSchemaRetriever] å®¹å™¨æœªæä¾›ï¼Œæ— æ³•ä½¿ç”¨LLMåˆ¤æ–­")
            raise RuntimeError("å®¹å™¨æœªæä¾›ï¼Œæ— æ³•ä½¿ç”¨LLMåˆ¤æ–­")
        
        # æ£€æŸ¥LLMæœåŠ¡æ˜¯å¦å¯ç”¨
        try:
            llm_service = self.container.llm
            if llm_service is None:
                logger.error("âŒ [IntelligentSchemaRetriever] LLMæœåŠ¡ä¸å¯ç”¨")
                raise RuntimeError("LLMæœåŠ¡ä¸å¯ç”¨")
            
            logger.info("âœ… [IntelligentSchemaRetriever] LLMæœåŠ¡éªŒè¯æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ [IntelligentSchemaRetriever] LLMæœåŠ¡éªŒè¯å¤±è´¥: {e}")
            raise RuntimeError(f"LLMæœåŠ¡éªŒè¯å¤±è´¥: {e}")

        self._initialized = True
        logger.info("âœ… [IntelligentSchemaRetriever] LLMæ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆ")

    async def retrieve(
        self,
        query: str,
        top_k: int = 5,
        stage: Optional[str] = None
    ) -> List[Tuple[str, float]]:
        """
        ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½æ£€ç´¢

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›æ•°é‡
            stage: æ‰§è¡Œé˜¶æ®µ

        Returns:
            List[Tuple[table_name, score]]: è¡¨åå’Œç›¸ä¼¼åº¦è¯„åˆ†åˆ—è¡¨
        """
        if not self._initialized:
            await self.initialize()

        # æ£€æŸ¥ç¼“å­˜
        cache_key = self._get_cache_key(query, top_k, stage)
        if self.config.enable_caching and cache_key in self._retrieval_cache:
            logger.debug(f"âœ… ä½¿ç”¨æ£€ç´¢ç¼“å­˜: {query[:50]}...")
            return self._retrieval_cache[cache_key]

        # ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½æ£€ç´¢
        results = await self._retrieve_with_llm(query, top_k, stage)

        # æ›´æ–°ç¼“å­˜
        if self.config.enable_caching:
            self._retrieval_cache[cache_key] = results
            # é™åˆ¶ç¼“å­˜å¤§å°
            if len(self._retrieval_cache) > 100:
                # åˆ é™¤æœ€æ—§çš„ç¼“å­˜
                oldest_key = next(iter(self._retrieval_cache))
                del self._retrieval_cache[oldest_key]

        return results

    async def _retrieve_with_llm(
        self,
        query: str,
        top_k: int,
        stage: Optional[str] = None
    ) -> List[Tuple[str, float]]:
        """ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½è¡¨ååŒ¹é…"""
        try:
            # è·å–LLMæœåŠ¡
            llm_service = self.container.llm
            if llm_service is None:
                raise RuntimeError("LLMæœåŠ¡ä¸å¯ç”¨")

            # æ„å»ºè¡¨ååˆ—è¡¨
            table_names = list(self.schema_cache.keys())
            if not table_names:
                logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨çš„è¡¨å")
                return []

            # æ„å»ºLLMæç¤ºè¯
            prompt = self._build_llm_prompt(query, table_names, top_k, stage)
            
            logger.info(f"ğŸ¤– [LLMæ£€ç´¢] æŸ¥è¯¢: {query[:100]}...")
            logger.info(f"ğŸ¤– [LLMæ£€ç´¢] å€™é€‰è¡¨æ•°é‡: {len(table_names)}")

            # ğŸ”¥ è°ƒç”¨LLM - ä½¿ç”¨å½“å‰ç”¨æˆ·ID
            # ä»containerä¸­è·å–å½“å‰ç”¨æˆ·IDï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
            current_user_id = getattr(self.container, '_current_user_id', None)
            if not current_user_id:
                # å°è¯•ä»context variableè·å–
                try:
                    from app.services.infrastructure.agents.llm_adapter import _CURRENT_USER_ID
                    current_user_id = _CURRENT_USER_ID.get()
                except:
                    current_user_id = None
            
            if not current_user_id:
                raise RuntimeError("æ— æ³•è·å–å½“å‰ç”¨æˆ·IDï¼Œè¯·ç¡®ä¿åœ¨è¯·æ±‚ä¸­æä¾›æœ‰æ•ˆçš„ç”¨æˆ·ID")
            
            response = await llm_service.ask(
                user_id=current_user_id,
                prompt=prompt,
                response_format="json"
            )

            # è§£æLLMå“åº”
            results = self._parse_llm_response(response, table_names)
            
            logger.info(f"âœ… [LLMæ£€ç´¢] è¿”å› {len(results)} ä¸ªè¡¨")
            return results

        except Exception as e:
            logger.error(f"âŒ [LLMæ£€ç´¢] å¤±è´¥: {e}")
            # ä¸è¿›è¡Œå›é€€ï¼Œç›´æ¥æŠ›å‡ºå¼‚å¸¸è®©ä¸»æµç¨‹å¤„ç†
            raise RuntimeError(f"LLMæ£€ç´¢å¤±è´¥: {e}")

    def _build_llm_prompt(
        self,
        query: str,
        table_names: List[str],
        top_k: int,
        stage: Optional[str] = None
    ) -> str:
        """æ„å»ºLLMæç¤ºè¯"""
        stage_context = f"å½“å‰æ‰§è¡Œé˜¶æ®µ: {stage}" if stage else "é€šç”¨æŸ¥è¯¢"
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ•°æ®åº“ä¸“å®¶ï¼Œéœ€è¦æ ¹æ®ç”¨æˆ·æŸ¥è¯¢æ‰¾åˆ°æœ€ç›¸å…³çš„æ•°æ®åº“è¡¨ã€‚

{stage_context}

ç”¨æˆ·æŸ¥è¯¢: {query}

å¯ç”¨çš„æ•°æ®åº“è¡¨:
{', '.join(table_names)}

è¯·åˆ†æç”¨æˆ·æŸ¥è¯¢çš„è¯­ä¹‰ï¼Œæ‰¾å‡ºæœ€ç›¸å…³çš„ {top_k} ä¸ªè¡¨ï¼Œå¹¶æŒ‰ç›¸å…³æ€§ä»é«˜åˆ°ä½æ’åºã€‚

è¿”å›JSONæ ¼å¼:
{{
  "reasoning": "ä½ çš„åˆ†æè¿‡ç¨‹",
  "tables": [
    {{"name": "è¡¨å1", "score": 0.95, "reason": "ä¸ºä»€ä¹ˆè¿™ä¸ªè¡¨ç›¸å…³"}},
    {{"name": "è¡¨å2", "score": 0.85, "reason": "ä¸ºä»€ä¹ˆè¿™ä¸ªè¡¨ç›¸å…³"}}
  ]
}}

è¯„åˆ†æ ‡å‡†:
- 1.0: å®Œå…¨åŒ¹é…
- 0.8-0.9: é«˜åº¦ç›¸å…³
- 0.6-0.7: ä¸­åº¦ç›¸å…³
- 0.4-0.5: ä½åº¦ç›¸å…³
- 0.0-0.3: ä¸ç›¸å…³

è¯·ç¡®ä¿è¿”å›çš„è¡¨ååœ¨å¯ç”¨è¡¨åˆ—è¡¨ä¸­ï¼Œè¯„åˆ†åœ¨0.0-1.0ä¹‹é—´ã€‚"""

        return prompt

    def _parse_llm_response(
        self,
        response: str,
        available_tables: List[str]
    ) -> List[Tuple[str, float]]:
        """è§£æLLMå“åº”"""
        try:
            # å°è¯•è§£æJSON
            if isinstance(response, str):
                # æå–JSONéƒ¨åˆ†
                start_idx = response.find('{')
                end_idx = response.rfind('}') + 1
                if start_idx != -1 and end_idx > start_idx:
                    json_str = response[start_idx:end_idx]
                    data = json.loads(json_str)
                else:
                    raise ValueError("å“åº”ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSON")
            else:
                data = response

            # ğŸ”¥ å¢å¼ºå“åº”æ ¼å¼éªŒè¯ï¼šæ”¯æŒå¤šç§æ ¼å¼
            if not isinstance(data, dict):
                raise ValueError("å“åº”ä¸æ˜¯æœ‰æ•ˆçš„å­—å…¸æ ¼å¼")
            
            # ğŸ”¥ å¤„ç†åµŒå¥—JSONæ ¼å¼
            if "response" in data and isinstance(data["response"], str):
                # å°è¯•è§£æåµŒå¥—çš„JSONå­—ç¬¦ä¸²
                try:
                    nested_data = json.loads(data["response"])
                    data = nested_data
                except json.JSONDecodeError:
                    pass
            
            # æ”¯æŒå¤šç§å“åº”æ ¼å¼
            tables = None
            if "tables" in data:
                tables = data["tables"]
            elif "matched_tables" in data:
                # å…¼å®¹æ—§æ ¼å¼
                tables = data["matched_tables"]
                if isinstance(tables, list) and all(isinstance(t, str) for t in tables):
                    # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                    tables = [{"name": t, "score": 0.8} for t in tables]
            elif "results" in data:
                tables = data["results"]
            
            if tables is None:
                raise ValueError("å“åº”ä¸­æœªæ‰¾åˆ°tablesã€matched_tablesæˆ–resultså­—æ®µ")

            if not isinstance(tables, list):
                raise ValueError("tableså­—æ®µä¸æ˜¯åˆ—è¡¨")

            # ğŸ”¥ å¢å¼ºè¡¨ä¿¡æ¯è§£æï¼šæ”¯æŒå¤šç§æ ¼å¼
            results = []
            for table_info in tables:
                if isinstance(table_info, str):
                    # ç®€å•å­—ç¬¦ä¸²æ ¼å¼
                    table_name = table_info
                    score = 0.8
                elif isinstance(table_info, dict):
                    # æ ‡å‡†æ ¼å¼
                    table_name = table_info.get("name", "")
                    score = table_info.get("score", 0.0)
                else:
                    continue
                
                # éªŒè¯è¡¨åæ˜¯å¦åœ¨å¯ç”¨åˆ—è¡¨ä¸­
                if table_name not in available_tables:
                    logger.warning(f"âš ï¸ LLMè¿”å›äº†ä¸å­˜åœ¨çš„è¡¨å: {table_name}")
                    continue
                
                # éªŒè¯è¯„åˆ†èŒƒå›´
                if not isinstance(score, (int, float)) or score < 0.0 or score > 1.0:
                    logger.warning(f"âš ï¸ LLMè¿”å›äº†æ— æ•ˆè¯„åˆ†: {score}")
                    score = 0.5  # é»˜è®¤è¯„åˆ†
                
                results.append((table_name, float(score)))

            # æŒ‰è¯„åˆ†æ’åº
            results.sort(key=lambda x: x[1], reverse=True)
            
            logger.info(f"ğŸ“Š [LLMæ£€ç´¢] è§£æç»“æœ: {len(results)} ä¸ªè¡¨")
            for table_name, score in results:
                logger.debug(f"   {table_name}: {score:.2f}")
            
            return results

        except Exception as e:
            logger.error(f"âŒ [LLMæ£€ç´¢] è§£æå“åº”å¤±è´¥: {e}")
            # ğŸ”¥ å®‰å…¨åœ°è®°å½•å“åº”å†…å®¹
            try:
                if isinstance(response, str):
                    logger.error(f"   åŸå§‹å“åº”: {response[:200]}...")
                else:
                    logger.error(f"   åŸå§‹å“åº”ç±»å‹: {type(response)}, å†…å®¹: {str(response)[:200]}...")
            except Exception as log_e:
                logger.error(f"   æ— æ³•è®°å½•å“åº”å†…å®¹: {log_e}")
            raise RuntimeError(f"LLMå“åº”è§£æå¤±è´¥: {e}")

    def _get_cache_key(self, query: str, top_k: int, stage: Optional[str]) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_data = f"{query}:{top_k}:{stage or 'default'}"
        return hashlib.md5(key_data.encode()).hexdigest()

    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self._retrieval_cache.clear()
        logger.info("ğŸ§¹ [IntelligentSchemaRetriever] æ¸…ç©ºæ£€ç´¢ç¼“å­˜")

    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        return {
            "cached_queries": len(self._retrieval_cache),
            "cache_size": f"{len(self._retrieval_cache)} queries"
        }


# å·¥å‚å‡½æ•°
def create_intelligent_retriever(
    schema_cache: Dict[str, Dict[str, Any]],
    config: Optional[RetrievalConfig] = None,
    container: Optional[Any] = None
) -> IntelligentSchemaRetriever:
    """åˆ›å»ºæ™ºèƒ½æ£€ç´¢å™¨"""
    return IntelligentSchemaRetriever(schema_cache, config, container)


def create_llm_retriever(
    schema_cache: Dict[str, Dict[str, Any]],
    container: Any,
    llm_model: str = "gpt-4o-mini"
) -> IntelligentSchemaRetriever:
    """åˆ›å»ºå¯ç”¨LLMçš„æ£€ç´¢å™¨"""
    config = RetrievalConfig(
        use_llm_judgment=True,
        llm_model=llm_model,
        llm_weight=0.8,
        keyword_weight=0.2
    )
    return IntelligentSchemaRetriever(schema_cache, config, container)