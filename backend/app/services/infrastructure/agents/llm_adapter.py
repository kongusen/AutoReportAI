"""
LLM é€‚é…å™¨

å°†å®¹å™¨çš„ LLM æœåŠ¡é€‚é…ä¸º Loom çš„ BaseLLM æ¥å£
åŸºäºç°æœ‰çš„ Container å’Œ RealLLMServiceAdapter å®ç°
"""

from __future__ import annotations

import json
import logging
import asyncio
import contextvars
import tiktoken
from typing import Any, Dict, List, Optional, Callable

from loom.interfaces.llm import BaseLLM

from .types import LLMConfig, ExecutionStage

logger = logging.getLogger(__name__)

# ä¸Šä¸‹æ–‡å˜é‡
_CURRENT_USER_ID = contextvars.ContextVar("loom_agent_user_id", default="")
_CURRENT_STAGE = contextvars.ContextVar("loom_agent_stage", default="agent_runtime")
_CURRENT_OUTPUT_KIND = contextvars.ContextVar("loom_agent_output_kind", default="text")


class ContainerLLMAdapter(BaseLLM):
    """Container LLM é€‚é…å™¨

    å°†å®¹å™¨çš„ LLM æœåŠ¡æ¡¥æ¥åˆ° Loom çš„ BaseLLM æ¥å£
    åŸºäºç°æœ‰çš„ RealLLMServiceAdapter å®ç°
    """

    def __init__(
        self,
        service: Any,
        logger: Optional[logging.Logger] = None, 
        default_user_id: str = "system",
        tool_call_callback: Optional[Callable[[str, Dict[str, Any]], None]] = None
    ) -> None:
        """åˆå§‹åŒ–é€‚é…å™¨

        Args:
            service: å®¹å™¨çš„ LLM æœåŠ¡ (RealLLMServiceAdapter)
            logger: æ—¥å¿—å™¨
            default_user_id: é»˜è®¤ç”¨æˆ· ID
            tool_call_callback: å·¥å…·è°ƒç”¨å›è°ƒå‡½æ•°
        """
        if not hasattr(service, "ask"):
            raise ValueError("Container LLM service must expose an async 'ask' method.")

        self._service = service
        self._logger = logger or logging.getLogger(self.__class__.__name__)
        self._default_user_id = default_user_id
        self._model_name = getattr(service, "default_model", "container-llm")
        self._tool_call_callback = tool_call_callback
        
        # åˆå§‹åŒ– tiktoken tokenizer
        try:
            # ä½¿ç”¨ GPT-4 çš„ tokenizer (cl100k_base)
            self._tokenizer = tiktoken.get_encoding("cl100k_base")
        except Exception as e:
            self._logger.warning(f"âš ï¸ æ— æ³•åˆå§‹åŒ– tiktokenï¼Œå°†ä½¿ç”¨ç®€å•ä¼°ç®—: {e}")
            self._tokenizer = None

    @property
    def model_name(self) -> str:
        """æ¨¡å‹åç§°"""
        return self._model_name

    @property
    def supports_tools(self) -> bool:
        """ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ ‡è®°æ­¤LLMæ”¯æŒå·¥å…·è°ƒç”¨

        è¿™ä¼šè®©Loomçš„AgentExecutorè°ƒç”¨generate_with_tools()è€Œä¸æ˜¯generate()
        ä»è€Œæ³¨å…¥å·¥å…·è°ƒç”¨æŒ‡ä»¤ï¼Œä½¿LLMèƒ½å¤Ÿæ­£ç¡®è°ƒç”¨å·¥å…·
        """
        return True

    def count_tokens(self, text: str) -> int:
        """ç²¾ç¡®è®¡ç®— token æ•°é‡"""
        if self._tokenizer:
            try:
                return len(self._tokenizer.encode(text))
            except Exception as e:
                self._logger.warning(f"âš ï¸ tiktoken è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨ä¼°ç®—: {e}")
                return self._estimate_tokens(text)
        else:
            return self._estimate_tokens(text)
    
    def _estimate_tokens(self, text: str) -> int:
        """ä¼°ç®— token æ•°é‡ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
        # ç®€å•ä¼°ç®—ï¼šä¸­æ–‡å­—ç¬¦æŒ‰ 1.5 ä¸ª token è®¡ç®—ï¼Œè‹±æ–‡æŒ‰ 0.25 ä¸ª token è®¡ç®—
        chinese_chars = len([c for c in text if '\u4e00' <= c <= '\u9fff'])
        other_chars = len(text) - chinese_chars
        return int(chinese_chars * 1.5 + other_chars * 0.25)
    
    def count_tokens_in_messages(self, messages: List[Dict]) -> int:
        """è®¡ç®—æ¶ˆæ¯åˆ—è¡¨çš„æ€» token æ•°"""
        total_tokens = 0
        for message in messages:
            content = message.get('content', '')
            if isinstance(content, str):
                total_tokens += self.count_tokens(content)
            elif isinstance(content, list):
                # å¤„ç†ç»“æ„åŒ–å†…å®¹
                for item in content:
                    if isinstance(item, dict) and 'text' in item:
                        total_tokens += self.count_tokens(item['text'])
        return total_tokens

    async def generate_response(self, messages: List[Dict], **kwargs) -> str:
        """å…¼å®¹æ€§æ–¹æ³•ï¼šä½¿ç”¨ generate() å®ç° generate_response æ¥å£"""
        self._logger.debug(f"ğŸ§  [ContainerLLMAdapter] generate_response called with {len(messages)} messages")
        try:
            result = await self.generate(messages)
            # ç¡®ä¿è¿”å›å­—ç¬¦ä¸²
            if isinstance(result, str):
                return result
            elif isinstance(result, dict):
                return result.get("content", str(result))
            else:
                return str(result)
        except Exception as e:
            self._logger.error(f"âŒ [ContainerLLMAdapter] generate_response failed: {e}")
            raise
    async def generate(self, messages: List[Dict]) -> str:
        """
        ç”Ÿæˆ LLM å“åº”
        
        ğŸ”¥ å…³é”®æ”¹è¿›ï¼šåˆå¹¶æ‰€æœ‰ messagesï¼ˆåŒ…æ‹¬ Loom æ³¨å…¥çš„ system messagesï¼‰
        è¿™æ ·å¯ä»¥ç¡®ä¿ ContextRetriever æ³¨å…¥çš„ schema context è¢«ä¼ é€’ç»™ LLM
        """
        # ğŸ”¥ åˆå¹¶æ‰€æœ‰ messages ä¸ºä¸€ä¸ªå®Œæ•´çš„ prompt
        prompt = self._compose_full_prompt(messages)
        user_id = self._extract_user_id(messages)

        self._logger.info(f"ğŸ§  [ContainerLLMAdapter] Composed prompt length: {len(prompt)} chars")
        self._logger.debug(f"   Message count: {len(messages)}, user_id: {user_id}")

        try:
            response = await self._service.ask(
                user_id=user_id,
                prompt=prompt,
                response_format={"type": "json_object"},
                llm_policy={
                    "stage": _CURRENT_STAGE.get("agent_runtime"),
                    "output_kind": _CURRENT_OUTPUT_KIND.get("text"),
                },
            )

            # âœ… æ·»åŠ å“åº”éªŒè¯
            if not response:
                self._logger.error("âŒ å®¹å™¨LLMæœåŠ¡è¿”å›ç©ºå“åº”")
                raise ValueError("LLM returned empty response")

            if isinstance(response, str) and not response.strip():
                self._logger.error("âŒ å“åº”ä¸ºç©ºå­—ç¬¦ä¸²")
                raise ValueError("LLM returned empty string")

        except Exception as exc:  # pragma: no cover - container side errors
            self._logger.error("âŒ å®¹å™¨LLMæœåŠ¡å¤±è´¥: %s", exc)
            raise

        if isinstance(response, dict):
            for key in ("response", "result", "text", "sql", "content"):
                if response.get(key):
                    self._logger.debug(
                        "ğŸ§  [ContainerLLMAdapter] user=%s key=%s preview=%s",
                        user_id,
                        key,
                        str(response[key])[:80],
                    )
                    return response[key]
            return str(response)
        
        self._logger.debug(
            "ğŸ§  [ContainerLLMAdapter] user=%s raw_response=%s", user_id, str(response)[:80]
        )
        return str(response)

    async def generate_with_tools(self, messages: List[Dict], tools: List[Dict]) -> Dict:
        """
        ç”Ÿæˆå¸¦å·¥å…·è°ƒç”¨çš„å“åº”
        
        ğŸ”¥ å…³é”®åŠŸèƒ½ï¼š
        1. å°†å·¥å…·æè¿°æ³¨å…¥åˆ° system message ä¸­
        2. æŒ‡ç¤º LLM ä»¥ JSON æ ¼å¼è¿”å›å“åº”ï¼ˆåŒ…å« tool_callsï¼‰
        3. è§£æ LLM å“åº”ï¼Œæå–å·¥å…·è°ƒç”¨
        
        è¿”å›æ ¼å¼ï¼š
        {
            "content": "...",       # LLM çš„æ–‡æœ¬å“åº”
            "tool_calls": [         # å·¥å…·è°ƒç”¨æ•°ç»„ï¼ˆå¯é€‰ï¼‰
                {
                    "id": "unique_id",
                    "type": "function",
                    "function": {
                        "name": "tool_name",
                        "arguments": "{...}"  # JSON å­—ç¬¦ä¸²
                    }
                }
            ]
        }
        """
        import uuid

        # ğŸ”¥ Step 1: æ„å»ºå·¥å…·æè¿°
        tools_desc = self._format_tools_description(tools)
        
        # ğŸ“Š ç›‘æ§ Token ä½¿ç”¨
        tools_tokens = self.count_tokens(tools_desc)
        self._logger.info(f"ğŸ“Š å·¥å…·æè¿° tokens: {tools_tokens}")

        # ğŸ”¥ Step 2: æ·»åŠ å·¥å…·è°ƒç”¨æŒ‡ä»¤åˆ° system message
        tool_system_msg = f"""
# SQLç”ŸæˆAgent - æ™ºèƒ½æ‰§è¡Œæ¨¡å¼

**æ ¸å¿ƒä»»åŠ¡**: æ ¹æ®ç”¨æˆ·éœ€æ±‚ç”Ÿæˆå‡†ç¡®çš„SQLæŸ¥è¯¢

**æ‰§è¡Œæµç¨‹**:
1. **åˆ†æéœ€æ±‚**: ç†è§£ç”¨æˆ·è¦æŸ¥è¯¢ä»€ä¹ˆæ•°æ®
2. **è·å–è¡¨ç»“æ„**: ä½¿ç”¨schemaå·¥å…·äº†è§£æ•°æ®åº“ç»“æ„
3. **ç”ŸæˆSQL**: åŸºäºè¡¨ç»“æ„ç”ŸæˆSQLæŸ¥è¯¢
4. **éªŒè¯SQL**: ç¡®ä¿SQLè¯­æ³•æ­£ç¡®å’Œé€»è¾‘åˆç†

**å¯ç”¨å·¥å…·**:
{tools_desc}

## å“åº”æ ¼å¼

**è°ƒç”¨å·¥å…·**:
```json
{{
  "reasoning": "éœ€è¦äº†è§£ods_refundè¡¨ç»“æ„",
  "action": "tool_call", 
  "tool_calls": [{{"name": "schema_discovery", "arguments": {{"tables": ["ods_refund"], "discovery_type": "columns"}}}}]
}}
```

**ç”ŸæˆSQL**:
```json
{{
  "reasoning": "å·²äº†è§£è¡¨ç»“æ„ï¼Œç”ŸæˆSQL",
  "action": "finish",
  "content": "SELECT COUNT(*) FROM ods_refund WHERE status = 'é€€è´§æˆåŠŸ'"
}}
```

**å…³é”®åŸåˆ™**:
- âœ… **ä¼˜å…ˆä½¿ç”¨schema_retrieval**: ç›´æ¥è·å–ç‰¹å®šè¡¨ç»“æ„ï¼Œä¸è¦ä½¿ç”¨schema_discovery
- âœ… **é¿å…é‡å¤è°ƒç”¨**: ä¸è¦é‡å¤è°ƒç”¨ç›¸åŒå·¥å…·
- âœ… **ç”Ÿæˆæœ‰æ•ˆSQL**: ç¡®ä¿SQLè¯­æ³•æ­£ç¡®ï¼Œå­—æ®µå­˜åœ¨
- âŒ **ä¸è¦ä½¿ç”¨schema_discovery**: ä¼šè¿”å›æ‰€æœ‰è¡¨ä¿¡æ¯ï¼Œæµªè´¹èµ„æº
- âŒ **ä¸è¦æ— é™å¾ªç¯**: æœ€å¤š2æ¬¡å·¥å…·è°ƒç”¨ï¼Œç„¶åå¿…é¡»ç”ŸæˆSQL
- âŒ **ä¸è¦ç©ºå“åº”**: å¿…é¡»è¿”å›æœ‰æ•ˆçš„SQLæŸ¥è¯¢

**ç¤ºä¾‹**:
ç”¨æˆ·: "ç»Ÿè®¡é€€è´§æˆåŠŸçš„é€€è´§å•æ•°é‡"
1. åˆ†æéœ€æ±‚ï¼šéœ€è¦ç»Ÿè®¡é€€è´§å•æ•°é‡ï¼Œæ¡ä»¶æ˜¯çŠ¶æ€ä¸º"é€€è´§æˆåŠŸ"
2. è°ƒç”¨schema_retrievalè·å–ods_refundè¡¨ç»“æ„: `{{"name": "schema_retrieval", "arguments": {{"table_names": ["ods_refund"]}}}}`
3. åŸºäºods_refundè¡¨ç”ŸæˆSQL: "SELECT COUNT(*) FROM ods_refund WHERE status = 'é€€è´§æˆåŠŸ'"
4. å®Œæˆä»»åŠ¡

**é‡è¦**: ä¼˜å…ˆä½¿ç”¨schema_retrievalå·¥å…·è·å–ç‰¹å®šè¡¨ç»“æ„ï¼Œé¿å…ä½¿ç”¨schema_discoveryè·å–æ‰€æœ‰è¡¨ä¿¡æ¯ï¼
"""

        # ğŸ”¥ Step 3: æ³¨å…¥å·¥å…·è°ƒç”¨æŒ‡ä»¤
        enhanced_messages = [{"role": "system", "content": tool_system_msg}] + messages

        # ğŸ”¥ Step 4: è°ƒç”¨ LLM
        prompt = self._compose_full_prompt(enhanced_messages)
        user_id = self._extract_user_id(enhanced_messages)

        # è®¡ç®— token æ•°é‡
        input_tokens = self.count_tokens(prompt)
        self._logger.info(f"ğŸ”§ [ContainerLLMAdapter] Calling LLM with {len(tools)} tools available")
        self._logger.info(f"ğŸ“Š [ContainerLLMAdapter] Input tokens: {input_tokens}")
        
        # ğŸ“ [DEBUG] æ·»åŠ è¯¦ç»†è°ƒè¯•æ—¥å¿—
        self._logger.info(f"ğŸ“ [DEBUG] å‘é€ç»™ LLM çš„ prompt é•¿åº¦: {len(prompt)} å­—ç¬¦")
        self._logger.info(f"ğŸ“ [DEBUG] å·¥å…·æ•°é‡: {len(tools)}")
        self._logger.info(f"ğŸ“ [DEBUG] Prompt å‰500å­—ç¬¦:\n{prompt[:500]}")

        try:
            response = await self._service.ask(
                user_id=user_id,
                prompt=prompt,
                response_format={"type": "json_object"},
                llm_policy={
                    "stage": _CURRENT_STAGE.get("agent_runtime"),
                    "output_kind": _CURRENT_OUTPUT_KIND.get("text"),
                },
            )

            # âœ… æ·»åŠ å“åº”éªŒè¯
            if not response:
                self._logger.error("âŒ å®¹å™¨LLMæœåŠ¡è¿”å›ç©ºå“åº”")
                raise ValueError("LLM returned empty response")

            if isinstance(response, str) and not response.strip():
                self._logger.error("âŒ å“åº”ä¸ºç©ºå­—ç¬¦ä¸²")
                raise ValueError("LLM returned empty string")

        except Exception as exc:
            self._logger.error("âŒ å®¹å™¨LLMæœåŠ¡å¤±è´¥: %s", exc)
            raise

        # ğŸ”¥ Step 5: è§£æå“åº”
        result = self._parse_tool_response(response)

        # ğŸ“ [DEBUG] æ·»åŠ å“åº”è§£æè°ƒè¯•æ—¥å¿—
        self._logger.info(f"ğŸ“ [DEBUG] LLM åŸå§‹å“åº”: {str(response)[:500]}")
        self._logger.info(f"ğŸ“ [DEBUG] tool_calls æ•°é‡: {len(result.get('tool_calls', []))}")

        # è®°å½•è§£æç»“æœ
        if isinstance(result, dict):
            output_content = result.get('content', '')
            tool_calls = result.get('tool_calls', [])

            # è®¡ç®—è¾“å‡º tokens
            if isinstance(output_content, str):
                output_tokens = self.count_tokens(output_content)
                self._logger.info(f"ğŸ“Š [ContainerLLMAdapter] Output tokens: {output_tokens}")

            # è®°å½•å·¥å…·è°ƒç”¨æƒ…å†µ
            if tool_calls:
                self._logger.info(f"âœ… [ContainerLLMAdapter] æˆåŠŸè§£æ {len(tool_calls)} ä¸ªå·¥å…·è°ƒç”¨")
                for i, tc in enumerate(tool_calls):
                    self._logger.info(f"   å·¥å…· {i+1}: {tc.get('name')} (id: {tc.get('id')})")
            else:
                self._logger.info(f"âœ… [ContainerLLMAdapter] LLM è¿”å›æœ€ç»ˆç­”æ¡ˆï¼ˆæ— å·¥å…·è°ƒç”¨ï¼‰")

        return result

    def _format_tools_description(self, tools: List[Dict]) -> str:
        """
        æ ¼å¼åŒ–å·¥å…·æè¿°
        
        Loom çš„å·¥å…·æ ¼å¼ï¼š
        {
            "type": "function",
            "function": {
                "name": "tool_name",
                "description": "...",
                "parameters": {...}
            }
        }
        """
        lines = []
        for tool in tools:
            # ğŸ”¥ å¤„ç† Loom çš„å·¥å…·æ ¼å¼
            if "function" in tool:
                func_spec = tool["function"]
                name = func_spec.get("name", "unknown")
                desc = func_spec.get("description", "")
                params = func_spec.get("parameters", {})
            else:
                # å…¼å®¹å…¶ä»–æ ¼å¼
                name = tool.get("name", "unknown")
                desc = tool.get("description", "")
                params = tool.get("parameters", {})

            # æå–å‚æ•°ä¿¡æ¯
            params_desc = []
            if isinstance(params, dict):
                properties = params.get("properties", {})
                required = params.get("required", [])

                for param_name, param_info in properties.items():
                    param_type = param_info.get("type", "any")
                    param_desc = param_info.get("description", "")
                    is_required = param_name in required
                    req_marker = "å¿…éœ€" if is_required else "å¯é€‰"
                    params_desc.append(f"  - {param_name} ({param_type}, {req_marker}): {param_desc}")

            tool_block = f"### {name}\n{desc}\n"
            if params_desc:
                tool_block += "å‚æ•°ï¼š\n" + "\n".join(params_desc)

            lines.append(tool_block)

        return "\n\n".join(lines)

    def _parse_tool_response(self, response: Any) -> Dict:
        """
        è§£æ LLM å“åº”ï¼Œæå–å·¥å…·è°ƒç”¨
        
        æœŸæœ›çš„ response æ ¼å¼ï¼š
        {
            "reasoning": "...",
            "action": "tool_call" | "finish",
            "tool_calls": [...],  # å¦‚æœ action == "tool_call"
            "content": "..."      # å¦‚æœ action == "finish"
        }
        """
        import uuid

        # å¤„ç†ä¸åŒçš„å“åº”æ ¼å¼
        if isinstance(response, str):
            try:
                parsed = json.loads(response)
            except json.JSONDecodeError:
                # å¦‚æœä¸æ˜¯ JSONï¼Œå½“ä½œæ™®é€šæ–‡æœ¬å“åº”
                self._logger.warning("âš ï¸ LLM response is not JSON, treating as text")
                return {"content": response, "tool_calls": []}
        elif isinstance(response, dict):
            # å°è¯•ä» dict ä¸­æå–å“åº”
            parsed = None
            for key in ("response", "result", "text", "content"):
                if key in response and response[key]:
                    try:
                        parsed = json.loads(response[key]) if isinstance(response[key], str) else response[key]
                        break
                    except (json.JSONDecodeError, TypeError):
                        continue

            if parsed is None:
                # å¦‚æœæ‰¾ä¸åˆ°æ˜ç¡®çš„å“åº”å­—æ®µï¼Œå°è¯•ç›´æ¥ä½¿ç”¨ response
                parsed = response
        else:
            self._logger.warning(f"âš ï¸ Unexpected response type: {type(response)}")
            return {"content": str(response), "tool_calls": []}

        if not isinstance(parsed, dict):
            self._logger.warning("âš ï¸ Parsed response is not a dict")
            return {"content": str(parsed), "tool_calls": []}

        # æ£€æŸ¥ action å­—æ®µ
        action = parsed.get("action", "finish")

        # ğŸ”§ è°ƒè¯•æ—¥å¿—ï¼šè®°å½•è§£æçš„ action
        self._logger.info(f"ğŸ“ [DEBUG] è§£æåçš„ action: {action}")

        if action == "tool_call":
            # æå–å·¥å…·è°ƒç”¨
            raw_tool_calls = parsed.get("tool_calls", [])
            tool_calls = []

            for tc in raw_tool_calls:
                if not isinstance(tc, dict):
                    continue

                tool_name = tc.get("name")
                tool_args = tc.get("arguments", {})

                if not tool_name:
                    self._logger.warning("âš ï¸ Tool call missing name, skipping")
                    continue

                # ğŸ”¥ è½¬æ¢ä¸º Loom æœŸæœ›çš„æ ¼å¼ï¼ˆæ‰å¹³ç»“æ„ï¼‰
                tool_calls.append({
                    "id": str(uuid.uuid4()),  # ç”Ÿæˆå”¯ä¸€ ID
                    "name": tool_name,
                    "arguments": tool_args if isinstance(tool_args, dict) else {}
                })

            self._logger.info(f"ğŸ”§ [ContainerLLMAdapter] Extracted {len(tool_calls)} tool calls")

            # è§¦å‘å·¥å…·è°ƒç”¨å›è°ƒ
            if self._tool_call_callback:
                for tool_call in tool_calls:
                    try:
                        self._tool_call_callback(tool_call["name"], tool_call["arguments"])
                    except Exception as e:
                        self._logger.warning(f"âš ï¸ Tool call callback failed: {e}")

            # è¿”å›ç»“æœ
            return {
                "content": parsed.get("reasoning", ""),  # ä½¿ç”¨ reasoning ä½œä¸º content
                "tool_calls": tool_calls
            }
        else:
            # action == "finish" æˆ–å…¶ä»–
            content = parsed.get("content") or parsed.get("sql") or parsed.get("result") or ""

            self._logger.info("âœ… [ContainerLLMAdapter] LLM returned final answer (no tool calls)")

            return {
                "content": content,
                "tool_calls": []
            }

    async def stream(self, messages: List[Dict]):
        """æµå¼ç”Ÿæˆå“åº”"""
        text = await self.generate(messages)
        for ch in text:
            await asyncio.sleep(0)
            yield ch

    async def chat_completion(self, messages: List[Dict], **kwargs) -> str:
        """
        å…¼å®¹æ€§æ–¹æ³•ï¼šä½¿ç”¨ generate() å®ç° chat_completion æ¥å£
        æŸäº›ç»„ä»¶ï¼ˆå¦‚ quality_scorerï¼‰å¯èƒ½éœ€è¦è¿™ä¸ªæ–¹æ³•

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            LLM å“åº”æ–‡æœ¬
        """
        self._logger.debug(f"ğŸ§  [ContainerLLMAdapter] chat_completion called with {len(messages)} messages")
        try:
            result = await self.generate(messages)
            # ç¡®ä¿è¿”å›å­—ç¬¦ä¸²
            if isinstance(result, str):
                return result
            elif isinstance(result, dict):
                return result.get("content", str(result))
            else:
                return str(result)
        except Exception as e:
            self._logger.error(f"âŒ [ContainerLLMAdapter] chat_completion failed: {e}")
            raise

    def _compose_full_prompt(self, messages: List[Dict], max_tokens: int = 12000) -> str:
        """
        åˆå¹¶æ‰€æœ‰ messages ä¸ºä¸€ä¸ªå®Œæ•´çš„ promptï¼Œå¹¶è¿›è¡Œæ™ºèƒ½ token ç®¡ç†
        
        ğŸ”¥ å…³é”®åŠŸèƒ½ï¼š
        1. ç¡®ä¿ Loom æ³¨å…¥çš„ system messagesï¼ˆschema contextï¼‰è¢«åŒ…å«
        2. ä½¿ç”¨æ»‘åŠ¨çª—å£æœºåˆ¶ï¼Œé¿å…é€’å½’è¿‡ç¨‹ä¸­çš„ token ç´¯ç§¯çˆ†ç‚¸
        3. ä¿ç•™æœ€é‡è¦çš„ä¿¡æ¯ï¼ˆsystem + æœ€æ–°çš„å¯¹è¯ï¼‰
        
        Token é¢„ç®—åˆ†é…ï¼š
        - System messages: æœ€å¤š 4000 tokensï¼ˆschema contextï¼‰
        - Recent conversation: æœ€å¤š 8000 tokensï¼ˆæœ€è¿‘çš„å¯¹è¯å†å²ï¼‰
        - Total: æœ€å¤š 12000 tokens
        
        æ”¯æŒçš„ message ç±»å‹ï¼š
        - system: ç³»ç»ŸæŒ‡ä»¤ï¼ˆåŒ…æ‹¬ ContextRetriever æ³¨å…¥çš„ schemaï¼‰
        - user: ç”¨æˆ·è¾“å…¥
        - assistant: åŠ©æ‰‹å“åº”
        - tool: å·¥å…·æ‰§è¡Œç»“æœ
        """
        # ç²—ç•¥ä¼°ç®—ï¼š4 chars â‰ˆ 1 token
        CHARS_PER_TOKEN = 4
        max_chars = max_tokens * CHARS_PER_TOKEN

        sections = []

        # 1. æ”¶é›†æ‰€æœ‰ system messagesï¼ˆåŒ…æ‹¬ schema contextï¼‰
        # System messages ä¼˜å…ˆçº§æœ€é«˜ï¼Œå¿…é¡»ä¿ç•™
        system_messages = [
            m.get("content", "")
            for m in messages
            if m.get("role") == "system" and m.get("content")
        ]

        system_content = ""
        if system_messages:
            # ğŸ”¥ Schema context ä¼šåœ¨è¿™é‡Œè¢«åŒ…å«ï¼
            system_content = "\n\n".join(system_messages)
            system_chars = len(system_content)
            system_tokens = system_chars // CHARS_PER_TOKEN

            self._logger.debug(f"ğŸ“‹ [ContainerLLMAdapter] System messages count: {len(system_messages)}, chars: {system_chars}, est. tokens: {system_tokens}")

            # å¦‚æœ system content è¶…è¿‡é¢„ç®—ï¼Œè£å‰ªï¼ˆä½†è¿™ä¸åº”è¯¥å‘ç”Ÿï¼Œå› ä¸º ContextAssembler å·²ç»æ§åˆ¶äº†ï¼‰
            if system_chars > (max_chars // 3):  # System æœ€å¤šå  1/3
                self._logger.warning(f"âš ï¸ [ContainerLLMAdapter] System content too large ({system_tokens} tokens), truncating")
                system_content = system_content[:max_chars // 3]

            sections.append("# SYSTEM INSTRUCTIONS\n\n" + system_content)

        # 2. æ”¶é›†å¯¹è¯å†å²ï¼ˆuser, assistant, toolï¼‰
        # ä½¿ç”¨æ»‘åŠ¨çª—å£ï¼šåªä¿ç•™æœ€è¿‘çš„ N æ¡æ¶ˆæ¯
        conversation_messages = []
        for m in messages:
            role = m.get("role")
            content = m.get("content", "")

            if role == "user":
                conversation_messages.append(f"# USER\n{content}")
            elif role == "assistant":
                conversation_messages.append(f"# ASSISTANT\n{content}")
            elif role == "tool":
                tool_name = m.get("name", "unknown")
                conversation_messages.append(f"# TOOL RESULT ({tool_name})\n{content}")

        # ğŸ”¥ æ»‘åŠ¨çª—å£æœºåˆ¶ï¼šä»æœ€æ–°çš„æ¶ˆæ¯å¼€å§‹ï¼Œé€æ­¥æ·»åŠ ï¼Œç›´åˆ°è¾¾åˆ° token é™åˆ¶
        conversation_chars_budget = max_chars - len(system_content) - 200  # ä¿ç•™ 200 chars ä½œä¸ºåˆ†éš”ç¬¦
        conversation = []
        current_chars = 0

        # ä»æœ€æ–°çš„æ¶ˆæ¯å¼€å§‹ï¼ˆreverseï¼‰
        for msg in reversed(conversation_messages):
            msg_chars = len(msg)
            if current_chars + msg_chars <= conversation_chars_budget:
                conversation.insert(0, msg)  # æ’å…¥åˆ°å¼€å¤´ï¼Œä¿æŒæ—¶é—´é¡ºåº
                current_chars += msg_chars
            else:
                # è¶…è¿‡é¢„ç®—ï¼Œåœæ­¢æ·»åŠ 
                self._logger.warning(
                    f"âš ï¸ [ContainerLLMAdapter] Conversation truncated: "
                    f"kept {len(conversation)}/{len(conversation_messages)} messages, "
                    f"{current_chars} chars, est. {current_chars // CHARS_PER_TOKEN} tokens"
                )
                break

        if conversation:
            sections.append("\n\n".join(conversation))

        # 3. åˆå¹¶æ‰€æœ‰éƒ¨åˆ†
        separator = "\n\n" + "=" * 80 + "\n\n"
        full_prompt = separator.join(sections)

        # æœ€ç»ˆæ£€æŸ¥
        final_chars = len(full_prompt)
        final_tokens = final_chars // CHARS_PER_TOKEN

        self._logger.info(
            f"ğŸ§  [ContainerLLMAdapter] Prompt composed: {final_chars} chars, "
            f"est. {final_tokens} tokens (budget: {max_tokens})"
        )
        if final_tokens > max_tokens:
            self._logger.error(
                f"âŒ [ContainerLLMAdapter] Prompt exceeds token budget! "
                f"{final_tokens} > {max_tokens}"
            )

        return full_prompt

    def _extract_user_id(self, messages: List[Dict]) -> str:
        """ä»æ¶ˆæ¯ä¸­æå–ç”¨æˆ· ID"""
        # ğŸ”¥ ä¼˜å…ˆä»context variableè·å–
        ctx_user = _CURRENT_USER_ID.get()
        if ctx_user:
            return ctx_user
        
        # ğŸ”¥ ä»æ¶ˆæ¯metadataä¸­è·å–
        for m in reversed(messages):
            metadata = m.get("metadata") or {}
            if isinstance(metadata, dict) and metadata.get("user_id"):
                return metadata["user_id"]
        
        # ğŸ”¥ å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç”¨æˆ·IDï¼ŒæŠ›å‡ºå¼‚å¸¸è€Œä¸æ˜¯ä½¿ç”¨system
        raise ValueError("æ— æ³•æå–ç”¨æˆ·IDï¼Œè¯·ç¡®ä¿åœ¨è¯·æ±‚ä¸­æä¾›æœ‰æ•ˆçš„ç”¨æˆ·ID")


# ğŸ”¥ LLMé€‚é…å™¨ç¼“å­˜ç®¡ç†å™¨
class LLMAdapterCache:
    """LLMé€‚é…å™¨ç¼“å­˜ç®¡ç†å™¨ï¼Œé¿å…é‡å¤åˆ›å»ºé€‚é…å™¨"""
    
    def __init__(self):
        self._cache: Dict[str, ContainerLLMAdapter] = {}
    
    def _generate_cache_key(self, container_id: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return f"llm_adapter:{container_id}"
    
    def get_adapter(self, container_id: str) -> Optional[ContainerLLMAdapter]:
        """è·å–ç¼“å­˜çš„é€‚é…å™¨å®ä¾‹"""
        cache_key = self._generate_cache_key(container_id)
        return self._cache.get(cache_key)
    
    def set_adapter(self, container_id: str, adapter: ContainerLLMAdapter):
        """ç¼“å­˜é€‚é…å™¨å®ä¾‹"""
        cache_key = self._generate_cache_key(container_id)
        self._cache[cache_key] = adapter
        logger.debug(f"ğŸ”§ [LLMAdapterCache] ç¼“å­˜é€‚é…å™¨: {container_id}")
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self._cache.clear()
        logger.info("ğŸ§¹ [LLMAdapterCache] æ¸…ç©ºé€‚é…å™¨ç¼“å­˜")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        return {
            "cached_adapters": len(self._cache),
            "memory_usage": f"{len(self._cache)} adapters"
        }


# å…¨å±€LLMé€‚é…å™¨ç¼“å­˜å®ä¾‹
_llm_adapter_cache = LLMAdapterCache()


def create_llm_adapter(container: Any) -> ContainerLLMAdapter:
    """åˆ›å»º LLM é€‚é…å™¨ï¼ˆå¸¦ç¼“å­˜ï¼‰

    Args:
        container: ä¾èµ–å®¹å™¨

    Returns:
        LLM é€‚é…å™¨å®ä¾‹
    """
    # ğŸ”¥ ä½¿ç”¨ç¼“å­˜æœºåˆ¶
    container_id = str(id(container))
    cached_adapter = _llm_adapter_cache.get_adapter(container_id)
    
    if cached_adapter:
        logger.info(f"â™»ï¸ [LLMAdapter] ä½¿ç”¨ç¼“å­˜é€‚é…å™¨: {container_id}")
        return cached_adapter
    
    # åˆ›å»ºæ–°çš„é€‚é…å™¨
    logger.info(f"ğŸ”§ [LLMAdapter] åˆ›å»ºæ–°é€‚é…å™¨: {container_id}")
    
    # ä» container è·å– LLM æœåŠ¡
    llm_service = container.llm
    
    adapter = ContainerLLMAdapter(
        service=llm_service,
        default_user_id=None  # ğŸ”¥ ä¸è®¾ç½®é»˜è®¤å€¼ï¼Œè®©_extract_user_idæ–¹æ³•å¤„ç†
    )
    
    # ç¼“å­˜é€‚é…å™¨
    _llm_adapter_cache.set_adapter(container_id, adapter)
    
    # è®°å½•ç¼“å­˜ç»Ÿè®¡
    cache_stats = _llm_adapter_cache.get_cache_stats()
    logger.info(f"ğŸ“¦ [LLMAdapter] é€‚é…å™¨åˆ›å»ºå®Œæˆï¼Œç¼“å­˜ç»Ÿè®¡: {cache_stats}")
    
    return adapter


def create_llm_adapter_from_config(config: LLMConfig, container: Any) -> ContainerLLMAdapter:
    """ä»é…ç½®åˆ›å»º LLM é€‚é…å™¨
    
    Args:
        config: LLM é…ç½®
        container: ä¾èµ–å®¹å™¨
        
    Returns:
        LLM é€‚é…å™¨å®ä¾‹
    """
    adapter = create_llm_adapter(container)
    
    # åº”ç”¨é…ç½®
    if config.temperature is not None:
        # æ³¨æ„ï¼šContainerLLMAdapter ä¸ç›´æ¥æ”¯æŒ temperature é…ç½®
        # è¿™éœ€è¦é€šè¿‡ LLM ç­–ç•¥ä¼ é€’
        pass
    
    return adapter


# åˆ«åå‡½æ•°
def get_llm_adapter(container: Any) -> ContainerLLMAdapter:
    """get_llm_adapter åˆ«åå‡½æ•°ï¼Œå‘åå…¼å®¹"""
    return create_llm_adapter(container)


# å¯¼å‡º
__all__ = [
    "ContainerLLMAdapter",
    "create_llm_adapter",
    "get_llm_adapter",  # æ·»åŠ åˆ«å
    "create_llm_adapter_from_config",
]