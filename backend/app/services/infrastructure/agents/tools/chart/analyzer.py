from __future__ import annotations

from loom.interfaces.tool import BaseTool
"""
å›¾è¡¨åˆ†æå·¥å…·

åˆ†æå›¾è¡¨æ•°æ®ç‰¹å¾å’Œæ¨¡å¼
æä¾›å›¾è¡¨ä¼˜åŒ–å»ºè®®å’Œæ´å¯Ÿ
"""


import logging
import math
from typing import Any, Dict, List, Optional, Union, Literal
from dataclasses import dataclass
from enum import Enum
from pydantic import BaseModel, Field


from ...types import ToolCategory, ContextInfo

logger = logging.getLogger(__name__)


class AnalysisFocus(str, Enum):
    """åˆ†æé‡ç‚¹"""
    PATTERNS = "patterns"           # æ¨¡å¼è¯†åˆ«
    TRENDS = "trends"              # è¶‹åŠ¿åˆ†æ
    OUTLIERS = "outliers"          # å¼‚å¸¸å€¼æ£€æµ‹
    CORRELATIONS = "correlations"  # ç›¸å…³æ€§åˆ†æ
    DISTRIBUTION = "distribution"   # åˆ†å¸ƒåˆ†æ
    OPTIMIZATION = "optimization"   # ä¼˜åŒ–å»ºè®®


class ChartQuality(str, Enum):
    """å›¾è¡¨è´¨é‡"""
    EXCELLENT = "excellent"
    GOOD = "good"
    FAIR = "fair"
    POOR = "poor"


@dataclass
class AnalysisConfig:
    """åˆ†æé…ç½®"""
    analysis_focus: List[AnalysisFocus]
    sensitivity: float = 0.5  # 0-1ä¹‹é—´ï¼Œåˆ†ææ•æ„Ÿåº¦
    include_recommendations: bool = True
    metadata: Dict[str, Any] = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


@dataclass
class AnalysisResult:
    """åˆ†æç»“æœ"""
    analysis_focus: AnalysisFocus
    findings: List[str]
    metrics: Dict[str, Any]
    quality_score: float
    recommendations: List[str]
    metadata: Dict[str, Any] = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


@dataclass
class ComprehensiveChartAnalysis:
    """ç»¼åˆå›¾è¡¨åˆ†æ"""
    chart_quality: ChartQuality
    overall_score: float
    analysis_results: List[AnalysisResult]
    key_insights: List[str]
    optimization_suggestions: List[str]
    metadata: Dict[str, Any] = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


class ChartAnalyzerTool(BaseTool):
    """å›¾è¡¨åˆ†æå·¥å…·"""
    
    def __init__(self, container: Any):
        """
        Args:
            container: æœåŠ¡å®¹å™¨
        """
        super().__init__()

        self.name = "chart_analyzer"

        self.category = ToolCategory.CHART

        self.description = "åˆ†æå›¾è¡¨æ•°æ®ç‰¹å¾å’Œæ¨¡å¼" 
        self.container = container
        
        # ä½¿ç”¨ Pydantic å®šä¹‰å‚æ•°æ¨¡å¼ï¼ˆargs_schemaï¼‰
        class ChartAnalyzerArgs(BaseModel):
            chart_data: Dict[str, Any] = Field(description="å›¾è¡¨æ•°æ®")
            chart_config: Optional[Dict[str, Any]] = Field(default=None, description="å›¾è¡¨é…ç½®")
            analysis_focus: Optional[List[Literal[
                "patterns", "trends", "outliers", "correlations", "distribution", "optimization"
            ]]] = Field(default=["patterns", "trends", "outliers"], description="åˆ†æé‡ç‚¹")
            sensitivity: float = Field(default=0.5, description="åˆ†ææ•æ„Ÿåº¦ (0-1)")
            include_recommendations: bool = Field(default=True, description="æ˜¯å¦åŒ…å«å»ºè®®")

        self.args_schema = ChartAnalyzerArgs
    
    def get_schema(self) -> Dict[str, Any]:
        """è·å–å·¥å…·å‚æ•°æ¨¡å¼ï¼ˆåŸºäº args_schema ç”Ÿæˆï¼‰"""
        try:
            parameters = self.args_schema.model_json_schema()
        except Exception:
            parameters = self.args_schema.schema()  # type: ignore[attr-defined]
        return {
            "type": "function",
            "function": {
                "name": "chart_analyzer",
                "description": "åˆ†æå›¾è¡¨æ•°æ®ç‰¹å¾å’Œæ¨¡å¼",
                "parameters": parameters,
            },
        }
    
    async def run(

    
        self,
        chart_data: Dict[str, Any],
        chart_config: Optional[Dict[str, Any]] = None,
        analysis_focus: Optional[List[str]] = None,
        sensitivity: float = 0.5,
        include_recommendations: bool = True,
        **kwargs
    

    
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œå›¾è¡¨åˆ†æ
        
        Args:
            chart_data: å›¾è¡¨æ•°æ®
            chart_config: å›¾è¡¨é…ç½®
            analysis_focus: åˆ†æé‡ç‚¹
            sensitivity: åˆ†ææ•æ„Ÿåº¦
            include_recommendations: æ˜¯å¦åŒ…å«å»ºè®®
            
        Returns:
            Dict[str, Any]: åˆ†æç»“æœ
        """
        logger.info(f"ğŸ” [ChartAnalyzerTool] åˆ†æå›¾è¡¨")
        logger.info(f"   åˆ†æé‡ç‚¹: {analysis_focus}")

    
    async def execute(self, **kwargs) -> Dict[str, Any]:

    
        """å‘åå…¼å®¹çš„executeæ–¹æ³•"""

    
        return await self.run(**kwargs)
        logger.info(f"   æ•æ„Ÿåº¦: {sensitivity}")
        
        try:
            if not chart_data:
                return {
                    "success": False,
                    "error": "å›¾è¡¨æ•°æ®ä¸ºç©º",
                    "result": None
                }
            
            # è®¾ç½®é»˜è®¤åˆ†æé‡ç‚¹
            if analysis_focus is None:
                analysis_focus = ["patterns", "trends", "outliers"]
            
            # æ„å»ºåˆ†æé…ç½®
            config = AnalysisConfig(
                analysis_focus=[AnalysisFocus(f) for f in analysis_focus],
                sensitivity=sensitivity,
                include_recommendations=include_recommendations
            )
            
            # æ‰§è¡Œåˆ†æ
            analysis_results = []
            for focus in config.analysis_focus:
                result = await self._perform_analysis(chart_data, chart_config, focus, config)
                analysis_results.append(result)
            
            # è®¡ç®—æ•´ä½“è´¨é‡åˆ†æ•°
            overall_score = self._calculate_overall_score(analysis_results)
            chart_quality = self._determine_chart_quality(overall_score)
            
            # æå–å…³é”®æ´å¯Ÿ
            key_insights = self._extract_key_insights(analysis_results)
            
            # ç”Ÿæˆä¼˜åŒ–å»ºè®®
            optimization_suggestions = []
            if include_recommendations:
                optimization_suggestions = self._generate_optimization_suggestions(
                    chart_data, chart_config, analysis_results
                )
            
            comprehensive_analysis = ComprehensiveChartAnalysis(
                chart_quality=chart_quality,
                overall_score=overall_score,
                analysis_results=analysis_results,
                key_insights=key_insights,
                optimization_suggestions=optimization_suggestions
            )
            
            return {
                "success": True,
                "result": comprehensive_analysis,
                "metadata": {
                    "analysis_focus": analysis_focus,
                    "sensitivity": sensitivity,
                    "overall_score": overall_score,
                    "chart_quality": chart_quality.value,
                    "insights_count": len(key_insights),
                    "suggestions_count": len(optimization_suggestions)
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ [ChartAnalyzerTool] åˆ†æå¤±è´¥: {e}", exc_info=True)
            return {
                "success": False,
                "error": str(e),
                "result": None
            }
    
    async def _perform_analysis(
        self,
        chart_data: Dict[str, Any],
        chart_config: Optional[Dict[str, Any]],
        focus: AnalysisFocus,
        config: AnalysisConfig
    ) -> AnalysisResult:
        """æ‰§è¡Œç‰¹å®šåˆ†æ"""
        if focus == AnalysisFocus.PATTERNS:
            return self._analyze_patterns(chart_data, chart_config, config)
        elif focus == AnalysisFocus.TRENDS:
            return self._analyze_trends(chart_data, chart_config, config)
        elif focus == AnalysisFocus.OUTLIERS:
            return self._analyze_outliers(chart_data, chart_config, config)
        elif focus == AnalysisFocus.CORRELATIONS:
            return self._analyze_correlations(chart_data, chart_config, config)
        elif focus == AnalysisFocus.DISTRIBUTION:
            return self._analyze_distribution(chart_data, chart_config, config)
        elif focus == AnalysisFocus.OPTIMIZATION:
            return self._analyze_optimization(chart_data, chart_config, config)
        else:
            return AnalysisResult(
                analysis_focus=focus,
                findings=[],
                metrics={},
                quality_score=0.0,
                recommendations=[]
            )
    
    def _analyze_patterns(self, chart_data: Dict[str, Any], chart_config: Optional[Dict[str, Any]], config: AnalysisConfig) -> AnalysisResult:
        """åˆ†ææ¨¡å¼"""
        findings = []
        metrics = {}
        recommendations = []
        
        # åˆ†ææ•°æ®æ¨¡å¼
        values = self._extract_values(chart_data)
        
        if values:
            # æ£€æŸ¥å‘¨æœŸæ€§
            periodicity = self._detect_periodicity(values)
            if periodicity > 0:
                findings.append(f"æ£€æµ‹åˆ°å‘¨æœŸæ€§æ¨¡å¼ï¼Œå‘¨æœŸé•¿åº¦: {periodicity}")
                metrics["periodicity"] = periodicity
            
            # æ£€æŸ¥å•è°ƒæ€§
            monotonicity = self._detect_monotonicity(values)
            if monotonicity != "none":
                findings.append(f"æ£€æµ‹åˆ°{monotonicity}å•è°ƒæ¨¡å¼")
                metrics["monotonicity"] = monotonicity
            
            # æ£€æŸ¥æ³¢åŠ¨æ€§
            volatility = self._calculate_volatility(values)
            metrics["volatility"] = volatility
            
            if volatility > config.sensitivity:
                findings.append("æ•°æ®æ³¢åŠ¨æ€§è¾ƒå¤§")
                recommendations.append("è€ƒè™‘ä½¿ç”¨å¹³æ»‘æŠ€æœ¯å‡å°‘å™ªå£°")
            else:
                findings.append("æ•°æ®ç›¸å¯¹ç¨³å®š")
        
        # åˆ†æåˆ†ç±»æ¨¡å¼
        categories = chart_data.get("categories", [])
        if categories:
            category_distribution = self._analyze_category_distribution(categories, values)
            metrics["category_distribution"] = category_distribution
            
            if category_distribution.get("dominant_category"):
                findings.append("å­˜åœ¨ä¸»å¯¼åˆ†ç±»")
                recommendations.append("è€ƒè™‘çªå‡ºæ˜¾ç¤ºä¸»å¯¼åˆ†ç±»æˆ–åˆ†æå…¶ä»–åˆ†ç±»")
        
        quality_score = self._calculate_pattern_quality_score(metrics)
        
        return AnalysisResult(
            analysis_focus=AnalysisFocus.PATTERNS,
            findings=findings,
            metrics=metrics,
            quality_score=quality_score,
            recommendations=recommendations
        )
    
    def _analyze_trends(self, chart_data: Dict[str, Any], chart_config: Optional[Dict[str, Any]], config: AnalysisConfig) -> AnalysisResult:
        """åˆ†æè¶‹åŠ¿"""
        findings = []
        metrics = {}
        recommendations = []
        
        values = self._extract_values(chart_data)
        
        if len(values) >= 3:
            # è®¡ç®—è¶‹åŠ¿
            trend_info = self._calculate_trend(values)
            metrics["trend"] = trend_info
            
            trend_direction = trend_info.get("direction", "stable")
            trend_strength = trend_info.get("strength", 0)
            
            if trend_direction == "increasing":
                findings.append("æ•°æ®å‘ˆä¸Šå‡è¶‹åŠ¿")
                if trend_strength > 0.7:
                    findings.append("è¶‹åŠ¿å¼ºåº¦è¾ƒå¼º")
            elif trend_direction == "decreasing":
                findings.append("æ•°æ®å‘ˆä¸‹é™è¶‹åŠ¿")
                if trend_strength > 0.7:
                    findings.append("è¶‹åŠ¿å¼ºåº¦è¾ƒå¼º")
            else:
                findings.append("æ•°æ®è¶‹åŠ¿ç›¸å¯¹ç¨³å®š")
            
            # åˆ†æè¶‹åŠ¿å˜åŒ–ç‚¹
            change_points = self._detect_change_points(values)
            if change_points:
                findings.append(f"æ£€æµ‹åˆ° {len(change_points)} ä¸ªè¶‹åŠ¿å˜åŒ–ç‚¹")
                metrics["change_points"] = change_points
                recommendations.append("å…³æ³¨è¶‹åŠ¿å˜åŒ–ç‚¹ï¼Œå¯èƒ½åŒ…å«é‡è¦ä¿¡æ¯")
            
            # é¢„æµ‹è¶‹åŠ¿
            if len(values) >= 5:
                prediction = self._predict_trend(values)
                metrics["prediction"] = prediction
                
                if prediction.get("confidence", 0) > 0.6:
                    findings.append(f"é¢„æµ‹è¶‹åŠ¿: {prediction.get('direction', 'unknown')}")
        
        quality_score = self._calculate_trend_quality_score(metrics)
        
        return AnalysisResult(
            analysis_focus=AnalysisFocus.TRENDS,
            findings=findings,
            metrics=metrics,
            quality_score=quality_score,
            recommendations=recommendations
        )
    
    def _analyze_outliers(self, chart_data: Dict[str, Any], chart_config: Optional[Dict[str, Any]], config: AnalysisConfig) -> AnalysisResult:
        """åˆ†æå¼‚å¸¸å€¼"""
        findings = []
        metrics = {}
        recommendations = []
        
        values = self._extract_values(chart_data)
        
        if len(values) >= 4:
            # æ£€æµ‹å¼‚å¸¸å€¼
            outliers = self._detect_outliers(values, config.sensitivity)
            metrics["outliers"] = outliers
            metrics["outlier_count"] = len(outliers)
            metrics["outlier_percentage"] = len(outliers) / len(values) * 100
            
            if outliers:
                findings.append(f"æ£€æµ‹åˆ° {len(outliers)} ä¸ªå¼‚å¸¸å€¼")
                
                if len(outliers) / len(values) > 0.1:  # è¶…è¿‡10%
                    findings.append("å¼‚å¸¸å€¼æ¯”ä¾‹è¾ƒé«˜")
                    recommendations.append("å»ºè®®æ£€æŸ¥æ•°æ®è´¨é‡æˆ–è€ƒè™‘å¼‚å¸¸å€¼å¤„ç†")
                else:
                    findings.append("å¼‚å¸¸å€¼æ¯”ä¾‹æ­£å¸¸")
                    recommendations.append("å¼‚å¸¸å€¼å¯èƒ½åŒ…å«é‡è¦ä¿¡æ¯ï¼Œå»ºè®®è¿›ä¸€æ­¥åˆ†æ")
            else:
                findings.append("æœªæ£€æµ‹åˆ°æ˜æ˜¾å¼‚å¸¸å€¼")
        
        quality_score = self._calculate_outlier_quality_score(metrics)
        
        return AnalysisResult(
            analysis_focus=AnalysisFocus.OUTLIERS,
            findings=findings,
            metrics=metrics,
            quality_score=quality_score,
            recommendations=recommendations
        )
    
    def _analyze_correlations(self, chart_data: Dict[str, Any], chart_config: Optional[Dict[str, Any]], config: AnalysisConfig) -> AnalysisResult:
        """åˆ†æç›¸å…³æ€§"""
        findings = []
        metrics = {}
        recommendations = []
        
        # åˆ†æå¤šç³»åˆ—æ•°æ®çš„ç›¸å…³æ€§
        series = chart_data.get("series", [])
        
        if len(series) >= 2:
            correlations = []
            
            for i in range(len(series)):
                for j in range(i + 1, len(series)):
                    series1_values = series[i].get("data", [])
                    series2_values = series[j].get("data", [])
                    
                    if len(series1_values) == len(series2_values) and len(series1_values) > 1:
                        correlation = self._calculate_correlation(series1_values, series2_values)
                        correlations.append({
                            "series1": series[i].get("name", f"Series {i}"),
                            "series2": series[j].get("name", f"Series {j}"),
                            "correlation": correlation
                        })
            
            metrics["correlations"] = correlations
            
            # åˆ†æå¼ºç›¸å…³æ€§
            strong_correlations = [c for c in correlations if abs(c["correlation"]) > 0.7]
            
            if strong_correlations:
                findings.append(f"å‘ç° {len(strong_correlations)} å¯¹å¼ºç›¸å…³å˜é‡")
                for corr in strong_correlations[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                    findings.append(f"'{corr['series1']}' å’Œ '{corr['series2']}' ç›¸å…³ç³»æ•°: {corr['correlation']:.3f}")
                
                recommendations.append("å¼ºç›¸å…³å˜é‡å¯èƒ½è¡¨ç¤ºå› æœå…³ç³»æˆ–å…±åŒå½±å“å› ç´ ")
            else:
                findings.append("æœªå‘ç°å¼ºç›¸å…³å…³ç³»")
        
        quality_score = self._calculate_correlation_quality_score(metrics)
        
        return AnalysisResult(
            analysis_focus=AnalysisFocus.CORRELATIONS,
            findings=findings,
            metrics=metrics,
            quality_score=quality_score,
            recommendations=recommendations
        )
    
    def _analyze_distribution(self, chart_data: Dict[str, Any], chart_config: Optional[Dict[str, Any]], config: AnalysisConfig) -> AnalysisResult:
        """åˆ†æåˆ†å¸ƒ"""
        findings = []
        metrics = {}
        recommendations = []
        
        values = self._extract_values(chart_data)
        
        if values:
            # åˆ†æåˆ†å¸ƒç‰¹å¾
            distribution_info = self._analyze_distribution_features(values)
            metrics["distribution"] = distribution_info
            
            # æ£€æŸ¥æ­£æ€æ€§
            if distribution_info.get("is_normal", False):
                findings.append("æ•°æ®è¿‘ä¼¼æ­£æ€åˆ†å¸ƒ")
            else:
                skewness = distribution_info.get("skewness", 0)
                if abs(skewness) > 1:
                    findings.append(f"æ•°æ®å‘ˆ{'å³' if skewness > 0 else 'å·¦'}ååˆ†å¸ƒ")
                    recommendations.append("åæ€åˆ†å¸ƒå¯èƒ½å½±å“ç»Ÿè®¡åˆ†æï¼Œè€ƒè™‘æ•°æ®å˜æ¢")
            
            # æ£€æŸ¥å³°åº¦
            kurtosis = distribution_info.get("kurtosis", 0)
            if kurtosis > 3:
                findings.append("æ•°æ®å‘ˆå°–å³°åˆ†å¸ƒ")
            elif kurtosis < 3:
                findings.append("æ•°æ®å‘ˆå¹³å³°åˆ†å¸ƒ")
            
            # åˆ†æé›†ä¸­åº¦
            concentration = self._calculate_concentration(values)
            metrics["concentration"] = concentration
            
            if concentration > 0.8:
                findings.append("æ•°æ®é«˜åº¦é›†ä¸­")
                recommendations.append("é«˜åº¦é›†ä¸­çš„æ•°æ®å¯èƒ½ç¼ºä¹å˜å¼‚æ€§")
            elif concentration < 0.3:
                findings.append("æ•°æ®åˆ†æ•£åº¦è¾ƒé«˜")
                recommendations.append("åˆ†æ•£çš„æ•°æ®å¯èƒ½åŒ…å«å¤šä¸ªå­ç¾¤ä½“")
        
        quality_score = self._calculate_distribution_quality_score(metrics)
        
        return AnalysisResult(
            analysis_focus=AnalysisFocus.DISTRIBUTION,
            findings=findings,
            metrics=metrics,
            quality_score=quality_score,
            recommendations=recommendations
        )
    
    def _analyze_optimization(self, chart_data: Dict[str, Any], chart_config: Optional[Dict[str, Any]], config: AnalysisConfig) -> AnalysisResult:
        """åˆ†æä¼˜åŒ–å»ºè®®"""
        findings = []
        metrics = {}
        recommendations = []
        
        # åˆ†æå›¾è¡¨é…ç½®
        if chart_config:
            config_analysis = self._analyze_chart_config(chart_config)
            metrics["config_analysis"] = config_analysis
            
            # æ£€æŸ¥æ ‡é¢˜
            if not chart_config.get("title", {}).get("text"):
                findings.append("ç¼ºå°‘å›¾è¡¨æ ‡é¢˜")
                recommendations.append("å»ºè®®æ·»åŠ æè¿°æ€§æ ‡é¢˜")
            
            # æ£€æŸ¥åæ ‡è½´æ ‡ç­¾
            if not chart_config.get("xAxis", {}).get("title"):
                findings.append("ç¼ºå°‘Xè½´æ ‡ç­¾")
                recommendations.append("å»ºè®®æ·»åŠ Xè½´æ ‡ç­¾")
            
            if not chart_config.get("yAxis", {}).get("title"):
                findings.append("ç¼ºå°‘Yè½´æ ‡ç­¾")
                recommendations.append("å»ºè®®æ·»åŠ Yè½´æ ‡ç­¾")
        
        # åˆ†ææ•°æ®è´¨é‡
        data_quality = self._analyze_data_quality(chart_data)
        metrics["data_quality"] = data_quality
        
        if data_quality.get("missing_data_percentage", 0) > 10:
            findings.append("æ•°æ®ç¼ºå¤±ç‡è¾ƒé«˜")
            recommendations.append("å»ºè®®å¤„ç†ç¼ºå¤±æ•°æ®æˆ–è¯´æ˜æ•°æ®é™åˆ¶")
        
        if data_quality.get("data_points", 0) < 10:
            findings.append("æ•°æ®ç‚¹è¾ƒå°‘")
            recommendations.append("å»ºè®®å¢åŠ æ•°æ®ç‚¹ä»¥æé«˜å¯ä¿¡åº¦")
        
        # åˆ†æå¯è§†åŒ–æ•ˆæœ
        visualization_score = self._calculate_visualization_score(chart_data, chart_config)
        metrics["visualization_score"] = visualization_score
        
        if visualization_score < 0.6:
            findings.append("å¯è§†åŒ–æ•ˆæœæœ‰å¾…æ”¹å–„")
            recommendations.append("å»ºè®®è°ƒæ•´å›¾è¡¨ç±»å‹æˆ–æ ·å¼")
        
        quality_score = self._calculate_optimization_quality_score(metrics)
        
        return AnalysisResult(
            analysis_focus=AnalysisFocus.OPTIMIZATION,
            findings=findings,
            metrics=metrics,
            quality_score=quality_score,
            recommendations=recommendations
        )
    
    def _extract_values(self, chart_data: Dict[str, Any]) -> List[float]:
        """æå–æ•°å€¼"""
        values = []
        
        # ä»seriesä¸­æå–
        series = chart_data.get("series", [])
        for s in series:
            data = s.get("data", [])
            for item in data:
                if isinstance(item, (int, float)):
                    values.append(float(item))
                elif isinstance(item, list) and len(item) >= 2:
                    values.append(float(item[1]))  # å‡è®¾æ˜¯[x, y]æ ¼å¼
        
        # ä»valuesä¸­æå–
        if not values:
            values = chart_data.get("values", [])
            values = [float(v) for v in values if isinstance(v, (int, float))]
        
        return values
    
    def _detect_periodicity(self, values: List[float]) -> int:
        """æ£€æµ‹å‘¨æœŸæ€§"""
        if len(values) < 6:
            return 0
        
        # ç®€åŒ–çš„å‘¨æœŸæ€§æ£€æµ‹
        n = len(values)
        max_period = min(n // 2, 20)  # æœ€å¤§å‘¨æœŸé•¿åº¦
        
        best_period = 0
        best_score = 0
        
        for period in range(2, max_period + 1):
            score = 0
            for i in range(period, n):
                if abs(values[i] - values[i - period]) < 0.1:  # ç®€åŒ–çš„ç›¸ä¼¼æ€§æ£€æŸ¥
                    score += 1
            
            if score > best_score:
                best_score = score
                best_period = period
        
        return best_period if best_score > len(values) * 0.3 else 0
    
    def _detect_monotonicity(self, values: List[float]) -> str:
        """æ£€æµ‹å•è°ƒæ€§"""
        if len(values) < 2:
            return "none"
        
        increasing_count = 0
        decreasing_count = 0
        
        for i in range(1, len(values)):
            if values[i] > values[i - 1]:
                increasing_count += 1
            elif values[i] < values[i - 1]:
                decreasing_count += 1
        
        total_changes = increasing_count + decreasing_count
        if total_changes == 0:
            return "stable"
        
        increasing_ratio = increasing_count / total_changes
        decreasing_ratio = decreasing_count / total_changes
        
        if increasing_ratio > 0.8:
            return "increasing"
        elif decreasing_ratio > 0.8:
            return "decreasing"
        else:
            return "mixed"
    
    def _calculate_volatility(self, values: List[float]) -> float:
        """è®¡ç®—æ³¢åŠ¨æ€§"""
        if len(values) < 2:
            return 0.0
        
        mean = sum(values) / len(values)
        variance = sum((x - mean) ** 2 for x in values) / len(values)
        std = math.sqrt(variance)
        
        return std / mean if mean != 0 else 0.0
    
    def _analyze_category_distribution(self, categories: List[str], values: List[float]) -> Dict[str, Any]:
        """åˆ†æåˆ†ç±»åˆ†å¸ƒ"""
        if not categories or not values:
            return {}
        
        # è®¡ç®—æ¯ä¸ªåˆ†ç±»çš„å€¼
        category_values = {}
        for i, category in enumerate(categories):
            if i < len(values):
                category_values[category] = values[i]
        
        # æ‰¾åˆ°ä¸»å¯¼åˆ†ç±»
        if category_values:
            max_value = max(category_values.values())
            dominant_category = max(category_values.items(), key=lambda x: x[1])[0]
            
            return {
                "dominant_category": dominant_category,
                "dominant_percentage": max_value / sum(category_values.values()) * 100,
                "category_count": len(categories),
                "distribution_entropy": self._calculate_entropy(list(category_values.values()))
            }
        
        return {}
    
    def _calculate_entropy(self, values: List[float]) -> float:
        """è®¡ç®—ç†µ"""
        if not values:
            return 0.0
        
        total = sum(values)
        if total == 0:
            return 0.0
        
        entropy = 0.0
        for value in values:
            if value > 0:
                p = value / total
                entropy -= p * math.log2(p)
        
        return entropy
    
    def _calculate_trend(self, values: List[float]) -> Dict[str, Any]:
        """è®¡ç®—è¶‹åŠ¿"""
        if len(values) < 2:
            return {"direction": "stable", "strength": 0.0}
        
        n = len(values)
        x_values = list(range(n))
        
        # è®¡ç®—çº¿æ€§å›å½’
        mean_x = sum(x_values) / n
        mean_y = sum(values) / n
        
        numerator = sum((x - mean_x) * (y - mean_y) for x, y in zip(x_values, values))
        denominator = sum((x - mean_x) ** 2 for x in x_values)
        
        if denominator == 0:
            slope = 0
        else:
            slope = numerator / denominator
        
        # è®¡ç®—RÂ²
        y_pred = [mean_y + slope * (x - mean_x) for x in x_values]
        ss_res = sum((y - pred) ** 2 for y, pred in zip(values, y_pred))
        ss_tot = sum((y - mean_y) ** 2 for y in values)
        
        r_squared = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0
        
        # ç¡®å®šè¶‹åŠ¿æ–¹å‘
        if slope > 0.01:
            direction = "increasing"
        elif slope < -0.01:
            direction = "decreasing"
        else:
            direction = "stable"
        
        return {
            "direction": direction,
            "slope": slope,
            "strength": r_squared,
            "r_squared": r_squared
        }
    
    def _detect_change_points(self, values: List[float]) -> List[int]:
        """æ£€æµ‹å˜åŒ–ç‚¹"""
        if len(values) < 5:
            return []
        
        change_points = []
        
        # ç®€åŒ–çš„å˜åŒ–ç‚¹æ£€æµ‹
        for i in range(2, len(values) - 2):
            # è®¡ç®—å‰åçª—å£çš„å¹³å‡å€¼
            before_avg = sum(values[i-2:i]) / 2
            after_avg = sum(values[i:i+2]) / 2
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ˜¾è‘—å˜åŒ–
            if abs(after_avg - before_avg) > 2 * self._calculate_volatility(values[:i]):
                change_points.append(i)
        
        return change_points
    
    def _predict_trend(self, values: List[float]) -> Dict[str, Any]:
        """é¢„æµ‹è¶‹åŠ¿"""
        if len(values) < 5:
            return {"direction": "unknown", "confidence": 0.0}
        
        # ä½¿ç”¨æœ€åå‡ ä¸ªç‚¹è¿›è¡Œç®€å•é¢„æµ‹
        recent_values = values[-5:]
        trend = self._calculate_trend(recent_values)
        
        return {
            "direction": trend["direction"],
            "confidence": min(trend["strength"], 0.9)
        }
    
    def _detect_outliers(self, values: List[float], sensitivity: float) -> List[float]:
        """æ£€æµ‹å¼‚å¸¸å€¼"""
        if len(values) < 4:
            return []
        
        # ä½¿ç”¨IQRæ–¹æ³•
        sorted_values = sorted(values)
        n = len(sorted_values)
        
        q1 = sorted_values[int(n * 0.25)]
        q3 = sorted_values[int(n * 0.75)]
        iqr = q3 - q1
        
        # æ ¹æ®æ•æ„Ÿåº¦è°ƒæ•´é˜ˆå€¼
        threshold = 1.5 + (1 - sensitivity) * 1.0
        
        lower_bound = q1 - threshold * iqr
        upper_bound = q3 + threshold * iqr
        
        outliers = [x for x in values if x < lower_bound or x > upper_bound]
        return outliers
    
    def _calculate_correlation(self, values1: List[float], values2: List[float]) -> float:
        """è®¡ç®—ç›¸å…³ç³»æ•°"""
        if len(values1) != len(values2) or len(values1) < 2:
            return 0.0
        
        n = len(values1)
        mean1 = sum(values1) / n
        mean2 = sum(values2) / n
        
        numerator = sum((x - mean1) * (y - mean2) for x, y in zip(values1, values2))
        denominator = math.sqrt(sum((x - mean1) ** 2 for x in values1) * sum((y - mean2) ** 2 for y in values2))
        
        if denominator == 0:
            return 0.0
        
        return numerator / denominator
    
    def _analyze_distribution_features(self, values: List[float]) -> Dict[str, Any]:
        """åˆ†æåˆ†å¸ƒç‰¹å¾"""
        if len(values) < 3:
            return {}
        
        n = len(values)
        mean = sum(values) / n
        
        # è®¡ç®—ååº¦
        variance = sum((x - mean) ** 2 for x in values) / n
        std = math.sqrt(variance)
        
        if std == 0:
            skewness = 0
            kurtosis = 0
        else:
            skewness = sum(((x - mean) / std) ** 3 for x in values) / n
            kurtosis = sum(((x - mean) / std) ** 4 for x in values) / n
        
        # ç®€åŒ–çš„æ­£æ€æ€§æ£€éªŒ
        is_normal = abs(skewness) < 0.5 and abs(kurtosis - 3) < 0.5
        
        return {
            "mean": mean,
            "std": std,
            "skewness": skewness,
            "kurtosis": kurtosis,
            "is_normal": is_normal
        }
    
    def _calculate_concentration(self, values: List[float]) -> float:
        """è®¡ç®—é›†ä¸­åº¦"""
        if not values:
            return 0.0
        
        total = sum(values)
        if total == 0:
            return 0.0
        
        # è®¡ç®—åŸºå°¼ç³»æ•°
        sorted_values = sorted(values)
        n = len(sorted_values)
        
        cumsum = 0
        gini = 0
        
        for i, value in enumerate(sorted_values):
            cumsum += value
            gini += (2 * (i + 1) - n - 1) * value
        
        gini = gini / (n * total)
        
        return gini
    
    def _analyze_chart_config(self, chart_config: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æå›¾è¡¨é…ç½®"""
        return {
            "has_title": bool(chart_config.get("title", {}).get("text")),
            "has_x_axis_label": bool(chart_config.get("xAxis", {}).get("title")),
            "has_y_axis_label": bool(chart_config.get("yAxis", {}).get("title")),
            "has_legend": chart_config.get("legend", {}).get("show", False),
            "has_grid": chart_config.get("grid", {}).get("show", False)
        }
    
    def _analyze_data_quality(self, chart_data: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææ•°æ®è´¨é‡"""
        values = self._extract_values(chart_data)
        
        if not values:
            return {"data_points": 0, "missing_data_percentage": 100}
        
        # è®¡ç®—ç¼ºå¤±æ•°æ®
        missing_count = sum(1 for v in values if v is None or (isinstance(v, float) and math.isnan(v)))
        missing_percentage = missing_count / len(values) * 100
        
        return {
            "data_points": len(values),
            "missing_data_percentage": missing_percentage,
            "unique_values": len(set(values)),
            "data_range": max(values) - min(values) if values else 0
        }
    
    def _calculate_visualization_score(self, chart_data: Dict[str, Any], chart_config: Optional[Dict[str, Any]]) -> float:
        """è®¡ç®—å¯è§†åŒ–åˆ†æ•°"""
        score = 0.0
        
        # æ•°æ®è´¨é‡åˆ†æ•°
        data_quality = self._analyze_data_quality(chart_data)
        if data_quality["data_points"] > 10:
            score += 0.3
        if data_quality["missing_data_percentage"] < 5:
            score += 0.2
        
        # é…ç½®åˆ†æ•°
        if chart_config:
            config_analysis = self._analyze_chart_config(chart_config)
            if config_analysis["has_title"]:
                score += 0.1
            if config_analysis["has_x_axis_label"]:
                score += 0.1
            if config_analysis["has_y_axis_label"]:
                score += 0.1
            if config_analysis["has_legend"]:
                score += 0.1
        
        return min(score, 1.0)
    
    def _calculate_pattern_quality_score(self, metrics: Dict[str, Any]) -> float:
        """è®¡ç®—æ¨¡å¼è´¨é‡åˆ†æ•°"""
        score = 0.5  # åŸºç¡€åˆ†æ•°
        
        if metrics.get("periodicity", 0) > 0:
            score += 0.2
        
        if metrics.get("monotonicity") != "none":
            score += 0.1
        
        volatility = metrics.get("volatility", 0)
        if 0.1 < volatility < 0.5:  # é€‚ä¸­çš„æ³¢åŠ¨æ€§
            score += 0.2
        
        return min(score, 1.0)
    
    def _calculate_trend_quality_score(self, metrics: Dict[str, Any]) -> float:
        """è®¡ç®—è¶‹åŠ¿è´¨é‡åˆ†æ•°"""
        trend = metrics.get("trend", {})
        strength = trend.get("strength", 0)
        
        return min(strength, 1.0)
    
    def _calculate_outlier_quality_score(self, metrics: Dict[str, Any]) -> float:
        """è®¡ç®—å¼‚å¸¸å€¼è´¨é‡åˆ†æ•°"""
        outlier_percentage = metrics.get("outlier_percentage", 0)
        
        if outlier_percentage < 5:
            return 0.9
        elif outlier_percentage < 15:
            return 0.7
        else:
            return 0.3
    
    def _calculate_correlation_quality_score(self, metrics: Dict[str, Any]) -> float:
        """è®¡ç®—ç›¸å…³æ€§è´¨é‡åˆ†æ•°"""
        correlations = metrics.get("correlations", [])
        
        if not correlations:
            return 0.5
        
        strong_correlations = [c for c in correlations if abs(c["correlation"]) > 0.7]
        
        if len(strong_correlations) > 0:
            return 0.8
        else:
            return 0.6
    
    def _calculate_distribution_quality_score(self, metrics: Dict[str, Any]) -> float:
        """è®¡ç®—åˆ†å¸ƒè´¨é‡åˆ†æ•°"""
        distribution = metrics.get("distribution", {})
        
        if distribution.get("is_normal", False):
            return 0.9
        
        skewness = abs(distribution.get("skewness", 0))
        if skewness < 1:
            return 0.7
        else:
            return 0.4
    
    def _calculate_optimization_quality_score(self, metrics: Dict[str, Any]) -> float:
        """è®¡ç®—ä¼˜åŒ–è´¨é‡åˆ†æ•°"""
        visualization_score = metrics.get("visualization_score", 0)
        data_quality = metrics.get("data_quality", {})
        
        score = visualization_score * 0.6
        
        if data_quality.get("missing_data_percentage", 0) < 5:
            score += 0.2
        
        if data_quality.get("data_points", 0) > 20:
            score += 0.2
        
        return min(score, 1.0)
    
    def _calculate_overall_score(self, analysis_results: List[AnalysisResult]) -> float:
        """è®¡ç®—æ•´ä½“åˆ†æ•°"""
        if not analysis_results:
            return 0.0
        
        scores = [result.quality_score for result in analysis_results]
        return sum(scores) / len(scores)
    
    def _determine_chart_quality(self, score: float) -> ChartQuality:
        """ç¡®å®šå›¾è¡¨è´¨é‡"""
        if score >= 0.8:
            return ChartQuality.EXCELLENT
        elif score >= 0.6:
            return ChartQuality.GOOD
        elif score >= 0.4:
            return ChartQuality.FAIR
        else:
            return ChartQuality.POOR
    
    def _extract_key_insights(self, analysis_results: List[AnalysisResult]) -> List[str]:
        """æå–å…³é”®æ´å¯Ÿ"""
        insights = []
        
        for result in analysis_results:
            # é€‰æ‹©æœ€é‡è¦çš„å‘ç°
            if result.findings:
                insights.extend(result.findings[:2])  # æ¯ä¸ªåˆ†ææœ€å¤š2ä¸ªæ´å¯Ÿ
        
        return insights[:5]  # æœ€å¤š5ä¸ªå…³é”®æ´å¯Ÿ
    
    def _generate_optimization_suggestions(self, chart_data: Dict[str, Any], chart_config: Optional[Dict[str, Any]], analysis_results: List[AnalysisResult]) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        suggestions = []
        
        for result in analysis_results:
            suggestions.extend(result.recommendations)
        
        # å»é‡å¹¶é™åˆ¶æ•°é‡
        unique_suggestions = list(set(suggestions))
        return unique_suggestions[:8]  # æœ€å¤š8ä¸ªå»ºè®®


def create_chart_analyzer_tool(container: Any) -> ChartAnalyzerTool:
    """
    åˆ›å»ºå›¾è¡¨åˆ†æå·¥å…·

    Args:
        container: æœåŠ¡å®¹å™¨

    Returns:
        ChartAnalyzerTool å®ä¾‹
    """
    return ChartAnalyzerTool(container)


# å¯¼å‡º
__all__ = [
    "ChartAnalyzerTool",
    "AnalysisFocus",
    "ChartQuality",
    "AnalysisConfig",
    "AnalysisResult",
    "ComprehensiveChartAnalysis",
    "create_chart_analyzer_tool",
]