from __future__ import annotations

from loom.interfaces.tool import BaseTool
"""
æ•°æ®åˆ†æå·¥å…·

åˆ†ææ•°æ®ç‰¹å¾ã€ç»Ÿè®¡ä¿¡æ¯å’Œæ¨¡å¼
æ”¯æŒæè¿°æ€§ç»Ÿè®¡ã€ç›¸å…³æ€§åˆ†æå’Œå¼‚å¸¸æ£€æµ‹
"""


import logging
import math
import statistics
from typing import Any, Dict, List, Optional, Union, Tuple, Literal
from dataclasses import dataclass
from enum import Enum
from pydantic import BaseModel, Field


from ...types import ToolCategory, ContextInfo

logger = logging.getLogger(__name__)


class AnalysisType(str, Enum):
    """åˆ†æç±»å‹"""
    DESCRIPTIVE = "descriptive"     # æè¿°æ€§ç»Ÿè®¡
    CORRELATION = "correlation"     # ç›¸å…³æ€§åˆ†æ
    DISTRIBUTION = "distribution"    # åˆ†å¸ƒåˆ†æ
    OUTLIER = "outlier"            # å¼‚å¸¸å€¼æ£€æµ‹
    TREND = "trend"               # è¶‹åŠ¿åˆ†æ
    PATTERN = "pattern"            # æ¨¡å¼è¯†åˆ«


class StatisticalMeasure(str, Enum):
    """ç»Ÿè®¡é‡"""
    MEAN = "mean"
    MEDIAN = "median"
    MODE = "mode"
    STD = "std"
    VARIANCE = "variance"
    MIN = "min"
    MAX = "max"
    QUARTILE_25 = "q25"
    QUARTILE_75 = "q75"
    SKEWNESS = "skewness"
    KURTOSIS = "kurtosis"


@dataclass
class AnalysisConfig:
    """åˆ†æé…ç½®"""
    analysis_types: List[AnalysisType]
    confidence_level: float = 0.95
    outlier_method: str = "iqr"  # iqr, zscore, modified_zscore
    correlation_method: str = "pearson"  # pearson, spearman, kendall
    metadata: Dict[str, Any] = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


@dataclass
class AnalysisResult:
    """åˆ†æç»“æœ"""
    analysis_type: AnalysisType
    results: Dict[str, Any]
    insights: List[str]
    recommendations: List[str]
    metadata: Dict[str, Any] = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


@dataclass
class ComprehensiveAnalysisReport:
    """ç»¼åˆåˆ†ææŠ¥å‘Š"""
    data_summary: Dict[str, Any]
    descriptive_stats: Dict[str, Any]
    correlation_matrix: Dict[str, Any]
    outlier_analysis: Dict[str, Any]
    distribution_analysis: Dict[str, Any]
    insights: List[str]
    recommendations: List[str]
    metadata: Dict[str, Any] = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


class DataAnalyzerTool(BaseTool):
    """æ•°æ®åˆ†æå·¥å…·"""
    
    def __init__(self, container: Any):
        """
        Args:
            container: æœåŠ¡å®¹å™¨
        """
        super().__init__()

        self.name = "data_analyzer"

        self.category = ToolCategory.DATA

        self.description = "åˆ†ææ•°æ®ç‰¹å¾ã€ç»Ÿè®¡ä¿¡æ¯å’Œæ¨¡å¼" 
        self.container = container
        
        # ä½¿ç”¨ Pydantic å®šä¹‰å‚æ•°æ¨¡å¼ï¼ˆargs_schemaï¼‰
        class DataAnalyzerArgs(BaseModel):
            data: List[Dict[str, Any]] = Field(description="è¦åˆ†æçš„æ•°æ®")
            analysis_types: Optional[List[Literal[
                "descriptive", "correlation", "distribution", "outlier", "trend", "pattern"
            ]]] = Field(default=["descriptive", "correlation", "outlier"], description="è¦æ‰§è¡Œçš„åˆ†æç±»å‹")
            target_columns: Optional[List[str]] = Field(default=None, description="ç›®æ ‡åˆ†æåˆ—")
            confidence_level: float = Field(default=0.95, description="ç½®ä¿¡æ°´å¹³")
            outlier_method: Literal["iqr", "zscore", "modified_zscore"] = Field(default="iqr", description="å¼‚å¸¸å€¼æ£€æµ‹æ–¹æ³•")
            correlation_method: Literal["pearson", "spearman", "kendall"] = Field(default="pearson", description="ç›¸å…³æ€§åˆ†ææ–¹æ³•")
            generate_insights: bool = Field(default=True, description="æ˜¯å¦ç”Ÿæˆæ´å¯Ÿ")

        self.args_schema = DataAnalyzerArgs
    
    def get_schema(self) -> Dict[str, Any]:
        """è·å–å·¥å…·å‚æ•°æ¨¡å¼ï¼ˆåŸºäº args_schema ç”Ÿæˆï¼‰"""
        try:
            parameters = self.args_schema.model_json_schema()
        except Exception:
            parameters = self.args_schema.schema()  # type: ignore[attr-defined]
        return {
            "type": "function",
            "function": {
                "name": "data_analyzer",
                "description": "åˆ†ææ•°æ®ç‰¹å¾ã€ç»Ÿè®¡ä¿¡æ¯å’Œæ¨¡å¼",
                "parameters": parameters,
            },
        }
    
    async def run(

    
        self,
        data: List[Dict[str, Any]],
        analysis_types: Optional[List[str]] = None,
        target_columns: Optional[List[str]] = None,
        confidence_level: float = 0.95,
        outlier_method: str = "iqr",
        correlation_method: str = "pearson",
        generate_insights: bool = True,
        **kwargs
    

    
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ•°æ®åˆ†æ

    Args:
            data: è¦åˆ†æçš„æ•°æ®
            analysis_types: è¦æ‰§è¡Œçš„åˆ†æç±»å‹
            target_columns: ç›®æ ‡åˆ†æåˆ—
            confidence_level: ç½®ä¿¡æ°´å¹³
            outlier_method: å¼‚å¸¸å€¼æ£€æµ‹æ–¹æ³•
            correlation_method: ç›¸å…³æ€§åˆ†ææ–¹æ³•
            generate_insights: æ˜¯å¦ç”Ÿæˆæ´å¯Ÿ

    Returns:
            Dict[str, Any]: åˆ†æç»“æœ
        """
        logger.info(f"ğŸ“ˆ [DataAnalyzerTool] å¼€å§‹åˆ†æ")
        logger.info(f"   æ•°æ®è¡Œæ•°: {len(data)}")

    
    async def execute(self, **kwargs) -> Dict[str, Any]:

    
        """å‘åå…¼å®¹çš„executeæ–¹æ³•"""

    
        return await self.run(**kwargs)
        logger.info(f"   åˆ†æç±»å‹: {analysis_types}")
        
        try:
            if not data:
                return {
                    "success": False,
                    "error": "æ•°æ®ä¸ºç©º",
                    "result": None
                }
            
            # è®¾ç½®é»˜è®¤åˆ†æç±»å‹
            if analysis_types is None:
                analysis_types = ["descriptive", "correlation", "outlier"]
            
            # æ„å»ºåˆ†æé…ç½®
            config = AnalysisConfig(
                analysis_types=[AnalysisType(t) for t in analysis_types],
                confidence_level=confidence_level,
                outlier_method=outlier_method,
                correlation_method=correlation_method
            )
            
            # ç¡®å®šç›®æ ‡åˆ—
            if target_columns is None:
                target_columns = list(data[0].keys()) if data else []
            
            # æ‰§è¡Œåˆ†æ
            results = {}
            insights = []
            recommendations = []
            
            for analysis_type in config.analysis_types:
                result = await self._perform_analysis(data, analysis_type, target_columns, config)
                results[analysis_type.value] = result.results
                insights.extend(result.insights)
                recommendations.extend(result.recommendations)
            
            # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
            report = ComprehensiveAnalysisReport(
                data_summary=self._generate_data_summary(data),
                descriptive_stats=results.get("descriptive", {}),
                correlation_matrix=results.get("correlation", {}),
                outlier_analysis=results.get("outlier", {}),
                distribution_analysis=results.get("distribution", {}),
                insights=insights,
                recommendations=recommendations
            )
            
            return {
                "success": True,
                "result": report,
                "metadata": {
                    "analysis_types": analysis_types,
                    "target_columns": target_columns,
                    "data_rows": len(data),
                    "data_columns": len(target_columns),
                    "insights_count": len(insights),
                    "recommendations_count": len(recommendations)
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ [DataAnalyzerTool] åˆ†æå¤±è´¥: {e}", exc_info=True)
            return {
                "success": False,
                "error": str(e),
                "result": None
            }
    
    async def _perform_analysis(
        self,
        data: List[Dict[str, Any]],
        analysis_type: AnalysisType,
        target_columns: List[str],
        config: AnalysisConfig
    ) -> AnalysisResult:
        """æ‰§è¡Œç‰¹å®šç±»å‹çš„åˆ†æ"""
        if analysis_type == AnalysisType.DESCRIPTIVE:
            return self._descriptive_analysis(data, target_columns)
        elif analysis_type == AnalysisType.CORRELATION:
            return self._correlation_analysis(data, target_columns, config)
        elif analysis_type == AnalysisType.DISTRIBUTION:
            return self._distribution_analysis(data, target_columns)
        elif analysis_type == AnalysisType.OUTLIER:
            return self._outlier_analysis(data, target_columns, config)
        elif analysis_type == AnalysisType.TREND:
            return self._trend_analysis(data, target_columns)
        elif analysis_type == AnalysisType.PATTERN:
            return self._pattern_analysis(data, target_columns)
        else:
            return AnalysisResult(
                analysis_type=analysis_type,
                results={},
                insights=[],
                recommendations=[]
            )
    
    def _descriptive_analysis(self, data: List[Dict[str, Any]], target_columns: List[str]) -> AnalysisResult:
        """æè¿°æ€§ç»Ÿè®¡åˆ†æ"""
        results = {}
        insights = []
        recommendations = []
        
        for column in target_columns:
            values = self._extract_column_values(data, column)
            numeric_values = self._convert_to_numeric(values)
            
            if numeric_values:
                stats = self._calculate_numeric_statistics(numeric_values)
                results[column] = stats
                
                # ç”Ÿæˆæ´å¯Ÿ
                if stats.get("std", 0) > stats.get("mean", 0) * 0.5:
                    insights.append(f"åˆ— '{column}' çš„æ•°æ®å˜å¼‚æ€§è¾ƒå¤§")
                
                if stats.get("skewness", 0) > 1:
                    insights.append(f"åˆ— '{column}' å‘ˆæ­£åæ€åˆ†å¸ƒ")
                elif stats.get("skewness", 0) < -1:
                    insights.append(f"åˆ— '{column}' å‘ˆè´Ÿåæ€åˆ†å¸ƒ")
                
                # ç”Ÿæˆå»ºè®®
                if stats.get("null_percentage", 0) > 20:
                    recommendations.append(f"åˆ— '{column}' ç¼ºå¤±å€¼è¾ƒå¤šï¼Œå»ºè®®æ£€æŸ¥æ•°æ®è´¨é‡")
                
                if stats.get("unique_percentage", 0) > 90:
                    recommendations.append(f"åˆ— '{column}' å”¯ä¸€å€¼æ¯”ä¾‹å¾ˆé«˜ï¼Œå¯èƒ½æ˜¯æ ‡è¯†ç¬¦")
            else:
                # åˆ†ç±»æ•°æ®ç»Ÿè®¡
                stats = self._calculate_categorical_statistics(values)
                results[column] = stats
                
                if stats.get("unique_count", 0) < 10:
                    insights.append(f"åˆ— '{column}' æ˜¯åˆ†ç±»å˜é‡ï¼Œæœ‰ {stats['unique_count']} ä¸ªç±»åˆ«")
                
                if stats.get("most_common_percentage", 0) > 80:
                    recommendations.append(f"åˆ— '{column}' å­˜åœ¨ä¸»å¯¼ç±»åˆ«ï¼Œå¯èƒ½å½±å“åˆ†æç»“æœ")
        
        return AnalysisResult(
            analysis_type=AnalysisType.DESCRIPTIVE,
            results=results,
            insights=insights,
            recommendations=recommendations
        )
    
    def _correlation_analysis(self, data: List[Dict[str, Any]], target_columns: List[str], config: AnalysisConfig) -> AnalysisResult:
        """ç›¸å…³æ€§åˆ†æ"""
        results = {}
        insights = []
        recommendations = []
        
        # æå–æ•°å€¼åˆ—
        numeric_columns = []
        for column in target_columns:
            values = self._extract_column_values(data, column)
            numeric_values = self._convert_to_numeric(values)
            if numeric_values and len(numeric_values) > 1:
                numeric_columns.append(column)
        
        if len(numeric_columns) < 2:
            insights.append("æ•°å€¼åˆ—ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œç›¸å…³æ€§åˆ†æ")
            return AnalysisResult(
                analysis_type=AnalysisType.CORRELATION,
                results={},
                insights=insights,
                recommendations=recommendations
            )
        
        # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
        correlation_matrix = {}
        for col1 in numeric_columns:
            correlation_matrix[col1] = {}
            values1 = self._convert_to_numeric(self._extract_column_values(data, col1))
            
            for col2 in numeric_columns:
                values2 = self._convert_to_numeric(self._extract_column_values(data, col2))
                
                if len(values1) == len(values2):
                    correlation = self._calculate_correlation(values1, values2, config.correlation_method)
                    correlation_matrix[col1][col2] = correlation
        
        results["correlation_matrix"] = correlation_matrix
        results["numeric_columns"] = numeric_columns
        
        # åˆ†æå¼ºç›¸å…³æ€§
        strong_correlations = []
        for col1 in numeric_columns:
            for col2 in numeric_columns:
                if col1 != col2:
                    corr = correlation_matrix.get(col1, {}).get(col2, 0)
                    if abs(corr) > 0.7:
                        strong_correlations.append((col1, col2, corr))
        
        if strong_correlations:
            insights.append(f"å‘ç° {len(strong_correlations)} å¯¹å¼ºç›¸å…³å˜é‡")
            for col1, col2, corr in strong_correlations[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                insights.append(f"'{col1}' å’Œ '{col2}' çš„ç›¸å…³ç³»æ•°ä¸º {corr:.3f}")
        
        # ç”Ÿæˆå»ºè®®
        if strong_correlations:
            recommendations.append("å­˜åœ¨å¼ºç›¸å…³å˜é‡ï¼Œè€ƒè™‘è¿›è¡Œé™ç»´åˆ†æ")
        
        return AnalysisResult(
            analysis_type=AnalysisType.CORRELATION,
            results=results,
            insights=insights,
            recommendations=recommendations
        )
    
    def _distribution_analysis(self, data: List[Dict[str, Any]], target_columns: List[str]) -> AnalysisResult:
        """åˆ†å¸ƒåˆ†æ"""
        results = {}
        insights = []
        recommendations = []
        
        for column in target_columns:
            values = self._extract_column_values(data, column)
            numeric_values = self._convert_to_numeric(values)
            
            if numeric_values:
                distribution_info = self._analyze_distribution(numeric_values)
                results[column] = distribution_info
                
                # åˆ†æåˆ†å¸ƒç‰¹å¾
                if distribution_info.get("is_normal", False):
                    insights.append(f"åˆ— '{column}' è¿‘ä¼¼æ­£æ€åˆ†å¸ƒ")
                elif distribution_info.get("skewness", 0) > 1:
                    insights.append(f"åˆ— '{column}' å‘ˆå³ååˆ†å¸ƒ")
                elif distribution_info.get("skewness", 0) < -1:
                    insights.append(f"åˆ— '{column}' å‘ˆå·¦ååˆ†å¸ƒ")
                
                if distribution_info.get("kurtosis", 0) > 3:
                    insights.append(f"åˆ— '{column}' å‘ˆå°–å³°åˆ†å¸ƒ")
                elif distribution_info.get("kurtosis", 0) < 3:
                    insights.append(f"åˆ— '{column}' å‘ˆå¹³å³°åˆ†å¸ƒ")
        
        return AnalysisResult(
            analysis_type=AnalysisType.DISTRIBUTION,
            results=results,
            insights=insights,
            recommendations=recommendations
        )
    
    def _outlier_analysis(self, data: List[Dict[str, Any]], target_columns: List[str], config: AnalysisConfig) -> AnalysisResult:
        """å¼‚å¸¸å€¼åˆ†æ"""
        results = {}
        insights = []
        recommendations = []
        
        for column in target_columns:
            values = self._extract_column_values(data, column)
            numeric_values = self._convert_to_numeric(values)
            
            if numeric_values and len(numeric_values) > 4:
                outliers = self._detect_outliers(numeric_values, config.outlier_method)
                outlier_info = {
                    "outlier_count": len(outliers),
                    "outlier_percentage": len(outliers) / len(numeric_values) * 100,
                    "outlier_values": outliers[:10],  # åªæ˜¾ç¤ºå‰10ä¸ª
                    "method": config.outlier_method
                }
                results[column] = outlier_info
                
                if len(outliers) > 0:
                    insights.append(f"åˆ— '{column}' å‘ç° {len(outliers)} ä¸ªå¼‚å¸¸å€¼")
                    
                    if outlier_info["outlier_percentage"] > 5:
                        recommendations.append(f"åˆ— '{column}' å¼‚å¸¸å€¼æ¯”ä¾‹è¾ƒé«˜ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®è´¨é‡")
        
        return AnalysisResult(
            analysis_type=AnalysisType.OUTLIER,
            results=results,
            insights=insights,
            recommendations=recommendations
        )
    
    def _trend_analysis(self, data: List[Dict[str, Any]], target_columns: List[str]) -> AnalysisResult:
        """è¶‹åŠ¿åˆ†æ"""
        results = {}
        insights = []
        recommendations = []
        
        # ç®€åŒ–å®ç°ï¼Œå‡è®¾æ•°æ®æŒ‰æ—¶é—´é¡ºåºæ’åˆ—
        for column in target_columns:
            values = self._extract_column_values(data, column)
            numeric_values = self._convert_to_numeric(values)
            
            if numeric_values and len(numeric_values) > 2:
                trend_info = self._analyze_trend(numeric_values)
                results[column] = trend_info
                
                if trend_info.get("trend", "stable") == "increasing":
                    insights.append(f"åˆ— '{column}' å‘ˆä¸Šå‡è¶‹åŠ¿")
                elif trend_info.get("trend", "stable") == "decreasing":
                    insights.append(f"åˆ— '{column}' å‘ˆä¸‹é™è¶‹åŠ¿")
        
        return AnalysisResult(
            analysis_type=AnalysisType.TREND,
            results=results,
            insights=insights,
            recommendations=recommendations
        )
    
    def _pattern_analysis(self, data: List[Dict[str, Any]], target_columns: List[str]) -> AnalysisResult:
        """æ¨¡å¼è¯†åˆ«"""
        results = {}
        insights = []
        recommendations = []
        
        # ç®€åŒ–å®ç°
        for column in target_columns:
            values = self._extract_column_values(data, column)
            
            # æ£€æŸ¥é‡å¤æ¨¡å¼
            value_counts = {}
            for value in values:
                value_str = str(value)
                value_counts[value_str] = value_counts.get(value_str, 0) + 1
            
            patterns = {
                "most_common": max(value_counts.items(), key=lambda x: x[1]) if value_counts else None,
                "unique_values": len(value_counts),
                "duplicate_percentage": (len(values) - len(value_counts)) / len(values) * 100 if values else 0
            }
            
            results[column] = patterns
            
            if patterns["duplicate_percentage"] > 50:
                insights.append(f"åˆ— '{column}' å­˜åœ¨å¤§é‡é‡å¤å€¼")
        
        return AnalysisResult(
            analysis_type=AnalysisType.PATTERN,
            results=results,
            insights=insights,
            recommendations=recommendations
        )
    
    def _extract_column_values(self, data: List[Dict[str, Any]], column: str) -> List[Any]:
        """æå–åˆ—å€¼"""
        values = []
        for row in data:
            if column in row:
                values.append(row[column])
        return values
    
    def _convert_to_numeric(self, values: List[Any]) -> List[float]:
        """è½¬æ¢ä¸ºæ•°å€¼"""
        numeric_values = []
        for value in values:
            if value is None:
                continue
            try:
                numeric_values.append(float(str(value)))
            except (ValueError, TypeError):
                continue
        return numeric_values
    
    def _calculate_numeric_statistics(self, values: List[float]) -> Dict[str, Any]:
        """è®¡ç®—æ•°å€¼ç»Ÿè®¡é‡"""
        if not values:
            return {}
        
        stats = {
            "count": len(values),
            "mean": statistics.mean(values),
            "median": statistics.median(values),
            "std": statistics.stdev(values) if len(values) > 1 else 0,
            "variance": statistics.variance(values) if len(values) > 1 else 0,
            "min": min(values),
            "max": max(values),
            "range": max(values) - min(values),
            "null_count": 0,
            "null_percentage": 0,
            "unique_count": len(set(values)),
            "unique_percentage": len(set(values)) / len(values) * 100
        }
        
        # è®¡ç®—åˆ†ä½æ•°
        sorted_values = sorted(values)
        n = len(sorted_values)
        stats["q25"] = sorted_values[int(n * 0.25)] if n > 0 else 0
        stats["q75"] = sorted_values[int(n * 0.75)] if n > 0 else 0
        
        # è®¡ç®—ååº¦å’Œå³°åº¦
        if len(values) > 2:
            stats["skewness"] = self._calculate_skewness(values)
            stats["kurtosis"] = self._calculate_kurtosis(values)
        
        return stats
    
    def _calculate_categorical_statistics(self, values: List[Any]) -> Dict[str, Any]:
        """è®¡ç®—åˆ†ç±»ç»Ÿè®¡é‡"""
        if not values:
            return {}
        
        value_counts = {}
        null_count = 0
        
        for value in values:
            if value is None:
                null_count += 1
            else:
                value_str = str(value)
                value_counts[value_str] = value_counts.get(value_str, 0) + 1
        
        total_count = len(values)
        unique_count = len(value_counts)
        
        stats = {
            "count": total_count,
            "null_count": null_count,
            "null_percentage": null_count / total_count * 100,
            "unique_count": unique_count,
            "unique_percentage": unique_count / total_count * 100,
            "value_counts": value_counts,
            "most_common": max(value_counts.items(), key=lambda x: x[1]) if value_counts else None,
            "most_common_percentage": max(value_counts.values()) / total_count * 100 if value_counts else 0
        }
        
        return stats
    
    def _calculate_correlation(self, values1: List[float], values2: List[float], method: str) -> float:
        """è®¡ç®—ç›¸å…³æ€§"""
        if len(values1) != len(values2) or len(values1) < 2:
            return 0.0
        
        if method == "pearson":
            return self._pearson_correlation(values1, values2)
        elif method == "spearman":
            return self._spearman_correlation(values1, values2)
        else:
            return self._pearson_correlation(values1, values2)
    
    def _pearson_correlation(self, values1: List[float], values2: List[float]) -> float:
        """çš®å°”é€Šç›¸å…³ç³»æ•°"""
        n = len(values1)
        if n < 2:
            return 0.0
        
        mean1 = statistics.mean(values1)
        mean2 = statistics.mean(values2)
        
        numerator = sum((x - mean1) * (y - mean2) for x, y in zip(values1, values2))
        denominator = math.sqrt(sum((x - mean1) ** 2 for x in values1) * sum((y - mean2) ** 2 for y in values2))
        
        if denominator == 0:
            return 0.0
        
        return numerator / denominator
    
    def _spearman_correlation(self, values1: List[float], values2: List[float]) -> float:
        """æ–¯çš®å°”æ›¼ç›¸å…³ç³»æ•°"""
        # ç®€åŒ–å®ç°ï¼Œä½¿ç”¨çš®å°”é€Šç›¸å…³ç³»æ•°
        return self._pearson_correlation(values1, values2)
    
    def _analyze_distribution(self, values: List[float]) -> Dict[str, Any]:
        """åˆ†æåˆ†å¸ƒ"""
        if not values:
            return {}
        
        stats = self._calculate_numeric_statistics(values)
        
        # ç®€åŒ–çš„æ­£æ€æ€§æ£€éªŒ
        skewness = abs(stats.get("skewness", 0))
        kurtosis = abs(stats.get("kurtosis", 0))
        
        is_normal = skewness < 0.5 and abs(kurtosis - 3) < 0.5
        
        return {
            "is_normal": is_normal,
            "skewness": stats.get("skewness", 0),
            "kurtosis": stats.get("kurtosis", 0),
            "distribution_type": "normal" if is_normal else "non-normal"
        }
    
    def _detect_outliers(self, values: List[float], method: str) -> List[float]:
        """æ£€æµ‹å¼‚å¸¸å€¼"""
        if len(values) < 4:
            return []
        
        if method == "iqr":
            return self._iqr_outliers(values)
        elif method == "zscore":
            return self._zscore_outliers(values)
        else:
            return self._iqr_outliers(values)
    
    def _iqr_outliers(self, values: List[float]) -> List[float]:
        """IQRæ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼"""
        sorted_values = sorted(values)
        n = len(sorted_values)
        
        q1 = sorted_values[int(n * 0.25)]
        q3 = sorted_values[int(n * 0.75)]
        iqr = q3 - q1
        
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr
        
        outliers = [x for x in values if x < lower_bound or x > upper_bound]
        return outliers
    
    def _zscore_outliers(self, values: List[float]) -> List[float]:
        """Z-scoreæ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼"""
        if len(values) < 2:
            return []
        
        mean = statistics.mean(values)
        std = statistics.stdev(values)
        
        if std == 0:
            return []
        
        outliers = [x for x in values if abs((x - mean) / std) > 2]
        return outliers
    
    def _analyze_trend(self, values: List[float]) -> Dict[str, Any]:
        """åˆ†æè¶‹åŠ¿"""
        if len(values) < 2:
            return {"trend": "insufficient_data"}
        
        # ç®€å•çš„çº¿æ€§è¶‹åŠ¿åˆ†æ
        n = len(values)
        x_values = list(range(n))
        
        # è®¡ç®—æ–œç‡
        mean_x = statistics.mean(x_values)
        mean_y = statistics.mean(values)
        
        numerator = sum((x - mean_x) * (y - mean_y) for x, y in zip(x_values, values))
        denominator = sum((x - mean_x) ** 2 for x in x_values)
        
        if denominator == 0:
            slope = 0
        else:
            slope = numerator / denominator
        
        # åˆ¤æ–­è¶‹åŠ¿
        if slope > 0.01:
            trend = "increasing"
        elif slope < -0.01:
            trend = "decreasing"
        else:
            trend = "stable"
        
        return {
            "trend": trend,
            "slope": slope,
            "strength": abs(slope)
        }
    
    def _calculate_skewness(self, values: List[float]) -> float:
        """è®¡ç®—ååº¦"""
        if len(values) < 3:
            return 0.0
        
        mean = statistics.mean(values)
        std = statistics.stdev(values)
        
        if std == 0:
            return 0.0
        
        n = len(values)
        skewness = (n / ((n - 1) * (n - 2))) * sum(((x - mean) / std) ** 3 for x in values)
        return skewness
    
    def _calculate_kurtosis(self, values: List[float]) -> float:
        """è®¡ç®—å³°åº¦"""
        if len(values) < 4:
            return 0.0
        
        mean = statistics.mean(values)
        std = statistics.stdev(values)
        
        if std == 0:
            return 0.0
        
        n = len(values)
        kurtosis = (n * (n + 1) / ((n - 1) * (n - 2) * (n - 3))) * sum(((x - mean) / std) ** 4 for x in values) - (3 * (n - 1) ** 2 / ((n - 2) * (n - 3)))
        return kurtosis
    
    def _generate_data_summary(self, data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆæ•°æ®æ‘˜è¦"""
        if not data:
            return {}
        
        return {
            "total_rows": len(data),
            "total_columns": len(data[0]) if data else 0,
            "columns": list(data[0].keys()) if data else [],
            "memory_usage_estimate": len(str(data))  # ç²—ç•¥ä¼°è®¡
        }


def create_data_analyzer_tool(container: Any) -> DataAnalyzerTool:
    """
    åˆ›å»ºæ•°æ®åˆ†æå·¥å…·
    
    Args:
        container: æœåŠ¡å®¹å™¨
        
    Returns:
        DataAnalyzerTool å®ä¾‹
    """
    return DataAnalyzerTool(container)


# å¯¼å‡º
__all__ = [
    "DataAnalyzerTool",
    "AnalysisType",
    "StatisticalMeasure",
    "AnalysisConfig",
    "AnalysisResult",
    "ComprehensiveAnalysisReport",
    "create_data_analyzer_tool",
]