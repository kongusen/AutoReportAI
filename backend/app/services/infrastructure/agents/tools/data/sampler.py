from __future__ import annotations

from loom.interfaces.tool import BaseTool
"""
ТЋ░ТЇ«жЄЄТаитиЦтЁи

С╗јТЋ░ТЇ«Т║љСИГжЄЄТаиТЋ░ТЇ«У┐ЏУАїтѕєТъљ
Тћ»ТїЂтцџуДЇжЄЄТаиуГќуЋЦтњїТЋ░ТЇ«тцёуљє
"""


import logging
import random
import math
from typing import Any, Dict, List, Optional, Union, Literal
from dataclasses import dataclass
from enum import Enum
from pydantic import BaseModel, Field


from ...types import ToolCategory, ContextInfo

logger = logging.getLogger(__name__)


class SamplingStrategy(str, Enum):
    """жЄЄТаиуГќуЋЦ"""
    RANDOM = "random"           # жџЈТю║жЄЄТаи
    SYSTEMATIC = "systematic"   # у│╗у╗ЪжЄЄТаи
    STRATIFIED = "stratified"   # тѕєт▒ѓжЄЄТаи
    CLUSTER = "cluster"         # УЂџу▒╗жЄЄТаи
    CONVENIENCE = "convenience" # СЙ┐тѕЕжЄЄТаи


class DataType(str, Enum):
    """ТЋ░ТЇ«у▒╗тъІ"""
    NUMERIC = "numeric"         # ТЋ░тђ╝тъІ
    CATEGORICAL = "categorical" # тѕєу▒╗тъІ
    TEXT = "text"              # ТќЄТюгтъІ
    DATETIME = "datetime"      # ТЌЦТюЪТЌХжЌ┤тъІ
    BOOLEAN = "boolean"        # тИЃт░ћтъІ


@dataclass
class SamplingConfig:
    """жЄЄТаижЁЇуй«"""
    strategy: SamplingStrategy
    sample_size: int
    random_seed: Optional[int] = None
    strata_column: Optional[str] = None
    cluster_column: Optional[str] = None
    metadata: Dict[str, Any] = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


@dataclass
class SamplingResult:
    """жЄЄТаиу╗ЊТъю"""
    data: List[Dict[str, Any]]
    sample_size: int
    total_size: int
    sampling_rate: float
    strategy: SamplingStrategy
    columns: List[str]
    data_types: Dict[str, DataType]
    statistics: Dict[str, Any]
    metadata: Dict[str, Any] = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


class DataSamplerTool(BaseTool):
    """ТЋ░ТЇ«жЄЄТаитиЦтЁи"""
    
    def __init__(self, container: Any):
        """
        Args:
            container: ТюЇтіАт«╣тЎе
        """
        super().__init__()

        self.name = "data_sampler"

        self.category = ToolCategory.DATA

        self.description = "С╗јТЋ░ТЇ«Т║љСИГжЄЄТаиТЋ░ТЇ«У┐ЏУАїтѕєТъљ" 
        self.container = container
        self._data_source_service = None
        
        # Сй┐уће Pydantic т«џС╣ЅтЈѓТЋ░ТеАт╝Ј№╝ѕargs_schema№╝Ѕ
        class DataSamplerArgs(BaseModel):
            sql: str = Field(description="УдЂжЄЄТаиуџё SQL ТЪЦУ»б")
            connection_config: Dict[str, Any] = Field(description="ТЋ░ТЇ«Т║љУ┐ъТјЦжЁЇуй«")
            strategy: Literal["random", "systematic", "stratified", "cluster", "convenience"] = Field(
                default="random", description="жЄЄТаиуГќуЋЦ"
            )
            sample_size: int = Field(default=1000, description="жЄЄТаитцДт░Ј")
            random_seed: Optional[int] = Field(default=None, description="жџЈТю║уДЇтГљ")
            strata_column: Optional[str] = Field(default=None, description="тѕєт▒ѓтѕЌтљЇ№╝ѕућеС║јтѕєт▒ѓжЄЄТаи№╝Ѕ")
            cluster_column: Optional[str] = Field(default=None, description="УЂџу▒╗тѕЌтљЇ№╝ѕућеС║јУЂџу▒╗жЄЄТаи№╝Ѕ")
            max_total_size: int = Field(default=100000, description="ТюђтцДТђ╗ТЋ░ТЇ«жЄЈ")
            analyze_data_types: bool = Field(default=True, description="Тў»тљдтѕєТъљТЋ░ТЇ«у▒╗тъІ")

        self.args_schema = DataSamplerArgs
    
    async def _get_data_source_service(self):
        """УјитЈќТЋ░ТЇ«Т║љТюЇтіА"""
        if self._data_source_service is None:
            self._data_source_service = getattr(
                self.container, 'data_source', None
            ) or getattr(self.container, 'data_source_service', None)
        return self._data_source_service
    
    def get_schema(self) -> Dict[str, Any]:
        """УјитЈќтиЦтЁитЈѓТЋ░ТеАт╝Ј№╝ѕтЪ║С║ј args_schema ућЪТѕљ№╝Ѕ"""
        try:
            parameters = self.args_schema.model_json_schema()
        except Exception:
            parameters = self.args_schema.schema()  # type: ignore[attr-defined]
        return {
            "type": "function",
            "function": {
                "name": "data_sampler",
                "description": "С╗јТЋ░ТЇ«Т║љСИГжЄЄТаиТЋ░ТЇ«У┐ЏУАїтѕєТъљ",
                "parameters": parameters,
            },
        }
    
    async def run(

    
        self,
        sql: str,
        connection_config: Dict[str, Any],
        strategy: str = "random",
        sample_size: int = 1000,
        random_seed: Optional[int] = None,
        strata_column: Optional[str] = None,
        cluster_column: Optional[str] = None,
        max_total_size: int = 100000,
        analyze_data_types: bool = True,
        **kwargs
    

    
    ) -> Dict[str, Any]:
        """
        ТЅДУАїТЋ░ТЇ«жЄЄТаи
        
        Args:
            sql: УдЂжЄЄТаиуџё SQL ТЪЦУ»б
            connection_config: ТЋ░ТЇ«Т║љУ┐ъТјЦжЁЇуй«
            strategy: жЄЄТаиуГќуЋЦ
            sample_size: жЄЄТаитцДт░Ј
            random_seed: жџЈТю║уДЇтГљ
            strata_column: тѕєт▒ѓтѕЌтљЇ
            cluster_column: УЂџу▒╗тѕЌтљЇ
            max_total_size: ТюђтцДТђ╗ТЋ░ТЇ«жЄЈ
            analyze_data_types: Тў»тљдтѕєТъљТЋ░ТЇ«у▒╗тъІ
            
        Returns:
            Dict[str, Any]: жЄЄТаиу╗ЊТъю
        """
        logger.info(f"­ЪЊі [DataSamplerTool] т╝ђтДІжЄЄТаи")
        logger.info(f"   жЄЄТаиуГќуЋЦ: {strategy}")

    
    async def execute(self, **kwargs) -> Dict[str, Any]:

    
        """тљЉтљјтЁ╝т«╣уџёexecuteТќ╣Т│Ћ"""

    
        return await self.run(**kwargs)
        logger.info(f"   жЄЄТаитцДт░Ј: {sample_size}")
        
        try:
            # УјитЈќТЋ░ТЇ«Т║љТюЇтіА
            data_source_service = await self._get_data_source_service()
            if not data_source_service:
                return {
                    "success": False,
                    "error": "ТЋ░ТЇ«Т║љТюЇтіАСИЇтЈ»уће",
                    "result": None
                }
            
            # Тъёт╗║жЄЄТаижЁЇуй«
            config = SamplingConfig(
                strategy=SamplingStrategy(strategy),
                sample_size=sample_size,
                random_seed=random_seed,
                strata_column=strata_column,
                cluster_column=cluster_column
            )
            
            # ТЅДУАїжЄЄТаи
            result = await self._execute_sampling(
                data_source_service, connection_config, sql, config, max_total_size, analyze_data_types
            )
            
            return {
                "success": True,
                "result": result,
                "metadata": {
                    "strategy": strategy,
                    "sample_size": sample_size,
                    "total_size": result.total_size,
                    "sampling_rate": result.sampling_rate,
                    "columns_count": len(result.columns)
                }
            }
            
        except Exception as e:
            logger.error(f"РЮї [DataSamplerTool] жЄЄТаитц▒У┤Ц: {e}", exc_info=True)
            return {
                "success": False,
                "error": str(e),
                "result": None
            }
    
    async def _execute_sampling(
        self,
        data_source_service: Any,
        connection_config: Dict[str, Any],
        sql: str,
        config: SamplingConfig,
        max_total_size: int,
        analyze_data_types: bool
    ) -> SamplingResult:
        """ТЅДУАїжЄЄТаи"""
        # ждќтЁѕУјитЈќТђ╗ТЋ░ТЇ«жЄЈ
        total_size = await self._get_total_size(data_source_service, connection_config, sql)
        
        if total_size > max_total_size:
            logger.warning(f"Рџа№ИЈ ТЋ░ТЇ«жЄЈУ┐ЄтцД ({total_size})№╝їжЎљтѕХСИ║ {max_total_size}")
            total_size = max_total_size
        
        # У░ЃТЋ┤жЄЄТаитцДт░Ј
        if config.sample_size > total_size:
            config.sample_size = total_size
            logger.info(f"­ЪЊЮ У░ЃТЋ┤жЄЄТаитцДт░ЈСИ║ {config.sample_size}")
        
        # Та╣ТЇ«уГќуЋЦТЅДУАїжЄЄТаи
        if config.strategy == SamplingStrategy.RANDOM:
            sampled_data = await self._random_sampling(
                data_source_service, connection_config, sql, config, total_size
            )
        elif config.strategy == SamplingStrategy.SYSTEMATIC:
            sampled_data = await self._systematic_sampling(
                data_source_service, connection_config, sql, config, total_size
            )
        elif config.strategy == SamplingStrategy.STRATIFIED:
            sampled_data = await self._stratified_sampling(
                data_source_service, connection_config, sql, config, total_size
            )
        elif config.strategy == SamplingStrategy.CLUSTER:
            sampled_data = await self._cluster_sampling(
                data_source_service, connection_config, sql, config, total_size
            )
        else:  # CONVENIENCE
            sampled_data = await self._convenience_sampling(
                data_source_service, connection_config, sql, config, total_size
            )
        
        # тѕєТъљТЋ░ТЇ«у▒╗тъІ
        data_types = {}
        if analyze_data_types and sampled_data:
            data_types = self._analyze_data_types(sampled_data)
        
        # У«Ау«Ќу╗ЪУ«АС┐АТЂ»
        statistics = self._calculate_statistics(sampled_data, data_types)
        
        # ТЈљтЈќтѕЌтљЇ
        columns = list(sampled_data[0].keys()) if sampled_data else []
        
        return SamplingResult(
            data=sampled_data,
            sample_size=len(sampled_data),
            total_size=total_size,
            sampling_rate=len(sampled_data) / total_size if total_size > 0 else 0,
            strategy=config.strategy,
            columns=columns,
            data_types=data_types,
            statistics=statistics
        )
    
    async def _get_total_size(
        self,
        data_source_service: Any,
        connection_config: Dict[str, Any],
        sql: str
    ) -> int:
        """УјитЈќТђ╗ТЋ░ТЇ«жЄЈ"""
        try:
            # Тъёт╗║У«АТЋ░ТЪЦУ»б
            count_sql = f"SELECT COUNT(*) as total FROM ({sql}) as subquery"
            
            result = await data_source_service.run_query(
                connection_config=connection_config,
                sql=count_sql,
                limit=1
            )
            
            if result.get("success"):
                rows = result.get("rows", []) or result.get("data", [])
                if rows and isinstance(rows[0], dict):
                    return int(rows[0].get("total", 0))
                elif rows and isinstance(rows[0], (list, tuple)):
                    return int(rows[0][0])
            
            return 0
            
        except Exception as e:
            logger.warning(f"Рџа№ИЈ УјитЈќТђ╗ТЋ░ТЇ«жЄЈтц▒У┤Ц: {e}")
            return 0
    
    async def _random_sampling(
        self,
        data_source_service: Any,
        connection_config: Dict[str, Any],
        sql: str,
        config: SamplingConfig,
        total_size: int
    ) -> List[Dict[str, Any]]:
        """жџЈТю║жЄЄТаи"""
        try:
            # У«Йуй«жџЈТю║уДЇтГљ
            if config.random_seed is not None:
                random.seed(config.random_seed)
            
            # ућЪТѕљжџЈТю║УАїтЈи
            sample_indices = sorted(random.sample(range(total_size), config.sample_size))
            
            # тѕєТЅ╣УјитЈќТЋ░ТЇ«
            sampled_data = []
            batch_size = 1000
            
            for i in range(0, len(sample_indices), batch_size):
                batch_indices = sample_indices[i:i + batch_size]
                
                # Тъёт╗║ТЪЦУ»бУјитЈќТїЄт«џУАїуџёТЋ░ТЇ«
                # У┐ЎжЄїСй┐уће LIMIT тњї OFFSET уџёу«ђтїќт«ъуј░
                for idx in batch_indices:
                    offset_sql = f"{sql} LIMIT 1 OFFSET {idx}"
                    
                    result = await data_source_service.run_query(
                        connection_config=connection_config,
                        sql=offset_sql,
                        limit=1
                    )
                    
                    if result.get("success"):
                        rows = result.get("rows", []) or result.get("data", [])
                        if rows:
                            sampled_data.extend(self._format_rows(rows))
            
            return sampled_data
            
        except Exception as e:
            logger.error(f"РЮї жџЈТю║жЄЄТаитц▒У┤Ц: {e}")
            return []
    
    async def _systematic_sampling(
        self,
        data_source_service: Any,
        connection_config: Dict[str, Any],
        sql: str,
        config: SamplingConfig,
        total_size: int
    ) -> List[Dict[str, Any]]:
        """у│╗у╗ЪжЄЄТаи"""
        try:
            # У«Ау«ЌжЄЄТаижЌ┤жџћ
            interval = total_size // config.sample_size
            
            # жџЈТю║жђЅТІЕУхитДІуѓ╣
            if config.random_seed is not None:
                random.seed(config.random_seed)
            start_point = random.randint(0, interval - 1)
            
            # ућЪТѕљжЄЄТаиуѓ╣
            sample_indices = [start_point + i * interval for i in range(config.sample_size)]
            sample_indices = [idx for idx in sample_indices if idx < total_size]
            
            # УјитЈќТЋ░ТЇ«
            sampled_data = []
            for idx in sample_indices:
                offset_sql = f"{sql} LIMIT 1 OFFSET {idx}"
                
                result = await data_source_service.run_query(
                    connection_config=connection_config,
                    sql=offset_sql,
                    limit=1
                )
                
                if result.get("success"):
                    rows = result.get("rows", []) or result.get("data", [])
                    if rows:
                        sampled_data.extend(self._format_rows(rows))
            
            return sampled_data
            
        except Exception as e:
            logger.error(f"РЮї у│╗у╗ЪжЄЄТаитц▒У┤Ц: {e}")
            return []
    
    async def _stratified_sampling(
        self,
        data_source_service: Any,
        connection_config: Dict[str, Any],
        sql: str,
        config: SamplingConfig,
        total_size: int
    ) -> List[Dict[str, Any]]:
        """тѕєт▒ѓжЄЄТаи"""
        try:
            if not config.strata_column:
                logger.warning("Рџа№ИЈ тѕєт▒ѓжЄЄТаижюђУдЂТїЄт«џ strata_column")
                return await self._random_sampling(data_source_service, connection_config, sql, config, total_size)
            
            # УјитЈќтѕєт▒ѓС┐АТЂ»
            strata_sql = f"""
            SELECT {config.strata_column}, COUNT(*) as count
            FROM ({sql}) as subquery
            GROUP BY {config.strata_column}
            """
            
            result = await data_source_service.run_query(
                connection_config=connection_config,
                sql=strata_sql,
                limit=1000
            )
            
            if not result.get("success"):
                logger.warning("Рџа№ИЈ УјитЈќтѕєт▒ѓС┐АТЂ»тц▒У┤Ц№╝їСй┐ућежџЈТю║жЄЄТаи")
                return await self._random_sampling(data_source_service, connection_config, sql, config, total_size)
            
            strata_info = result.get("rows", []) or result.get("data", [])
            
            # У«Ау«ЌТ»Јт▒ѓуџёжЄЄТаиТЋ░жЄЈ
            sampled_data = []
            for stratum in strata_info:
                stratum_value = stratum.get(config.strata_column)
                stratum_count = stratum.get("count", 0)
                
                if stratum_count > 0:
                    stratum_sample_size = max(1, int(config.sample_size * stratum_count / total_size))
                    
                    # С╗јТ»Јт▒ѓжЄЄТаи
                    stratum_sql = f"""
                    SELECT * FROM ({sql}) as subquery
                    WHERE {config.strata_column} = '{stratum_value}'
                    LIMIT {stratum_sample_size}
                    """
                    
                    stratum_result = await data_source_service.run_query(
                        connection_config=connection_config,
                        sql=stratum_sql,
                        limit=stratum_sample_size
                    )
                    
                    if stratum_result.get("success"):
                        rows = stratum_result.get("rows", []) or stratum_result.get("data", [])
                        sampled_data.extend(self._format_rows(rows))
            
            return sampled_data
            
        except Exception as e:
            logger.error(f"РЮї тѕєт▒ѓжЄЄТаитц▒У┤Ц: {e}")
            return await self._random_sampling(data_source_service, connection_config, sql, config, total_size)
    
    async def _cluster_sampling(
        self,
        data_source_service: Any,
        connection_config: Dict[str, Any],
        sql: str,
        config: SamplingConfig,
        total_size: int
    ) -> List[Dict[str, Any]]:
        """УЂџу▒╗жЄЄТаи"""
        try:
            if not config.cluster_column:
                logger.warning("Рџа№ИЈ УЂџу▒╗жЄЄТаижюђУдЂТїЄт«џ cluster_column")
                return await self._random_sampling(data_source_service, connection_config, sql, config, total_size)
            
            # УјитЈќУЂџу▒╗С┐АТЂ»
            cluster_sql = f"""
            SELECT {config.cluster_column}, COUNT(*) as count
            FROM ({sql}) as subquery
            GROUP BY {config.cluster_column}
            """
            
            result = await data_source_service.run_query(
                connection_config=connection_config,
                sql=cluster_sql,
                limit=1000
            )
            
            if not result.get("success"):
                logger.warning("Рџа№ИЈ УјитЈќУЂџу▒╗С┐АТЂ»тц▒У┤Ц№╝їСй┐ућежџЈТю║жЄЄТаи")
                return await self._random_sampling(data_source_service, connection_config, sql, config, total_size)
            
            clusters = result.get("rows", []) or result.get("data", [])
            
            # жџЈТю║жђЅТІЕУЂџу▒╗
            if config.random_seed is not None:
                random.seed(config.random_seed)
            
            selected_clusters = random.sample(clusters, min(len(clusters), config.sample_size))
            
            # С╗јжђЅСИГуџёУЂџу▒╗СИГУјитЈќТЋ░ТЇ«
            sampled_data = []
            for cluster in selected_clusters:
                cluster_value = cluster.get(config.cluster_column)
                
                cluster_sql = f"""
                SELECT * FROM ({sql}) as subquery
                WHERE {config.cluster_column} = '{cluster_value}'
                """
                
                cluster_result = await data_source_service.run_query(
                    connection_config=connection_config,
                    sql=cluster_sql,
                    limit=1000
                )
                
                if cluster_result.get("success"):
                    rows = cluster_result.get("rows", []) or cluster_result.get("data", [])
                    sampled_data.extend(self._format_rows(rows))
            
            return sampled_data
            
        except Exception as e:
            logger.error(f"РЮї УЂџу▒╗жЄЄТаитц▒У┤Ц: {e}")
            return await self._random_sampling(data_source_service, connection_config, sql, config, total_size)
    
    async def _convenience_sampling(
        self,
        data_source_service: Any,
        connection_config: Dict[str, Any],
        sql: str,
        config: SamplingConfig,
        total_size: int
    ) -> List[Dict[str, Any]]:
        """СЙ┐тѕЕжЄЄТаи№╝ѕтЈќтЅЇNТЮА№╝Ѕ"""
        try:
            convenience_sql = f"{sql} LIMIT {config.sample_size}"
            
            result = await data_source_service.run_query(
                connection_config=connection_config,
                sql=convenience_sql,
                limit=config.sample_size
            )
            
            if result.get("success"):
                rows = result.get("rows", []) or result.get("data", [])
                return self._format_rows(rows)
            
            return []
            
        except Exception as e:
            logger.error(f"РЮї СЙ┐тѕЕжЄЄТаитц▒У┤Ц: {e}")
            return []
    
    def _format_rows(self, rows: List[Any]) -> List[Dict[str, Any]]:
        """Та╝т╝ЈтїќУАїТЋ░ТЇ«"""
        formatted_rows = []
        
        for row in rows:
            if isinstance(row, dict):
                formatted_rows.append(row)
            elif isinstance(row, (list, tuple)):
                # УйгТЇбСИ║тГЌтЁИТа╝т╝Ј
                row_dict = {}
                for i, value in enumerate(row):
                    row_dict[f"column_{i}"] = value
                formatted_rows.append(row_dict)
            else:
                formatted_rows.append({"value": row})
        
        return formatted_rows
    
    def _analyze_data_types(self, data: List[Dict[str, Any]]) -> Dict[str, DataType]:
        """тѕєТъљТЋ░ТЇ«у▒╗тъІ"""
        if not data:
            return {}
        
        data_types = {}
        columns = list(data[0].keys())
        
        for column in columns:
            values = [row.get(column) for row in data if row.get(column) is not None]
            
            if not values:
                data_types[column] = DataType.TEXT
                continue
            
            # ТБђТЪЦТЋ░тђ╝тъІ
            numeric_count = 0
            for value in values:
                try:
                    float(str(value))
                    numeric_count += 1
                except (ValueError, TypeError):
                    break
            
            if numeric_count == len(values):
                data_types[column] = DataType.NUMERIC
                continue
            
            # ТБђТЪЦтИЃт░ћтъІ
            boolean_count = 0
            for value in values:
                if str(value).lower() in ['true', 'false', '1', '0', 'yes', 'no']:
                    boolean_count += 1
            
            if boolean_count == len(values):
                data_types[column] = DataType.BOOLEAN
                continue
            
            # ТБђТЪЦТЌЦТюЪТЌХжЌ┤тъІ
            datetime_count = 0
            for value in values:
                try:
                    import datetime
                    if isinstance(value, (datetime.datetime, datetime.date)):
                        datetime_count += 1
                    elif isinstance(value, str) and len(str(value)) > 8:
                        # у«ђтЇЋТБђТЪЦТЌЦТюЪТа╝т╝Ј
                        if any(char in str(value) for char in ['-', '/', ':']):
                            datetime_count += 1
                except:
                    pass
            
            if datetime_count > len(values) * 0.8:  # 80% С╗ЦСИіТў»ТЌЦТюЪТЌХжЌ┤
                data_types[column] = DataType.DATETIME
                continue
            
            # ТБђТЪЦтѕєу▒╗тъІ№╝ѕтћ»СИђтђ╝УЙЃт░Љ№╝Ѕ
            unique_values = set(str(value) for value in values)
            if len(unique_values) < min(20, len(values) * 0.1):  # т░ЉС║ј20СИфтћ»СИђтђ╝Тѕќт░ЉС║ј10%уџётћ»СИђтђ╝
                data_types[column] = DataType.CATEGORICAL
                continue
            
            # ж╗ўУ«цСИ║ТќЄТюгтъІ
            data_types[column] = DataType.TEXT
        
        return data_types
    
    def _calculate_statistics(self, data: List[Dict[str, Any]], data_types: Dict[str, DataType]) -> Dict[str, Any]:
        """У«Ау«Ќу╗ЪУ«АС┐АТЂ»"""
        if not data:
            return {}
        
        statistics = {
            "total_rows": len(data),
            "total_columns": len(data[0]) if data else 0,
            "column_statistics": {}
        }
        
        for column, data_type in data_types.items():
            values = [row.get(column) for row in data if row.get(column) is not None]
            
            if not values:
                statistics["column_statistics"][column] = {
                    "type": data_type.value,
                    "null_count": len(data),
                    "null_percentage": 100.0
                }
                continue
            
            null_count = len(data) - len(values)
            null_percentage = (null_count / len(data)) * 100
            
            column_stats = {
                "type": data_type.value,
                "null_count": null_count,
                "null_percentage": null_percentage,
                "non_null_count": len(values),
                "unique_count": len(set(str(v) for v in values))
            }
            
            if data_type == DataType.NUMERIC:
                numeric_values = []
                for value in values:
                    try:
                        numeric_values.append(float(str(value)))
                    except (ValueError, TypeError):
                        pass
                
                if numeric_values:
                    column_stats.update({
                        "min": min(numeric_values),
                        "max": max(numeric_values),
                        "mean": sum(numeric_values) / len(numeric_values),
                        "median": sorted(numeric_values)[len(numeric_values) // 2]
                    })
            
            elif data_type == DataType.CATEGORICAL:
                value_counts = {}
                for value in values:
                    value_str = str(value)
                    value_counts[value_str] = value_counts.get(value_str, 0) + 1
                
                column_stats["value_counts"] = value_counts
                column_stats["most_common"] = max(value_counts.items(), key=lambda x: x[1]) if value_counts else None
            
            statistics["column_statistics"][column] = column_stats
        
        return statistics


def create_data_sampler_tool(container: Any) -> DataSamplerTool:
    """
    тѕЏт╗║ТЋ░ТЇ«жЄЄТаитиЦтЁи
    
    Args:
        container: ТюЇтіАт«╣тЎе
        
    Returns:
        DataSamplerTool т«ъСЙІ
    """
    return DataSamplerTool(container)


# т»╝тЄ║
__all__ = [
    "DataSamplerTool",
    "SamplingStrategy",
    "DataType",
    "SamplingConfig",
    "SamplingResult",
    "create_data_sampler_tool",
]