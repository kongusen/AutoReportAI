from __future__ import annotations

from loom.interfaces.tool import BaseTool
"""
æ•°æ®é‡‡æ ·å·¥å…·

ä»æ•°æ®æºä¸­é‡‡æ ·æ•°æ®è¿›è¡Œåˆ†æ
æ”¯æŒå¤šç§é‡‡æ ·ç­–ç•¥å’Œæ•°æ®å¤„ç†
"""


import logging
import random
import math
from typing import Any, Dict, List, Optional, Union, Literal
from dataclasses import dataclass
from enum import Enum
from pydantic import BaseModel, Field


from ...types import ToolCategory, ContextInfo

logger = logging.getLogger(__name__)


class SamplingStrategy(str, Enum):
    """é‡‡æ ·ç­–ç•¥"""
    RANDOM = "random"           # éšæœºé‡‡æ ·
    SYSTEMATIC = "systematic"   # ç³»ç»Ÿé‡‡æ ·
    STRATIFIED = "stratified"   # åˆ†å±‚é‡‡æ ·
    CLUSTER = "cluster"         # èšç±»é‡‡æ ·
    CONVENIENCE = "convenience" # ä¾¿åˆ©é‡‡æ ·


class DataType(str, Enum):
    """æ•°æ®ç±»å‹"""
    NUMERIC = "numeric"         # æ•°å€¼å‹
    CATEGORICAL = "categorical" # åˆ†ç±»å‹
    TEXT = "text"              # æ–‡æœ¬å‹
    DATETIME = "datetime"      # æ—¥æœŸæ—¶é—´å‹
    BOOLEAN = "boolean"        # å¸ƒå°”å‹


@dataclass
class SamplingConfig:
    """é‡‡æ ·é…ç½®"""
    strategy: SamplingStrategy
    sample_size: int
    random_seed: Optional[int] = None
    strata_column: Optional[str] = None
    cluster_column: Optional[str] = None
    metadata: Dict[str, Any] = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


@dataclass
class SamplingResult:
    """é‡‡æ ·ç»“æœ"""
    data: List[Dict[str, Any]]
    sample_size: int
    total_size: int
    sampling_rate: float
    strategy: SamplingStrategy
    columns: List[str]
    data_types: Dict[str, DataType]
    statistics: Dict[str, Any]
    metadata: Dict[str, Any] = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


class DataSamplerTool(BaseTool):
    """æ•°æ®é‡‡æ ·å·¥å…·"""
    
    def __init__(self, container: Any):
        """
        Args:
            container: æœåŠ¡å®¹å™¨
        """
        super().__init__()

        self.name = "data_sampler"

        self.category = ToolCategory.DATA

        self.description = "ä»æ•°æ®æºä¸­é‡‡æ ·æ•°æ®è¿›è¡Œåˆ†æ" 
        self.container = container
        self._data_source_service = None
        
        # ä½¿ç”¨ Pydantic å®šä¹‰å‚æ•°æ¨¡å¼ï¼ˆargs_schemaï¼‰
        class DataSamplerArgs(BaseModel):
            sql: str = Field(description="è¦é‡‡æ ·çš„ SQL æŸ¥è¯¢")
            connection_config: Dict[str, Any] = Field(description="æ•°æ®æºè¿æ¥é…ç½®")
            strategy: Literal["random", "systematic", "stratified", "cluster", "convenience"] = Field(
                default="random", description="é‡‡æ ·ç­–ç•¥"
            )
            sample_size: int = Field(default=1000, description="é‡‡æ ·å¤§å°")
            random_seed: Optional[int] = Field(default=None, description="éšæœºç§å­")
            strata_column: Optional[str] = Field(default=None, description="åˆ†å±‚åˆ—åï¼ˆç”¨äºåˆ†å±‚é‡‡æ ·ï¼‰")
            cluster_column: Optional[str] = Field(default=None, description="èšç±»åˆ—åï¼ˆç”¨äºèšç±»é‡‡æ ·ï¼‰")
            max_total_size: int = Field(default=100000, description="æœ€å¤§æ€»æ•°æ®é‡")
            analyze_data_types: bool = Field(default=True, description="æ˜¯å¦åˆ†ææ•°æ®ç±»å‹")

        self.args_schema = DataSamplerArgs
    
    async def _get_data_source_service(self):
        """è·å–æ•°æ®æºæœåŠ¡"""
        if self._data_source_service is None:
            self._data_source_service = getattr(
                self.container, 'data_source', None
            ) or getattr(self.container, 'data_source_service', None)
        return self._data_source_service
    
    def get_schema(self) -> Dict[str, Any]:
        """è·å–å·¥å…·å‚æ•°æ¨¡å¼ï¼ˆåŸºäº args_schema ç”Ÿæˆï¼‰"""
        try:
            parameters = self.args_schema.model_json_schema()
        except Exception:
            parameters = self.args_schema.schema()  # type: ignore[attr-defined]
        return {
            "type": "function",
            "function": {
                "name": "data_sampler",
                "description": "ä»æ•°æ®æºä¸­é‡‡æ ·æ•°æ®è¿›è¡Œåˆ†æ",
                "parameters": parameters,
            },
        }
    
    async def run(

    
        self,
        sql: str,
        connection_config: Dict[str, Any],
        strategy: str = "random",
        sample_size: int = 1000,
        random_seed: Optional[int] = None,
        strata_column: Optional[str] = None,
        cluster_column: Optional[str] = None,
        max_total_size: int = 100000,
        analyze_data_types: bool = True,
        **kwargs
    

    
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ•°æ®é‡‡æ ·
        
        Args:
            sql: è¦é‡‡æ ·çš„ SQL æŸ¥è¯¢
            connection_config: æ•°æ®æºè¿æ¥é…ç½®
            strategy: é‡‡æ ·ç­–ç•¥
            sample_size: é‡‡æ ·å¤§å°
            random_seed: éšæœºç§å­
            strata_column: åˆ†å±‚åˆ—å
            cluster_column: èšç±»åˆ—å
            max_total_size: æœ€å¤§æ€»æ•°æ®é‡
            analyze_data_types: æ˜¯å¦åˆ†ææ•°æ®ç±»å‹
            
        Returns:
            Dict[str, Any]: é‡‡æ ·ç»“æœ
        """
        logger.info(f"ğŸ“Š [DataSamplerTool] å¼€å§‹é‡‡æ ·")
        logger.info(f"   é‡‡æ ·ç­–ç•¥: {strategy}")

    
    async def execute(self, **kwargs) -> Dict[str, Any]:

    
        """å‘åå…¼å®¹çš„executeæ–¹æ³•"""

    
        return await self.run(**kwargs)
        logger.info(f"   é‡‡æ ·å¤§å°: {sample_size}")
        
        try:
            # è·å–æ•°æ®æºæœåŠ¡
            data_source_service = await self._get_data_source_service()
            if not data_source_service:
                return {
                    "success": False,
                    "error": "æ•°æ®æºæœåŠ¡ä¸å¯ç”¨",
                    "result": None
                }
            
            # æ„å»ºé‡‡æ ·é…ç½®
            config = SamplingConfig(
                strategy=SamplingStrategy(strategy),
                sample_size=sample_size,
                random_seed=random_seed,
                strata_column=strata_column,
                cluster_column=cluster_column
            )
            
            # æ‰§è¡Œé‡‡æ ·
            result = await self._execute_sampling(
                data_source_service, connection_config, sql, config, max_total_size, analyze_data_types
            )
            
            return {
                "success": True,
                "result": result,
                "metadata": {
                    "strategy": strategy,
                    "sample_size": sample_size,
                    "total_size": result.total_size,
                    "sampling_rate": result.sampling_rate,
                    "columns_count": len(result.columns)
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ [DataSamplerTool] é‡‡æ ·å¤±è´¥: {e}", exc_info=True)
            return {
                "success": False,
                "error": str(e),
                "result": None
            }
    
    async def _execute_sampling(
        self,
        data_source_service: Any,
        connection_config: Dict[str, Any],
        sql: str,
        config: SamplingConfig,
        max_total_size: int,
        analyze_data_types: bool
    ) -> SamplingResult:
        """æ‰§è¡Œé‡‡æ ·"""
        # é¦–å…ˆè·å–æ€»æ•°æ®é‡
        total_size = await self._get_total_size(data_source_service, connection_config, sql)
        
        if total_size > max_total_size:
            logger.warning(f"âš ï¸ æ•°æ®é‡è¿‡å¤§ ({total_size})ï¼Œé™åˆ¶ä¸º {max_total_size}")
            total_size = max_total_size
        
        # è°ƒæ•´é‡‡æ ·å¤§å°
        if config.sample_size > total_size:
            config.sample_size = total_size
            logger.info(f"ğŸ“ è°ƒæ•´é‡‡æ ·å¤§å°ä¸º {config.sample_size}")
        
        # æ ¹æ®ç­–ç•¥æ‰§è¡Œé‡‡æ ·
        if config.strategy == SamplingStrategy.RANDOM:
            sampled_data = await self._random_sampling(
                data_source_service, connection_config, sql, config, total_size
            )
        elif config.strategy == SamplingStrategy.SYSTEMATIC:
            sampled_data = await self._systematic_sampling(
                data_source_service, connection_config, sql, config, total_size
            )
        elif config.strategy == SamplingStrategy.STRATIFIED:
            sampled_data = await self._stratified_sampling(
                data_source_service, connection_config, sql, config, total_size
            )
        elif config.strategy == SamplingStrategy.CLUSTER:
            sampled_data = await self._cluster_sampling(
                data_source_service, connection_config, sql, config, total_size
            )
        else:  # CONVENIENCE
            sampled_data = await self._convenience_sampling(
                data_source_service, connection_config, sql, config, total_size
            )
        
        # åˆ†ææ•°æ®ç±»å‹
        data_types = {}
        if analyze_data_types and sampled_data:
            data_types = self._analyze_data_types(sampled_data)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        statistics = self._calculate_statistics(sampled_data, data_types)
        
        # æå–åˆ—å
        columns = list(sampled_data[0].keys()) if sampled_data else []
        
        return SamplingResult(
            data=sampled_data,
            sample_size=len(sampled_data),
            total_size=total_size,
            sampling_rate=len(sampled_data) / total_size if total_size > 0 else 0,
            strategy=config.strategy,
            columns=columns,
            data_types=data_types,
            statistics=statistics
        )
    
    async def _get_total_size(
        self,
        data_source_service: Any,
        connection_config: Dict[str, Any],
        sql: str
    ) -> int:
        """è·å–æ€»æ•°æ®é‡"""
        try:
            # æ„å»ºè®¡æ•°æŸ¥è¯¢
            count_sql = f"SELECT COUNT(*) as total FROM ({sql}) as subquery"
            
            result = await data_source_service.run_query(
                connection_config=connection_config,
                sql=count_sql,
                limit=1
            )
            
            if result.get("success"):
                rows = result.get("rows", []) or result.get("data", [])
                if rows and isinstance(rows[0], dict):
                    return int(rows[0].get("total", 0))
                elif rows and isinstance(rows[0], (list, tuple)):
                    return int(rows[0][0])
            
            return 0
            
        except Exception as e:
            logger.warning(f"âš ï¸ è·å–æ€»æ•°æ®é‡å¤±è´¥: {e}")
            return 0
    
    async def _random_sampling(
        self,
        data_source_service: Any,
        connection_config: Dict[str, Any],
        sql: str,
        config: SamplingConfig,
        total_size: int
    ) -> List[Dict[str, Any]]:
        """éšæœºé‡‡æ ·"""
        try:
            # è®¾ç½®éšæœºç§å­
            if config.random_seed is not None:
                random.seed(config.random_seed)
            
            # ç”Ÿæˆéšæœºè¡Œå·
            sample_indices = sorted(random.sample(range(total_size), config.sample_size))
            
            # åˆ†æ‰¹è·å–æ•°æ®
            sampled_data = []
            batch_size = 1000
            
            for i in range(0, len(sample_indices), batch_size):
                batch_indices = sample_indices[i:i + batch_size]
                
                # æ„å»ºæŸ¥è¯¢è·å–æŒ‡å®šè¡Œçš„æ•°æ®
                # è¿™é‡Œä½¿ç”¨ LIMIT å’Œ OFFSET çš„ç®€åŒ–å®ç°
                for idx in batch_indices:
                    offset_sql = f"{sql} LIMIT 1 OFFSET {idx}"
                    
                    result = await data_source_service.run_query(
                        connection_config=connection_config,
                        sql=offset_sql,
                        limit=1
                    )
                    
                    if result.get("success"):
                        rows = result.get("rows", []) or result.get("data", [])
                        if rows:
                            sampled_data.extend(self._format_rows(rows))
            
            return sampled_data
            
        except Exception as e:
            logger.error(f"âŒ éšæœºé‡‡æ ·å¤±è´¥: {e}")
            return []
    
    async def _systematic_sampling(
        self,
        data_source_service: Any,
        connection_config: Dict[str, Any],
        sql: str,
        config: SamplingConfig,
        total_size: int
    ) -> List[Dict[str, Any]]:
        """ç³»ç»Ÿé‡‡æ ·"""
        try:
            # è®¡ç®—é‡‡æ ·é—´éš”
            interval = total_size // config.sample_size
            
            # éšæœºé€‰æ‹©èµ·å§‹ç‚¹
            if config.random_seed is not None:
                random.seed(config.random_seed)
            start_point = random.randint(0, interval - 1)
            
            # ç”Ÿæˆé‡‡æ ·ç‚¹
            sample_indices = [start_point + i * interval for i in range(config.sample_size)]
            sample_indices = [idx for idx in sample_indices if idx < total_size]
            
            # è·å–æ•°æ®
            sampled_data = []
            for idx in sample_indices:
                offset_sql = f"{sql} LIMIT 1 OFFSET {idx}"
                
                result = await data_source_service.run_query(
                    connection_config=connection_config,
                    sql=offset_sql,
                    limit=1
                )
                
                if result.get("success"):
                    rows = result.get("rows", []) or result.get("data", [])
                    if rows:
                        sampled_data.extend(self._format_rows(rows))
            
            return sampled_data
            
        except Exception as e:
            logger.error(f"âŒ ç³»ç»Ÿé‡‡æ ·å¤±è´¥: {e}")
            return []
    
    async def _stratified_sampling(
        self,
        data_source_service: Any,
        connection_config: Dict[str, Any],
        sql: str,
        config: SamplingConfig,
        total_size: int
    ) -> List[Dict[str, Any]]:
        """åˆ†å±‚é‡‡æ ·"""
        try:
            if not config.strata_column:
                logger.warning("âš ï¸ åˆ†å±‚é‡‡æ ·éœ€è¦æŒ‡å®š strata_column")
                return await self._random_sampling(data_source_service, connection_config, sql, config, total_size)
            
            # è·å–åˆ†å±‚ä¿¡æ¯
            strata_sql = f"""
            SELECT {config.strata_column}, COUNT(*) as count
            FROM ({sql}) as subquery
            GROUP BY {config.strata_column}
            """
            
            result = await data_source_service.run_query(
                connection_config=connection_config,
                sql=strata_sql,
                limit=1000
            )
            
            if not result.get("success"):
                logger.warning("âš ï¸ è·å–åˆ†å±‚ä¿¡æ¯å¤±è´¥ï¼Œä½¿ç”¨éšæœºé‡‡æ ·")
                return await self._random_sampling(data_source_service, connection_config, sql, config, total_size)
            
            strata_info = result.get("rows", []) or result.get("data", [])
            
            # è®¡ç®—æ¯å±‚çš„é‡‡æ ·æ•°é‡
            sampled_data = []
            for stratum in strata_info:
                stratum_value = stratum.get(config.strata_column)
                stratum_count = stratum.get("count", 0)
                
                if stratum_count > 0:
                    stratum_sample_size = max(1, int(config.sample_size * stratum_count / total_size))
                    
                    # ä»æ¯å±‚é‡‡æ ·
                    stratum_sql = f"""
                    SELECT * FROM ({sql}) as subquery
                    WHERE {config.strata_column} = '{stratum_value}'
                    LIMIT {stratum_sample_size}
                    """
                    
                    stratum_result = await data_source_service.run_query(
                        connection_config=connection_config,
                        sql=stratum_sql,
                        limit=stratum_sample_size
                    )
                    
                    if stratum_result.get("success"):
                        rows = stratum_result.get("rows", []) or stratum_result.get("data", [])
                        sampled_data.extend(self._format_rows(rows))
            
            return sampled_data
            
        except Exception as e:
            logger.error(f"âŒ åˆ†å±‚é‡‡æ ·å¤±è´¥: {e}")
            return await self._random_sampling(data_source_service, connection_config, sql, config, total_size)
    
    async def _cluster_sampling(
        self,
        data_source_service: Any,
        connection_config: Dict[str, Any],
        sql: str,
        config: SamplingConfig,
        total_size: int
    ) -> List[Dict[str, Any]]:
        """èšç±»é‡‡æ ·"""
        try:
            if not config.cluster_column:
                logger.warning("âš ï¸ èšç±»é‡‡æ ·éœ€è¦æŒ‡å®š cluster_column")
                return await self._random_sampling(data_source_service, connection_config, sql, config, total_size)
            
            # è·å–èšç±»ä¿¡æ¯
            cluster_sql = f"""
            SELECT {config.cluster_column}, COUNT(*) as count
            FROM ({sql}) as subquery
            GROUP BY {config.cluster_column}
            """
            
            result = await data_source_service.run_query(
                connection_config=connection_config,
                sql=cluster_sql,
                limit=1000
            )
            
            if not result.get("success"):
                logger.warning("âš ï¸ è·å–èšç±»ä¿¡æ¯å¤±è´¥ï¼Œä½¿ç”¨éšæœºé‡‡æ ·")
                return await self._random_sampling(data_source_service, connection_config, sql, config, total_size)
            
            clusters = result.get("rows", []) or result.get("data", [])
            
            # éšæœºé€‰æ‹©èšç±»
            if config.random_seed is not None:
                random.seed(config.random_seed)
            
            selected_clusters = random.sample(clusters, min(len(clusters), config.sample_size))
            
            # ä»é€‰ä¸­çš„èšç±»ä¸­è·å–æ•°æ®
            sampled_data = []
            for cluster in selected_clusters:
                cluster_value = cluster.get(config.cluster_column)
                
                cluster_sql = f"""
                SELECT * FROM ({sql}) as subquery
                WHERE {config.cluster_column} = '{cluster_value}'
                """
                
                cluster_result = await data_source_service.run_query(
                    connection_config=connection_config,
                    sql=cluster_sql,
                    limit=1000
                )
                
                if cluster_result.get("success"):
                    rows = cluster_result.get("rows", []) or cluster_result.get("data", [])
                    sampled_data.extend(self._format_rows(rows))
            
            return sampled_data
            
        except Exception as e:
            logger.error(f"âŒ èšç±»é‡‡æ ·å¤±è´¥: {e}")
            return await self._random_sampling(data_source_service, connection_config, sql, config, total_size)
    
    async def _convenience_sampling(
        self,
        data_source_service: Any,
        connection_config: Dict[str, Any],
        sql: str,
        config: SamplingConfig,
        total_size: int
    ) -> List[Dict[str, Any]]:
        """ä¾¿åˆ©é‡‡æ ·ï¼ˆå–å‰Næ¡ï¼‰"""
        try:
            convenience_sql = f"{sql} LIMIT {config.sample_size}"
            
            result = await data_source_service.run_query(
                connection_config=connection_config,
                sql=convenience_sql,
                limit=config.sample_size
            )
            
            if result.get("success"):
                rows = result.get("rows", []) or result.get("data", [])
                return self._format_rows(rows)
            
            return []
            
        except Exception as e:
            logger.error(f"âŒ ä¾¿åˆ©é‡‡æ ·å¤±è´¥: {e}")
            return []
    
    def _format_rows(self, rows: List[Any]) -> List[Dict[str, Any]]:
        """æ ¼å¼åŒ–è¡Œæ•°æ®"""
        formatted_rows = []
        
        for row in rows:
            if isinstance(row, dict):
                formatted_rows.append(row)
            elif isinstance(row, (list, tuple)):
                # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                row_dict = {}
                for i, value in enumerate(row):
                    row_dict[f"column_{i}"] = value
                formatted_rows.append(row_dict)
            else:
                formatted_rows.append({"value": row})
        
        return formatted_rows
    
    def _analyze_data_types(self, data: List[Dict[str, Any]]) -> Dict[str, DataType]:
        """åˆ†ææ•°æ®ç±»å‹"""
        if not data:
            return {}
        
        data_types = {}
        columns = list(data[0].keys())
        
        for column in columns:
            values = [row.get(column) for row in data if row.get(column) is not None]
            
            if not values:
                data_types[column] = DataType.TEXT
                continue
            
            # æ£€æŸ¥æ•°å€¼å‹
            numeric_count = 0
            for value in values:
                try:
                    float(str(value))
                    numeric_count += 1
                except (ValueError, TypeError):
                    break
            
            if numeric_count == len(values):
                data_types[column] = DataType.NUMERIC
                continue
            
            # æ£€æŸ¥å¸ƒå°”å‹
            boolean_count = 0
            for value in values:
                if str(value).lower() in ['true', 'false', '1', '0', 'yes', 'no']:
                    boolean_count += 1
            
            if boolean_count == len(values):
                data_types[column] = DataType.BOOLEAN
                continue
            
            # æ£€æŸ¥æ—¥æœŸæ—¶é—´å‹
            datetime_count = 0
            for value in values:
                try:
                    import datetime
                    if isinstance(value, (datetime.datetime, datetime.date)):
                        datetime_count += 1
                    elif isinstance(value, str) and len(str(value)) > 8:
                        # ç®€å•æ£€æŸ¥æ—¥æœŸæ ¼å¼
                        if any(char in str(value) for char in ['-', '/', ':']):
                            datetime_count += 1
                except:
                    pass
            
            if datetime_count > len(values) * 0.8:  # 80% ä»¥ä¸Šæ˜¯æ—¥æœŸæ—¶é—´
                data_types[column] = DataType.DATETIME
                continue
            
            # æ£€æŸ¥åˆ†ç±»å‹ï¼ˆå”¯ä¸€å€¼è¾ƒå°‘ï¼‰
            unique_values = set(str(value) for value in values)
            if len(unique_values) < min(20, len(values) * 0.1):  # å°‘äº20ä¸ªå”¯ä¸€å€¼æˆ–å°‘äº10%çš„å”¯ä¸€å€¼
                data_types[column] = DataType.CATEGORICAL
                continue
            
            # é»˜è®¤ä¸ºæ–‡æœ¬å‹
            data_types[column] = DataType.TEXT
        
        return data_types
    
    def _calculate_statistics(self, data: List[Dict[str, Any]], data_types: Dict[str, DataType]) -> Dict[str, Any]:
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        if not data:
            return {}
        
        statistics = {
            "total_rows": len(data),
            "total_columns": len(data[0]) if data else 0,
            "column_statistics": {}
        }
        
        for column, data_type in data_types.items():
            values = [row.get(column) for row in data if row.get(column) is not None]
            
            if not values:
                statistics["column_statistics"][column] = {
                    "type": data_type.value,
                    "null_count": len(data),
                    "null_percentage": 100.0
                }
                continue
            
            null_count = len(data) - len(values)
            null_percentage = (null_count / len(data)) * 100
            
            column_stats = {
                "type": data_type.value,
                "null_count": null_count,
                "null_percentage": null_percentage,
                "non_null_count": len(values),
                "unique_count": len(set(str(v) for v in values))
            }
            
            if data_type == DataType.NUMERIC:
                numeric_values = []
                for value in values:
                    try:
                        numeric_values.append(float(str(value)))
                    except (ValueError, TypeError):
                        pass
                
                if numeric_values:
                    column_stats.update({
                        "min": min(numeric_values),
                        "max": max(numeric_values),
                        "mean": sum(numeric_values) / len(numeric_values),
                        "median": sorted(numeric_values)[len(numeric_values) // 2]
                    })
            
            elif data_type == DataType.CATEGORICAL:
                value_counts = {}
                for value in values:
                    value_str = str(value)
                    value_counts[value_str] = value_counts.get(value_str, 0) + 1
                
                column_stats["value_counts"] = value_counts
                column_stats["most_common"] = max(value_counts.items(), key=lambda x: x[1]) if value_counts else None
            
            statistics["column_statistics"][column] = column_stats
        
        return statistics


def create_data_sampler_tool(container: Any) -> DataSamplerTool:
    """
    åˆ›å»ºæ•°æ®é‡‡æ ·å·¥å…·
    
    Args:
        container: æœåŠ¡å®¹å™¨
        
    Returns:
        DataSamplerTool å®ä¾‹
    """
    return DataSamplerTool(container)


# å¯¼å‡º
__all__ = [
    "DataSamplerTool",
    "SamplingStrategy",
    "DataType",
    "SamplingConfig",
    "SamplingResult",
    "create_data_sampler_tool",
]