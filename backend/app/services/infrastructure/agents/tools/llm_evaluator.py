#!/usr/bin/env python3
"""
LLMå¤æ‚åº¦è¯„ä¼°å™¨å’Œæ¨¡åž‹é€‰æ‹©å™¨

ä½¿ç”¨çœŸå®žçš„LLMæœåŠ¡è¿›è¡Œä»»åŠ¡å¤æ‚åº¦è¯„ä¼°å’Œæ¨¡åž‹é€‰æ‹©
"""

import json
import logging
from typing import Optional, Dict, Any, List
from dataclasses import dataclass, asdict, is_dataclass

logger = logging.getLogger(__name__)

from app.services.infrastructure.agents.llm_adapter import _CURRENT_STAGE


def _to_serializable(payload: Any) -> Any:  # noqa: F841
    """
    å°†æ•°æ®å¯¹è±¡è½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼ï¼ˆæ”¯æŒdataclasså’ŒPydanticï¼‰

    Note: æ­¤å‡½æ•°ä¿ç•™ç”¨äºŽæœªæ¥çš„åºåˆ—åŒ–éœ€æ±‚
    """
    if is_dataclass(payload):
        return asdict(payload)
    if hasattr(payload, "dict") and callable(payload.dict):
        return payload.dict()
    if hasattr(payload, "model_dump") and callable(payload.model_dump):
        return payload.model_dump()
    return payload


@dataclass
class TaskComplexityAssessment:
    """ä»»åŠ¡å¤æ‚åº¦è¯„ä¼°ç»“æžœ"""
    complexity_score: float  # 0.0-1.0
    reasoning: str
    factors: List[str]
    confidence: float  # 0.0-1.0
    dimension_scores: Optional[Dict[str, float]] = None


@dataclass
class ModelSelectionDecision:
    """æ¨¡åž‹é€‰æ‹©å†³ç­–ç»“æžœ"""
    selected_model: str
    model_type: str  # "default" or "think"
    reasoning: str
    expected_performance: str
    fallback_model: Optional[str] = None
    confidence: float = 0.8


class LLMComplexityEvaluator:
    """ä½¿ç”¨LLMè¿›è¡Œä»»åŠ¡å¤æ‚åº¦è¯„ä¼°"""

    def __init__(self, container):
        """
        Args:
            container: æœåŠ¡å®¹å™¨ï¼ŒåŒ…å«llm_adapter
        """
        self.container = container
        self.llm_adapter = None

    async def initialize(self):
        """åˆå§‹åŒ–LLMé€‚é…å™¨"""
        if not self.llm_adapter:
            from ..llm_adapter import get_llm_adapter
            self.llm_adapter = get_llm_adapter(self.container)

    async def evaluate_complexity(
        self,
        task_description: str,
        context: Optional[Dict[str, Any]] = None,
        *,
        user_id: Optional[str] = None
    ) -> TaskComplexityAssessment:
        """
        ä½¿ç”¨LLMè¯„ä¼°ä»»åŠ¡å¤æ‚åº¦

        Args:
            task_description: ä»»åŠ¡æè¿°
            context: ä»»åŠ¡ä¸Šä¸‹æ–‡

        Returns:
            TaskComplexityAssessment: å¤æ‚åº¦è¯„ä¼°ç»“æžœ
        """
        await self.initialize()

        # æž„å»ºè¯„ä¼°æç¤º
        evaluation_prompt = self._build_evaluation_prompt(task_description, context)

        # è°ƒç”¨LLM
        messages = [
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»»åŠ¡å¤æ‚åº¦è¯„ä¼°ä¸“å®¶ã€‚ä½ éœ€è¦åˆ†æžä»»åŠ¡å¹¶è¯„ä¼°å…¶å¤æ‚åº¦ã€‚"
            },
            {
                "role": "user",
                "content": evaluation_prompt
            }
        ]

        try:
            # ðŸ”¥ è®¾ç½®ç”¨æˆ·IDå’Œé˜¶æ®µçš„context variables
            from ..llm_adapter import _CURRENT_USER_ID
            if not user_id:
                raise ValueError("evaluate_complexity éœ€è¦æä¾›æœ‰æ•ˆçš„ user_id")
            user_token = _CURRENT_USER_ID.set(user_id)
            stage_token = _CURRENT_STAGE.set("complexity_assessment")
            try:
                response = await self.llm_adapter.chat_completion(
                    messages=messages,
                    temperature=0.0,  # ä½¿ç”¨ç¡®å®šæ€§è¾“å‡º
                    response_format={"type": "json_object"}  # è¦æ±‚JSONæ ¼å¼
                )
            finally:
                _CURRENT_STAGE.reset(stage_token)
                _CURRENT_USER_ID.reset(user_token)

            # æ£€æŸ¥ç©ºå“åº”
            if not response or not response.strip():
                logger.error("âŒ LLMè¿”å›žç©ºå“åº”ï¼Œæ— æ³•è¿›è¡Œå¤æ‚åº¦è¯„ä¼°")
                raise ValueError("LLM returned empty response")

            # è§£æžå“åº”
            result = self._parse_llm_response(response)
            logger.info(f"âœ… LLMå¤æ‚åº¦è¯„ä¼°å®Œæˆ: {result.complexity_score:.2f}")
            return result

        except Exception as e:
            logger.error(f"âŒ LLMå¤æ‚åº¦è¯„ä¼°å¤±è´¥: {e}")
            # å›žé€€åˆ°è§„åˆ™åŸºç¡€è¯„ä¼°
            return self._fallback_assessment(task_description, context)

    def _build_evaluation_prompt(
        self,
        task_description: str,
        context: Optional[Dict[str, Any]]
    ) -> str:
        """æž„å»ºè¯„ä¼°æç¤º"""

        prompt = f"""è¯·è¯„ä¼°ä»¥ä¸‹ä»»åŠ¡çš„å¤æ‚åº¦ï¼š

## ä»»åŠ¡æè¿°
{task_description}

## è¯„ä¼°ç»´åº¦

### 1. æ•°æ®æŸ¥è¯¢å¤æ‚åº¦ (0.0-0.3)
- 0.0-0.1: å•è¡¨æŸ¥è¯¢ï¼Œç®€å•æ¡ä»¶
- 0.1-0.2: å¤šè¡¨JOINï¼ŒåŸºç¡€èšåˆ
- 0.2-0.3: å¤æ‚JOINï¼Œå­æŸ¥è¯¢ï¼Œçª—å£å‡½æ•°

### 2. ä¸šåŠ¡é€»è¾‘å¤æ‚åº¦ (0.0-0.3)
- 0.0-0.1: å•ä¸€æŒ‡æ ‡ï¼Œç›´æŽ¥è®¡ç®—
- 0.1-0.2: å¤šä¸ªæŒ‡æ ‡ï¼Œç®€å•é€»è¾‘
- 0.2-0.3: å¤æ‚ä¸šåŠ¡è§„åˆ™ï¼Œå¤šç»´åº¦åˆ†æž

### 3. è®¡ç®—å¤æ‚åº¦ (0.0-0.2)
- 0.0-0.1: åŸºç¡€ç»Ÿè®¡ï¼ˆSUM, AVG, COUNTï¼‰
- 0.1-0.2: å¤æ‚è®¡ç®—ï¼ˆåŒæ¯”ã€çŽ¯æ¯”ã€è¶‹åŠ¿åˆ†æžï¼‰

### 4. ä¸Šä¸‹æ–‡ç†è§£å¤æ‚åº¦ (0.0-0.2)
- 0.0-0.1: ç›´æŽ¥æ˜Žç¡®çš„éœ€æ±‚
- 0.1-0.2: éœ€è¦æŽ¨ç†å’Œç†è§£éšå«éœ€æ±‚

"""

        if context:
            prompt += f"\n## ä»»åŠ¡ä¸Šä¸‹æ–‡\n{json.dumps(context, ensure_ascii=False, indent=2)}\n"

        prompt += """
## è¾“å‡ºæ ¼å¼

è¯·ä»¥JSONæ ¼å¼è¿”å›žè¯„ä¼°ç»“æžœï¼š

```json
{
    "complexity_score": 0.75,  // æ€»å¤æ‚åº¦è¯„åˆ† (0.0-1.0)
    "reasoning": "ä»»åŠ¡æ¶‰åŠå¤šè¡¨å…³è”æŸ¥è¯¢å’Œå¤æ‚çš„æ—¶é—´åºåˆ—åˆ†æž...",
    "factors": [
        "å¤šè¡¨JOINæŸ¥è¯¢",
        "æ—¶é—´åºåˆ—åˆ†æž",
        "å¤æ‚èšåˆè®¡ç®—"
    ],
    "confidence": 0.85,  // è¯„ä¼°ç½®ä¿¡åº¦ (0.0-1.0)
    "dimension_scores": {
        "data_query": 0.25,
        "business_logic": 0.20,
        "computation": 0.15,
        "context_understanding": 0.15
    }
}
```

è¯·æ ¹æ®ä¸Šè¿°è¯„ä¼°ç»´åº¦ï¼Œç»¼åˆåˆ†æžä»»åŠ¡å¤æ‚åº¦ã€‚
"""

        return prompt

    def _parse_llm_response(self, response: str) -> TaskComplexityAssessment:
        """è§£æžLLMå“åº”"""
        try:
            # å°è¯•è§£æžJSON
            data = json.loads(response)

            return TaskComplexityAssessment(
                complexity_score=data.get("complexity_score", 0.5),
                reasoning=data.get("reasoning", ""),
                factors=data.get("factors", []),
                confidence=data.get("confidence", 0.8),
                dimension_scores=data.get("dimension_scores")
            )
        except json.JSONDecodeError as e:
            logger.error(f"è§£æžLLMå“åº”å¤±è´¥: {e}")
            # è¿”å›žé»˜è®¤å€¼
            return TaskComplexityAssessment(
                complexity_score=0.5,
                reasoning="è§£æžå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å¤æ‚åº¦",
                factors=["è§£æžå¤±è´¥"],
                confidence=0.3
            )

    def _fallback_assessment(
        self,
        task_description: str,
        context: Optional[Dict[str, Any]]  # noqa: ARG002 - ä¿ç•™ç”¨äºŽæŽ¥å£ä¸€è‡´æ€§
    ) -> TaskComplexityAssessment:
        """å›žé€€è¯„ä¼°æ–¹æ³•ï¼ˆå½“LLMè¯„ä¼°å¤±è´¥æ—¶ä½¿ç”¨ï¼‰

        Args:
            task_description: ä»»åŠ¡æè¿°
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆä¿ç•™å‚æ•°ï¼Œæœªæ¥å¯èƒ½ä½¿ç”¨ï¼‰

        Returns:
            TaskComplexityAssessment: å¤æ‚åº¦è¯„ä¼°ç»“æžœ
        """
        # ä½¿ç”¨ç®€å•è§„åˆ™è¯„ä¼°
        complexity_score = 0.5

        # åŸºäºŽå…³é”®è¯çš„ç®€å•è¯„ä¼°
        keywords_complex = ["å¤æ‚", "å¤šè¡¨", "èšåˆ", "åˆ†æž", "JOIN", "å­æŸ¥è¯¢", "çª—å£å‡½æ•°"]
        keywords_simple = ["ç®€å•", "å•ä¸€", "åŸºç¡€", "æŸ¥è¯¢", "ç»Ÿè®¡"]

        text = task_description.lower()

        if any(kw in text for kw in keywords_complex):
            complexity_score = 0.7
        elif any(kw in text for kw in keywords_simple):
            complexity_score = 0.3

        return TaskComplexityAssessment(
            complexity_score=complexity_score,
            reasoning="LLMè¯„ä¼°å¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™åŸºç¡€è¯„ä¼°",
            factors=["è§„åˆ™è¯„ä¼°"],
            confidence=0.6
        )


class LLMModelSelector:
    """ä½¿ç”¨LLMè¿›è¡Œæ¨¡åž‹é€‰æ‹©"""

    def __init__(self, container, user_model_resolver):
        self.container = container
        self.user_model_resolver = user_model_resolver
        self.llm_adapter = None

    async def initialize(self):
        """åˆå§‹åŒ–LLMé€‚é…å™¨"""
        if not self.llm_adapter:
            from ..llm_adapter import get_llm_adapter
            self.llm_adapter = get_llm_adapter(self.container)

    async def select_model(
        self,
        task_description: str,
        complexity_assessment: TaskComplexityAssessment,
        user_id: str,
        task_type: str,
        available_models: List[Dict[str, Any]]
    ) -> ModelSelectionDecision:
        """
        ä½¿ç”¨LLMé€‰æ‹©æœ€åˆé€‚çš„æ¨¡åž‹

        Args:
            task_description: ä»»åŠ¡æè¿°
            complexity_assessment: å¤æ‚åº¦è¯„ä¼°ç»“æžœ
            user_id: ç”¨æˆ·IDï¼ˆä¿ç•™å‚æ•°ï¼Œæœªæ¥å¯èƒ½ç”¨äºŽä¸ªæ€§åŒ–ï¼‰
            task_type: ä»»åŠ¡ç±»åž‹ï¼ˆä¿ç•™å‚æ•°ï¼Œæœªæ¥å¯èƒ½ç”¨äºŽåˆ†ç±»ï¼‰
            available_models: å¯ç”¨æ¨¡åž‹åˆ—è¡¨

        Returns:
            ModelSelectionDecision: æ¨¡åž‹é€‰æ‹©å†³ç­–
        """
        await self.initialize()

        # æž„å»ºé€‰æ‹©æç¤º
        selection_prompt = self._build_selection_prompt(
            task_description,
            complexity_assessment,
            available_models
        )

        # è°ƒç”¨LLM
        messages = [
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªAIæ¨¡åž‹é€‰æ‹©ä¸“å®¶ï¼Œèƒ½å¤Ÿæ ¹æ®ä»»åŠ¡ç‰¹ç‚¹é€‰æ‹©æœ€åˆé€‚çš„æ¨¡åž‹ã€‚"
            },
            {
                "role": "user",
                "content": selection_prompt
            }
        ]

        try:
            # ðŸ”¥ è®¾ç½®ç”¨æˆ·IDå’Œé˜¶æ®µçš„context variables
            from ..llm_adapter import _CURRENT_USER_ID
            if not user_id:
                raise ValueError("select_model éœ€è¦æä¾›æœ‰æ•ˆçš„ user_id")
            user_token = _CURRENT_USER_ID.set(user_id)
            stage_token = _CURRENT_STAGE.set("model_selection")
            try:
                response = await self.llm_adapter.chat_completion(
                    messages=messages,
                    temperature=0.0,
                    response_format={"type": "json_object"}
                )
            finally:
                _CURRENT_STAGE.reset(stage_token)
                _CURRENT_USER_ID.reset(user_token)

            # æ£€æŸ¥ç©ºå“åº”
            if not response or not response.strip():
                logger.error("âŒ LLMè¿”å›žç©ºå“åº”ï¼Œæ— æ³•è¿›è¡Œæ¨¡åž‹é€‰æ‹©")
                raise ValueError("LLM returned empty response")

            # è§£æžå“åº”
            decision = self._parse_selection_response(response, available_models)
            logger.info(f"âœ… LLMæ¨¡åž‹é€‰æ‹©å®Œæˆ: {decision.selected_model}")
            return decision

        except Exception as e:
            logger.error(f"âŒ LLMæ¨¡åž‹é€‰æ‹©å¤±è´¥: {e}")
            # ä¸å†ä½¿ç”¨å›žé€€é€‰æ‹©ï¼Œç›´æŽ¥æŠ›å‡ºå¼‚å¸¸
            raise ValueError(f"æ¨¡åž‹é€‰æ‹©å¤±è´¥: {e}")

    def _build_selection_prompt(
        self,
        task_description: str,
        complexity_assessment: TaskComplexityAssessment,
        available_models: List[Dict[str, Any]]
    ) -> str:
        """æž„å»ºæ¨¡åž‹é€‰æ‹©æç¤º"""

        prompt = f"""è¯·ä¸ºä»¥ä¸‹ä»»åŠ¡é€‰æ‹©æœ€åˆé€‚çš„AIæ¨¡åž‹ï¼š

## ä»»åŠ¡æè¿°
{task_description}

## å¤æ‚åº¦è¯„ä¼°
- å¤æ‚åº¦è¯„åˆ†: {complexity_assessment.complexity_score:.2f}
- è¯„ä¼°æŽ¨ç†: {complexity_assessment.reasoning}
- å½±å“å› ç´ : {', '.join(complexity_assessment.factors)}
- è¯„ä¼°ç½®ä¿¡åº¦: {complexity_assessment.confidence:.2f}

## å¯ç”¨æ¨¡åž‹

"""

        for i, model in enumerate(available_models, 1):
            prompt += f"""
### {i}. {model['name']}
- ç±»åž‹: {model['type']}
- èƒ½åŠ›: {model['capabilities']}
- æŽ¨ç†èƒ½åŠ›: {model['reasoning_level']}
- é€Ÿåº¦: {model['speed']}
- æˆæœ¬: {model['cost']}
- é€‚ç”¨åœºæ™¯: {model['use_cases']}
"""

        prompt += f"""
## âš ï¸ é‡è¦æç¤º

**ä½ å¿…é¡»ä»Žä¸Šé¢åˆ—å‡ºçš„ {len(available_models)} ä¸ªå¯ç”¨æ¨¡åž‹ä¸­é€‰æ‹©ä¸€ä¸ªï¼**

å¯ç”¨æ¨¡åž‹åç§°ï¼š{', '.join([f'"{m["name"]}"' for m in available_models])}

**ä¸è¦è¿”å›žå…¶ä»–æ¨¡åž‹åç§°ï¼Œå¦åˆ™é€‰æ‹©å°†è¢«è§†ä¸ºæ— æ•ˆï¼**

## é€‰æ‹©æ ‡å‡†

1. **ä»»åŠ¡åŒ¹é…åº¦**: æ¨¡åž‹èƒ½åŠ›æ˜¯å¦åŒ¹é…ä»»åŠ¡éœ€æ±‚
2. **æ€§èƒ½éœ€æ±‚**: ä»»åŠ¡å¤æ‚åº¦ä¸Žæ¨¡åž‹æŽ¨ç†èƒ½åŠ›çš„åŒ¹é…
3. **æˆæœ¬æ•ˆç›Š**: åœ¨æ»¡è¶³éœ€æ±‚çš„å‰æä¸‹é€‰æ‹©æ€§ä»·æ¯”æœ€é«˜çš„æ¨¡åž‹
4. **é€Ÿåº¦è¦æ±‚**: è€ƒè™‘ä»»åŠ¡çš„æ—¶æ•ˆæ€§éœ€æ±‚

## è¾“å‡ºæ ¼å¼

```json
{{
    "selected_model": "å¿…é¡»æ˜¯å¯ç”¨æ¨¡åž‹ä¹‹ä¸€",
    "model_type": "default æˆ– think",
    "reasoning": "ä»»åŠ¡å¤æ‚åº¦è¾ƒé«˜(0.75)ï¼Œéœ€è¦å¼ºå¤§çš„æŽ¨ç†èƒ½åŠ›...",
    "expected_performance": "é«˜æ€§èƒ½ï¼Œé¢„è®¡å‡†ç¡®çŽ‡95%+",
    "fallback_model": "default",
    "confidence": 0.9
}}
```

è¯·æ ¹æ®ä»»åŠ¡ç‰¹ç‚¹å’Œå¯ç”¨æ¨¡åž‹ï¼Œä»Žå¯ç”¨æ¨¡åž‹åˆ—è¡¨ä¸­é€‰æ‹©æœ€åˆé€‚çš„æ¨¡åž‹ã€‚
"""

        return prompt

    def _parse_selection_response(
        self,
        response: str,
        available_models: List[Dict[str, Any]]
    ) -> ModelSelectionDecision:
        """è§£æžæ¨¡åž‹é€‰æ‹©å“åº”"""
        try:
            data = json.loads(response)

            # ðŸ”¥ å…³é”®ä¿®å¤ï¼šéªŒè¯ LLM è¿”å›žçš„æ¨¡åž‹æ˜¯å¦åœ¨ç”¨æˆ·é…ç½®çš„æ¨¡åž‹åˆ—è¡¨ä¸­
            selected_model_name = data.get("selected_model", "")
            available_model_names = [m["name"] for m in available_models]

            # å¦‚æžœ LLM é€‰æ‹©çš„æ¨¡åž‹ä¸åœ¨å¯ç”¨åˆ—è¡¨ä¸­ï¼Œç›´æŽ¥æŠ¥é”™è€Œä¸æ˜¯å›žé€€
            if selected_model_name not in available_model_names:
                logger.error(
                    f"âŒ LLM é€‰æ‹©çš„æ¨¡åž‹ '{selected_model_name}' ä¸åœ¨ç”¨æˆ·é…ç½®çš„æ¨¡åž‹åˆ—è¡¨ä¸­: {available_model_names}"
                )
                raise ValueError(f"é€‰æ‹©çš„æ¨¡åž‹ '{selected_model_name}' ä¸åœ¨ç”¨æˆ·é…ç½®çš„æ¨¡åž‹åˆ—è¡¨ä¸­")

            # æ ¹æ®é€‰æ‹©çš„æ¨¡åž‹åç§°æŸ¥æ‰¾æ¨¡åž‹ç±»åž‹
            selected_model_type = "default"
            for model in available_models:
                if model["name"] == selected_model_name:
                    selected_model_type = model.get("type", "default")
                    break

            return ModelSelectionDecision(
                selected_model=selected_model_name,
                model_type=selected_model_type,
                reasoning=data.get("reasoning", ""),
                expected_performance=data.get("expected_performance", "æ ‡å‡†æ€§èƒ½"),
                fallback_model=data.get("fallback_model"),
                confidence=data.get("confidence", 0.8)
            )
        except json.JSONDecodeError as e:
            logger.error(f"è§£æžæ¨¡åž‹é€‰æ‹©å“åº”å¤±è´¥: {e}")
            raise ValueError(f"æ— æ³•è§£æžLLMå“åº”: {e}")
        except ValueError as e:
            # é‡æ–°æŠ›å‡ºéªŒè¯é”™è¯¯
            raise e

    def _fallback_selection(
        self,
        available_models: List[Dict[str, Any]],
        complexity_assessment: TaskComplexityAssessment
    ) -> ModelSelectionDecision:
        """å›žé€€é€‰æ‹©æ–¹æ³•"""
        # ä½¿ç”¨è§„åˆ™é€‰æ‹©
        if complexity_assessment.complexity_score > 0.7:
            # é«˜å¤æ‚åº¦ä»»åŠ¡ï¼Œä¼˜å…ˆé€‰æ‹©æ€è€ƒæ¨¡åž‹
            for model in available_models:
                if model.get("type") == "think":
                    return ModelSelectionDecision(
                        selected_model=model["name"],
                        model_type="think",
                        reasoning="LLMé€‰æ‹©å¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™é€‰æ‹©ï¼ˆé«˜å¤æ‚åº¦ï¼‰",
                        expected_performance="é«˜æ€§èƒ½",
                        fallback_model=available_models[0]["name"] if available_models else None
                    )
        
        # é»˜è®¤é€‰æ‹©ç¬¬ä¸€ä¸ªæ¨¡åž‹
        return ModelSelectionDecision(
            selected_model=available_models[0]["name"],
            model_type=available_models[0].get("type", "default"),
            reasoning="LLMé€‰æ‹©å¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™é€‰æ‹©",
            expected_performance="æ ‡å‡†æ€§èƒ½",
            fallback_model=available_models[1]["name"] if len(available_models) > 1 else None
        )
