from __future__ import annotations

"""
æ¨¡å‹é€‰æ‹©å·¥å…·

è®©LLMè‡ªä¸»åˆ¤æ–­ä»»åŠ¡å¤æ‚åº¦å¹¶é€‰æ‹©åˆé€‚çš„æ¨¡å‹
"""


import logging
from typing import Any, Dict, List, Optional
from dataclasses import dataclass, asdict, is_dataclass

from loom.interfaces.tool import BaseTool
from pydantic import BaseModel, Field

from ..config.user_model_resolver import UserModelResolver, get_user_model_config
from .llm_evaluator import LLMComplexityEvaluator, LLMModelSelector, TaskComplexityAssessment as LLMTaskComplexityAssessment, ModelSelectionDecision as LLMModelSelectionDecision

logger = logging.getLogger(__name__)


def _to_serializable(payload: Any) -> Any:
    """å°†æ¨¡å‹é€‰æ‹©ç»“æœè½¬æ¢ä¸ºå¯åºåˆ—åŒ–ç»“æ„"""
    if is_dataclass(payload):
        return asdict(payload)
    if hasattr(payload, "dict") and callable(payload.dict):
        return payload.dict()
    if hasattr(payload, "model_dump") and callable(payload.model_dump):
        return payload.model_dump()
    return payload


class TaskComplexityAssessment(BaseModel):
    """ä»»åŠ¡å¤æ‚åº¦è¯„ä¼°ç»“æœ"""
    complexity_score: float = Field(..., description="ä»»åŠ¡å¤æ‚åº¦è¯„åˆ† (0.0-1.0)", ge=0.0, le=1.0)
    reasoning: str = Field(..., description="å¤æ‚åº¦è¯„ä¼°çš„æ¨ç†è¿‡ç¨‹")
    factors: List[str] = Field(..., description="å½±å“å¤æ‚åº¦çš„å› ç´ ")
    confidence: float = Field(..., description="è¯„ä¼°ç½®ä¿¡åº¦ (0.0-1.0)", ge=0.0, le=1.0)


class ModelSelectionDecision(BaseModel):
    """æ¨¡å‹é€‰æ‹©å†³ç­–ç»“æœ"""
    selected_model: str = Field(..., description="é€‰æ‹©çš„æ¨¡å‹åç§°")
    model_type: str = Field(..., description="æ¨¡å‹ç±»å‹ (default/think)")
    reasoning: str = Field(..., description="é€‰æ‹©æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹")
    expected_performance: str = Field(..., description="é¢„æœŸæ€§èƒ½è¡¨ç°")
    fallback_model: Optional[str] = Field(None, description="å¤‡ç”¨æ¨¡å‹")


class TaskComplexityAssessmentTool(BaseTool):
    """ä»»åŠ¡å¤æ‚åº¦è¯„ä¼°å·¥å…·ï¼ˆä½¿ç”¨çœŸå®LLMï¼‰"""
    
    def __init__(self, container, user_model_resolver: UserModelResolver):
        super().__init__()
        self.container = container
        self.user_model_resolver = user_model_resolver
        self.evaluator = LLMComplexityEvaluator(container)
        self.name = "assess_task_complexity"
        self.description = """
        ä½¿ç”¨LLMè¯„ä¼°ä»»åŠ¡çš„å¤æ‚åº¦ï¼Œå¸®åŠ©å†³å®šæ˜¯å¦éœ€è¦ä½¿ç”¨æ›´å¼ºå¤§çš„æ¨¡å‹ã€‚
        
        è¾“å…¥å‚æ•°ï¼š
        - task_description: ä»»åŠ¡æè¿°
        - context: ä»»åŠ¡ä¸Šä¸‹æ–‡ä¿¡æ¯
        - user_preferences: ç”¨æˆ·åå¥½è®¾ç½®
        
        è¿”å›ï¼š
        - complexity_score: å¤æ‚åº¦è¯„åˆ† (0.0-1.0)
        - reasoning: è¯„ä¼°æ¨ç†è¿‡ç¨‹
        - factors: å½±å“å¤æ‚åº¦çš„å› ç´ 
        - confidence: è¯„ä¼°ç½®ä¿¡åº¦
        """
    
    async def run(self, task_description: str, context: Optional[Dict[str, Any]] = None, **kwargs) -> LLMTaskComplexityAssessment:
        """Loomæ¡†æ¶è¦æ±‚çš„runæ–¹æ³•"""
        return await self.arun(task_description, context, **kwargs)
    
    async def arun(self, task_description: str, context: Optional[Dict[str, Any]] = None, **kwargs) -> LLMTaskComplexityAssessment:
        """
        è¯„ä¼°ä»»åŠ¡å¤æ‚åº¦
        
        Args:
            task_description: ä»»åŠ¡æè¿°
            context: ä»»åŠ¡ä¸Šä¸‹æ–‡
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            TaskComplexityAssessment: å¤æ‚åº¦è¯„ä¼°ç»“æœ
        """
        try:
            logger.info(f"ğŸ” å¼€å§‹LLMè¯„ä¼°ä»»åŠ¡å¤æ‚åº¦: {task_description[:100]}...")
            
            # ä½¿ç”¨çœŸå®LLMè¯„ä¼°
            result = await self.evaluator.evaluate_complexity(
                task_description=task_description,
                context=context,
                user_id=kwargs.get("user_id")
            )
            
            logger.info(f"âœ… LLMè¯„ä¼°å®Œæˆ: {result.complexity_score:.2f}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ LLMè¯„ä¼°å¤±è´¥: {e}")
            # å›é€€åˆ°è§„åˆ™åŸºç¡€è¯„ä¼°
            return self._fallback_assessment(task_description, context)
    
    def _fallback_assessment(
        self,
        task_description: str,
        context: Optional[Dict[str, Any]]
    ) -> TaskComplexityAssessment:
        """å›é€€è¯„ä¼°æ–¹æ³•"""
        # ä½¿ç”¨ç®€å•è§„åˆ™è¯„ä¼°
        complexity_score = 0.5

        # åŸºäºå…³é”®è¯çš„ç®€å•è¯„ä¼°
        keywords_complex = ["å¤æ‚", "å¤šè¡¨", "èšåˆ", "åˆ†æ", "JOIN", "å­æŸ¥è¯¢", "çª—å£å‡½æ•°"]
        keywords_simple = ["ç®€å•", "å•ä¸€", "åŸºç¡€", "æŸ¥è¯¢", "ç»Ÿè®¡"]

        text = task_description.lower()

        if any(kw in text for kw in keywords_complex):
            complexity_score = 0.7
        elif any(kw in text for kw in keywords_simple):
            complexity_score = 0.3

        return LLMTaskComplexityAssessment(
            complexity_score=complexity_score,
            reasoning="LLMè¯„ä¼°å¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™åŸºç¡€è¯„ä¼°",
            factors=["è§„åˆ™è¯„ä¼°"],
            confidence=0.6
        )


class ModelSelectionTool(BaseTool):
    """æ¨¡å‹é€‰æ‹©å·¥å…·ï¼ˆä½¿ç”¨çœŸå®LLMï¼‰"""
    
    def __init__(self, container, user_model_resolver: UserModelResolver):
        super().__init__()
        self.container = container
        self.user_model_resolver = user_model_resolver
        self.selector = LLMModelSelector(container, user_model_resolver)
        self.name = "select_optimal_model"
        self.description = """
        ä½¿ç”¨LLMé€‰æ‹©æœ€åˆé€‚çš„æ¨¡å‹ã€‚
        
        è¾“å…¥å‚æ•°ï¼š
        - task_description: ä»»åŠ¡æè¿°
        - complexity_score: ä»»åŠ¡å¤æ‚åº¦è¯„åˆ†
        - user_id: ç”¨æˆ·ID
        - task_type: ä»»åŠ¡ç±»å‹
        
        è¿”å›ï¼š
        - selected_model: é€‰æ‹©çš„æ¨¡å‹åç§°
        - model_type: æ¨¡å‹ç±»å‹
        - reasoning: é€‰æ‹©æ¨ç†è¿‡ç¨‹
        - expected_performance: é¢„æœŸæ€§èƒ½
        - fallback_model: å¤‡ç”¨æ¨¡å‹
        """
    
    async def run(self, task_description: str, complexity_score: float, user_id: str, task_type: str = "placeholder_analysis", **kwargs) -> LLMModelSelectionDecision:
        """Loomæ¡†æ¶è¦æ±‚çš„runæ–¹æ³•"""
        return await self.arun(task_description, complexity_score, user_id, task_type, **kwargs)
    
    async def arun(
        self,
        task_description: str,
        complexity_score: float,
        user_id: str,
        task_type: str = "placeholder_analysis",
        **kwargs
    ) -> LLMModelSelectionDecision:
        """
        é€‰æ‹©åˆé€‚çš„æ¨¡å‹
        
        Args:
            task_description: ä»»åŠ¡æè¿°
            complexity_score: ä»»åŠ¡å¤æ‚åº¦è¯„åˆ†
            user_id: ç”¨æˆ·ID
            task_type: ä»»åŠ¡ç±»å‹
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            ModelSelectionDecision: æ¨¡å‹é€‰æ‹©å†³ç­–
        """
        try:
            logger.info(f"ğŸ¤– å¼€å§‹LLMæ¨¡å‹é€‰æ‹©: complexity={complexity_score:.2f}")

            # è·å–ç”¨æˆ·é…ç½®
            user_config = await get_user_model_config(user_id, task_type)

            # å‡†å¤‡å¯ç”¨æ¨¡å‹åˆ—è¡¨
            available_models = self._prepare_available_models(user_config)

            # åˆ›å»ºå¤æ‚åº¦è¯„ä¼°å¯¹è±¡
            complexity_assessment = LLMTaskComplexityAssessment(
                complexity_score=complexity_score,
                reasoning=kwargs.get("complexity_reasoning", ""),
                factors=kwargs.get("complexity_factors", []),
                confidence=kwargs.get("complexity_confidence", 0.8)
            )

            # ä½¿ç”¨LLMé€‰æ‹©æ¨¡å‹
            decision = await self.selector.select_model(
                task_description=task_description,
                complexity_assessment=complexity_assessment,
                user_id=user_id,
                task_type=task_type,
                available_models=available_models
            )

            logger.info(f"âœ… LLMé€‰æ‹©å®Œæˆ: {decision.selected_model}")
            return decision

        except Exception as e:
            logger.error(f"âŒ LLMæ¨¡å‹é€‰æ‹©å¤±è´¥: {e}")
            # ä¸å†ä½¿ç”¨å›é€€é€‰æ‹©ï¼Œç›´æ¥æŠ›å‡ºå¼‚å¸¸
            raise ValueError(f"æ¨¡å‹é€‰æ‹©å¤±è´¥: {e}")

    def _prepare_available_models(
        self,
        user_config
    ) -> List[Dict[str, Any]]:
        """å‡†å¤‡å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        models = []

        # æ·»åŠ é»˜è®¤æ¨¡å‹
        if user_config.default_model:
            models.append({
                "name": user_config.default_model.model_name,
                "type": user_config.default_model.model_type,
                "capabilities": "é€šç”¨ä»»åŠ¡å¤„ç†",
                "reasoning_level": "ä¸­ç­‰",
                "speed": "å¿«é€Ÿ",
                "cost": "ä½",
                "use_cases": "ç®€å•åˆ°ä¸­ç­‰å¤æ‚åº¦ä»»åŠ¡"
            })

        # æ·»åŠ æ€è€ƒæ¨¡å‹ï¼ˆå¦‚æœå­˜åœ¨ä¸”ä¸åŒäºé»˜è®¤æ¨¡å‹ï¼‰
        if user_config.think_model and user_config.think_model.model_name != user_config.default_model.model_name:
            models.append({
                "name": user_config.think_model.model_name,
                "type": user_config.think_model.model_type,
                "capabilities": "æ·±åº¦æ¨ç†å’Œå¤æ‚ä»»åŠ¡å¤„ç†",
                "reasoning_level": "é«˜",
                "speed": "è¾ƒæ…¢",
                "cost": "é«˜",
                "use_cases": "å¤æ‚æ¨ç†ã€å¤šæ­¥éª¤ä»»åŠ¡"
            })

        logger.info(f"å‡†å¤‡å¯ç”¨æ¨¡å‹åˆ—è¡¨: {[m['name'] for m in models]}")
        return models

    def _fallback_selection(
        self,
        user_config,
        complexity_score: float
    ) -> ModelSelectionDecision:
        """å›é€€é€‰æ‹©æ–¹æ³•"""
        # ä½¿ç”¨è§„åˆ™é€‰æ‹©
        selected_model_config = self.user_model_resolver.select_model_for_task(
            user_config, complexity_score, "placeholder_analysis"
        )

        return LLMModelSelectionDecision(
            selected_model=selected_model_config.model_name,
            model_type=selected_model_config.model_type,
            reasoning="LLMé€‰æ‹©å¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™é€‰æ‹©",
            expected_performance="æ ‡å‡†æ€§èƒ½",
            fallback_model=None
        )


class DynamicModelSwitcher:
    """åŠ¨æ€æ¨¡å‹åˆ‡æ¢å™¨"""
    
    def __init__(self, container, user_model_resolver: UserModelResolver):
        self.container = container
        self.user_model_resolver = user_model_resolver
        self.complexity_tool = TaskComplexityAssessmentTool(container, user_model_resolver)
        self.selection_tool = ModelSelectionTool(container, user_model_resolver)
    
    async def assess_and_select_model(
        self,
        task_description: str,
        user_id: str,
        context: Optional[Dict[str, Any]] = None,
        task_type: str = "placeholder_analysis"
    ) -> Dict[str, Any]:
        """
        è¯„ä¼°ä»»åŠ¡å¤æ‚åº¦å¹¶é€‰æ‹©åˆé€‚çš„æ¨¡å‹
        
        Args:
            task_description: ä»»åŠ¡æè¿°
            user_id: ç”¨æˆ·ID
            context: ä»»åŠ¡ä¸Šä¸‹æ–‡
            task_type: ä»»åŠ¡ç±»å‹
            
        Returns:
            Dict[str, Any]: åŒ…å«è¯„ä¼°ç»“æœå’Œæ¨¡å‹é€‰æ‹©çš„ç»“æœ
        """
        try:
            logger.info(f"ğŸ”„ å¼€å§‹åŠ¨æ€æ¨¡å‹è¯„ä¼°å’Œé€‰æ‹©: user_id={user_id}")
            
            # 1. è¯„ä¼°ä»»åŠ¡å¤æ‚åº¦
            complexity_assessment = await self.complexity_tool.arun(
                task_description=task_description,
                context=context,
                user_id=user_id  # ğŸ”¥ ä¼ é€’ç”¨æˆ·ID
            )
            
            # 2. é€‰æ‹©åˆé€‚æ¨¡å‹
            model_decision = await self.selection_tool.arun(
                task_description=task_description,
                complexity_score=complexity_assessment.complexity_score,
                user_id=user_id,
                task_type=task_type
            )
            
            # 3. è·å–ç”¨æˆ·é…ç½®
            user_config = await get_user_model_config(user_id, task_type)
            selected_model_config = self.user_model_resolver.select_model_for_task(
                user_config, complexity_assessment.complexity_score, task_type
            )
            
            # 4. è®¡ç®—æœ€å¤§ä¸Šä¸‹æ–‡tokens
            max_context_tokens = self.user_model_resolver.get_max_context_tokens(
                user_config, selected_model_config
            )
            
            result = {
                "complexity_assessment": {
                    "complexity_score": complexity_assessment.complexity_score,
                    "reasoning": complexity_assessment.reasoning,
                    "factors": complexity_assessment.factors,
                    "confidence": complexity_assessment.confidence,
                    "dimension_scores": complexity_assessment.dimension_scores
                },
                "model_decision": _to_serializable(model_decision),
                "selected_model_config": {
                    "model_name": selected_model_config.model_name,
                    "model_type": selected_model_config.model_type,
                    "max_tokens": selected_model_config.max_tokens,
                    "temperature": selected_model_config.temperature,
                    "supports_function_calls": selected_model_config.supports_function_calls,
                    "supports_thinking": selected_model_config.supports_thinking
                },
                "max_context_tokens": max_context_tokens,
                "user_config": {
                    "auto_model_selection": user_config.auto_model_selection,
                    "think_model_threshold": user_config.think_model_threshold
                }
            }
            
            logger.info(f"âœ… åŠ¨æ€æ¨¡å‹è¯„ä¼°å®Œæˆ: {model_decision.selected_model}({model_decision.model_type})")
            return result
            
        except Exception as e:
            logger.error(f"âŒ åŠ¨æ€æ¨¡å‹è¯„ä¼°å¤±è´¥: {e}")
            raise


# å…¨å±€å®ä¾‹
user_model_resolver = UserModelResolver()

# æ³¨æ„ï¼šdynamic_model_switcher éœ€è¦ container å‚æ•°ï¼Œè¿™é‡Œæä¾›ä¸€ä¸ªå·¥å‚å‡½æ•°
def create_dynamic_model_switcher(container) -> DynamicModelSwitcher:
    """åˆ›å»ºåŠ¨æ€æ¨¡å‹åˆ‡æ¢å™¨å®ä¾‹"""
    return DynamicModelSwitcher(container, user_model_resolver)


# ä¾¿æ·å‡½æ•°
async def assess_task_complexity(
    task_description: str,
    context: Optional[Dict[str, Any]] = None,
    container: Optional[Any] = None
) -> LLMTaskComplexityAssessment:
    """è¯„ä¼°ä»»åŠ¡å¤æ‚åº¦çš„ä¾¿æ·å‡½æ•°"""
    if container is None:
        raise ValueError("container å‚æ•°æ˜¯å¿…éœ€çš„")
    
    switcher = create_dynamic_model_switcher(container)
    return await switcher.complexity_tool.arun(
        task_description=task_description,
        context=context
    )


async def select_optimal_model(
    task_description: str,
    complexity_score: float,
    user_id: str,
    task_type: str = "placeholder_analysis",
    container: Optional[Any] = None
) -> LLMModelSelectionDecision:
    """é€‰æ‹©æœ€ä¼˜æ¨¡å‹çš„ä¾¿æ·å‡½æ•°"""
    if container is None:
        raise ValueError("container å‚æ•°æ˜¯å¿…éœ€çš„")
    
    switcher = create_dynamic_model_switcher(container)
    return await switcher.selection_tool.arun(
        task_description=task_description,
        complexity_score=complexity_score,
        user_id=user_id,
        task_type=task_type
    )


async def assess_and_select_model(
    task_description: str,
    user_id: str,
    context: Optional[Dict[str, Any]] = None,
    task_type: str = "placeholder_analysis",
    container: Optional[Any] = None
) -> Dict[str, Any]:
    """è¯„ä¼°ä»»åŠ¡å¤æ‚åº¦å¹¶é€‰æ‹©æ¨¡å‹çš„ä¾¿æ·å‡½æ•°"""
    if container is None:
        raise ValueError("container å‚æ•°æ˜¯å¿…éœ€çš„")
    
    switcher = create_dynamic_model_switcher(container)
    return await switcher.assess_and_select_model(
        task_description=task_description,
        user_id=user_id,
        context=context,
        task_type=task_type
    )
