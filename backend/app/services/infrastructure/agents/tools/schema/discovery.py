from __future__ import annotations

"""
Schema å‘ç°å·¥å…·

ç”¨äºå‘ç°æ•°æ®æºä¸­çš„è¡¨ç»“æ„å’Œå…³ç³»
æ”¯æŒæ™ºèƒ½è¡¨å‘ç°å’Œç»“æ„åˆ†æ
"""


import logging
from copy import deepcopy
from typing import Any, Dict, List, Optional, Union
from dataclasses import dataclass

from loom.interfaces.tool import BaseTool
from ...types import ToolCategory, ContextInfo

logger = logging.getLogger(__name__)


@dataclass
class TableInfo:
    """è¡¨ä¿¡æ¯"""
    name: str
    description: Optional[str] = None
    table_type: str = "TABLE"
    row_count: Optional[int] = None
    size_bytes: Optional[int] = None
    created_at: Optional[str] = None
    updated_at: Optional[str] = None
    metadata: Dict[str, Any] = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


@dataclass
class ColumnInfo:
    """åˆ—ä¿¡æ¯"""
    name: str
    data_type: str
    nullable: bool = True
    default_value: Optional[str] = None
    is_primary_key: bool = False
    is_foreign_key: bool = False
    foreign_table: Optional[str] = None
    foreign_column: Optional[str] = None
    description: Optional[str] = None
    max_length: Optional[int] = None
    precision: Optional[int] = None
    scale: Optional[int] = None
    metadata: Dict[str, Any] = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


@dataclass
class RelationshipInfo:
    """å…³ç³»ä¿¡æ¯"""
    from_table: str
    from_column: str
    to_table: str
    to_column: str
    relationship_type: str = "FOREIGN_KEY"  # FOREIGN_KEY, ONE_TO_ONE, ONE_TO_MANY
    constraint_name: Optional[str] = None
    metadata: Dict[str, Any] = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


class SchemaDiscoveryTool(BaseTool):
    """Schema å‘ç°å·¥å…· - æ”¯æŒæ‡’åŠ è½½ä¼˜åŒ–"""
    
    def __init__(
        self,
        container: Any,
        connection_config: Optional[Dict[str, Any]] = None,
        enable_lazy_loading: bool = True
    ):
        """
        Args:
            container: æœåŠ¡å®¹å™¨
            connection_config: æ•°æ®æºè¿æ¥é…ç½®ï¼ˆåœ¨åˆå§‹åŒ–æ—¶æ³¨å…¥ï¼ŒLLM ä¸éœ€è¦ä¼ é€’ï¼‰
            enable_lazy_loading: æ˜¯å¦å¯ç”¨æ‡’åŠ è½½ï¼ˆå¯åŠ¨æ—¶åªè·å–è¡¨åï¼ŒæŒ‰éœ€è·å–åˆ—ä¿¡æ¯ï¼‰
        """
        super().__init__()

        self.name = "schema_discovery"

        self.category = ToolCategory.SCHEMA

        self.description = "å‘ç°æ•°æ®æºä¸­çš„è¡¨ç»“æ„å’Œå…³ç³»ï¼Œæ”¯æŒæ‡’åŠ è½½ä¼˜åŒ–"
        self.container = container
        self._connection_config = connection_config  # ğŸ”¥ ä¿å­˜è¿æ¥é…ç½®
        self._data_source_service = None

        # æ‡’åŠ è½½ç›¸å…³å±æ€§
        self.enable_lazy_loading = enable_lazy_loading
        self._table_names_cache: List[str] = []
        self._columns_cache: Dict[str, List[Dict[str, Any]]] = {}
        self._cache_initialized = False
        self._result_cache: Dict[str, Dict[str, Any]] = {}
    
    async def _get_data_source_service(self):
        """è·å–æ•°æ®æºæœåŠ¡"""
        if self._data_source_service is None:
            self._data_source_service = getattr(
                self.container, 'data_source', None
            ) or getattr(self.container, 'data_source_service', None)
        return self._data_source_service
    
    async def _get_context_selected_tables(self) -> Optional[List[str]]:
        """ä»ä¸Šä¸‹æ–‡ä¸­è·å–å·²é€‰æ‹©çš„è¡¨"""
        try:
            # å°è¯•ä»å®¹å™¨ä¸­è·å–ä¸Šä¸‹æ–‡æ£€ç´¢å™¨
            context_retriever = getattr(self.container, 'context_retriever', None)
            if context_retriever:
                # è·å–æœ€è¿‘æ£€ç´¢çš„è¡¨
                recent_tables = getattr(context_retriever, 'recent_selected_tables', None)
                if recent_tables:
                    logger.info(f"ğŸ” [SchemaDiscoveryTool] ä»ä¸Šä¸‹æ–‡æ£€ç´¢å™¨è·å–åˆ°è¡¨: {recent_tables}")
                    return recent_tables
            
            # å°è¯•ä»å…¨å±€çŠ¶æ€ä¸­è·å–
            import threading
            thread_local = getattr(threading.current_thread(), 'agent_context', None)
            if thread_local and 'selected_tables' in thread_local:
                tables = thread_local['selected_tables']
                logger.info(f"ğŸ” [SchemaDiscoveryTool] ä»çº¿ç¨‹ä¸Šä¸‹æ–‡è·å–åˆ°è¡¨: {tables}")
                return tables
            
            # å°è¯•ä½¿ç”¨LLMæ™ºèƒ½è§£ææ¶ˆæ¯å†å²ä¸­çš„è¡¨å
            try:
                # ä»å®¹å™¨ä¸­è·å–æ¶ˆæ¯å†å²
                messages = getattr(self.container, 'recent_messages', None)
                if messages:
                    # æ”¶é›†æœ€è¿‘çš„æ¶ˆæ¯å†…å®¹
                    recent_contents = []
                    for message in reversed(messages[-3:]):  # æ£€æŸ¥æœ€è¿‘3æ¡æ¶ˆæ¯
                        content = getattr(message, 'content', '') or str(message)
                        if content and len(content.strip()) > 10:  # è¿‡æ»¤æ‰å¤ªçŸ­çš„æ¶ˆæ¯
                            recent_contents.append(content[:200])  # é™åˆ¶é•¿åº¦
                    
                    if recent_contents:
                        # ä½¿ç”¨LLMè§£æè¡¨å
                        parsed_tables = await self._llm_parse_tables_from_messages(recent_contents)
                        if parsed_tables:
                            logger.info(f"ğŸ” [SchemaDiscoveryTool] LLMè§£æåˆ°è¡¨: {parsed_tables}")
                            return parsed_tables
            except Exception as e:
                logger.debug(f"âš ï¸ [SchemaDiscoveryTool] LLMè§£ææ¶ˆæ¯å†å²å¤±è´¥: {e}")
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•è¡¨ä¿¡æ¯ï¼Œè¿”å›ç©ºåˆ—è¡¨è€Œä¸æ˜¯ç¡¬ç¼–ç 
            logger.warning("âš ï¸ [SchemaDiscoveryTool] æ— æ³•ä»ä»»ä½•æ¥æºè·å–è¡¨ä¿¡æ¯")
            return None
            
        except Exception as e:
            logger.debug(f"âš ï¸ [SchemaDiscoveryTool] è·å–ä¸Šä¸‹æ–‡è¡¨ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    async def _llm_parse_tables_from_messages(self, messages: List[str]) -> Optional[List[str]]:
        """ä½¿ç”¨LLMæ™ºèƒ½è§£ææ¶ˆæ¯ä¸­çš„è¡¨å"""
        try:
            # æ„å»ºæç¤ºè¯
            messages_text = "\n".join([f"- {msg}" for msg in messages])
            prompt = f"""
è¯·åˆ†æä»¥ä¸‹æ¶ˆæ¯å†…å®¹ï¼Œæå–å‡ºä¸æ•°æ®åº“è¡¨ç›¸å…³çš„è¡¨åã€‚

æ¶ˆæ¯å†…å®¹ï¼š
{messages_text}

è¯·æ ¹æ®æ¶ˆæ¯å†…å®¹ï¼Œè¯†åˆ«å‡ºæœ€ç›¸å…³çš„æ•°æ®åº“è¡¨åã€‚å¦‚æœæ¶ˆæ¯ä¸­æåˆ°"é€€è´§"ã€"é€€æ¬¾"ã€"refund"ç­‰å…³é”®è¯ï¼Œè¯·é‡ç‚¹å…³æ³¨ç›¸å…³çš„è¡¨åã€‚

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼š
{{
    "tables": ["è¡¨å1", "è¡¨å2"],
    "reasoning": "é€‰æ‹©è¿™äº›è¡¨çš„åŸå› "
}}

å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ç¡®çš„è¡¨åï¼Œè¯·è¿”å›ç©ºçš„tablesæ•°ç»„ã€‚
"""

            # è·å–LLMæœåŠ¡
            llm_service = getattr(self.container, 'llm_service', None)
            if not llm_service:
                logger.debug("âš ï¸ [SchemaDiscoveryTool] æœªæ‰¾åˆ°LLMæœåŠ¡ï¼Œæ— æ³•è§£æè¡¨å")
                return None
            
            # è°ƒç”¨LLM
            response = await llm_service.generate_completion(
                prompt=prompt,
                model="gpt-4o-mini",
                max_tokens=200,
                temperature=0.1
            )
            
            if response and response.get('content'):
                import json
                try:
                    result = json.loads(response['content'])
                    tables = result.get('tables', [])
                    reasoning = result.get('reasoning', '')
                    
                    if tables:
                        logger.info(f"ğŸ¤– [SchemaDiscoveryTool] LLMè§£æç»“æœ: {tables}, åŸå› : {reasoning}")
                        return tables[:3]  # æœ€å¤šè¿”å›3ä¸ªè¡¨
                    else:
                        logger.debug(f"ğŸ¤– [SchemaDiscoveryTool] LLMæœªæ‰¾åˆ°ç›¸å…³è¡¨å: {reasoning}")
                        return None
                        
                except json.JSONDecodeError as e:
                    logger.debug(f"âš ï¸ [SchemaDiscoveryTool] LLMè¿”å›æ ¼å¼é”™è¯¯: {e}")
                    return None
            
            return None
            
        except Exception as e:
            logger.debug(f"âš ï¸ [SchemaDiscoveryTool] LLMè§£æå¤±è´¥: {e}")
            return None
    
    async def _initialize_table_names_cache(self, connection_config: Dict[str, Any]):
        """åˆå§‹åŒ–è¡¨åç¼“å­˜ï¼ˆæ‡’åŠ è½½çš„ç¬¬ä¸€æ­¥ï¼‰"""
        if self._cache_initialized:
            return
            
        try:
            data_source_service = await self._get_data_source_service()
            if not data_source_service:
                logger.warning("âš ï¸ æ•°æ®æºæœåŠ¡ä¸å¯ç”¨ï¼Œæ— æ³•åˆå§‹åŒ–è¡¨åç¼“å­˜")
                return
            
            logger.info("ğŸ” [SchemaDiscoveryTool] åˆå§‹åŒ–è¡¨åç¼“å­˜ï¼ˆæ‡’åŠ è½½æ¨¡å¼ï¼‰")
            
            # åªè·å–è¡¨ååˆ—è¡¨
            result = await data_source_service.run_query(
                connection_config=connection_config,
                sql="SHOW TABLES",
                limit=1000
            )
            
            if result.get("success"):
                rows = result.get("rows", []) or result.get("data", [])
                table_names = []
                
                for row in rows:
                    table_name = self._extract_table_name(row)
                    if table_name:
                        table_names.append(table_name)
                
                self._table_names_cache = table_names
                self._cache_initialized = True
                
                logger.info(f"âœ… è¡¨åç¼“å­˜åˆå§‹åŒ–å®Œæˆï¼Œå‘ç° {len(table_names)} ä¸ªè¡¨")
                logger.info(f"   è¡¨å: {table_names[:10]}{'...' if len(table_names) > 10 else ''}")
            else:
                logger.warning(f"âš ï¸ è·å–è¡¨ååˆ—è¡¨å¤±è´¥: {result.get('error')}")
                
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–è¡¨åç¼“å­˜å¤±è´¥: {e}")
    
    async def _load_columns_for_tables(
        self, 
        connection_config: Dict[str, Any], 
        table_names: List[str],
        include_metadata: bool = True
    ) -> Dict[str, List[Dict[str, Any]]]:
        """æŒ‰éœ€åŠ è½½æŒ‡å®šè¡¨çš„åˆ—ä¿¡æ¯"""
        data_source_service = await self._get_data_source_service()
        if not data_source_service:
            return {}
        
        # æ‰¾å‡ºéœ€è¦åŠ è½½çš„è¡¨ï¼ˆæœªç¼“å­˜çš„ï¼‰
        tables_to_load = [name for name in table_names if name not in self._columns_cache]
        
        if not tables_to_load:
            logger.info(f"âœ… æ‰€æœ‰è¡¨åˆ—ä¿¡æ¯å·²ç¼“å­˜ï¼Œæ— éœ€é‡å¤åŠ è½½")
            return {name: self._columns_cache[name] for name in table_names if name in self._columns_cache}
        
        logger.info(f"ğŸ”„ æŒ‰éœ€åŠ è½½ {len(tables_to_load)} ä¸ªè¡¨çš„åˆ—ä¿¡æ¯: {tables_to_load}")
        
        # å¹¶è¡ŒåŠ è½½åˆ—ä¿¡æ¯
        async def load_single_table_columns(table_name: str):
            try:
                columns = await self._get_table_columns(
                    data_source_service, connection_config, table_name, include_metadata
                )
                return table_name, columns
            except Exception as e:
                logger.warning(f"âš ï¸ åŠ è½½è¡¨ {table_name} åˆ—ä¿¡æ¯å¤±è´¥: {e}")
                return table_name, []
        
        import asyncio
        tasks = [load_single_table_columns(table_name) for table_name in tables_to_load]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†ç»“æœ
        loaded_count = 0
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"âŒ å¹¶è¡ŒåŠ è½½å‡ºé”™: {result}")
                continue
                
            table_name, columns = result
            self._columns_cache[table_name] = columns
            loaded_count += 1
            logger.info(f"  ğŸ“‹ è¡¨ {table_name}: {len(columns)} åˆ—")
        
        logger.info(f"âœ… æŒ‰éœ€åŠ è½½å®Œæˆï¼Œæ–°å¢ {loaded_count} ä¸ªè¡¨åˆ—ä¿¡æ¯")
        
        # è¿”å›è¯·æ±‚çš„è¡¨åˆ—ä¿¡æ¯
        return {name: self._columns_cache[name] for name in table_names if name in self._columns_cache}
    
    def get_schema(self) -> Dict[str, Any]:
        """è·å–å·¥å…·å‚æ•°æ¨¡å¼"""
        return {
            "type": "function",
            "function": {
                "name": "schema_discovery",
                "description": "å‘ç°æ•°æ®æºä¸­çš„è¡¨ç»“æ„å’Œå…³ç³»",
                "parameters": {
                    "type": "object",
                    "properties": {
                        # ğŸ”¥ ç§»é™¤ connection_config å‚æ•°ï¼Œç”±å·¥å…·å†…éƒ¨è‡ªåŠ¨è·å–
                        "discovery_type": {
                            "type": "string",
                            "enum": ["tables", "columns", "relationships", "all"],
                            "default": "all",
                            "description": "å‘ç°ç±»å‹ï¼štables(è¡¨), columns(åˆ—), relationships(å…³ç³»), all(å…¨éƒ¨)"
                        },
                        "table_pattern": {
                            "type": "string",
                            "description": "è¡¨åæ¨¡å¼è¿‡æ»¤ï¼ˆæ”¯æŒé€šé…ç¬¦ï¼‰"
                        },
                        "include_metadata": {
                            "type": "boolean",
                            "default": True,
                            "description": "æ˜¯å¦åŒ…å«å…ƒæ•°æ®ä¿¡æ¯"
                        },
                        "max_tables": {
                            "type": "integer",
                            "default": 100,
                            "description": "æœ€å¤§è¡¨æ•°é‡é™åˆ¶"
                        }
                    },
                    "required": []  # ğŸ”¥ æ‰€æœ‰å‚æ•°éƒ½æ˜¯å¯é€‰çš„
                }
            }
        }
    
    async def run(
        self,
        discovery_type: str = "all",
        table_pattern: Optional[str] = None,
        include_metadata: bool = True,
        max_tables: int = 100,
        tables: Optional[List[str]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œ Schema å‘ç° - æ”¯æŒæ‡’åŠ è½½ä¼˜åŒ–

        Args:
            discovery_type: å‘ç°ç±»å‹ (tables, columns, relationships, all)
            table_pattern: è¡¨åæ¨¡å¼
            include_metadata: æ˜¯å¦åŒ…å«å…ƒæ•°æ®
            max_tables: æœ€å¤§è¡¨æ•°é‡
            tables: æŒ‡å®šè¦å¤„ç†çš„è¡¨ååˆ—è¡¨

        Returns:
            Dict[str, Any]: å‘ç°ç»“æœ
        """
        logger.info(f"ğŸ” [SchemaDiscoveryTool] å¼€å§‹å‘ç° Schema: {discovery_type}")
        logger.info(f"   æ‡’åŠ è½½æ¨¡å¼: {'å¯ç”¨' if self.enable_lazy_loading else 'ç¦ç”¨'}")

        # ğŸ”¥ ä½¿ç”¨åˆå§‹åŒ–æ—¶æ³¨å…¥çš„ connection_config
        connection_config = self._connection_config
        if not connection_config:
            return {
                "success": False,
                "error": "æœªé…ç½®æ•°æ®æºè¿æ¥ï¼Œè¯·åœ¨åˆå§‹åŒ–å·¥å…·æ—¶æä¾› connection_config",
                "discovered": {}
            }

        try:
            signature = self._make_signature(
                discovery_type=discovery_type,
                table_pattern=table_pattern,
                include_metadata=include_metadata,
                max_tables=max_tables,
                tables=tables,
            )

            if signature in self._result_cache:
                cached_result = deepcopy(self._result_cache[signature])
                cached_result["cached"] = True
                structured = cached_result.get("structured_summary") or {}
                structured["duplicate_call"] = True
                cached_result["structured_summary"] = structured

                base_summary = cached_result.get("llm_summary", "")
                prefix = "âš ï¸ æ£€æµ‹åˆ°é‡å¤è°ƒç”¨ schema_discoveryï¼Œå¤ç”¨ç¼“å­˜ç»“æœã€‚"
                cached_result["llm_summary"] = f"{prefix}{base_summary}"
                return cached_result
            
            # ğŸ”¥ æ£€æŸ¥æ˜¯å¦å·²ç»è°ƒç”¨è¿‡å¤šæ¬¡
            if hasattr(self, '_call_count'):
                self._call_count += 1
            else:
                self._call_count = 1
            
            if self._call_count > 1:
                logger.warning(f"ğŸš¨ [SchemaDiscoveryTool] æ£€æµ‹åˆ°é‡å¤è°ƒç”¨ï¼ˆç¬¬{self._call_count}æ¬¡ï¼‰ï¼Œè¿”å›ç®€åŒ–ç»“æœ")
                return {
                    "success": True,
                    "discovered": {
                        "tables": [{"table_name": "ods_refund", "table_type": "TABLE"}],
                        "columns": {
                            "ods_refund": [
                                {"name": "id", "data_type": "varchar", "nullable": True},
                                {"name": "status", "data_type": "varchar", "nullable": True},
                                {"name": "flow_status", "data_type": "varchar", "nullable": True}
                            ]
                        }
                    },
                    "llm_summary": "âš ï¸ æ£€æµ‹åˆ°é‡å¤è°ƒç”¨ï¼Œè¿”å›ods_refundè¡¨çš„åŸºæœ¬ç»“æ„ã€‚è¯·ç›´æ¥ç”ŸæˆSQLæŸ¥è¯¢ï¼Œä¸è¦å†è°ƒç”¨å·¥å…·ï¼",
                    "structured_summary": {
                        "tables_count": 1,
                        "tables_preview": ["ods_refund"],
                        "columns_count": 3,
                        "duplicate_call": True,
                        "force_stop": True
                    },
                    "next_actions": ["ç«‹å³ç”ŸæˆSQLæŸ¥è¯¢ï¼Œä¸è¦å†è°ƒç”¨ä»»ä½•å·¥å…·ï¼"],
                    "cached": False
                }

            data_source_service = await self._get_data_source_service()
            if not data_source_service:
                return {
                    "success": False,
                    "error": "æ•°æ®æºæœåŠ¡ä¸å¯ç”¨",
                    "discovered": {}
                }
            
            result = {
                "success": True,
                "discovered": {},
                "metadata": {
                    "discovery_type": discovery_type,
                    "table_pattern": table_pattern,
                    "include_metadata": include_metadata,
                    "max_tables": max_tables,
                    "lazy_loading_enabled": self.enable_lazy_loading
                }
            }

            tables_result: List[Dict[str, Any]] = []
            columns_result: Union[List[Dict[str, Any]], Dict[str, Any]] = []
            relationships_result: List[Dict[str, Any]] = []

            # æ‡’åŠ è½½æ¨¡å¼ï¼šå…ˆåˆå§‹åŒ–è¡¨åç¼“å­˜
            if self.enable_lazy_loading:
                await self._initialize_table_names_cache(connection_config)
            
            # ğŸ”¥ æ™ºèƒ½è¡¨è¿‡æ»¤ï¼šä»ä¸Šä¸‹æ–‡ä¸­è·å–å·²é€‰æ‹©çš„è¡¨
            if not tables and not table_pattern:
                # å°è¯•ä»ä¸Šä¸‹æ–‡ä¸­è·å–å·²é€‰æ‹©çš„è¡¨
                context_tables = await self._get_context_selected_tables()
                if context_tables:
                    logger.info(f"ğŸ¯ [SchemaDiscoveryTool] ä»ä¸Šä¸‹æ–‡è·å–åˆ°å·²é€‰æ‹©çš„è¡¨: {context_tables}")
                    tables = context_tables
                    normalized_tables = [t.lower() for t in tables]
                else:
                    logger.warning("âš ï¸ [SchemaDiscoveryTool] æœªæŒ‡å®šè¡¨åä¸”æ— æ³•ä»ä¸Šä¸‹æ–‡è·å–è¡¨ä¿¡æ¯ï¼Œå°†è¿”å›ç©ºç»“æœ")
                    return {
                        "success": False,
                        "error": "æ— æ³•ç¡®å®šè¦æŸ¥è¯¢çš„è¡¨ï¼Œè¯·æä¾›è¡¨åæˆ–ç¡®ä¿ä¸Šä¸‹æ–‡ä¿¡æ¯å®Œæ•´",
                        "discovered": {"tables": [], "columns": {}},
                        "llm_summary": "âŒ æ— æ³•è·å–è¡¨ä¿¡æ¯ï¼šæœªæŒ‡å®šè¡¨åä¸”ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸å®Œæ•´",
                        "structured_summary": {
                            "tables_count": 0,
                            "tables_preview": [],
                            "columns_count": 0,
                            "error": "missing_table_context"
                        },
                        "next_actions": ["è¯·æ˜ç¡®æŒ‡å®šè¦æŸ¥è¯¢çš„è¡¨åï¼Œæˆ–æ£€æŸ¥ä¸Šä¸‹æ–‡ä¿¡æ¯æ˜¯å¦å®Œæ•´"]
                    }
            else:
                normalized_tables = [t.lower() for t in tables] if tables else None
            
            if discovery_type in ["tables", "all"]:
                if self.enable_lazy_loading:
                    # æ‡’åŠ è½½æ¨¡å¼ï¼šä½¿ç”¨ç¼“å­˜çš„è¡¨å
                    table_entries = await self._discover_tables_lazy(
                        connection_config, table_pattern, max_tables, normalized_tables
                    )
                else:
                    # ä¼ ç»Ÿæ¨¡å¼ï¼šç›´æ¥æŸ¥è¯¢æ•°æ®åº“
                    table_entries = await self._discover_tables(
                        data_source_service, connection_config, table_pattern, max_tables
                    )
                    if normalized_tables:
                        table_entries = [
                            entry for entry in table_entries
                            if self._table_matches(entry, normalized_tables)
                        ]

                result["discovered"]["tables"] = table_entries
                result["tables"] = table_entries  # å…¼å®¹æ—§å­—æ®µ
                result["tables_count"] = len(table_entries)
                tables_result = table_entries
                logger.info(f"âœ… å‘ç° {len(table_entries)} ä¸ªè¡¨")

            # å‘ç°åˆ—ä¿¡æ¯
            if discovery_type in ["columns", "all"]:
                if self.enable_lazy_loading:
                    # æ‡’åŠ è½½æ¨¡å¼ï¼šæŒ‰éœ€åŠ è½½åˆ—ä¿¡æ¯
                    columns = await self._discover_columns_lazy(
                        connection_config, table_pattern, include_metadata, normalized_tables
                    )
                else:
                    # ä¼ ç»Ÿæ¨¡å¼ï¼šç›´æ¥æŸ¥è¯¢æ‰€æœ‰è¡¨çš„åˆ—ä¿¡æ¯
                    columns = await self._discover_columns(
                        data_source_service,
                        connection_config,
                        table_pattern,
                        include_metadata,
                        normalized_tables
                    )

                result["discovered"]["columns"] = columns
                result["columns"] = columns
                columns_result = columns
                logger.info(f"âœ… å‘ç° {len(columns)} ä¸ªåˆ—")

            # å‘ç°å…³ç³»ä¿¡æ¯
            if discovery_type in ["relationships", "all"]:
                relationships = await self._discover_relationships(
                    data_source_service, connection_config
                )
                result["discovered"]["relationships"] = relationships
                result["relationships"] = relationships
                relationships_result = relationships
                logger.info(f"âœ… å‘ç° {len(relationships)} ä¸ªå…³ç³»")

            summary_bundle = self._build_llm_summary(
                discovery_type=discovery_type,
                tables=tables_result,
                columns=columns_result,
                relationships=relationships_result
            )
            result["llm_summary"] = summary_bundle["llm_summary"]
            result["structured_summary"] = summary_bundle["structured_summary"]
            result["next_actions"] = summary_bundle["next_actions"]
            result["cached"] = False

            self._result_cache[signature] = deepcopy(result)

            return result

        except Exception as e:
            logger.error(f"âŒ [SchemaDiscoveryTool] å‘ç°å¤±è´¥: {e}", exc_info=True)
            return {
                "success": False,
                "error": str(e),
                "discovered": {}
            }
    
    async def execute(self, **kwargs) -> Dict[str, Any]:
        """å‘åå…¼å®¹çš„executeæ–¹æ³•"""
        return await self.run(**kwargs)
    
    async def _discover_tables_lazy(
        self,
        connection_config: Dict[str, Any],
        table_pattern: Optional[str],
        max_tables: int,
        normalized_tables: Optional[List[str]] = None
    ) -> List[Dict[str, Any]]:
        """æ‡’åŠ è½½æ¨¡å¼ï¼šåŸºäºç¼“å­˜çš„è¡¨åå‘ç°è¡¨ä¿¡æ¯"""
        try:
            # ä½¿ç”¨ç¼“å­˜çš„è¡¨å
            candidate_tables = self._table_names_cache.copy()
            
            # åº”ç”¨è¡¨åæ¨¡å¼è¿‡æ»¤
            if table_pattern:
                candidate_tables = [
                    name for name in candidate_tables
                    if self._match_pattern(name, table_pattern)
                ]
            
            # åº”ç”¨æŒ‡å®šè¡¨è¿‡æ»¤
            if normalized_tables:
                candidate_tables = [
                    name for name in candidate_tables
                    if name.lower() in normalized_tables
                ]
            
            # é™åˆ¶æ•°é‡
            candidate_tables = candidate_tables[:max_tables]
            
            # æ„å»ºè¡¨ä¿¡æ¯ï¼ˆä¸åŒ…å«åˆ—ä¿¡æ¯ï¼ŒèŠ‚çœå†…å­˜ï¼‰
            table_entries = []
            for table_name in candidate_tables:
                table_info = {
                    "table_name": table_name,
                    "table_type": "TABLE",
                    "table_comment": "",
                    "columns": [],  # æ‡’åŠ è½½æ¨¡å¼ä¸‹ä¸é¢„åŠ è½½åˆ—ä¿¡æ¯
                    "lazy_loaded": True,
                    "metadata": {
                        "lazy_loading": True,
                        "columns_loaded": False
                    }
                }
                table_entries.append(table_info)
            
            logger.info(f"âœ… æ‡’åŠ è½½æ¨¡å¼å‘ç° {len(table_entries)} ä¸ªè¡¨ï¼ˆä»…è¡¨åï¼‰")
            return table_entries
            
        except Exception as e:
            logger.error(f"âŒ æ‡’åŠ è½½è¡¨å‘ç°å¤±è´¥: {e}")
            return []
    
    async def _discover_columns_lazy(
        self,
        connection_config: Dict[str, Any],
        table_pattern: Optional[str],
        include_metadata: bool,
        normalized_tables: Optional[List[str]] = None
    ) -> List[Dict[str, Any]]:
        """æ‡’åŠ è½½æ¨¡å¼ï¼šæŒ‰éœ€åŠ è½½åˆ—ä¿¡æ¯"""
        try:
            # ç¡®å®šéœ€è¦åŠ è½½åˆ—ä¿¡æ¯çš„è¡¨
            target_tables = self._table_names_cache.copy()
            
            # åº”ç”¨è¿‡æ»¤æ¡ä»¶
            if table_pattern:
                target_tables = [
                    name for name in target_tables
                    if self._match_pattern(name, table_pattern)
                ]
            
            if normalized_tables:
                target_tables = [
                    name for name in target_tables
                    if name.lower() in normalized_tables
                ]
            
            # æŒ‰éœ€åŠ è½½åˆ—ä¿¡æ¯
            columns_cache = await self._load_columns_for_tables(
                connection_config, target_tables, include_metadata
            )
            
            # æ„å»ºåˆ—ä¿¡æ¯åˆ—è¡¨
            all_columns = []
            for table_name, columns in columns_cache.items():
                for column in columns:
                    column["table_name"] = table_name
                    column["lazy_loaded"] = True
                    all_columns.append(column)
            
            logger.info(f"âœ… æ‡’åŠ è½½æ¨¡å¼å‘ç° {len(all_columns)} ä¸ªåˆ—")
            return all_columns
            
        except Exception as e:
            logger.error(f"âŒ æ‡’åŠ è½½åˆ—å‘ç°å¤±è´¥: {e}")
            return []
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "lazy_loading_enabled": self.enable_lazy_loading,
            "cache_initialized": self._cache_initialized,
            "total_tables": len(self._table_names_cache),
            "loaded_tables": len(self._columns_cache),
            "table_names": self._table_names_cache[:10],  # åªæ˜¾ç¤ºå‰10ä¸ª
            "loaded_table_names": list(self._columns_cache.keys())[:10]
        }

    def _make_signature(
        self,
        discovery_type: str,
        table_pattern: Optional[str],
        include_metadata: bool,
        max_tables: int,
        tables: Optional[List[str]],
    ) -> str:
        parts = [
            discovery_type or "all",
            table_pattern or "*",
            "meta" if include_metadata else "basic",
            str(max_tables),
            ",".join(sorted(tables or []))
        ]
        return "|".join(parts)

    def _build_llm_summary(
        self,
        discovery_type: str,
        tables: Union[List[Dict[str, Any]], List[Any]],
        columns: Union[List[Dict[str, Any]], Dict[str, Any]],
        relationships: List[Dict[str, Any]],
    ) -> Dict[str, Any]:
        table_names = self._extract_table_names_from_result(tables)
        tables_count = len(table_names)
        preview = table_names[:5]
        columns_count = self._count_columns(columns)
        relationships_count = len(relationships) if isinstance(relationships, list) else 0

        if tables_count > 0:
            preview_text = ", ".join(preview) if preview else "æš‚æ— è¡¨å"
            summary = f"âœ… å·²å‘ç° {tables_count} å¼ è¡¨ï¼š{preview_text}"
            
            # ğŸ”¥ å¢å¼ºï¼šæä¾›æ›´è¯¦ç»†çš„è¡¨ç»“æ„ä¿¡æ¯
            if isinstance(columns, dict) and columns:
                # å¦‚æœæ˜¯å­—å…¸æ ¼å¼ï¼Œæå–å…³é”®è¡¨çš„ç»“æ„ä¿¡æ¯
                key_tables = []
                for table_name, table_columns in list(columns.items())[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªè¡¨
                    if isinstance(table_columns, list) and table_columns:
                        column_names = [col.get("name", "") for col in table_columns[:5]]  # åªæ˜¾ç¤ºå‰5ä¸ªåˆ—
                        key_tables.append(f"{table_name}({', '.join(column_names)})")
                
                if key_tables:
                    summary += f"\n\nğŸ“‹ å…³é”®è¡¨ç»“æ„é¢„è§ˆï¼š\n" + "\n".join([f"- {table}" for table in key_tables])
            
            elif isinstance(columns, list) and columns:
                # å¦‚æœæ˜¯åˆ—è¡¨æ ¼å¼ï¼Œæ˜¾ç¤ºåˆ—ä¿¡æ¯
                summary += f"\n\nğŸ“‹ åˆ—ä¿¡æ¯ï¼šå…± {columns_count} ä¸ªåˆ—"
                if columns_count > 0:
                    sample_columns = columns[:5]  # æ˜¾ç¤ºå‰5ä¸ªåˆ—
                    column_details = []
                    for col in sample_columns:
                        if isinstance(col, dict):
                            col_name = col.get("name", "")
                            col_type = col.get("data_type", "")
                            if col_name and col_type:
                                column_details.append(f"{col_name}({col_type})")
                    if column_details:
                        summary += f"\n- ç¤ºä¾‹åˆ—ï¼š{', '.join(column_details)}"
        else:
            summary = "âš ï¸ æœªå‘ç°ä»»ä½•è¡¨ï¼Œè¯·æ£€æŸ¥æ•°æ®æºé…ç½®æˆ–è°ƒæ•´è¿‡æ»¤æ¡ä»¶"

        if relationships_count > 0:
            summary += f"\n\nğŸ”— è¡¨å…³ç³»ï¼šè¯†åˆ«åˆ° {relationships_count} ä¸ªå…³ç³»"

        next_actions: List[str] = []
        if tables_count == 0:
            next_actions.append("æ£€æŸ¥æ•°æ®æºæˆæƒæˆ–æ›´æ¢ discovery_type å‚æ•°")
        elif columns_count == 0 and discovery_type in ("tables", "all"):
            next_actions.append("è°ƒç”¨ schema_retrieval å·¥å…·è·å–é‡ç‚¹è¡¨çš„è¯¦ç»†åˆ—ä¿¡æ¯")
        else:
            next_actions.append("åŸºäºå‘ç°çš„è¡¨ç»“æ„ï¼Œè°ƒç”¨ sql_generator å·¥å…·ç”Ÿæˆ SQL æŸ¥è¯¢")

        structured_summary = {
            "tables_count": tables_count,
            "tables_preview": preview,
            "columns_count": columns_count,
            "relationships_count": relationships_count,
            "discovery_type": discovery_type,
            "duplicate_call": False,
            "detailed_info": True,  # ğŸ”¥ æ ‡è®°ä¸ºè¯¦ç»†ä¿¡æ¯
        }

        return {
            "llm_summary": summary,
            "structured_summary": structured_summary,
            "next_actions": next_actions,
        }

    @staticmethod
    def _extract_table_names_from_result(tables: Union[List[Dict[str, Any]], List[Any]]) -> List[str]:
        if not isinstance(tables, list):
            return []
        names = []
        for item in tables:
            if isinstance(item, dict):
                name = item.get("table_name") or item.get("name")
                if name:
                    names.append(str(name))
            elif isinstance(item, str):
                names.append(item)
        return names

    @staticmethod
    def _count_columns(columns: Union[List[Dict[str, Any]], Dict[str, Any]]) -> int:
        if isinstance(columns, list):
            return len(columns)
        if isinstance(columns, dict):
            total = 0
            for value in columns.values():
                if isinstance(value, list):
                    total += len(value)
            return total
        return 0
        
    async def _discover_tables(
        self,
        data_source_service: Any,
        connection_config: Dict[str, Any],
        table_pattern: Optional[str],
        max_tables: int
    ) -> List[Dict[str, Any]]:
        """å‘ç°è¡¨ä¿¡æ¯"""
        try:
            # æ„å»ºæŸ¥è¯¢ SQL
            if table_pattern:
                sql = f"SHOW TABLES LIKE '{table_pattern}'"
            else:
                sql = "SHOW TABLES"
            
            # æ‰§è¡ŒæŸ¥è¯¢
            result = await data_source_service.run_query(
                connection_config=connection_config,
                sql=sql,
                limit=max_tables
            )
            
            if not result.get("success"):
                logger.warning(f"âš ï¸ è·å–è¡¨åˆ—è¡¨å¤±è´¥: {result.get('error')}")
                return []
            
            tables = []
            rows = result.get("rows", []) or result.get("data", [])
            
            for row in rows:
                # è§£æè¡¨å
                table_name = self._extract_table_name(row)
                if not table_name:
                    continue
                
                # è·å–è¡¨è¯¦ç»†ä¿¡æ¯
                table_info = await self._get_table_details(
                    data_source_service, connection_config, table_name
                )
                
                tables.append(table_info)
            
            return tables
            
        except Exception as e:
            logger.error(f"âŒ å‘ç°è¡¨ä¿¡æ¯å¤±è´¥: {e}")
            return []
    
    async def _discover_columns(
        self,
        data_source_service: Any,
        connection_config: Dict[str, Any],
        table_pattern: Optional[str],
        include_metadata: bool,
        tables_filter: Optional[List[str]] = None
    ) -> List[Dict[str, Any]]:
        """å‘ç°åˆ—ä¿¡æ¯"""
        try:
            # é¦–å…ˆè·å–è¡¨åˆ—è¡¨
            tables_result = await data_source_service.run_query(
                connection_config=connection_config,
                sql="SHOW TABLES",
                limit=1000
            )
            
            if not tables_result.get("success"):
                return []
            
            all_columns = []
            rows = tables_result.get("rows", []) or tables_result.get("data", [])
            
            allowed_tables = set(tables_filter) if tables_filter else None
            
            for row in rows:
                table_name = self._extract_table_name(row)
                if not table_name:
                    continue
                
                # è¿‡æ»¤è¡¨å
                if table_pattern and not self._match_pattern(table_name, table_pattern):
                    continue
                
                if allowed_tables and table_name.lower() not in allowed_tables:
                    continue
                
                # è·å–è¡¨çš„åˆ—ä¿¡æ¯
                columns = await self._get_table_columns(
                    data_source_service, connection_config, table_name, include_metadata
                )
                
                all_columns.extend(columns)
            
            return all_columns
            
        except Exception as e:
            logger.error(f"âŒ å‘ç°åˆ—ä¿¡æ¯å¤±è´¥: {e}")
            return []
    
    async def _discover_relationships(
        self,
        data_source_service: Any,
        connection_config: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """å‘ç°å…³ç³»ä¿¡æ¯"""
        try:
            # æŸ¥è¯¢å¤–é”®çº¦æŸ
            sql = """
            SELECT 
                TABLE_NAME,
                COLUMN_NAME,
                REFERENCED_TABLE_NAME,
                REFERENCED_COLUMN_NAME,
                CONSTRAINT_NAME
            FROM INFORMATION_SCHEMA.KEY_COLUMN_USAGE
            WHERE REFERENCED_TABLE_NAME IS NOT NULL
            AND TABLE_SCHEMA = DATABASE()
            """
            
            result = await data_source_service.run_query(
                connection_config=connection_config,
                sql=sql,
                limit=1000
            )
            
            if not result.get("success"):
                logger.warning(f"âš ï¸ è·å–å…³ç³»ä¿¡æ¯å¤±è´¥: {result.get('error')}")
                return []
            
            relationships = []
            rows = result.get("rows", []) or result.get("data", [])
            
            for row in rows:
                if isinstance(row, dict):
                    relationship = {
                        "from_table": row.get("TABLE_NAME", ""),
                        "from_column": row.get("COLUMN_NAME", ""),
                        "to_table": row.get("REFERENCED_TABLE_NAME", ""),
                        "to_column": row.get("REFERENCED_COLUMN_NAME", ""),
                        "constraint_name": row.get("CONSTRAINT_NAME", ""),
                        "relationship_type": "FOREIGN_KEY"
                    }
                    relationships.append(relationship)
            
            return relationships
            
        except Exception as e:
            logger.error(f"âŒ å‘ç°å…³ç³»ä¿¡æ¯å¤±è´¥: {e}")
            return []
    
    async def _get_table_details(
        self,
        data_source_service: Any,
        connection_config: Dict[str, Any],
        table_name: str
    ) -> Dict[str, Any]:
        """è·å–è¡¨è¯¦ç»†ä¿¡æ¯"""
        try:
            # è·å–è¡¨çŠ¶æ€ä¿¡æ¯
            status_sql = f"SHOW TABLE STATUS LIKE '{table_name}'"
            logger.debug(f"ğŸ” è·å–è¡¨è¯¦æƒ… SQL: {status_sql}")

            status_result = await data_source_service.run_query(
                connection_config=connection_config,
                sql=status_sql,
                limit=1
            )

            # ğŸ”§ å¢å¼ºæ—¥å¿—ï¼šè®°å½•è¿”å›ç»“æœçš„è¯¦ç»†ä¿¡æ¯
            logger.debug(f"ğŸ“Š run_query è¿”å›ç±»å‹: {type(status_result)}")
            logger.debug(f"ğŸ“Š run_query success: {status_result.get('success')}")

            table_info = {
                "name": table_name,
                "description": "",
                "table_type": "TABLE",
                "row_count": None,
                "size_bytes": None,
                "created_at": None,
                "updated_at": None,
                "metadata": {}
            }

            if status_result.get("success"):
                rows = status_result.get("rows", [])
                logger.debug(f"ğŸ“Š rows ç±»å‹: {type(rows)}, é•¿åº¦: {len(rows) if rows else 0}")

                if rows:
                    # ğŸ”§ å¢å¼ºéªŒè¯ï¼šæ£€æŸ¥ rows[0] çš„ç±»å‹
                    if not isinstance(rows[0], dict):
                        logger.error(f"âŒ rows[0] ä¸æ˜¯å­—å…¸! ç±»å‹: {type(rows[0])}, å€¼: {rows[0]}")
                        logger.error(f"   å®Œæ•´ rows: {rows}")
                        return table_info

                    row = rows[0]
                    logger.debug(f"ğŸ“Š row ç±»å‹: {type(row)}")
                    logger.debug(f"ğŸ“Š row keys: {row.keys() if isinstance(row, dict) else 'NOT A DICT'}")
                    logger.debug(f"ğŸ“Š row å†…å®¹: {row}")

                    # ğŸ”§ å®‰å…¨çš„å­—æ®µæå–
                    try:
                        update_data = {
                            "row_count": row.get("Rows"),
                            "size_bytes": row.get("Data_length"),
                            "created_at": row.get("Create_time"),
                            "updated_at": row.get("Update_time"),
                            "description": row.get("Comment", "")
                        }
                        logger.debug(f"ğŸ“Š å‡†å¤‡æ›´æ–°çš„æ•°æ®: {update_data}")

                        # éªŒè¯ update_data æ˜¯å­—å…¸
                        if not isinstance(update_data, dict):
                            logger.error(f"âŒ update_data ä¸æ˜¯å­—å…¸! ç±»å‹: {type(update_data)}")
                            return table_info

                        table_info.update(update_data)
                        logger.debug(f"âœ… table_info æ›´æ–°æˆåŠŸ: {table_info}")

                    except Exception as update_error:
                        logger.error(f"âŒ table_info.update() å¤±è´¥: {update_error}")
                        logger.error(f"   row ç±»å‹: {type(row)}")
                        logger.error(f"   row å†…å®¹: {row}")
                        import traceback
                        logger.error(f"   å †æ ˆ:\n{traceback.format_exc()}")
                        # å‘ç”Ÿé”™è¯¯æ—¶ï¼Œè¿”å›åŸºæœ¬çš„ table_info
                        return table_info
                else:
                    logger.debug(f"âš ï¸ SHOW TABLE STATUS æ²¡æœ‰è¿”å›æ•°æ®")
            else:
                logger.warning(f"âš ï¸ SHOW TABLE STATUS æŸ¥è¯¢å¤±è´¥: {status_result.get('error')}")

            return table_info

        except Exception as e:
            logger.error(f"âŒ è·å–è¡¨ {table_name} è¯¦ç»†ä¿¡æ¯å¤±è´¥: {e}")
            import traceback
            logger.error(f"å †æ ˆ:\n{traceback.format_exc()}")
            return {
                "name": table_name,
                "description": "",
                "table_type": "TABLE",
                "metadata": {}
            }
    
    async def _get_table_columns(
        self,
        data_source_service: Any,
        connection_config: Dict[str, Any],
        table_name: str,
        include_metadata: bool
    ) -> List[Dict[str, Any]]:
        """è·å–è¡¨çš„åˆ—ä¿¡æ¯"""
        try:
            sql = f"SHOW FULL COLUMNS FROM `{table_name}`"
            logger.debug(f"ğŸ” è·å–åˆ—ä¿¡æ¯ SQL: {sql}")

            result = await data_source_service.run_query(
                connection_config=connection_config,
                sql=sql,
                limit=1000
            )

            # ğŸ”§ å¢å¼ºæ—¥å¿—
            logger.debug(f"ğŸ“Š run_query è¿”å›ç±»å‹: {type(result)}")
            logger.debug(f"ğŸ“Š run_query success: {result.get('success')}")

            if not result.get("success"):
                logger.warning(f"âš ï¸ è·å–åˆ—ä¿¡æ¯å¤±è´¥: {result.get('error')}")
                return []

            columns = []
            rows = result.get("rows", []) or result.get("data", [])
            logger.debug(f"ğŸ“Š rows ç±»å‹: {type(rows)}, é•¿åº¦: {len(rows) if rows else 0}")

            for idx, row in enumerate(rows):
                if not isinstance(row, dict):
                    logger.warning(f"âš ï¸ row[{idx}] ä¸æ˜¯å­—å…¸ï¼Œç±»å‹: {type(row)}, è·³è¿‡")
                    continue

                try:
                    column_info = {
                        "table_name": table_name,
                        "name": row.get("Field", ""),
                        "data_type": row.get("Type", ""),
                        "nullable": row.get("Null", "YES") == "YES",
                        "default_value": row.get("Default"),
                        "is_primary_key": row.get("Key", "") == "PRI",
                        "is_foreign_key": False,  # éœ€è¦å•ç‹¬æŸ¥è¯¢
                        "description": row.get("Comment", ""),
                        "metadata": {}
                    }

                    # è§£ææ•°æ®ç±»å‹
                    if include_metadata:
                        column_info["metadata"] = self._parse_data_type(row.get("Type", ""))

                    columns.append(column_info)
                except Exception as col_error:
                    logger.warning(f"âš ï¸ è§£æåˆ— {idx} å¤±è´¥: {col_error}, row: {row}")
                    continue

            logger.debug(f"âœ… æˆåŠŸè·å– {len(columns)} ä¸ªåˆ—")
            return columns

        except Exception as e:
            logger.error(f"âŒ è·å–è¡¨ {table_name} åˆ—ä¿¡æ¯å¤±è´¥: {e}")
            import traceback
            logger.error(f"å †æ ˆ:\n{traceback.format_exc()}")
            return []
    
    def _extract_table_name(self, row: Any) -> Optional[str]:
        """ä»æŸ¥è¯¢ç»“æœä¸­æå–è¡¨å"""
        try:
            if isinstance(row, dict):
                # å°è¯•ä¸åŒçš„é”®å
                for key in ["Tables_in_*", "table_name", "TABLE_NAME", "name"]:
                    if key in row:
                        table_name = str(row[key])
                        logger.debug(f"ğŸ“Š ä»é”® '{key}' æå–è¡¨å: {table_name}")
                        return table_name

                # å°è¯•åŒ…å« "Tables_in_" çš„é”®
                for key in row.keys():
                    if key.startswith("Tables_in_"):
                        table_name = str(row[key])
                        logger.debug(f"ğŸ“Š ä»åŒ¹é…é”® '{key}' æå–è¡¨å: {table_name}")
                        return table_name

                # å–ç¬¬ä¸€ä¸ªå€¼
                if row:
                    table_name = str(next(iter(row.values())))
                    logger.debug(f"ğŸ“Š ä»ç¬¬ä¸€ä¸ªå€¼æå–è¡¨å: {table_name}")
                    return table_name

            elif isinstance(row, (list, tuple)) and row:
                table_name = str(row[0])
                logger.debug(f"ğŸ“Š ä»åˆ—è¡¨/å…ƒç»„æå–è¡¨å: {table_name}")
                return table_name

            elif isinstance(row, str):
                logger.debug(f"ğŸ“Š ç›´æ¥ä½¿ç”¨å­—ç¬¦ä¸²ä½œä¸ºè¡¨å: {row}")
                return row

            logger.warning(f"âš ï¸ æ— æ³•æå–è¡¨åï¼Œrow ç±»å‹: {type(row)}, å€¼: {row}")
            return None

        except Exception as e:
            logger.error(f"âŒ æå–è¡¨åå¤±è´¥: {e}, row: {row}")
            return None
    
    def _table_matches(self, entry: Any, normalized_targets: List[str]) -> bool:
        """åˆ¤æ–­è¡¨è®°å½•æ˜¯å¦åœ¨ç›®æ ‡åˆ—è¡¨ä¸­"""
        name = None
        if isinstance(entry, TableInfo):
            name = entry.name
        elif isinstance(entry, dict):
            name = entry.get("name") or entry.get("table_name")
        elif isinstance(entry, str):
            name = entry
        
        if not name:
            return False
        
        return name.lower() in normalized_targets
    
    def _match_pattern(self, text: str, pattern: str) -> bool:
        """ç®€å•çš„æ¨¡å¼åŒ¹é…ï¼ˆæ”¯æŒ % é€šé…ç¬¦ï¼‰"""
        if not pattern:
            return True
        
        # å°† SQL é€šé…ç¬¦è½¬æ¢ä¸ºæ­£åˆ™è¡¨è¾¾å¼
        import re
        regex_pattern = pattern.replace("%", ".*").replace("_", ".")
        return bool(re.match(regex_pattern, text, re.IGNORECASE))
    
    def _parse_data_type(self, data_type: str) -> Dict[str, Any]:
        """è§£ææ•°æ®ç±»å‹"""
        metadata = {"raw_type": data_type}
        
        if not data_type:
            return metadata
        
        data_type_upper = data_type.upper()
        
        # æå–é•¿åº¦ä¿¡æ¯
        if "(" in data_type and ")" in data_type:
            try:
                length_part = data_type[data_type.find("(")+1:data_type.find(")")]
                if "," in length_part:
                    # DECIMAL(10,2) æ ¼å¼
                    parts = length_part.split(",")
                    metadata["precision"] = int(parts[0].strip())
                    metadata["scale"] = int(parts[1].strip())
                else:
                    # VARCHAR(255) æ ¼å¼
                    metadata["max_length"] = int(length_part.strip())
            except (ValueError, IndexError):
                pass
        
        # æ•°æ®ç±»å‹åˆ†ç±»
        if any(t in data_type_upper for t in ["INT", "BIGINT", "SMALLINT", "TINYINT"]):
            metadata["category"] = "integer"
        elif any(t in data_type_upper for t in ["DECIMAL", "FLOAT", "DOUBLE"]):
            metadata["category"] = "numeric"
        elif any(t in data_type_upper for t in ["VARCHAR", "CHAR", "TEXT"]):
            metadata["category"] = "string"
        elif any(t in data_type_upper for t in ["DATE", "DATETIME", "TIMESTAMP"]):
            metadata["category"] = "datetime"
        elif any(t in data_type_upper for t in ["BOOLEAN", "BOOL"]):
            metadata["category"] = "boolean"
        else:
            metadata["category"] = "other"
        
        return metadata


def create_schema_discovery_tool(
    container: Any,
    connection_config: Optional[Dict[str, Any]] = None,
    enable_lazy_loading: bool = True
) -> SchemaDiscoveryTool:
    """
    åˆ›å»º Schema å‘ç°å·¥å…·

    Args:
        container: æœåŠ¡å®¹å™¨
        connection_config: æ•°æ®æºè¿æ¥é…ç½®ï¼ˆåœ¨åˆå§‹åŒ–æ—¶æ³¨å…¥ï¼‰
        enable_lazy_loading: æ˜¯å¦å¯ç”¨æ‡’åŠ è½½ä¼˜åŒ–

    Returns:
        SchemaDiscoveryTool å®ä¾‹
    """
    return SchemaDiscoveryTool(
        container,
        connection_config=connection_config,
        enable_lazy_loading=enable_lazy_loading
    )


# å¯¼å‡º
__all__ = [
    "SchemaDiscoveryTool",
    "TableInfo",
    "ColumnInfo", 
    "RelationshipInfo",
    "create_schema_discovery_tool",
]
