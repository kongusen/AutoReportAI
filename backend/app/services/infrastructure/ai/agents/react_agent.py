"""
Reactæ™ºèƒ½ä»£ç†

åŸºäºReact Agentæ¶æ„çš„æ™ºèƒ½ä»£ç†å®ç°
éœ€è¦user_idå‚æ•°è¿›è¡Œç”¨æˆ·ä¸ªæ€§åŒ–é…ç½®
"""

import asyncio
import logging
import time
import hashlib
import json
from typing import Dict, List, Any, Optional
from datetime import datetime

logger = logging.getLogger(__name__)


class ReactAgent:
    """
    Reactæ™ºèƒ½ä»£ç†
    
    æ ¸å¿ƒç‰¹æ€§:
    1. åŸºäºç”¨æˆ·é…ç½®çš„ä¸ªæ€§åŒ–AIæœåŠ¡
    2. çº¯æ•°æ®åº“é©±åŠ¨ï¼Œæ— é…ç½®æ–‡ä»¶ä¾èµ–
    3. æ™ºèƒ½æ¨¡å‹é€‰æ‹©å’Œèµ„æºä¼˜åŒ–
    4. å­¦ä¹ å¼åå¥½è®°å½•å’Œä¼˜åŒ–
    """
    
    def __init__(
        self,
        user_id: str,  # ğŸ”‘ å¿…éœ€å‚æ•°ï¼šç”¨æˆ·ID
        tools: Optional[List] = None,
        memory_token_limit: int = 4000,
        max_iterations: int = 15,
        verbose: bool = True,
        system_prompt: Optional[str] = None
    ):
        if not user_id:
            raise ValueError("user_idæ˜¯å¿…éœ€å‚æ•°ï¼Œçº¯æ•°æ®åº“é©±åŠ¨Agentå¿…é¡»æŒ‡å®šç”¨æˆ·ID")
        
        self.user_id = user_id
        self.tools = tools or []
        self.memory_token_limit = memory_token_limit
        self.max_iterations = max_iterations
        self.verbose = verbose
        self.system_prompt = system_prompt
        
        self.agent: Optional[Any] = None
        self.initialized = False
        self.llm_manager = None
        self.selected_model = None
        self.current_task_type = "reasoning"  # é»˜è®¤ä»»åŠ¡ç±»å‹
        
        # æ™ºèƒ½ç¼“å­˜é…ç½®
        self.enable_cache = True
        self.cache_ttl = 3600  # 1å°æ—¶ç¼“å­˜æ—¶é—´
        self._cache = {}  # ç®€å•å†…å­˜ç¼“å­˜
        self._cache_stats = {
            "hits": 0,
            "misses": 0,
            "size": 0
        }
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.total_conversations = 0
        self.successful_conversations = 0
        self.total_tool_calls = 0
        self.start_time = datetime.utcnow()
        
        logger.info(f"ä¸ºç”¨æˆ· {user_id} åˆå§‹åŒ–Reactæ™ºèƒ½ä»£ç†")
    
    async def initialize(self):
        """åˆå§‹åŒ–Reactä»£ç† - çº¯æ•°æ®åº“é©±åŠ¨"""
        if self.initialized:
            return
        
        logger.info(f"åˆå§‹åŒ–ç”¨æˆ· {self.user_id} çš„Reactæ™ºèƒ½ä»£ç†...")
        
        try:
            # 1. è·å–æ•°æ®åº“é©±åŠ¨çš„LLMç®¡ç†å™¨
            from ..llm import get_llm_manager
            self.llm_manager = await get_llm_manager()
            
            # 2. ä¸ºReact Agenté€‰æ‹©æœ€ä½³æ¨¡å‹ - åŸºäºç”¨æˆ·é…ç½®
            try:
                best_model = await self.llm_manager.select_best_model_for_user(
                    user_id=self.user_id,
                    task_type="reasoning",  # Reactéœ€è¦æ¨ç†èƒ½åŠ›
                    complexity="medium",
                    constraints={
                        "max_cost": 0.02,  # åˆç†çš„æˆæœ¬æ§åˆ¶
                        "preferred_providers": ["anthropic", "openai"]  # æ¨ç†ä»»åŠ¡åå¥½
                    },
                    agent_id="react_agent"
                )
                
                logger.info(f"React Agentä¸ºç”¨æˆ· {self.user_id} é€‰æ‹©æ¨¡å‹: {best_model['provider']}:{best_model['model']} (ç½®ä¿¡åº¦: {best_model['confidence']:.1%})")
                
                # è®°å½•æ¨¡å‹é€‰æ‹©ä¿¡æ¯
                self.selected_model = best_model
                
            except Exception as e:
                logger.error(f"ä¸ºç”¨æˆ· {self.user_id} é€‰æ‹©æ¨¡å‹å¤±è´¥: {e}")
                raise ValueError(f"æ— æ³•ä¸ºç”¨æˆ·é€‰æ‹©åˆé€‚çš„æ¨¡å‹: {str(e)}")
            
            # 3. åˆ›å»ºå·¥å…·é›†åˆï¼ˆå¦‚æœæœªæä¾›ï¼‰
            if not self.tools:
                try:
                    # æ·»åŠ å›¾è¡¨ç”Ÿæˆå·¥å…·
                    from ..tools.chart_generator_tool import generate_chart
                    from llama_index.core.tools import FunctionTool
                    
                    # åˆ›å»ºå›¾è¡¨ç”Ÿæˆå·¥å…·
                    chart_tool = FunctionTool.from_defaults(
                        fn=generate_chart,
                        name="generate_chart",
                        description="ç”Ÿæˆä¸“ä¸šå›¾è¡¨ï¼Œæ”¯æŒæŸ±çŠ¶å›¾ã€æŠ˜çº¿å›¾ã€é¥¼å›¾ã€é¢ç§¯å›¾ã€‚æ¥å—JSONé…ç½®å‚æ•°ï¼Œç”Ÿæˆå®é™…å›¾ç‰‡æ–‡ä»¶ã€‚"
                    )
                    
                    self.tools = [chart_tool]
                    
                    # å°è¯•æ·»åŠ å…¶ä»–å·¥å…·
                    try:
                        from ..tools import get_ai_tools_factory
                        tools_factory = await get_ai_tools_factory()
                        additional_tools = await tools_factory.create_all_tools()
                        self.tools.extend(additional_tools)
                    except Exception as e:
                        logger.warning(f"åˆ›å»ºé¢å¤–å·¥å…·å¤±è´¥: {e}")
                        
                except Exception as e:
                    logger.warning(f"åˆ›å»ºå·¥å…·å¤±è´¥: {e}")
                    self.tools = []
            
            # 4. åˆ›å»ºReactä»£ç†ï¼ˆä½¿ç”¨å®é™…çš„LlamaIndexï¼‰
            try:
                from llama_index.core.agent import ReActAgent
                from llama_index.core.memory import ChatMemoryBuffer
                
                # ä»selected_modelåˆ›å»ºLLMå®ä¾‹
                llm = await self._create_llm_from_model()
                
                self.agent = ReActAgent.from_tools(
                    tools=self.tools,
                    llm=llm,
                    memory=ChatMemoryBuffer.from_defaults(token_limit=self.memory_token_limit),
                    verbose=self.verbose,
                    max_iterations=self.max_iterations,
                    system_prompt=self.system_prompt or self._get_react_system_prompt()
                )
                
            except (ImportError, Exception) as e:
                logger.error(f"LlamaIndex React Agentåˆ›å»ºå¤±è´¥: {e}")
                raise ValueError(f"æ— æ³•åˆ›å»ºReactä»£ç†: {str(e)}")
            
            self.initialized = True
            logger.info(f"ç”¨æˆ· {self.user_id} çš„Reactä»£ç†åˆå§‹åŒ–å®Œæˆ - å·¥å…·: {len(self.tools)}, æ¨¡å‹: {self.selected_model['provider']}:{self.selected_model['model']}")
            
        except Exception as e:
            logger.error(f"Reactä»£ç†åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def _create_llm_from_model(self):
        """ä»é€‰æ‹©çš„æ¨¡å‹åˆ›å»ºLLMå®ä¾‹"""
        if not self.selected_model:
            raise ValueError("No model selected")
        
        try:
            # æ ¹æ®provideråˆ›å»ºç›¸åº”çš„LLM
            if self.selected_model['provider'] == 'anthropic':
                from llama_index.llms.anthropic import Anthropic
                return Anthropic(model=self.selected_model['model'])
            elif self.selected_model['provider'] == 'openai':
                from llama_index.llms.openai import OpenAI
                return OpenAI(model=self.selected_model['model'])
            else:
                # ä½¿ç”¨é€šç”¨æ¨¡å‹åˆ›å»ºæ–¹æ³•
                from app.services.infrastructure.ai.llm.model_executor import create_llm_from_model
                return await create_llm_from_model(self.selected_model)
                
        except Exception as e:
            logger.error(f"åˆ›å»ºLLMå®ä¾‹å¤±è´¥: {e}")
            raise
    
    def _get_react_system_prompt(self) -> str:
        """è·å–Reactç³»ç»Ÿæç¤ºè¯"""
        tools_info = self._get_tools_description()
        
        return f"""
ä½ æ˜¯ç”¨æˆ· {self.user_id} çš„ä¸“å±AIæ™ºèƒ½ä»£ç†ï¼ŒåŸºäºReActï¼ˆReasoning + Actingï¼‰æ¡†æ¶å·¥ä½œã€‚

ä½ çš„å·¥ä½œæµç¨‹ï¼š
1. **Thoughtï¼ˆæ€è€ƒï¼‰**: åˆ†æå½“å‰é—®é¢˜ï¼Œåˆ¶å®šè§£å†³ç­–ç•¥
2. **Actionï¼ˆè¡ŒåŠ¨ï¼‰**: é€‰æ‹©åˆé€‚çš„å·¥å…·æ‰§è¡Œå…·ä½“æ“ä½œ
3. **Observationï¼ˆè§‚å¯Ÿï¼‰**: åˆ†æå·¥å…·æ‰§è¡Œç»“æœ
4. **åå¤è¿­ä»£**: ç›´åˆ°é—®é¢˜å®Œå…¨è§£å†³

å¯ç”¨å·¥å…·ï¼š
{tools_info}

å·¥ä½œåŸåˆ™ï¼š
- å§‹ç»ˆå…ˆæ€è€ƒå†è¡ŒåŠ¨
- é€‰æ‹©æœ€åˆé€‚çš„å·¥å…·å®Œæˆä»»åŠ¡
- åŸºäºè§‚å¯Ÿç»“æœè°ƒæ•´ç­–ç•¥
- ç¡®ä¿æœ€ç»ˆç­”æ¡ˆå‡†ç¡®å®Œæ•´
- å¦‚æœé‡åˆ°é—®é¢˜ï¼Œå°è¯•ä¸åŒçš„æ–¹æ³•
- è®°ä½ä½ æœåŠ¡çš„ç”¨æˆ·IDæ˜¯ {self.user_id}

è¯·æŒ‰ç…§ Thought -> Action -> Observation çš„å¾ªç¯æ¥å¤„ç†ç”¨æˆ·è¯·æ±‚ã€‚
"""
    
    def _get_tools_description(self) -> str:
        """è·å–å·¥å…·æè¿°"""
        if not self.tools:
            return "æš‚æ— å¯ç”¨å·¥å…·"
        
        descriptions = []
        for i, tool in enumerate(self.tools, 1):
            if hasattr(tool, 'metadata'):
                name = getattr(tool.metadata, 'name', f'å·¥å…·{i}')
                desc = getattr(tool.metadata, 'description', 'æ— æè¿°')
                descriptions.append(f"- {name}: {desc}")
            else:
                descriptions.append(f"- å·¥å…·{i}: æ— æè¿°ä¿¡æ¯")
        
        return "\n".join(descriptions)
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total_requests = self._cache_stats["hits"] + self._cache_stats["misses"]
        hit_rate = self._cache_stats["hits"] / total_requests if total_requests > 0 else 0
        
        return {
            "cache_enabled": self.enable_cache,
            "cache_hits": self._cache_stats["hits"],
            "cache_misses": self._cache_stats["misses"],
            "hit_rate": round(hit_rate, 3),
            "cache_size": self._cache_stats["size"],
            "cache_ttl_seconds": self.cache_ttl,
            "user_id": self.user_id
        }
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self._cache.clear()
        self._cache_stats["size"] = 0
        logger.info(f"Reactä»£ç†(ç”¨æˆ·:{self.user_id})ç¼“å­˜å·²æ¸…ç©º")
    
    def _generate_cache_key(self, message: str, context: Optional[Dict[str, Any]] = None) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        cache_data = {
            "user_id": self.user_id,
            "message": message,
            "context": context or {},
            "tools_count": len(self.tools)
        }
        cache_str = json.dumps(cache_data, sort_keys=True)
        return hashlib.md5(cache_str.encode()).hexdigest()
    
    def _is_cache_valid(self, cache_entry: Dict[str, Any]) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if not cache_entry:
            return False
        
        created_at = cache_entry.get("created_at")
        if not created_at:
            return False
        
        # æ£€æŸ¥TTL
        elapsed = time.time() - created_at
        return elapsed < self.cache_ttl
    
    async def chat(
        self, 
        message: str,
        context: Optional[Dict[str, Any]] = None,
        task_type: str = "auto"
    ) -> Dict[str, Any]:
        """
        è¿›è¡Œå¯¹è¯ - çº¯æ•°æ®åº“é©±åŠ¨ï¼Œæ”¯æŒæ™ºèƒ½ç¼“å­˜
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            context: é¢å¤–ä¸Šä¸‹æ–‡ä¿¡æ¯
            task_type: ä»»åŠ¡ç±»å‹ ("auto" ä¸ºè‡ªåŠ¨åˆ¤æ–­, "reasoning" ä¸ºæ¨ç†ä»»åŠ¡, "general" ä¸ºå¸¸è§„å¯¹è¯)
            
        Returns:
            å¯¹è¯ç»“æœï¼ŒåŒ…å«æ¨¡å‹é€‰æ‹©å’Œä½¿ç”¨ç»Ÿè®¡
        """
        if not self.initialized:
            await self.initialize()
        
        # 1. åŠ¨æ€ä»»åŠ¡ç±»å‹åˆ¤æ–­
        if task_type == "auto":
            detected_task_type = self._analyze_task_type(message)
            logger.info(f"è‡ªåŠ¨æ£€æµ‹ä»»åŠ¡ç±»å‹: {detected_task_type} (ç”¨æˆ·: {self.user_id})")
        else:
            detected_task_type = task_type
        
        # 2. æ ¹æ®ä»»åŠ¡ç±»å‹é‡æ–°é€‰æ‹©æ¨¡å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
        model_switched = False
        if detected_task_type != self.current_task_type:
            model_switched = await self._reselect_model_for_task(detected_task_type)
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = None
        if self.enable_cache:
            cache_key = self._generate_cache_key(message, context)
            cached_result = self._cache.get(cache_key)
            
            if cached_result and self._is_cache_valid(cached_result):
                self._cache_stats["hits"] += 1
                logger.info(f"Reactä»£ç†ç¼“å­˜å‘½ä¸­ - ç”¨æˆ·:{self.user_id}, é”®:{cache_key[:8]}...")
                
                # è¿”å›ç¼“å­˜ç»“æœï¼Œæ·»åŠ ç¼“å­˜æ ‡è®°
                result = cached_result["result"].copy()
                result["from_cache"] = True
                result["cache_hit_time"] = datetime.utcnow().isoformat()
                return result
            else:
                self._cache_stats["misses"] += 1
        
        conversation_start = time.time()
        
        try:
            logger.info(f"Reactä»£ç†(ç”¨æˆ·:{self.user_id})å¼€å§‹å¤„ç†: {message[:100]}{'...' if len(message) > 100 else ''}")
            
            # æ„å»ºå®Œæ•´çš„è¾“å…¥æ¶ˆæ¯
            full_message = message
            if context:
                context_str = "\n".join([f"{k}: {v}" for k, v in context.items()])
                full_message = f"ä¸Šä¸‹æ–‡ä¿¡æ¯:\n{context_str}\n\nç”¨æˆ·è¯·æ±‚: {message}"
            
            # è°ƒç”¨ä»£ç†
            if hasattr(self.agent, 'achat'):
                response = await self.agent.achat(full_message)
            else:
                response = self.agent.chat(full_message)
            
            conversation_time = time.time() - conversation_start
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.total_conversations += 1
            self.successful_conversations += 1
            
            # è®°å½•ä½¿ç”¨åé¦ˆåˆ°æ•°æ®åº“
            if hasattr(self, 'selected_model') and self.llm_manager:
                from ..llm import record_usage_feedback
                
                record_usage_feedback(
                    user_id=self.user_id,
                    model=self.selected_model['model'],
                    provider=self.selected_model['provider'],
                    success=True,
                    satisfaction_score=0.9,  # åŸºäºæˆåŠŸå®Œæˆä»»åŠ¡çš„æ»¡æ„åº¦
                    actual_cost=self.selected_model.get('expected_cost'),
                    actual_latency=int(conversation_time * 1000),
                    agent_id="react_agent",
                    task_type="reasoning"
                )
            
            # æ„å»ºç»“æœ
            result = {
                "status": "success",
                "response": str(response),
                "conversation_time": conversation_time,
                "reasoning_steps": self._extract_reasoning_steps(response),
                "tool_calls": self._extract_tool_calls(response),
                "sources": getattr(response, 'sources', []),
                "metadata": {
                    "user_id": self.user_id,
                    "agent_type": "react",
                    "task_type_detected": detected_task_type,
                    "task_type_requested": task_type,
                    "model_switched": model_switched,
                    "current_task_type": self.current_task_type,
                    "model_used": f"{self.selected_model['provider']}:{self.selected_model['model']}" if hasattr(self, 'selected_model') else 'unknown',
                    "model_confidence": self.selected_model.get('confidence') if hasattr(self, 'selected_model') else None,
                    "tools_available": len(self.tools),
                    "max_iterations": self.max_iterations,
                    "database_driven": True,
                    "smart_model_selection": True
                }
            }
            
            # å­˜å‚¨åˆ°ç¼“å­˜
            if self.enable_cache and cache_key:
                cache_entry = {
                    "result": result,
                    "created_at": time.time(),
                    "user_id": self.user_id
                }
                self._cache[cache_key] = cache_entry
                self._cache_stats["size"] = len(self._cache)
                
                # ç®€å•çš„LRUæ¸…ç†ï¼šå¦‚æœç¼“å­˜è¿‡å¤§ï¼Œåˆ é™¤ä¸€äº›æ—§æ¡ç›®
                if len(self._cache) > 100:
                    oldest_keys = sorted(
                        self._cache.keys(),
                        key=lambda k: self._cache[k]["created_at"]
                    )[:20]
                    for old_key in oldest_keys:
                        del self._cache[old_key]
                    self._cache_stats["size"] = len(self._cache)
                
                logger.debug(f"Reactä»£ç†ç»“æœå·²ç¼“å­˜ - é”®:{cache_key[:8]}...")
            
            logger.info(f"Reactä»£ç†(ç”¨æˆ·:{self.user_id})å¤„ç†å®Œæˆï¼Œç”¨æ—¶: {conversation_time:.2f}s")
            
            return result
            
        except Exception as e:
            conversation_time = time.time() - conversation_start
            logger.error(f"Reactä»£ç†(ç”¨æˆ·:{self.user_id})å¤„ç†å¤±è´¥: {e}")
            
            # è®°å½•å¤±è´¥åé¦ˆ
            if hasattr(self, 'selected_model') and self.llm_manager:
                from ..llm import record_usage_feedback
                
                record_usage_feedback(
                    user_id=self.user_id,
                    model=self.selected_model['model'],
                    provider=self.selected_model['provider'],
                    success=False,
                    satisfaction_score=0.3,
                    actual_latency=int(conversation_time * 1000),
                    agent_id="react_agent",
                    task_type="reasoning"
                )
            
            return {
                "status": "error",
                "error": str(e),
                "conversation_time": conversation_time,
                "response": f"å¤„ç†å‡ºç°é”™è¯¯ï¼š{str(e)}",
                "reasoning_steps": [],
                "tool_calls": [],
                "sources": [],
                "metadata": {
                    "user_id": self.user_id,
                    "agent_type": "react",
                    "error_type": type(e).__name__,
                    "database_driven": True
                }
            }
    
    def _extract_reasoning_steps(self, response: Any) -> List[Dict[str, Any]]:
        """ä»å“åº”ä¸­æå–æ¨ç†æ­¥éª¤"""
        steps = []
        response_text = str(response)
        
        if "Thought:" in response_text:
            parts = response_text.split("Thought:")
            for i, part in enumerate(parts[1:], 1):
                thought_end = part.find("Action:") if "Action:" in part else len(part)
                thought = part[:thought_end].strip()
                
                step = {
                    "step_number": i,
                    "type": "thought",
                    "content": thought
                }
                
                if "Action:" in part:
                    action_start = part.find("Action:") + 7
                    action_end = part.find("Observation:") if "Observation:" in part else len(part)
                    action = part[action_start:action_end].strip()
                    step["action"] = action
                    
                    if "Observation:" in part:
                        obs_start = part.find("Observation:") + 12
                        observation = part[obs_start:].strip()
                        step["observation"] = observation
                
                steps.append(step)
        
        return steps
    
    def _analyze_task_type(self, message: str) -> str:
        """åˆ†æä»»åŠ¡ç±»å‹ï¼Œåˆ¤æ–­åº”è¯¥ä½¿ç”¨ä»€ä¹ˆç±»å‹çš„æ¨¡å‹"""
        message_lower = message.lower()
        
        # éœ€è¦æ·±åº¦æ€è€ƒçš„ä»»åŠ¡å…³é”®è¯
        thinking_keywords = [
            "åˆ†æ", "æ¨ç†", "è®¡ç®—", "è§£å†³", "è®¾è®¡", "ç­–ç•¥", "è§„åˆ’", "åˆ¶å®š",
            "å¤æ‚", "æ·±å…¥", "è¯¦ç»†åˆ†æ", "å¤šæ­¥", "æ­¥éª¤", "æµç¨‹", "æ–¹æ¡ˆ",
            "å¯¹æ¯”", "æ¯”è¾ƒ", "è¯„ä¼°", "åˆ¤æ–­", "å†³ç­–", "ä¼˜åŒ–",
            "analyze", "reasoning", "solve", "complex", "strategy", "plan",
            "compare", "evaluate", "optimize", "decision"
        ]
        
        # ç®€å•å¯¹è¯ä»»åŠ¡å…³é”®è¯
        chat_keywords = [
            "ç¿»è¯‘", "æ€»ç»“", "ä»‹ç»", "è§£é‡Š", "æè¿°", "é—®ç­”", "å›ç­”",
            "ä»€ä¹ˆæ˜¯", "å¦‚ä½•", "å‘Šè¯‰æˆ‘", "ç®€å•è¯´", "å¿«é€Ÿ", "ç›´æ¥",
            "translate", "summarize", "explain", "describe", "what is", 
            "how to", "tell me", "simple", "quick"
        ]
        
        # ç»Ÿè®¡å…³é”®è¯å‡ºç°æ¬¡æ•°
        thinking_score = sum(1 for keyword in thinking_keywords if keyword in message_lower)
        chat_score = sum(1 for keyword in chat_keywords if keyword in message_lower)
        
        # åŸºäºæ¶ˆæ¯é•¿åº¦çš„å¯å‘å¼åˆ¤æ–­
        message_length_factor = len(message) / 100  # é•¿æ¶ˆæ¯å¯èƒ½éœ€è¦æ›´æ·±åº¦çš„å¤„ç†
        
        # ç»¼åˆåˆ¤æ–­
        if thinking_score > chat_score or message_length_factor > 2:
            return "reasoning"  # éœ€è¦THINKæ¨¡å‹
        else:
            return "general"   # ä½¿ç”¨DEFAULTæ¨¡å‹
    
    async def _reselect_model_for_task(self, task_type: str) -> bool:
        """æ ¹æ®ä»»åŠ¡ç±»å‹é‡æ–°é€‰æ‹©æ¨¡å‹ - é›†æˆç®€åŒ–é€‰æ‹©å™¨"""
        try:
            # ä½¿ç”¨ç®€åŒ–é€‰æ‹©å™¨è¿›è¡Œæ¨¡å‹é€‰æ‹©
            from ..llm.simple_model_selector import get_simple_model_selector, TaskRequirement
            
            # æ ¹æ®ä»»åŠ¡ç±»å‹æ„å»ºéœ€æ±‚
            if task_type == "reasoning":
                task_requirement = TaskRequirement(
                    requires_thinking=True,
                    cost_sensitive=False,
                    speed_priority=False
                )
            else:  # general/chat tasks
                task_requirement = TaskRequirement(
                    requires_thinking=False,
                    cost_sensitive=True,  # ç®€å•ä»»åŠ¡åå‘æˆæœ¬æ§åˆ¶
                    speed_priority=True   # ç®€å•ä»»åŠ¡åå‘é€Ÿåº¦
                )
            
            # ä½¿ç”¨ç®€åŒ–é€‰æ‹©å™¨é€‰æ‹©æ¨¡å‹
            selector = get_simple_model_selector()
            selection = selector.select_model_for_user(
                user_id=self.user_id,
                task_requirement=task_requirement
            )
            
            if selection:
                # æ„å»ºæ–°çš„æ¨¡å‹ä¿¡æ¯ï¼ˆå…¼å®¹åŸæœ‰æ ¼å¼ï¼‰
                new_model = {
                    "model": selection.model_name,
                    "provider": selection.server_name,
                    "model_id": selection.model_id,
                    "server_id": selection.server_id,
                    "provider_type": selection.provider_type,
                    "confidence": 0.9,  # ç®€åŒ–é€‰æ‹©å™¨çš„ç½®ä¿¡åº¦
                    "reasoning": selection.reasoning,
                    "fallback_model_id": selection.fallback_model_id
                }
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ‡æ¢æ¨¡å‹
                if (not self.selected_model or 
                    new_model['model'] != self.selected_model.get('model') or
                    new_model['provider'] != self.selected_model.get('provider')):
                    
                    self.selected_model = new_model
                    self.current_task_type = task_type
                    
                    logger.info(f"React Agentä¸ºç”¨æˆ· {self.user_id} åˆ‡æ¢æ¨¡å‹: "
                              f"{new_model['provider']}:{new_model['model']} "
                              f"(ä»»åŠ¡ç±»å‹: {task_type}, æ¨ç†: {selection.reasoning})")
                    return True
                    
            return False
            
        except Exception as e:
            logger.error(f"ä½¿ç”¨ç®€åŒ–é€‰æ‹©å™¨é‡æ–°é€‰æ‹©æ¨¡å‹å¤±è´¥: {e}")
            
            # é™çº§åˆ°åŸæœ‰çš„LLMç®¡ç†å™¨é€‰æ‹©ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.llm_manager:
                try:
                    new_model = await self.llm_manager.select_best_model_for_user(
                        user_id=self.user_id,
                        task_type=task_type,
                        complexity="medium" if task_type == "reasoning" else "simple",
                        constraints={
                            "max_cost": 0.03 if task_type == "reasoning" else 0.01,
                            "preferred_providers": ["anthropic", "openai"]
                        },
                        agent_id="react_agent"
                    )
                    
                    if new_model:
                        self.selected_model = new_model
                        self.current_task_type = task_type
                        logger.info(f"React Agenté™çº§ä½¿ç”¨LLMç®¡ç†å™¨é€‰æ‹©æ¨¡å‹: {new_model['provider']}:{new_model['model']}")
                        return True
                        
                except Exception as fallback_e:
                    logger.error(f"é™çº§æ¨¡å‹é€‰æ‹©ä¹Ÿå¤±è´¥: {fallback_e}")
            
            return False

    def _extract_tool_calls(self, response: Any) -> List[Dict[str, Any]]:
        """ä»å“åº”ä¸­æå–å·¥å…·è°ƒç”¨ä¿¡æ¯"""
        tool_calls = []
        
        if hasattr(response, 'source_nodes'):
            for node in response.source_nodes:
                if hasattr(node, 'metadata'):
                    tool_calls.append({
                        "tool_name": node.metadata.get("tool_name", "unknown"),
                        "result": node.text if hasattr(node, 'text') else str(node)
                    })
        
        self.total_tool_calls += len(tool_calls)
        return tool_calls
    
    def reset_conversation(self):
        """é‡ç½®å¯¹è¯å†å²"""
        if self.agent and hasattr(self.agent, 'reset'):
            self.agent.reset()
        logger.info(f"Reactä»£ç†(ç”¨æˆ·:{self.user_id})å¯¹è¯å†å²å·²é‡ç½®")
    
    def get_conversation_stats(self) -> Dict[str, Any]:
        """è·å–å¯¹è¯ç»Ÿè®¡ä¿¡æ¯"""
        uptime = (datetime.utcnow() - self.start_time).total_seconds()
        
        return {
            "user_id": self.user_id,
            "total_conversations": self.total_conversations,
            "successful_conversations": self.successful_conversations,
            "success_rate": self.successful_conversations / max(self.total_conversations, 1),
            "total_tool_calls": self.total_tool_calls,
            "average_tool_calls_per_conversation": self.total_tool_calls / max(self.total_conversations, 1),
            "uptime_seconds": uptime,
            "agent_config": {
                "max_iterations": self.max_iterations,
                "memory_token_limit": self.memory_token_limit,
                "tools_count": len(self.tools),
                "verbose": self.verbose,
                "database_driven": True
            },
            "selected_model": getattr(self, 'selected_model', None)
        }
    
    def get_service_info(self) -> Dict[str, Any]:
        """è·å–æœåŠ¡ä¿¡æ¯"""
        return {
            "service_name": "Pure Database Driven React Agent",
            "version": "2.0.0",
            "architecture": "Database-First User-Specific Agent",
            "user_id": self.user_id,
            "status": "initialized" if self.initialized else "uninitialized",
            "capabilities": [
                "ç”¨æˆ·ä¸“å±æ¨¡å‹é€‰æ‹©",
                "ReActæ¨ç†å¾ªç¯",
                "å¤šè½®å¯¹è¯æ”¯æŒ",
                "å·¥å…·è°ƒç”¨ç¼–æ’",
                "ä½¿ç”¨åé¦ˆå­¦ä¹ ",
                "ä¸ªæ€§åŒ–Agentåå¥½"
            ],
            "data_sources": [
                "ç”¨æˆ·LLMåå¥½é…ç½®",
                "æ•°æ®åº“é©±åŠ¨æ¨¡å‹é€‰æ‹©",
                "ç”¨æˆ·ä½¿ç”¨å†å²è®°å½•"
            ],
            "configuration": {
                "max_iterations": self.max_iterations,
                "memory_token_limit": self.memory_token_limit,
                "tools_count": len(self.tools),
                "verbose": self.verbose
            },
            "statistics": self.get_conversation_stats(),
            "selected_model": getattr(self, 'selected_model', None)
        }


# ä¾¿æ·å‡½æ•°
async def create_react_agent_for_user(
    user_id: str,
    tools: Optional[List] = None,
    memory_token_limit: int = 4000,
    max_iterations: int = 15,
    verbose: bool = True,
    system_prompt: Optional[str] = None
) -> ReactAgent:
    """ä¸ºæŒ‡å®šç”¨æˆ·åˆ›å»ºReactä»£ç†"""
    
    agent = ReactAgent(
        user_id=user_id,
        tools=tools,
        memory_token_limit=memory_token_limit,
        max_iterations=max_iterations,
        verbose=verbose,
        system_prompt=system_prompt
    )
    
    await agent.initialize()
    return agent