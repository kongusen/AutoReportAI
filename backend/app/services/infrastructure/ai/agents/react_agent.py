"""
Reactæ™ºèƒ½ä»£ç†

åŸºäºReact Agentæ¶æ„çš„æ™ºèƒ½ä»£ç†å®ç°
éœ€è¦user_idå‚æ•°è¿›è¡Œç”¨æˆ·ä¸ªæ€§åŒ–é…ç½®
"""

import asyncio
import logging
import time
import hashlib
import json
from typing import Dict, List, Any, Optional
from datetime import datetime

logger = logging.getLogger(__name__)


class ReactAgent:
    """
    Reactæ™ºèƒ½ä»£ç†
    
    æ ¸å¿ƒç‰¹æ€§:
    1. åŸºäºç”¨æˆ·é…ç½®çš„ä¸ªæ€§åŒ–AIæœåŠ¡
    2. çº¯æ•°æ®åº“é©±åŠ¨ï¼Œæ— é…ç½®æ–‡ä»¶ä¾èµ–
    3. æ™ºèƒ½æ¨¡å‹é€‰æ‹©å’Œèµ„æºä¼˜åŒ–
    4. å­¦ä¹ å¼åå¥½è®°å½•å’Œä¼˜åŒ–
    """
    
    def __init__(
        self,
        user_id: str,  # ğŸ”‘ å¿…éœ€å‚æ•°ï¼šç”¨æˆ·ID
        tools: Optional[List] = None,
        memory_token_limit: int = 4000,
        max_iterations: int = 15,
        verbose: bool = True,
        system_prompt: Optional[str] = None
    ):
        if not user_id:
            raise ValueError("user_idæ˜¯å¿…éœ€å‚æ•°ï¼Œçº¯æ•°æ®åº“é©±åŠ¨Agentå¿…é¡»æŒ‡å®šç”¨æˆ·ID")
        
        self.user_id = user_id
        self.tools = tools or []
        self.memory_token_limit = memory_token_limit
        self.max_iterations = max_iterations
        self.verbose = verbose
        self.system_prompt = system_prompt
        
        self.agent: Optional[Any] = None
        self.initialized = False
        self.llm_manager = None
        
        # æ™ºèƒ½ç¼“å­˜é…ç½®
        self.enable_cache = True
        self.cache_ttl = 3600  # 1å°æ—¶ç¼“å­˜æ—¶é—´
        self._cache = {}  # ç®€å•å†…å­˜ç¼“å­˜
        self._cache_stats = {
            "hits": 0,
            "misses": 0,
            "size": 0
        }
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.total_conversations = 0
        self.successful_conversations = 0
        self.total_tool_calls = 0
        self.start_time = datetime.utcnow()
        
        logger.info(f"ä¸ºç”¨æˆ· {user_id} åˆå§‹åŒ–Reactæ™ºèƒ½ä»£ç†")
    
    async def initialize(self):
        """åˆå§‹åŒ–Reactä»£ç† - çº¯æ•°æ®åº“é©±åŠ¨"""
        if self.initialized:
            return
        
        logger.info(f"åˆå§‹åŒ–ç”¨æˆ· {self.user_id} çš„Reactæ™ºèƒ½ä»£ç†...")
        
        try:
            # 1. è·å–æ•°æ®åº“é©±åŠ¨çš„LLMç®¡ç†å™¨
            from ..llm import get_llm_manager
            self.llm_manager = await get_llm_manager()
            
            # 2. ä¸ºReact Agenté€‰æ‹©æœ€ä½³æ¨¡å‹ - åŸºäºç”¨æˆ·é…ç½®
            try:
                best_model = await self.llm_manager.select_best_model_for_user(
                    user_id=self.user_id,
                    task_type="reasoning",  # Reactéœ€è¦æ¨ç†èƒ½åŠ›
                    complexity="medium",
                    constraints={
                        "max_cost": 0.02,  # åˆç†çš„æˆæœ¬æ§åˆ¶
                        "preferred_providers": ["anthropic", "openai"]  # æ¨ç†ä»»åŠ¡åå¥½
                    },
                    agent_id="react_agent"
                )
                
                logger.info(f"React Agentä¸ºç”¨æˆ· {self.user_id} é€‰æ‹©æ¨¡å‹: {best_model['provider']}:{best_model['model']} (ç½®ä¿¡åº¦: {best_model['confidence']:.1%})")
                
                # è®°å½•æ¨¡å‹é€‰æ‹©ä¿¡æ¯
                self.selected_model = best_model
                
            except Exception as e:
                logger.error(f"ä¸ºç”¨æˆ· {self.user_id} é€‰æ‹©æ¨¡å‹å¤±è´¥: {e}")
                raise ValueError(f"æ— æ³•ä¸ºç”¨æˆ·é€‰æ‹©åˆé€‚çš„æ¨¡å‹: {str(e)}")
            
            # 3. åˆ›å»ºå·¥å…·é›†åˆï¼ˆå¦‚æœæœªæä¾›ï¼‰
            if not self.tools:
                try:
                    # æ·»åŠ å›¾è¡¨ç”Ÿæˆå·¥å…·
                    from ..tools.chart_generator_tool import generate_chart
                    from llama_index.core.tools import FunctionTool
                    
                    # åˆ›å»ºå›¾è¡¨ç”Ÿæˆå·¥å…·
                    chart_tool = FunctionTool.from_defaults(
                        fn=generate_chart,
                        name="generate_chart",
                        description="ç”Ÿæˆä¸“ä¸šå›¾è¡¨ï¼Œæ”¯æŒæŸ±çŠ¶å›¾ã€æŠ˜çº¿å›¾ã€é¥¼å›¾ã€é¢ç§¯å›¾ã€‚æ¥å—JSONé…ç½®å‚æ•°ï¼Œç”Ÿæˆå®é™…å›¾ç‰‡æ–‡ä»¶ã€‚"
                    )
                    
                    self.tools = [chart_tool]
                    
                    # å°è¯•æ·»åŠ å…¶ä»–å·¥å…·
                    try:
                        from ..tools import get_ai_tools_factory
                        tools_factory = await get_ai_tools_factory()
                        additional_tools = await tools_factory.create_all_tools()
                        self.tools.extend(additional_tools)
                    except Exception as e:
                        logger.warning(f"åˆ›å»ºé¢å¤–å·¥å…·å¤±è´¥: {e}")
                        
                except Exception as e:
                    logger.warning(f"åˆ›å»ºå·¥å…·å¤±è´¥: {e}")
                    self.tools = []
            
            # 4. åˆ›å»ºReactä»£ç†ï¼ˆä¼˜å…ˆä½¿ç”¨LlamaIndexï¼Œé™çº§åˆ°æ¨¡æ‹Ÿï¼‰
            try:
                from llama_index.core.agent import ReActAgent
                from llama_index.core.memory import ChatMemoryBuffer
                
                # è¿™é‡Œåº”è¯¥ä½¿ç”¨å®é™…çš„LlamaIndex LLMå®ä¾‹
                # ç›®å‰ä½¿ç”¨æ¨¡æ‹Ÿï¼Œå®é™…å®ç°ä¸­éœ€è¦ä»selected_modelåˆ›å»ºLLMå®ä¾‹
                self.agent = ReActAgent.from_tools(
                    tools=self.tools,
                    llm=None,  # å®é™…å®ç°ä¸­ä»selected_modelåˆ›å»º
                    memory=ChatMemoryBuffer.from_defaults(token_limit=self.memory_token_limit),
                    verbose=self.verbose,
                    max_iterations=self.max_iterations,
                    system_prompt=self.system_prompt or self._get_react_system_prompt()
                )
                
            except (ImportError, Exception) as e:
                logger.warning(f"LlamaIndex React Agentåˆ›å»ºå¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿä»£ç†: {e}")
                self.agent = self._create_mock_agent()
            
            self.initialized = True
            logger.info(f"ç”¨æˆ· {self.user_id} çš„Reactä»£ç†åˆå§‹åŒ–å®Œæˆ - å·¥å…·: {len(self.tools)}, æ¨¡å‹: {self.selected_model['provider']}:{self.selected_model['model']}")
            
        except Exception as e:
            logger.error(f"Reactä»£ç†åˆå§‹åŒ–å¤±è´¥: {e}")
            # åˆ›å»ºå¤‡ç”¨æ¨¡æ‹Ÿä»£ç†
            self.agent = self._create_mock_agent()
            self.initialized = True
            raise
    
    def _create_mock_agent(self):
        """åˆ›å»ºæ¨¡æ‹ŸReactä»£ç†"""
        class MockReactAgent:
            def __init__(self, parent):
                self.parent = parent
            
            async def achat(self, message: str) -> Any:
                """æ¨¡æ‹Ÿå¼‚æ­¥èŠå¤©ï¼Œä½¿ç”¨æ•°æ®åº“é©±åŠ¨çš„LLMé€‰æ‹©ï¼Œæ”¯æŒå·¥å…·è°ƒç”¨"""
                try:
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦ç”Ÿæˆå›¾è¡¨
                    charts_info = []
                    if any(keyword in message.lower() for keyword in ['å›¾è¡¨', 'æŸ±çŠ¶å›¾', 'æŠ˜çº¿å›¾', 'é¥¼å›¾', 'å¯è§†åŒ–', 'chart']):
                        charts_info = self._generate_sample_charts(message)
                    
                    from app.services.infrastructure.ai.llm import ask_agent_for_user
                    
                    response = await ask_agent_for_user(
                        user_id=self.parent.user_id,
                        question=message,
                        agent_type="react_agent",
                        context="Reactä»£ç†æ¨ç†æ¨¡å¼ï¼Œæ”¯æŒå›¾è¡¨ç”Ÿæˆå·¥å…·",
                        task_type="reasoning",
                        complexity="medium"
                    )
                    
                    # å¦‚æœç”Ÿæˆäº†å›¾è¡¨ï¼Œæ·»åŠ åˆ°å“åº”ä¸­
                    if charts_info:
                        chart_text = "\n\nğŸ“Š **ç”Ÿæˆçš„å›¾è¡¨æ–‡ä»¶:**\n"
                        for chart in charts_info:
                            chart_text += f"- {chart['title']}: `{chart['filename']}`\n"
                        response = response + chart_text
                    
                    return MockAgentResponse(response, charts_info)
                    
                except Exception as e:
                    logger.error(f"æ¨¡æ‹ŸAgentè°ƒç”¨å¤±è´¥: {e}")
                    return MockAgentResponse(f"æ¨¡æ‹ŸReactä»£ç†å“åº”ï¼š{message}", [])
            
            def chat(self, message: str) -> Any:
                """æ¨¡æ‹ŸåŒæ­¥èŠå¤©"""
                return MockAgentResponse(f"æ¨¡æ‹ŸReactä»£ç†å“åº”ï¼š{message}")
            
            def reset(self):
                """é‡ç½®å¯¹è¯å†å²"""
                pass
            
            def _generate_sample_charts(self, message: str):
                """æ ¹æ®æ¶ˆæ¯ç”Ÿæˆç¤ºä¾‹å›¾è¡¨"""
                charts_generated = []
                try:
                    from ..tools.chart_generator_tool import generate_chart
                    import json
                    
                    # æ ¹æ®æ¶ˆæ¯å†…å®¹å†³å®šç”Ÿæˆä»€ä¹ˆç±»å‹çš„å›¾è¡¨
                    if 'é”€å”®' in message or 'ä¸šç»©' in message or 'æ”¶å…¥' in message:
                        # ç”Ÿæˆé”€å”®æŸ±çŠ¶å›¾
                        config = {
                            "type": "bar",
                            "title": "æœˆåº¦é”€å”®ä¸šç»©åˆ†æ",
                            "x_data": ["1æœˆ", "2æœˆ", "3æœˆ", "4æœˆ", "5æœˆ"],
                            "y_data": [120000, 150000, 180000, 160000, 200000],
                            "x_label": "æœˆä»½", 
                            "y_label": "é”€å”®é¢ (å…ƒ)"
                        }
                        result = json.loads(generate_chart(json.dumps(config)))
                        if result.get('success'):
                            charts_generated.append(result)
                    
                    if 'è¶‹åŠ¿' in message or 'å¢é•¿' in message or 'æŠ˜çº¿' in message:
                        # ç”Ÿæˆè¶‹åŠ¿æŠ˜çº¿å›¾
                        config = {
                            "type": "line",
                            "title": "ä¸šåŠ¡å¢é•¿è¶‹åŠ¿",
                            "x_data": ["Q1", "Q2", "Q3", "Q4"],
                            "series": [
                                {"name": "æ”¶å…¥", "data": [100, 120, 140, 180]},
                                {"name": "åˆ©æ¶¦", "data": [20, 25, 30, 40]}
                            ],
                            "x_label": "å­£åº¦",
                            "y_label": "é‡‘é¢ (ä¸‡å…ƒ)"
                        }
                        result = json.loads(generate_chart(json.dumps(config)))
                        if result.get('success'):
                            charts_generated.append(result)
                    
                    if 'å æ¯”' in message or 'åˆ†å¸ƒ' in message or 'é¥¼å›¾' in message:
                        # ç”Ÿæˆå æ¯”é¥¼å›¾
                        config = {
                            "type": "pie",
                            "title": "å¸‚åœºä»½é¢åˆ†å¸ƒ",
                            "labels": ["äº§å“A", "äº§å“B", "äº§å“C", "äº§å“D", "å…¶ä»–"],
                            "sizes": [35, 25, 20, 15, 5]
                        }
                        result = json.loads(generate_chart(json.dumps(config)))
                        if result.get('success'):
                            charts_generated.append(result)
                            
                except Exception as e:
                    logger.error(f"ç”Ÿæˆç¤ºä¾‹å›¾è¡¨å¤±è´¥: {e}")
                
                return charts_generated
                
        class MockAgentResponse:
            def __init__(self, response: str, charts: List[Dict] = None):
                self.response = response
                self.source_nodes = []
                self.sources = []
                self.charts = charts or []
                
            def __str__(self):
                return self.response
        
        return MockReactAgent(self)
    
    def _get_react_system_prompt(self) -> str:
        """è·å–Reactç³»ç»Ÿæç¤ºè¯"""
        tools_info = self._get_tools_description()
        
        return f"""
ä½ æ˜¯ç”¨æˆ· {self.user_id} çš„ä¸“å±AIæ™ºèƒ½ä»£ç†ï¼ŒåŸºäºReActï¼ˆReasoning + Actingï¼‰æ¡†æ¶å·¥ä½œã€‚

ä½ çš„å·¥ä½œæµç¨‹ï¼š
1. **Thoughtï¼ˆæ€è€ƒï¼‰**: åˆ†æå½“å‰é—®é¢˜ï¼Œåˆ¶å®šè§£å†³ç­–ç•¥
2. **Actionï¼ˆè¡ŒåŠ¨ï¼‰**: é€‰æ‹©åˆé€‚çš„å·¥å…·æ‰§è¡Œå…·ä½“æ“ä½œ
3. **Observationï¼ˆè§‚å¯Ÿï¼‰**: åˆ†æå·¥å…·æ‰§è¡Œç»“æœ
4. **åå¤è¿­ä»£**: ç›´åˆ°é—®é¢˜å®Œå…¨è§£å†³

å¯ç”¨å·¥å…·ï¼š
{tools_info}

å·¥ä½œåŸåˆ™ï¼š
- å§‹ç»ˆå…ˆæ€è€ƒå†è¡ŒåŠ¨
- é€‰æ‹©æœ€åˆé€‚çš„å·¥å…·å®Œæˆä»»åŠ¡
- åŸºäºè§‚å¯Ÿç»“æœè°ƒæ•´ç­–ç•¥
- ç¡®ä¿æœ€ç»ˆç­”æ¡ˆå‡†ç¡®å®Œæ•´
- å¦‚æœé‡åˆ°é—®é¢˜ï¼Œå°è¯•ä¸åŒçš„æ–¹æ³•
- è®°ä½ä½ æœåŠ¡çš„ç”¨æˆ·IDæ˜¯ {self.user_id}

è¯·æŒ‰ç…§ Thought -> Action -> Observation çš„å¾ªç¯æ¥å¤„ç†ç”¨æˆ·è¯·æ±‚ã€‚
"""
    
    def _get_tools_description(self) -> str:
        """è·å–å·¥å…·æè¿°"""
        if not self.tools:
            return "æš‚æ— å¯ç”¨å·¥å…·"
        
        descriptions = []
        for i, tool in enumerate(self.tools, 1):
            if hasattr(tool, 'metadata'):
                name = getattr(tool.metadata, 'name', f'å·¥å…·{i}')
                desc = getattr(tool.metadata, 'description', 'æ— æè¿°')
                descriptions.append(f"- {name}: {desc}")
            else:
                descriptions.append(f"- å·¥å…·{i}: æ— æè¿°ä¿¡æ¯")
        
        return "\n".join(descriptions)
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total_requests = self._cache_stats["hits"] + self._cache_stats["misses"]
        hit_rate = self._cache_stats["hits"] / total_requests if total_requests > 0 else 0
        
        return {
            "cache_enabled": self.enable_cache,
            "cache_hits": self._cache_stats["hits"],
            "cache_misses": self._cache_stats["misses"],
            "hit_rate": round(hit_rate, 3),
            "cache_size": self._cache_stats["size"],
            "cache_ttl_seconds": self.cache_ttl,
            "user_id": self.user_id
        }
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self._cache.clear()
        self._cache_stats["size"] = 0
        logger.info(f"Reactä»£ç†(ç”¨æˆ·:{self.user_id})ç¼“å­˜å·²æ¸…ç©º")
    
    def _generate_cache_key(self, message: str, context: Optional[Dict[str, Any]] = None) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        cache_data = {
            "user_id": self.user_id,
            "message": message,
            "context": context or {},
            "tools_count": len(self.tools)
        }
        cache_str = json.dumps(cache_data, sort_keys=True)
        return hashlib.md5(cache_str.encode()).hexdigest()
    
    def _is_cache_valid(self, cache_entry: Dict[str, Any]) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if not cache_entry:
            return False
        
        created_at = cache_entry.get("created_at")
        if not created_at:
            return False
        
        # æ£€æŸ¥TTL
        elapsed = time.time() - created_at
        return elapsed < self.cache_ttl
    
    async def chat(
        self, 
        message: str,
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        è¿›è¡Œå¯¹è¯ - çº¯æ•°æ®åº“é©±åŠ¨ï¼Œæ”¯æŒæ™ºèƒ½ç¼“å­˜
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            context: é¢å¤–ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Returns:
            å¯¹è¯ç»“æœï¼ŒåŒ…å«æ¨¡å‹é€‰æ‹©å’Œä½¿ç”¨ç»Ÿè®¡
        """
        if not self.initialized:
            await self.initialize()
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = None
        if self.enable_cache:
            cache_key = self._generate_cache_key(message, context)
            cached_result = self._cache.get(cache_key)
            
            if cached_result and self._is_cache_valid(cached_result):
                self._cache_stats["hits"] += 1
                logger.info(f"Reactä»£ç†ç¼“å­˜å‘½ä¸­ - ç”¨æˆ·:{self.user_id}, é”®:{cache_key[:8]}...")
                
                # è¿”å›ç¼“å­˜ç»“æœï¼Œæ·»åŠ ç¼“å­˜æ ‡è®°
                result = cached_result["result"].copy()
                result["from_cache"] = True
                result["cache_hit_time"] = datetime.utcnow().isoformat()
                return result
            else:
                self._cache_stats["misses"] += 1
        
        conversation_start = time.time()
        
        try:
            logger.info(f"Reactä»£ç†(ç”¨æˆ·:{self.user_id})å¼€å§‹å¤„ç†: {message[:100]}{'...' if len(message) > 100 else ''}")
            
            # æ„å»ºå®Œæ•´çš„è¾“å…¥æ¶ˆæ¯
            full_message = message
            if context:
                context_str = "\n".join([f"{k}: {v}" for k, v in context.items()])
                full_message = f"ä¸Šä¸‹æ–‡ä¿¡æ¯:\n{context_str}\n\nç”¨æˆ·è¯·æ±‚: {message}"
            
            # è°ƒç”¨ä»£ç†
            if hasattr(self.agent, 'achat'):
                response = await self.agent.achat(full_message)
            else:
                response = self.agent.chat(full_message)
            
            conversation_time = time.time() - conversation_start
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.total_conversations += 1
            self.successful_conversations += 1
            
            # è®°å½•ä½¿ç”¨åé¦ˆåˆ°æ•°æ®åº“
            if hasattr(self, 'selected_model') and self.llm_manager:
                from ..llm import record_usage_feedback
                
                record_usage_feedback(
                    user_id=self.user_id,
                    model=self.selected_model['model'],
                    provider=self.selected_model['provider'],
                    success=True,
                    satisfaction_score=0.9,  # åŸºäºæˆåŠŸå®Œæˆä»»åŠ¡çš„æ»¡æ„åº¦
                    actual_cost=self.selected_model.get('expected_cost'),
                    actual_latency=int(conversation_time * 1000),
                    agent_id="react_agent",
                    task_type="reasoning"
                )
            
            # æ„å»ºç»“æœ
            result = {
                "status": "success",
                "response": str(response),
                "conversation_time": conversation_time,
                "reasoning_steps": self._extract_reasoning_steps(response),
                "tool_calls": self._extract_tool_calls(response),
                "sources": getattr(response, 'sources', []),
                "metadata": {
                    "user_id": self.user_id,
                    "agent_type": "react",
                    "model_used": f"{self.selected_model['provider']}:{self.selected_model['model']}" if hasattr(self, 'selected_model') else 'unknown',
                    "model_confidence": self.selected_model.get('confidence') if hasattr(self, 'selected_model') else None,
                    "tools_available": len(self.tools),
                    "max_iterations": self.max_iterations,
                    "database_driven": True
                }
            }
            
            # å­˜å‚¨åˆ°ç¼“å­˜
            if self.enable_cache and cache_key:
                cache_entry = {
                    "result": result,
                    "created_at": time.time(),
                    "user_id": self.user_id
                }
                self._cache[cache_key] = cache_entry
                self._cache_stats["size"] = len(self._cache)
                
                # ç®€å•çš„LRUæ¸…ç†ï¼šå¦‚æœç¼“å­˜è¿‡å¤§ï¼Œåˆ é™¤ä¸€äº›æ—§æ¡ç›®
                if len(self._cache) > 100:
                    oldest_keys = sorted(
                        self._cache.keys(),
                        key=lambda k: self._cache[k]["created_at"]
                    )[:20]
                    for old_key in oldest_keys:
                        del self._cache[old_key]
                    self._cache_stats["size"] = len(self._cache)
                
                logger.debug(f"Reactä»£ç†ç»“æœå·²ç¼“å­˜ - é”®:{cache_key[:8]}...")
            
            logger.info(f"Reactä»£ç†(ç”¨æˆ·:{self.user_id})å¤„ç†å®Œæˆï¼Œç”¨æ—¶: {conversation_time:.2f}s")
            
            return result
            
        except Exception as e:
            conversation_time = time.time() - conversation_start
            logger.error(f"Reactä»£ç†(ç”¨æˆ·:{self.user_id})å¤„ç†å¤±è´¥: {e}")
            
            # è®°å½•å¤±è´¥åé¦ˆ
            if hasattr(self, 'selected_model') and self.llm_manager:
                from ..llm import record_usage_feedback
                
                record_usage_feedback(
                    user_id=self.user_id,
                    model=self.selected_model['model'],
                    provider=self.selected_model['provider'],
                    success=False,
                    satisfaction_score=0.3,
                    actual_latency=int(conversation_time * 1000),
                    agent_id="react_agent",
                    task_type="reasoning"
                )
            
            return {
                "status": "error",
                "error": str(e),
                "conversation_time": conversation_time,
                "response": f"å¤„ç†å‡ºç°é”™è¯¯ï¼š{str(e)}",
                "reasoning_steps": [],
                "tool_calls": [],
                "sources": [],
                "metadata": {
                    "user_id": self.user_id,
                    "agent_type": "react",
                    "error_type": type(e).__name__,
                    "database_driven": True
                }
            }
    
    def _extract_reasoning_steps(self, response: Any) -> List[Dict[str, Any]]:
        """ä»å“åº”ä¸­æå–æ¨ç†æ­¥éª¤"""
        steps = []
        response_text = str(response)
        
        if "Thought:" in response_text:
            parts = response_text.split("Thought:")
            for i, part in enumerate(parts[1:], 1):
                thought_end = part.find("Action:") if "Action:" in part else len(part)
                thought = part[:thought_end].strip()
                
                step = {
                    "step_number": i,
                    "type": "thought",
                    "content": thought
                }
                
                if "Action:" in part:
                    action_start = part.find("Action:") + 7
                    action_end = part.find("Observation:") if "Observation:" in part else len(part)
                    action = part[action_start:action_end].strip()
                    step["action"] = action
                    
                    if "Observation:" in part:
                        obs_start = part.find("Observation:") + 12
                        observation = part[obs_start:].strip()
                        step["observation"] = observation
                
                steps.append(step)
        
        return steps
    
    def _extract_tool_calls(self, response: Any) -> List[Dict[str, Any]]:
        """ä»å“åº”ä¸­æå–å·¥å…·è°ƒç”¨ä¿¡æ¯"""
        tool_calls = []
        
        if hasattr(response, 'source_nodes'):
            for node in response.source_nodes:
                if hasattr(node, 'metadata'):
                    tool_calls.append({
                        "tool_name": node.metadata.get("tool_name", "unknown"),
                        "result": node.text if hasattr(node, 'text') else str(node)
                    })
        
        self.total_tool_calls += len(tool_calls)
        return tool_calls
    
    def reset_conversation(self):
        """é‡ç½®å¯¹è¯å†å²"""
        if self.agent and hasattr(self.agent, 'reset'):
            self.agent.reset()
        logger.info(f"Reactä»£ç†(ç”¨æˆ·:{self.user_id})å¯¹è¯å†å²å·²é‡ç½®")
    
    def get_conversation_stats(self) -> Dict[str, Any]:
        """è·å–å¯¹è¯ç»Ÿè®¡ä¿¡æ¯"""
        uptime = (datetime.utcnow() - self.start_time).total_seconds()
        
        return {
            "user_id": self.user_id,
            "total_conversations": self.total_conversations,
            "successful_conversations": self.successful_conversations,
            "success_rate": self.successful_conversations / max(self.total_conversations, 1),
            "total_tool_calls": self.total_tool_calls,
            "average_tool_calls_per_conversation": self.total_tool_calls / max(self.total_conversations, 1),
            "uptime_seconds": uptime,
            "agent_config": {
                "max_iterations": self.max_iterations,
                "memory_token_limit": self.memory_token_limit,
                "tools_count": len(self.tools),
                "verbose": self.verbose,
                "database_driven": True
            },
            "selected_model": getattr(self, 'selected_model', None)
        }
    
    def get_service_info(self) -> Dict[str, Any]:
        """è·å–æœåŠ¡ä¿¡æ¯"""
        return {
            "service_name": "Pure Database Driven React Agent",
            "version": "2.0.0",
            "architecture": "Database-First User-Specific Agent",
            "user_id": self.user_id,
            "status": "initialized" if self.initialized else "uninitialized",
            "capabilities": [
                "ç”¨æˆ·ä¸“å±æ¨¡å‹é€‰æ‹©",
                "ReActæ¨ç†å¾ªç¯",
                "å¤šè½®å¯¹è¯æ”¯æŒ",
                "å·¥å…·è°ƒç”¨ç¼–æ’",
                "ä½¿ç”¨åé¦ˆå­¦ä¹ ",
                "ä¸ªæ€§åŒ–Agentåå¥½"
            ],
            "data_sources": [
                "ç”¨æˆ·LLMåå¥½é…ç½®",
                "æ•°æ®åº“é©±åŠ¨æ¨¡å‹é€‰æ‹©",
                "ç”¨æˆ·ä½¿ç”¨å†å²è®°å½•"
            ],
            "configuration": {
                "max_iterations": self.max_iterations,
                "memory_token_limit": self.memory_token_limit,
                "tools_count": len(self.tools),
                "verbose": self.verbose
            },
            "statistics": self.get_conversation_stats(),
            "selected_model": getattr(self, 'selected_model', None)
        }


# ä¾¿æ·å‡½æ•°
async def create_react_agent_for_user(
    user_id: str,
    tools: Optional[List] = None,
    memory_token_limit: int = 4000,
    max_iterations: int = 15,
    verbose: bool = True,
    system_prompt: Optional[str] = None
) -> ReactAgent:
    """ä¸ºæŒ‡å®šç”¨æˆ·åˆ›å»ºReactä»£ç†"""
    
    agent = ReactAgent(
        user_id=user_id,
        tools=tools,
        memory_token_limit=memory_token_limit,
        max_iterations=max_iterations,
        verbose=verbose,
        system_prompt=system_prompt
    )
    
    await agent.initialize()
    return agent