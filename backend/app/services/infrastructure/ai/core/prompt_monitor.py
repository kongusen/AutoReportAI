"""
æç¤ºè¯æ€§èƒ½ç›‘æ§ç³»ç»Ÿ
===================

ç›‘æ§æç¤ºè¯ä½¿ç”¨æƒ…å†µã€æ€§èƒ½è¡¨ç°å’Œä¼˜åŒ–å»ºè®®
"""

import logging
import json
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
from dataclasses import dataclass, asdict
from collections import defaultdict
import asyncio

logger = logging.getLogger(__name__)


@dataclass
class PromptUsageMetrics:
    """æç¤ºè¯ä½¿ç”¨æŒ‡æ ‡"""
    category: str
    prompt_type: str
    complexity: str
    timestamp: datetime
    success: bool
    execution_time_ms: float
    prompt_length: int
    error_message: Optional[str] = None
    user_id: Optional[str] = None
    context_size: int = 0
    iterations: int = 1


class PromptPerformanceMonitor:
    """æç¤ºè¯æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.metrics_buffer: List[PromptUsageMetrics] = []
        self.buffer_size = 1000
        self.logger = logger
        
        # æ€§èƒ½ç»Ÿè®¡ç¼“å­˜
        self.performance_cache: Dict[str, Dict[str, Any]] = {}
        self.cache_ttl = 300  # 5åˆ†é’Ÿç¼“å­˜
        self.last_cache_update = {}
    
    def record_usage(
        self,
        category: str,
        prompt_type: str,
        complexity: str,
        success: bool,
        execution_time_ms: float,
        prompt_length: int,
        error_message: Optional[str] = None,
        user_id: Optional[str] = None,
        context_size: int = 0,
        iterations: int = 1
    ):
        """è®°å½•æç¤ºè¯ä½¿ç”¨æƒ…å†µ"""
        
        metric = PromptUsageMetrics(
            category=category,
            prompt_type=prompt_type,
            complexity=complexity,
            timestamp=datetime.now(),
            success=success,
            execution_time_ms=execution_time_ms,
            prompt_length=prompt_length,
            error_message=error_message,
            user_id=user_id,
            context_size=context_size,
            iterations=iterations
        )
        
        self.metrics_buffer.append(metric)
        
        # ç®¡ç†ç¼“å†²åŒºå¤§å°
        if len(self.metrics_buffer) > self.buffer_size:
            self.metrics_buffer = self.metrics_buffer[-self.buffer_size:]
        
        self.logger.debug(f"è®°å½•æç¤ºè¯ä½¿ç”¨: {category}.{prompt_type} - {'æˆåŠŸ' if success else 'å¤±è´¥'}")
    
    def get_performance_summary(
        self,
        category: Optional[str] = None,
        prompt_type: Optional[str] = None,
        time_window_hours: int = 24
    ) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æ‘˜è¦"""
        
        cache_key = f"{category or 'all'}_{prompt_type or 'all'}_{time_window_hours}"
        
        # æ£€æŸ¥ç¼“å­˜
        if (cache_key in self.performance_cache and 
            cache_key in self.last_cache_update and
            (datetime.now() - self.last_cache_update[cache_key]).seconds < self.cache_ttl):
            return self.performance_cache[cache_key]
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        cutoff_time = datetime.now() - timedelta(hours=time_window_hours)
        
        filtered_metrics = [
            m for m in self.metrics_buffer
            if m.timestamp >= cutoff_time
        ]
        
        if category:
            filtered_metrics = [m for m in filtered_metrics if m.category == category]
        
        if prompt_type:
            filtered_metrics = [m for m in filtered_metrics if m.prompt_type == prompt_type]
        
        if not filtered_metrics:
            return {
                "total_usage": 0,
                "success_rate": 0,
                "average_execution_time": 0,
                "error_analysis": {},
                "complexity_distribution": {},
                "recommendations": []
            }
        
        # è®¡ç®—å„é¡¹æŒ‡æ ‡
        total_usage = len(filtered_metrics)
        successful_count = sum(1 for m in filtered_metrics if m.success)
        success_rate = successful_count / total_usage
        
        avg_execution_time = sum(m.execution_time_ms for m in filtered_metrics) / total_usage
        avg_prompt_length = sum(m.prompt_length for m in filtered_metrics) / total_usage
        avg_iterations = sum(m.iterations for m in filtered_metrics) / total_usage
        
        # é”™è¯¯åˆ†æ
        error_analysis = self._analyze_errors(filtered_metrics)
        
        # å¤æ‚åº¦åˆ†å¸ƒ
        complexity_dist = defaultdict(int)
        for m in filtered_metrics:
            complexity_dist[m.complexity] += 1
        
        complexity_distribution = dict(complexity_dist)
        
        # ç”Ÿæˆå»ºè®®
        recommendations = self._generate_recommendations(
            success_rate, avg_execution_time, avg_iterations, error_analysis
        )
        
        summary = {
            "total_usage": total_usage,
            "success_rate": success_rate,
            "successful_count": successful_count,
            "failed_count": total_usage - successful_count,
            "average_execution_time": avg_execution_time,
            "average_prompt_length": avg_prompt_length,
            "average_iterations": avg_iterations,
            "error_analysis": error_analysis,
            "complexity_distribution": complexity_distribution,
            "recommendations": recommendations,
            "time_window_hours": time_window_hours,
            "generated_at": datetime.now().isoformat()
        }
        
        # æ›´æ–°ç¼“å­˜
        self.performance_cache[cache_key] = summary
        self.last_cache_update[cache_key] = datetime.now()
        
        return summary
    
    def _analyze_errors(self, metrics: List[PromptUsageMetrics]) -> Dict[str, Any]:
        """åˆ†æé”™è¯¯æ¨¡å¼"""
        
        failed_metrics = [m for m in metrics if not m.success]
        if not failed_metrics:
            return {"total_errors": 0, "error_patterns": {}, "common_causes": []}
        
        error_patterns = defaultdict(int)
        for m in failed_metrics:
            if m.error_message:
                # ç®€å•çš„é”™è¯¯åˆ†ç±»
                error_msg = m.error_message.lower()
                if "table" in error_msg and "not" in error_msg:
                    error_patterns["table_not_found"] += 1
                elif "column" in error_msg and "not" in error_msg:
                    error_patterns["column_not_found"] += 1
                elif "syntax" in error_msg:
                    error_patterns["syntax_error"] += 1
                elif "timeout" in error_msg:
                    error_patterns["timeout"] += 1
                else:
                    error_patterns["other"] += 1
        
        # è¯†åˆ«å¸¸è§åŸå› 
        common_causes = []
        if error_patterns.get("table_not_found", 0) > 2:
            common_causes.append("æ•°æ®æºè¡¨ç»“æ„ä¿¡æ¯å¯èƒ½ä¸å®Œæ•´æˆ–è¿‡æ—¶")
        
        if error_patterns.get("column_not_found", 0) > 2:
            common_causes.append("å­—æ®µæ˜ å°„æœºåˆ¶éœ€è¦ä¼˜åŒ–")
        
        if error_patterns.get("syntax_error", 0) > 2:
            common_causes.append("SQLç”Ÿæˆæ¨¡æ¿éœ€è¦æ”¹è¿›")
        
        return {
            "total_errors": len(failed_metrics),
            "error_patterns": dict(error_patterns),
            "common_causes": common_causes
        }
    
    def _generate_recommendations(
        self,
        success_rate: float,
        avg_execution_time: float,
        avg_iterations: float,
        error_analysis: Dict[str, Any]
    ) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        
        recommendations = []
        
        # æˆåŠŸç‡å»ºè®®
        if success_rate < 0.7:
            recommendations.append("ğŸš¨ æˆåŠŸç‡è¿‡ä½ï¼Œå»ºè®®å¢åŠ æç¤ºè¯çº¦æŸæˆ–æ”¹è¿›é”™è¯¯æ¢å¤æœºåˆ¶")
        elif success_rate < 0.85:
            recommendations.append("âš ï¸ æˆåŠŸç‡æœ‰å¾…æ”¹å–„ï¼Œå¯è€ƒè™‘ä¼˜åŒ–æç¤ºè¯å¤æ‚åº¦")
        else:
            recommendations.append("âœ… æˆåŠŸç‡è¡¨ç°è‰¯å¥½")
        
        # æ‰§è¡Œæ—¶é—´å»ºè®®
        if avg_execution_time > 5000:  # 5ç§’
            recommendations.append("ğŸŒ å¹³å‡æ‰§è¡Œæ—¶é—´è¾ƒé•¿ï¼Œå»ºè®®ä¼˜åŒ–æç¤ºè¯é•¿åº¦æˆ–å¤æ‚åº¦")
        elif avg_execution_time > 2000:  # 2ç§’
            recommendations.append("â±ï¸ æ‰§è¡Œæ—¶é—´é€‚ä¸­ï¼Œå¯è€ƒè™‘è¿›ä¸€æ­¥ä¼˜åŒ–")
        else:
            recommendations.append("âš¡ æ‰§è¡Œæ•ˆç‡è‰¯å¥½")
        
        # è¿­ä»£æ¬¡æ•°å»ºè®®
        if avg_iterations > 3:
            recommendations.append("ğŸ”„ å¹³å‡è¿­ä»£æ¬¡æ•°è¾ƒå¤šï¼Œå»ºè®®æ”¹è¿›åˆæ¬¡ç”Ÿæˆè´¨é‡")
        elif avg_iterations > 2:
            recommendations.append("ğŸ”„ è¿­ä»£æ¬¡æ•°é€‚ä¸­ï¼Œå¯ä¼˜åŒ–é¦–æ¬¡æˆåŠŸç‡")
        else:
            recommendations.append("ğŸ¯ è¿­ä»£æ•ˆç‡è‰¯å¥½")
        
        # é”™è¯¯æ¨¡å¼å»ºè®®
        error_patterns = error_analysis.get("error_patterns", {})
        if error_patterns.get("table_not_found", 0) > 2:
            recommendations.append("ğŸ” å»ºè®®åŠ å¼ºè¡¨åéªŒè¯å’Œæ™ºèƒ½åŒ¹é…æœºåˆ¶")
        
        if error_patterns.get("column_not_found", 0) > 2:
            recommendations.append("ğŸ” å»ºè®®ä¼˜åŒ–å­—æ®µæ˜ å°„å’ŒéªŒè¯é€»è¾‘")
        
        return recommendations
    
    def export_metrics(self, format: str = "json") -> str:
        """å¯¼å‡ºç›‘æ§æŒ‡æ ‡"""
        
        if format.lower() == "json":
            metrics_data = [asdict(m) for m in self.metrics_buffer]
            # è½¬æ¢datetimeä¸ºå­—ç¬¦ä¸²
            for metric in metrics_data:
                metric['timestamp'] = metric['timestamp'].isoformat()
            
            return json.dumps({
                "export_time": datetime.now().isoformat(),
                "total_records": len(metrics_data),
                "metrics": metrics_data
            }, indent=2, ensure_ascii=False)
        
        elif format.lower() == "csv":
            import csv
            import io
            
            output = io.StringIO()
            if self.metrics_buffer:
                writer = csv.DictWriter(output, fieldnames=asdict(self.metrics_buffer[0]).keys())
                writer.writeheader()
                
                for metric in self.metrics_buffer:
                    row = asdict(metric)
                    row['timestamp'] = row['timestamp'].isoformat()
                    writer.writerow(row)
            
            return output.getvalue()
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ ¼å¼: {format}")
    
    def get_real_time_dashboard_data(self) -> Dict[str, Any]:
        """è·å–å®æ—¶ä»ªè¡¨æ¿æ•°æ®"""
        
        # æœ€è¿‘1å°æ—¶çš„æ•°æ®
        recent_summary = self.get_performance_summary(time_window_hours=1)
        
        # æœ€è¿‘24å°æ—¶çš„æ•°æ®
        daily_summary = self.get_performance_summary(time_window_hours=24)
        
        # è¶‹åŠ¿åˆ†æ
        hourly_trends = self._calculate_hourly_trends()
        
        return {
            "real_time": {
                "timestamp": datetime.now().isoformat(),
                "recent_hour": recent_summary,
                "last_24_hours": daily_summary,
                "trends": hourly_trends
            },
            "system_health": {
                "buffer_usage": len(self.metrics_buffer) / self.buffer_size,
                "cache_hit_rate": len(self.performance_cache) / max(len(self.last_cache_update), 1),
                "active_categories": len(set(m.category for m in self.metrics_buffer[-100:])),
                "active_users": len(set(m.user_id for m in self.metrics_buffer[-100:] if m.user_id))
            }
        }
    
    def _calculate_hourly_trends(self) -> Dict[str, List[float]]:
        """è®¡ç®—å°æ—¶è¶‹åŠ¿"""
        
        now = datetime.now()
        hourly_data = defaultdict(list)
        
        for i in range(24):
            hour_start = now - timedelta(hours=i+1)
            hour_end = now - timedelta(hours=i)
            
            hour_metrics = [
                m for m in self.metrics_buffer
                if hour_start <= m.timestamp < hour_end
            ]
            
            if hour_metrics:
                success_rate = sum(1 for m in hour_metrics if m.success) / len(hour_metrics)
                avg_time = sum(m.execution_time_ms for m in hour_metrics) / len(hour_metrics)
                total_usage = len(hour_metrics)
            else:
                success_rate = 0
                avg_time = 0
                total_usage = 0
            
            hourly_data["success_rates"].append(success_rate)
            hourly_data["execution_times"].append(avg_time)
            hourly_data["usage_counts"].append(total_usage)
        
        # åè½¬åˆ—è¡¨ä½¿å…¶æŒ‰æ—¶é—´é¡ºåº
        for key in hourly_data:
            hourly_data[key] = hourly_data[key][::-1]
        
        return dict(hourly_data)


# å…¨å±€ç›‘æ§å®ä¾‹
_monitor_instance: Optional[PromptPerformanceMonitor] = None

def get_prompt_monitor() -> PromptPerformanceMonitor:
    """è·å–æç¤ºè¯ç›‘æ§å®ä¾‹"""
    global _monitor_instance
    if _monitor_instance is None:
        _monitor_instance = PromptPerformanceMonitor()
    return _monitor_instance