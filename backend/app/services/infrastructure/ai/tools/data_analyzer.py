"""
æ™ºèƒ½æ•°æ®åˆ†æå™¨ v2.0
===============================================

åŸºäºä¼˜åŒ–æç¤ºè¯ç³»ç»Ÿçš„æ•°æ®åˆ†æå·¥å…·ï¼š
- æ™ºèƒ½æ•°æ®æºæ¢ç´¢å’Œè¡¨ç»“æ„åˆ†æ
- è‡ªåŠ¨æ•°æ®è´¨é‡è¯„ä¼°
- æ™ºèƒ½å­—æ®µæ˜ å°„å’Œå…³ç³»å‘ç°
- æ•°æ®ç»Ÿè®¡å’Œæ´å¯Ÿç”Ÿæˆ
"""

import json
import logging
from typing import Dict, Any, List, Optional, AsyncGenerator

from ..core.tools import IterativeTool, ToolContext, ToolResult, ToolResultType

logger = logging.getLogger(__name__)


class SmartDataAnalyzer(IterativeTool):
    """æ™ºèƒ½æ•°æ®åˆ†æå™¨"""
    
    def __init__(self):
        super().__init__(
            tool_name="smart_data_analyzer",
            tool_category="data_analysis"
        )
    
    async def execute(
        self,
        input_data: Dict[str, Any],
        context: ToolContext
    ) -> AsyncGenerator[ToolResult, None]:
        """
        æ‰§è¡Œæ•°æ®åˆ†æä»»åŠ¡
        
        Args:
            input_data: è¾“å…¥æ•°æ®ï¼ŒåŒ…å«åˆ†æç±»å‹ç­‰å‚æ•°
            context: å·¥å…·æ‰§è¡Œä¸Šä¸‹æ–‡
        """
        
        yield self.create_progress_result("ğŸ” å¯åŠ¨æ™ºèƒ½æ•°æ®åˆ†æå™¨")
        
        # ä»input_dataä¸­æå–å‚æ•°
        analysis_type = input_data.get("analysis_type", "comprehensive")
        
        # éªŒè¯è¾“å…¥
        validation_result = await self.validate_input_enhanced(input_data)
        if not validation_result.get("valid", False):
            errors = validation_result.get("errors", ["è¾“å…¥éªŒè¯å¤±è´¥"])
            yield self.create_error_result("; ".join(errors))
            return
        
        # ä½¿ç”¨ç»§æ‰¿çš„è¿­ä»£æ‰§è¡Œæ¡†æ¶
        async for result in super().execute(input_data, context):
            yield result
    
    async def _validate_specific_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯æ•°æ®åˆ†æç‰¹å®šè¾“å…¥"""
        validation_result = {
            "valid": True,
            "errors": [],
            "warnings": [],
            "suggestions": []
        }
        
        # éªŒè¯åˆ†æç±»å‹
        analysis_type = input_data.get("analysis_type", "comprehensive")
        valid_types = ["table_structure", "data_quality", "relationship", "comprehensive"]
        
        if analysis_type not in valid_types:
            validation_result["valid"] = False
            validation_result["errors"].append(f"æ— æ•ˆçš„åˆ†æç±»å‹: {analysis_type}")
            validation_result["suggestions"].append(f"æ”¯æŒçš„åˆ†æç±»å‹: {', '.join(valid_types)}")
        
        # æ£€æŸ¥æ•°æ®æºä¿¡æ¯ï¼ˆè¿™å°†åœ¨æ‰§è¡Œæ—¶æ£€æŸ¥contextï¼‰
        if not input_data.get("data_source_info"):
            validation_result["warnings"].append("æœªæä¾›æ•°æ®æºä¿¡æ¯ï¼Œå°†å°è¯•ä»ä¸Šä¸‹æ–‡è·å–")
        
        return validation_result
    
    async def execute_single_iteration(
        self,
        input_data: Dict[str, Any],
        context: ToolContext,
        iteration: int
    ) -> AsyncGenerator[ToolResult, None]:
        """æ‰§è¡Œå•æ¬¡æ•°æ®åˆ†æè¿­ä»£"""
        
        analysis_type = input_data.get("analysis_type", "comprehensive")
        
        yield self.create_progress_result(
            f"ğŸ” ç¬¬ {iteration + 1} è½®: å¼€å§‹{analysis_type}åˆ†æ",
            step="analysis",
            percentage=((iteration * 1) / self.max_iterations) * 100
        )
        
        # æ ¹æ®åˆ†æç±»å‹é€‰æ‹©æ‰§è¡Œæ–¹å¼
        if analysis_type == "table_structure":
            async for result in self._analyze_table_structure(context, **input_data):
                yield result
        elif analysis_type == "data_quality":
            async for result in self._analyze_data_quality(context, **input_data):
                yield result
        elif analysis_type == "relationship":
            async for result in self._analyze_relationships(context, **input_data):
                yield result
        elif analysis_type == "comprehensive":
            async for result in self._comprehensive_analysis(context, **input_data):
                yield result
        else:
            yield self.create_error_result(f"ä¸æ”¯æŒçš„åˆ†æç±»å‹: {analysis_type}")
    
    async def _analyze_table_structure(
        self,
        context: ToolContext,
        **kwargs
    ) -> AsyncGenerator[ToolResult, None]:
        """åˆ†æè¡¨ç»“æ„"""
        
        yield self.create_progress_result("ğŸ“Š å¼€å§‹è¡¨ç»“æ„åˆ†æ")
        
        try:
            # è·å–æ•°æ®æºä¿¡æ¯
            data_source_info = await self._ensure_data_source_info(context)
            if not data_source_info:
                yield self.create_error_result("æ— æ³•è·å–æ•°æ®æºä¿¡æ¯")
                return
            
            tables = data_source_info.get('tables', [])
            if not tables:
                yield self.create_error_result("æ•°æ®æºä¸­æ²¡æœ‰å¯ç”¨çš„è¡¨")
                return
            
            yield self.create_progress_result(f"å‘ç° {len(tables)} ä¸ªè¡¨ï¼Œå¼€å§‹è¯¦ç»†åˆ†æ")
            
            # åˆ†ææ¯ä¸ªè¡¨çš„ç»“æ„
            table_analysis = []
            for i, table_name in enumerate(tables):
                yield self.create_progress_result(
                    f"åˆ†æè¡¨ {i+1}/{len(tables)}: {table_name}"
                )
                
                table_info = await self._analyze_single_table(
                    context, table_name, data_source_info
                )
                if table_info:
                    table_analysis.append(table_info)
            
            # ç”Ÿæˆåˆ†æç»“æœ
            analysis_result = {
                "analysis_type": "table_structure",
                "data_source": {
                    "name": data_source_info.get('name', 'unknown'),
                    "type": data_source_info.get('type', 'unknown'),
                    "database": data_source_info.get('database', 'unknown')
                },
                "total_tables": len(tables),
                "analyzed_tables": len(table_analysis),
                "tables": table_analysis,
                "summary": self._generate_structure_summary(table_analysis),
                "recommendations": self._generate_structure_recommendations(table_analysis)
            }
            
            yield self.create_success_result(
                data=analysis_result,
                confidence=0.9,
                insights=[
                    f"åˆ†æäº† {len(table_analysis)} ä¸ªè¡¨çš„ç»“æ„",
                    f"å‘ç° {sum(len(t.get('columns', [])) for t in table_analysis)} ä¸ªå­—æ®µ",
                    "è¡¨ç»“æ„åˆ†æå®Œæˆ"
                ]
            )
            
        except Exception as e:
            self.logger.error(f"è¡¨ç»“æ„åˆ†æå¼‚å¸¸: {e}")
            yield self.create_error_result(f"è¡¨ç»“æ„åˆ†æå¤±è´¥: {str(e)}")
    
    async def _analyze_data_quality(
        self,
        context: ToolContext,
        **kwargs
    ) -> AsyncGenerator[ToolResult, None]:
        """åˆ†ææ•°æ®è´¨é‡"""
        
        yield self.create_progress_result("ğŸ” å¼€å§‹æ•°æ®è´¨é‡åˆ†æ")
        
        try:
            # è·å–æ•°æ®æºä¿¡æ¯
            data_source_info = await self._ensure_data_source_info(context)
            if not data_source_info:
                yield self.create_error_result("æ— æ³•è·å–æ•°æ®æºä¿¡æ¯")
                return
            
            # ç”Ÿæˆæ•°æ®è´¨é‡åˆ†ææç¤ºè¯
            quality_prompt = self._build_data_quality_prompt(data_source_info, context)
            
            yield self.create_progress_result("ğŸ¤– AIåˆ†ææ•°æ®è´¨é‡æ¨¡å¼")
            
            # è°ƒç”¨LLMè¿›è¡Œæ•°æ®è´¨é‡åˆ†æ
            quality_response = await self.ask_llm(
                prompt=quality_prompt,
                context=context,
                agent_type="data_analyst",
                task_type="data_quality_analysis"
            )
            
            # è§£æåˆ†æç»“æœ
            quality_result = self._parse_quality_analysis(quality_response)
            
            if quality_result:
                yield self.create_success_result(
                    data={
                        "analysis_type": "data_quality",
                        "data_source": data_source_info.get('name', 'unknown'),
                        **quality_result
                    },
                    confidence=quality_result.get('confidence', 0.7),
                    insights=quality_result.get('insights', [])
                )
            else:
                yield self.create_error_result("æ•°æ®è´¨é‡åˆ†æç»“æœè§£æå¤±è´¥")
                
        except Exception as e:
            self.logger.error(f"æ•°æ®è´¨é‡åˆ†æå¼‚å¸¸: {e}")
            yield self.create_error_result(f"æ•°æ®è´¨é‡åˆ†æå¤±è´¥: {str(e)}")
    
    async def _analyze_relationships(
        self,
        context: ToolContext,
        **kwargs
    ) -> AsyncGenerator[ToolResult, None]:
        """åˆ†æè¡¨é—´å…³ç³»"""
        
        yield self.create_progress_result("ğŸ”— å¼€å§‹å…³ç³»åˆ†æ")
        
        try:
            # è·å–æ•°æ®æºä¿¡æ¯
            data_source_info = await self._ensure_data_source_info(context)
            if not data_source_info:
                yield self.create_error_result("æ— æ³•è·å–æ•°æ®æºä¿¡æ¯")
                return
            
            tables = data_source_info.get('tables', [])
            table_details = data_source_info.get('table_details', [])
            
            if len(tables) < 2:
                yield self.create_warning_result(
                    "è¡¨æ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œå…³ç³»åˆ†æ",
                    data={"analysis_type": "relationship", "tables_count": len(tables)}
                )
                return
            
            # ç”Ÿæˆå…³ç³»åˆ†ææç¤ºè¯
            relationship_prompt = self._build_relationship_prompt(tables, table_details, context)
            
            yield self.create_progress_result("ğŸ¤– AIåˆ†æè¡¨é—´å…³ç³»")
            
            # è°ƒç”¨LLMè¿›è¡Œå…³ç³»åˆ†æ
            relationship_response = await self.ask_llm(
                prompt=relationship_prompt,
                context=context,
                agent_type="data_architect",
                task_type="relationship_analysis"
            )
            
            # è§£æå…³ç³»åˆ†æç»“æœ
            relationship_result = self._parse_relationship_analysis(relationship_response)
            
            if relationship_result:
                yield self.create_success_result(
                    data={
                        "analysis_type": "relationship",
                        "data_source": data_source_info.get('name', 'unknown'),
                        "tables_analyzed": len(tables),
                        **relationship_result
                    },
                    confidence=relationship_result.get('confidence', 0.7),
                    insights=relationship_result.get('insights', [])
                )
            else:
                yield self.create_error_result("å…³ç³»åˆ†æç»“æœè§£æå¤±è´¥")
                
        except Exception as e:
            self.logger.error(f"å…³ç³»åˆ†æå¼‚å¸¸: {e}")
            yield self.create_error_result(f"å…³ç³»åˆ†æå¤±è´¥: {str(e)}")
    
    async def _comprehensive_analysis(
        self,
        context: ToolContext,
        **kwargs
    ) -> AsyncGenerator[ToolResult, None]:
        """ç»¼åˆåˆ†æ"""
        
        yield self.create_progress_result("ğŸš€ å¼€å§‹ç»¼åˆæ•°æ®åˆ†æ")
        
        comprehensive_result = {
            "analysis_type": "comprehensive",
            "components": {}
        }
        
        try:
            # 1. è¡¨ç»“æ„åˆ†æ
            yield self.create_progress_result("ç¬¬1æ­¥: è¡¨ç»“æ„åˆ†æ")
            structure_results = []
            async for result in self._analyze_table_structure(context, **kwargs):
                structure_results.append(result)
                if result.type == ToolResultType.PROGRESS:
                    yield result
            
            if structure_results and structure_results[-1].type == ToolResultType.RESULT:
                comprehensive_result["components"]["structure"] = structure_results[-1].data
            
            # 2. æ•°æ®è´¨é‡åˆ†æ
            yield self.create_progress_result("ç¬¬2æ­¥: æ•°æ®è´¨é‡åˆ†æ")
            quality_results = []
            async for result in self._analyze_data_quality(context, **kwargs):
                quality_results.append(result)
                if result.type == ToolResultType.PROGRESS:
                    yield result
            
            if quality_results and quality_results[-1].type == ToolResultType.RESULT:
                comprehensive_result["components"]["quality"] = quality_results[-1].data
            
            # 3. å…³ç³»åˆ†æ
            yield self.create_progress_result("ç¬¬3æ­¥: è¡¨å…³ç³»åˆ†æ")
            relationship_results = []
            async for result in self._analyze_relationships(context, **kwargs):
                relationship_results.append(result)
                if result.type == ToolResultType.PROGRESS:
                    yield result
            
            if relationship_results and relationship_results[-1].type == ToolResultType.RESULT:
                comprehensive_result["components"]["relationships"] = relationship_results[-1].data
            
            # 4. ç”Ÿæˆç»¼åˆæ´å¯Ÿ
            yield self.create_progress_result("ç¬¬4æ­¥: ç”Ÿæˆç»¼åˆæ´å¯Ÿ")
            comprehensive_insights = self._generate_comprehensive_insights(comprehensive_result)
            comprehensive_result["insights"] = comprehensive_insights
            comprehensive_result["summary"] = self._generate_comprehensive_summary(comprehensive_result)
            
            # è®¡ç®—ç»¼åˆç½®ä¿¡åº¦
            confidences = []
            for component in comprehensive_result["components"].values():
                if isinstance(component, dict) and "confidence" in component:
                    confidences.append(component["confidence"])
            
            avg_confidence = sum(confidences) / len(confidences) if confidences else 0.5
            
            yield self.create_success_result(
                data=comprehensive_result,
                confidence=avg_confidence,
                insights=comprehensive_insights
            )
            
        except Exception as e:
            self.logger.error(f"ç»¼åˆåˆ†æå¼‚å¸¸: {e}")
            yield self.create_error_result(f"ç»¼åˆåˆ†æå¤±è´¥: {str(e)}")
    
    async def _ensure_data_source_info(self, context: ToolContext) -> Optional[Dict[str, Any]]:
        """ç¡®ä¿è·å–åˆ°æ•°æ®æºä¿¡æ¯"""
        
        if context.data_source_info:
            return context.data_source_info
        
        if context.data_source_id:
            # TODO: ä»æ•°æ®åº“è·å–æ•°æ®æºä¿¡æ¯
            # è¿™é‡Œåº”è¯¥è°ƒç”¨æ•°æ®æºæœåŠ¡è·å–è¯¦ç»†ä¿¡æ¯
            self.logger.warning("éœ€è¦å®ç°ä»æ•°æ®åº“è·å–æ•°æ®æºä¿¡æ¯çš„é€»è¾‘")
            return None
        
        return None
    
    async def _analyze_single_table(
        self,
        context: ToolContext,
        table_name: str,
        data_source_info: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """åˆ†æå•ä¸ªè¡¨çš„ç»“æ„"""
        
        try:
            # ä»table_detailsä¸­æŸ¥æ‰¾è¡¨ä¿¡æ¯
            table_details = data_source_info.get('table_details', [])
            table_info = None
            
            for detail in table_details:
                if detail.get('name') == table_name:
                    table_info = detail
                    break
            
            if not table_info:
                # å¦‚æœæ²¡æœ‰è¯¦ç»†ä¿¡æ¯ï¼Œåˆ›å»ºåŸºç¡€ä¿¡æ¯
                table_info = {"name": table_name, "all_columns": []}
            
            # åˆ†æå­—æ®µç±»å‹å’Œæ¨¡å¼
            columns = table_info.get('all_columns', [])
            analyzed_columns = []
            
            for column in columns:
                if isinstance(column, str):
                    # è§£æå­—æ®µåå’Œç±»å‹
                    if '(' in column and ')' in column:
                        parts = column.split('(')
                        field_name = parts[0].strip()
                        field_type = column[len(field_name):].strip()
                    else:
                        field_name = column.strip()
                        field_type = "unknown"
                    
                    analyzed_columns.append({
                        "name": field_name,
                        "type": field_type,
                        "category": self._categorize_field(field_name, field_type),
                        "business_meaning": self._infer_business_meaning(field_name)
                    })
            
            return {
                "name": table_name,
                "columns": analyzed_columns,
                "column_count": len(analyzed_columns),
                "estimated_rows": table_info.get('estimated_rows', 0),
                "business_category": self._categorize_table(table_name, analyzed_columns),
                "key_fields": self._identify_key_fields(analyzed_columns)
            }
            
        except Exception as e:
            self.logger.error(f"åˆ†æè¡¨ {table_name} å¼‚å¸¸: {e}")
            return None
    
    def _categorize_field(self, field_name: str, field_type: str) -> str:
        """å­—æ®µåˆ†ç±»"""
        field_name_lower = field_name.lower()
        field_type_lower = field_type.lower()
        
        # æ—¶é—´å­—æ®µ
        time_keywords = ['time', 'date', 'created', 'updated', 'modified', '_at', '_on']
        if any(keyword in field_name_lower for keyword in time_keywords):
            return "temporal"
        
        # IDå­—æ®µ
        if field_name_lower.endswith('_id') or field_name_lower == 'id':
            return "identifier"
        
        # æ•°å€¼å­—æ®µ
        if any(keyword in field_type_lower for keyword in ['int', 'decimal', 'float', 'number', 'bigint']):
            return "numeric"
        
        # æ–‡æœ¬å­—æ®µ
        if any(keyword in field_type_lower for keyword in ['varchar', 'text', 'char', 'string']):
            return "textual"
        
        return "other"
    
    def _categorize_table(self, table_name: str, columns: List[Dict[str, Any]]) -> str:
        """è¡¨åˆ†ç±»"""
        table_name_lower = table_name.lower()
        
        # äº‹å®è¡¨ç‰¹å¾
        if any(prefix in table_name_lower for prefix in ['fact_', 'f_']):
            return "fact_table"
        
        # ç»´åº¦è¡¨ç‰¹å¾
        if any(prefix in table_name_lower for prefix in ['dim_', 'd_']):
            return "dimension_table"
        
        # ODSè¡¨ç‰¹å¾
        if table_name_lower.startswith('ods_'):
            return "ods_table"
        
        # ä¸šåŠ¡è¡¨ç‰¹å¾åˆ¤æ–­
        business_keywords = {
            'user': 'user_data',
            'customer': 'customer_data',
            'order': 'transaction_data',
            'product': 'product_data',
            'complain': 'service_data',
            'sales': 'sales_data'
        }
        
        for keyword, category in business_keywords.items():
            if keyword in table_name_lower:
                return category
        
        return "general_table"
    
    def _infer_business_meaning(self, field_name: str) -> str:
        """æ¨æ–­å­—æ®µä¸šåŠ¡å«ä¹‰"""
        field_name_lower = field_name.lower()
        
        business_meanings = {
            'id': 'å”¯ä¸€æ ‡è¯†ç¬¦',
            'name': 'åç§°',
            'title': 'æ ‡é¢˜',
            'content': 'å†…å®¹',
            'description': 'æè¿°',
            'status': 'çŠ¶æ€',
            'type': 'ç±»å‹',
            'category': 'åˆ†ç±»',
            'amount': 'é‡‘é¢',
            'quantity': 'æ•°é‡',
            'price': 'ä»·æ ¼',
            'date': 'æ—¥æœŸ',
            'time': 'æ—¶é—´',
            'created': 'åˆ›å»ºæ—¶é—´',
            'updated': 'æ›´æ–°æ—¶é—´',
            'user': 'ç”¨æˆ·',
            'customer': 'å®¢æˆ·'
        }
        
        for keyword, meaning in business_meanings.items():
            if keyword in field_name_lower:
                return meaning
        
        return 'å¾…ç¡®å®šå«ä¹‰'
    
    def _identify_key_fields(self, columns: List[Dict[str, Any]]) -> List[str]:
        """è¯†åˆ«å…³é”®å­—æ®µ"""
        key_fields = []
        
        for column in columns:
            field_name = column.get('name', '').lower()
            field_category = column.get('category', '')
            
            # IDå­—æ®µé€šå¸¸æ˜¯å…³é”®å­—æ®µ
            if field_category == 'identifier':
                key_fields.append(column.get('name', ''))
            
            # æ—¶é—´å­—æ®µé€šå¸¸æ˜¯å…³é”®å­—æ®µ
            elif field_category == 'temporal':
                key_fields.append(column.get('name', ''))
            
            # çŠ¶æ€ã€ç±»å‹å­—æ®µé€šå¸¸æ˜¯å…³é”®å­—æ®µ
            elif any(keyword in field_name for keyword in ['status', 'type', 'category']):
                key_fields.append(column.get('name', ''))
        
        return key_fields
    
    def _generate_structure_summary(self, table_analysis: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆç»“æ„åˆ†ææ‘˜è¦"""
        total_columns = sum(len(table.get('columns', [])) for table in table_analysis)
        
        # ç»Ÿè®¡å­—æ®µç±»å‹åˆ†å¸ƒ
        field_categories = {}
        for table in table_analysis:
            for column in table.get('columns', []):
                category = column.get('category', 'other')
                field_categories[category] = field_categories.get(category, 0) + 1
        
        # ç»Ÿè®¡è¡¨ç±»å‹åˆ†å¸ƒ
        table_categories = {}
        for table in table_analysis:
            category = table.get('business_category', 'general_table')
            table_categories[category] = table_categories.get(category, 0) + 1
        
        return {
            "total_tables": len(table_analysis),
            "total_columns": total_columns,
            "avg_columns_per_table": total_columns / len(table_analysis) if table_analysis else 0,
            "field_type_distribution": field_categories,
            "table_type_distribution": table_categories
        }
    
    def _generate_structure_recommendations(self, table_analysis: List[Dict[str, Any]]) -> List[str]:
        """ç”Ÿæˆç»“æ„åˆ†æå»ºè®®"""
        recommendations = []
        
        # æ£€æŸ¥è¡¨æ•°é‡
        if len(table_analysis) < 3:
            recommendations.append("æ•°æ®æºè¡¨æ•°é‡è¾ƒå°‘ï¼Œå»ºè®®ç¡®è®¤æ˜¯å¦å®Œæ•´")
        
        # æ£€æŸ¥å­—æ®µæ•°é‡
        total_columns = sum(len(table.get('columns', [])) for table in table_analysis)
        if total_columns < 10:
            recommendations.append("æ€»å­—æ®µæ•°é‡è¾ƒå°‘ï¼Œå»ºè®®æ£€æŸ¥è¡¨ç»“æ„å®Œæ•´æ€§")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ—¶é—´å­—æ®µ
        has_time_field = any(
            any(col.get('category') == 'temporal' for col in table.get('columns', []))
            for table in table_analysis
        )
        if not has_time_field:
            recommendations.append("æœªå‘ç°æ—¶é—´å­—æ®µï¼Œå»ºè®®ç¡®è®¤æ•°æ®çš„æ—¶é—´ç»´åº¦")
        
        return recommendations
    
    def _build_data_quality_prompt(self, data_source_info: Dict[str, Any], context: ToolContext) -> str:
        """æ„å»ºæ•°æ®è´¨é‡åˆ†ææç¤ºè¯"""
        
        tables = data_source_info.get('tables', [])
        table_details = data_source_info.get('table_details', [])
        
        prompt_parts = [
            "è¯·åˆ†æä»¥ä¸‹æ•°æ®æºçš„æ•°æ®è´¨é‡ï¼š",
            "",
            f"æ•°æ®æºç±»å‹: {data_source_info.get('type', 'unknown')}",
            f"æ•°æ®åº“: {data_source_info.get('database', 'unknown')}",
            f"è¡¨æ•°é‡: {len(tables)}",
            "",
            "è¡¨ç»“æ„ä¿¡æ¯:"
        ]
        
        for i, table_name in enumerate(tables[:5]):  # æœ€å¤šæ˜¾ç¤º5ä¸ªè¡¨
            prompt_parts.append(f"{i+1}. {table_name}")
            
            # æŸ¥æ‰¾è¡¨è¯¦æƒ…
            for detail in table_details:
                if detail.get('name') == table_name:
                    columns = detail.get('all_columns', [])[:10]  # æœ€å¤šæ˜¾ç¤º10ä¸ªå­—æ®µ
                    prompt_parts.append(f"   å­—æ®µ: {', '.join(columns)}")
                    break
        
        if len(tables) > 5:
            prompt_parts.append(f"... è¿˜æœ‰ {len(tables) - 5} ä¸ªè¡¨")
        
        prompt_parts.extend([
            "",
            "è¯·ä»ä»¥ä¸‹ç»´åº¦è¯„ä¼°æ•°æ®è´¨é‡ï¼š",
            "1. è¡¨åå’Œå­—æ®µå‘½åè§„èŒƒæ€§",
            "2. æ•°æ®ç»“æ„åˆç†æ€§",
            "3. å¯èƒ½çš„æ•°æ®å®Œæ•´æ€§é—®é¢˜",
            "4. å­—æ®µç±»å‹é€‚å½“æ€§",
            "5. ä¸šåŠ¡é€»è¾‘ä¸€è‡´æ€§",
            "",
            "è¯·è¿”å›JSONæ ¼å¼ç»“æœï¼š",
            """{
    "overall_score": 0.8,
    "quality_dimensions": {
        "naming_convention": {"score": 0.9, "issues": []},
        "data_structure": {"score": 0.8, "issues": []},
        "completeness": {"score": 0.7, "issues": []},
        "consistency": {"score": 0.8, "issues": []}
    },
    "recommendations": ["å»ºè®®1", "å»ºè®®2"],
    "confidence": 0.8,
    "insights": ["æ´å¯Ÿ1", "æ´å¯Ÿ2"]
}"""
        ])
        
        return "\n".join(prompt_parts)
    
    def _build_relationship_prompt(
        self,
        tables: List[str],
        table_details: List[Dict[str, Any]],
        context: ToolContext
    ) -> str:
        """æ„å»ºå…³ç³»åˆ†ææç¤ºè¯"""
        
        prompt_parts = [
            "è¯·åˆ†æä»¥ä¸‹è¡¨ä¹‹é—´çš„æ½œåœ¨å…³ç³»ï¼š",
            "",
            f"è¡¨æ•°é‡: {len(tables)}",
            ""
        ]
        
        # æ˜¾ç¤ºè¡¨å’Œå…³é”®å­—æ®µ
        for i, table_name in enumerate(tables):
            prompt_parts.append(f"{i+1}. {table_name}")
            
            # æŸ¥æ‰¾è¡¨è¯¦æƒ…
            for detail in table_details:
                if detail.get('name') == table_name:
                    columns = detail.get('all_columns', [])
                    # æå–å¯èƒ½çš„å…³é”®å­—æ®µ
                    key_columns = [col for col in columns if 'id' in col.lower()][:5]
                    if key_columns:
                        prompt_parts.append(f"   å…³é”®å­—æ®µ: {', '.join(key_columns)}")
                    break
        
        prompt_parts.extend([
            "",
            "è¯·åˆ†æï¼š",
            "1. è¡¨ä¹‹é—´å¯èƒ½çš„ä¸»å¤–é”®å…³ç³»",
            "2. ä¸šåŠ¡å…³è”å…³ç³»",
            "3. æ•°æ®æµå‘å’Œä¾èµ–å…³ç³»",
            "4. å¯èƒ½çš„JOINæ“ä½œåœºæ™¯",
            "",
            "è¯·è¿”å›JSONæ ¼å¼ç»“æœï¼š",
            """{
    "relationships": [
        {
            "type": "foreign_key",
            "source_table": "table1",
            "source_field": "user_id", 
            "target_table": "table2",
            "target_field": "id",
            "confidence": 0.9,
            "reasoning": "å¤–é”®å…³ç³»è¯´æ˜"
        }
    ],
    "business_relationships": [
        {
            "tables": ["table1", "table2"],
            "relationship_type": "one_to_many",
            "business_meaning": "ä¸šåŠ¡å…³ç³»è¯´æ˜",
            "confidence": 0.8
        }
    ],
    "join_recommendations": [
        {
            "tables": ["table1", "table2"],
            "join_condition": "table1.id = table2.user_id",
            "use_case": "ä½¿ç”¨åœºæ™¯è¯´æ˜"
        }
    ],
    "confidence": 0.8,
    "insights": ["å…³ç³»æ´å¯Ÿ1", "å…³ç³»æ´å¯Ÿ2"]
}"""
        ])
        
        return "\n".join(prompt_parts)
    
    def _parse_quality_analysis(self, response: str) -> Optional[Dict[str, Any]]:
        """è§£ææ•°æ®è´¨é‡åˆ†æç»“æœ"""
        try:
            json_str = self._extract_json_from_response(response)
            if not json_str:
                return None
            
            result = json.loads(json_str)
            
            # éªŒè¯å¿…éœ€å­—æ®µ
            required_fields = ['overall_score', 'quality_dimensions']
            for field in required_fields:
                if field not in result:
                    self.logger.error(f"è´¨é‡åˆ†æç»“æœç¼ºå°‘å­—æ®µ: {field}")
                    return None
            
            return result
            
        except json.JSONDecodeError as e:
            self.logger.error(f"è´¨é‡åˆ†æç»“æœJSONè§£æå¤±è´¥: {e}")
            return None
        except Exception as e:
            self.logger.error(f"è´¨é‡åˆ†æç»“æœè§£æå¼‚å¸¸: {e}")
            return None
    
    def _parse_relationship_analysis(self, response: str) -> Optional[Dict[str, Any]]:
        """è§£æå…³ç³»åˆ†æç»“æœ"""
        try:
            self.logger.info(f"å¼€å§‹è§£æå…³ç³»åˆ†æå“åº”ï¼Œé•¿åº¦: {len(response)}")
            
            json_str = self._extract_json_from_response(response)
            if not json_str:
                self.logger.error("æ— æ³•ä»å“åº”ä¸­æå–JSONå­—ç¬¦ä¸²")
                self.logger.debug(f"å“åº”å†…å®¹å‰500å­—ç¬¦: {response[:500]}")
                return self._create_fallback_relationship_result()
            
            self.logger.info(f"æå–çš„JSONé•¿åº¦: {len(json_str)}")
            
            result = json.loads(json_str)
            
            # éªŒè¯å¿…éœ€å­—æ®µ
            required_fields = ['relationships']
            for field in required_fields:
                if field not in result:
                    self.logger.error(f"å…³ç³»åˆ†æç»“æœç¼ºå°‘å­—æ®µ: {field}")
                    # å°è¯•ä¿®å¤ç¼ºå¤±å­—æ®µ
                    result = self._fix_missing_relationship_fields(result)
                    break
            
            self.logger.info("å…³ç³»åˆ†æç»“æœè§£ææˆåŠŸ")
            return result
            
        except json.JSONDecodeError as e:
            self.logger.error(f"å…³ç³»åˆ†æç»“æœJSONè§£æå¤±è´¥: {e}")
            self.logger.debug(f"å¤±è´¥çš„JSONå­—ç¬¦ä¸²: {json_str[:200] if json_str else 'None'}")
            
            # å°è¯•ä»åŸå§‹å“åº”ä¸­æå–éƒ¨åˆ†ä¿¡æ¯
            return self._extract_partial_relationship_info(response)
            
        except Exception as e:
            self.logger.error(f"å…³ç³»åˆ†æç»“æœè§£æå¼‚å¸¸: {e}")
            import traceback
            self.logger.debug(f"å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
            return self._create_fallback_relationship_result()
    
    def _create_fallback_relationship_result(self) -> Dict[str, Any]:
        """åˆ›å»ºåå¤‡å…³ç³»åˆ†æç»“æœ"""
        return {
            "relationships": [],
            "business_relationships": [],
            "join_recommendations": [],
            "confidence": 0.3,
            "insights": ["å…³ç³»åˆ†æé‡åˆ°è§£æé—®é¢˜ï¼Œè¿”å›åŸºç¡€ç»“æœ"],
            "parsing_status": "fallback"
        }
    
    def _fix_missing_relationship_fields(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """ä¿®å¤ç¼ºå¤±çš„å…³ç³»å­—æ®µ"""
        if 'relationships' not in result:
            result['relationships'] = []
        
        if 'business_relationships' not in result:
            result['business_relationships'] = []
        
        if 'join_recommendations' not in result:
            result['join_recommendations'] = []
        
        if 'confidence' not in result:
            result['confidence'] = 0.5
        
        if 'insights' not in result:
            result['insights'] = ["å­—æ®µä¿®å¤åçš„å…³ç³»åˆ†æç»“æœ"]
        
        return result
    
    def _extract_partial_relationship_info(self, response: str) -> Optional[Dict[str, Any]]:
        """ä»åŸå§‹å“åº”ä¸­æå–éƒ¨åˆ†å…³ç³»ä¿¡æ¯"""
        try:
            import re
            
            # å°è¯•ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å…³ç³»ä¿¡æ¯
            relationships = []
            insights = []
            
            # æŸ¥æ‰¾å¯èƒ½çš„è¡¨å…³ç³»æè¿°
            table_patterns = [
                r'(\w+)\s*å’Œ\s*(\w+).*å…³ç³»',
                r'(\w+)\s*â†’\s*(\w+)',
                r'(\w+)\s*å…³è”\s*(\w+)'
            ]
            
            for pattern in table_patterns:
                matches = re.findall(pattern, response, re.IGNORECASE)
                for match in matches:
                    relationships.append({
                        "type": "inferred",
                        "source_table": match[0],
                        "target_table": match[1],
                        "confidence": 0.4,
                        "reasoning": "ä»æ–‡æœ¬ä¸­æ¨æ–­çš„å…³ç³»"
                    })
            
            # æå–æ´å¯Ÿä¿¡æ¯
            insight_patterns = [
                r'å»ºè®®.*?[ã€‚\n]',
                r'å‘ç°.*?[ã€‚\n]',
                r'å¯ä»¥.*?[ã€‚\n]'
            ]
            
            for pattern in insight_patterns:
                matches = re.findall(pattern, response, re.DOTALL)
                insights.extend(matches[:3])  # æœ€å¤š3ä¸ªæ´å¯Ÿ
            
            if not insights:
                insights = ["ä»æ–‡æœ¬åˆ†æä¸­æå–çš„åŸºç¡€å…³ç³»ä¿¡æ¯"]
            
            return {
                "relationships": relationships,
                "business_relationships": [],
                "join_recommendations": [],
                "confidence": 0.4,
                "insights": insights,
                "parsing_status": "partial_extraction"
            }
            
        except Exception as e:
            self.logger.error(f"éƒ¨åˆ†ä¿¡æ¯æå–å¼‚å¸¸: {e}")
            return self._create_fallback_relationship_result()
    
    def _extract_json_from_response(self, response: str) -> Optional[str]:
        """ä»å“åº”ä¸­æå–JSONå­—ç¬¦ä¸²"""
        try:
            response = response.strip()
            
            # ç§»é™¤markdownæ ‡è®°
            if response.startswith('```'):
                lines = response.split('\n')
                if lines[0].startswith('```'):
                    lines = lines[1:]
                if lines and lines[-1].strip() == '```':
                    lines = lines[:-1]
                response = '\n'.join(lines)
            
            # å°è¯•å¤šç§JSONæå–ç­–ç•¥
            
            # ç­–ç•¥1: æŸ¥æ‰¾å®Œæ•´çš„JSONå¯¹è±¡ï¼ˆå¹³è¡¡å¤§æ‹¬å·ï¼‰
            json_str = self._extract_balanced_json(response)
            if json_str and self._is_valid_json(json_str):
                return json_str
            
            # ç­–ç•¥2: æŸ¥æ‰¾ç¬¬ä¸€ä¸ªJSONå¯¹è±¡åˆ°æœ€åä¸€ä¸ªå¤§æ‹¬å·
            start = response.find('{')
            if start >= 0:
                # ä»ç¬¬ä¸€ä¸ª{å¼€å§‹ï¼Œæ‰¾åˆ°åŒ¹é…çš„}
                brace_count = 0
                end_pos = start
                
                for i in range(start, len(response)):
                    char = response[i]
                    if char == '{':
                        brace_count += 1
                    elif char == '}':
                        brace_count -= 1
                        if brace_count == 0:
                            end_pos = i
                            break
                
                if brace_count == 0:
                    json_str = response[start:end_pos+1]
                    if self._is_valid_json(json_str):
                        return json_str
            
            # ç­–ç•¥3: å°è¯•ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜
            cleaned_response = self._clean_json_response(response)
            if cleaned_response and self._is_valid_json(cleaned_response):
                return cleaned_response
            
            return None
            
        except Exception as e:
            self.logger.error(f"JSONæå–å¼‚å¸¸: {e}")
            return None
    
    def _extract_balanced_json(self, text: str) -> Optional[str]:
        """æå–å¹³è¡¡çš„JSONå¯¹è±¡"""
        try:
            start = text.find('{')
            if start == -1:
                return None
            
            brace_count = 0
            in_string = False
            escaped = False
            
            for i in range(start, len(text)):
                char = text[i]
                
                if escaped:
                    escaped = False
                    continue
                
                if char == '\\':
                    escaped = True
                    continue
                
                if char == '"' and not escaped:
                    in_string = not in_string
                    continue
                
                if not in_string:
                    if char == '{':
                        brace_count += 1
                    elif char == '}':
                        brace_count -= 1
                        
                        if brace_count == 0:
                            return text[start:i+1]
            
            return None
            
        except Exception as e:
            self.logger.error(f"å¹³è¡¡JSONæå–å¼‚å¸¸: {e}")
            return None
    
    def _clean_json_response(self, response: str) -> Optional[str]:
        """æ¸…ç†JSONå“åº”ä¸­çš„å¸¸è§é—®é¢˜"""
        try:
            import re
            
            # ç§»é™¤æ³¨é‡Š
            response = re.sub(r'//.*?$', '', response, flags=re.MULTILINE)
            response = re.sub(r'/\*.*?\*/', '', response, flags=re.DOTALL)
            
            # æŸ¥æ‰¾JSONå¯¹è±¡
            start = response.find('{')
            if start == -1:
                return None
            
            # ç§»é™¤trailing comma
            response = re.sub(r',\s*}', '}', response)
            response = re.sub(r',\s*]', ']', response)
            
            # æå–JSONéƒ¨åˆ†
            json_str = self._extract_balanced_json(response[start:])
            
            return json_str
            
        except Exception as e:
            self.logger.error(f"JSONæ¸…ç†å¼‚å¸¸: {e}")
            return None
    
    def _is_valid_json(self, json_str: str) -> bool:
        """éªŒè¯JSONå­—ç¬¦ä¸²æ˜¯å¦æœ‰æ•ˆ"""
        try:
            json.loads(json_str)
            return True
        except (json.JSONDecodeError, TypeError, ValueError):
            return False
    
    def _generate_comprehensive_insights(self, comprehensive_result: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆç»¼åˆæ´å¯Ÿ"""
        insights = []
        components = comprehensive_result.get("components", {})
        
        # ç»“æ„åˆ†ææ´å¯Ÿ
        if "structure" in components:
            structure = components["structure"]
            table_count = structure.get("total_tables", 0)
            insights.append(f"æ•°æ®æºåŒ…å« {table_count} ä¸ªè¡¨")
            
            if "summary" in structure:
                summary = structure["summary"]
                avg_columns = summary.get("avg_columns_per_table", 0)
                insights.append(f"å¹³å‡æ¯è¡¨ {avg_columns:.1f} ä¸ªå­—æ®µ")
        
        # è´¨é‡åˆ†ææ´å¯Ÿ
        if "quality" in components:
            quality = components["quality"]
            overall_score = quality.get("overall_score", 0)
            insights.append(f"æ•°æ®è´¨é‡è¯„åˆ†: {overall_score:.1f}")
        
        # å…³ç³»åˆ†ææ´å¯Ÿ
        if "relationships" in components:
            relationships = components["relationships"]
            rel_count = len(relationships.get("relationships", []))
            if rel_count > 0:
                insights.append(f"å‘ç° {rel_count} ä¸ªæ½œåœ¨è¡¨å…³ç³»")
        
        insights.append("ç»¼åˆæ•°æ®åˆ†æå®Œæˆ")
        return insights
    
    def _generate_comprehensive_summary(self, comprehensive_result: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆæ‘˜è¦"""
        components = comprehensive_result.get("components", {})
        
        summary = {
            "analysis_completed": len(components),
            "total_components": 3  # ç»“æ„ã€è´¨é‡ã€å…³ç³»
        }
        
        if "structure" in components:
            structure = components["structure"]
            summary["tables_analyzed"] = structure.get("total_tables", 0)
            summary["total_columns"] = structure.get("total_columns", 0)
        
        if "quality" in components:
            quality = components["quality"]
            summary["quality_score"] = quality.get("overall_score", 0)
        
        if "relationships" in components:
            relationships = components["relationships"]
            summary["relationships_found"] = len(relationships.get("relationships", []))
        
        return summary