"""
çº¯æ•°æ®åº“é©±åŠ¨çš„LLMç®¡ç†å™¨ - React Agentç³»ç»Ÿæ ¸å¿ƒ
ä»æ•°æ®åº“è¯»å– LLM æœåŠ¡å™¨/æ¨¡å‹ï¼Œæ”¯æŒæŒ‰ä»»åŠ¡é˜¶æ®µä¸å¤æ‚åº¦çš„ç­–ç•¥åŒ–é€‰æ‹©
"""

import logging
from typing import Dict, List, Optional, Any
from datetime import datetime

from .types import TaskRequirement, ModelSelection, LLMExecutionContext
from app.db.session import get_db_session
from app.crud.crud_llm_server import crud_llm_server
from app.crud.crud_llm_model import crud_llm_model
from app.models.llm_server import LLMModel, ModelType

logger = logging.getLogger(__name__)


class PureDatabaseLLMManager:
    """çº¯æ•°æ®åº“é©±åŠ¨çš„LLMç®¡ç†å™¨"""
    
    def __init__(self):
        self.is_initialized = False
    
    async def initialize(self):
        """åˆå§‹åŒ–ç®¡ç†å™¨"""
        if not self.is_initialized:
            logger.info("åˆå§‹åŒ–çº¯æ•°æ®åº“LLMç®¡ç†å™¨")
            self.is_initialized = True
    
    async def select_best_model_for_user(
        self,
        user_id: str,
        task_type: str,
        complexity: str = "medium",
        constraints: Optional[Dict[str, Any]] = None,
        agent_id: Optional[str] = None,
        stage: Optional[str] = None,
        output_kind: Optional[str] = None,
        tool_name: Optional[str] = None
    ) -> Dict[str, Any]:
        """ä¸ºç”¨æˆ·é€‰æ‹©æœ€ä½³æ¨¡å‹"""
        await self.initialize()

        constraints = constraints or {}
        need_json = constraints.get("json") is True

        # è®°å½•æ¨¡å‹é€‰æ‹©ä¸Šä¸‹æ–‡
        context = {
            "user_id": user_id,
            "task_type": task_type,
            "stage": stage,
            "complexity": complexity,
            "output_kind": output_kind,
            "tool_name": tool_name,
            "need_json": need_json,
            "agent_id": agent_id
        }

        logger.info(f"ğŸ¤– [ModelSelection] å¼€å§‹æ¨¡å‹é€‰æ‹©: {context}")

        # ç­–ç•¥ï¼šæ™ºèƒ½ç¡®å®šæœŸæœ›çš„æ¨¡å‹ç±»å‹ï¼ˆdefault/thinkï¼‰
        desired_type = ModelType.DEFAULT.value
        strategy_reasons = []

        # åˆ†æé˜¶æ®µç”¨thinkæ¨¡å‹ - éœ€è¦æ·±åº¦æ€è€ƒå’Œè§„åˆ’
        if task_type in ("plan", "finalize"):
            desired_type = ModelType.THINK.value
            strategy_reasons.append("åˆ†æè§„åˆ’é˜¶æ®µ")
        elif stage in ("plan", "finalize", "think", "analysis"):
            desired_type = ModelType.THINK.value
            strategy_reasons.append(f"æ·±åº¦åˆ†æé˜¶æ®µ({stage})")

        # SQLç”Ÿæˆæ ¹æ®å¤æ‚åº¦æ™ºèƒ½é€‰æ‹©
        elif tool_name == "sql.draft" or task_type == "sql_generation":
            # ç®€å•ç»Ÿè®¡ç”¨defaultï¼Œå¤æ‚åˆ†æç”¨think
            context_info = str(context or {}).lower()
            if (complexity in ("low", "medium") and
                any(word in context_info for word in ["ç»Ÿè®¡", "è®¡æ•°", "count", "sum", "æ€»æ•°"])):
                desired_type = ModelType.DEFAULT.value
                strategy_reasons.append("ç®€å•SQLç”Ÿæˆä»»åŠ¡")
            else:
                desired_type = ModelType.THINK.value
                strategy_reasons.append("å¤æ‚SQLåˆ†æä»»åŠ¡")

        # æ‰§è¡ŒéªŒè¯ç­‰æ“ä½œç”¨defaultæ¨¡å‹
        elif tool_name in ("sql.validate", "sql.execute", "schema.get_columns", "schema.list_tables"):
            desired_type = ModelType.DEFAULT.value
            strategy_reasons.append(f"æ‰§è¡ŒéªŒè¯æ“ä½œ({tool_name})")

        # JSONè¾“å‡ºéœ€æ±‚ï¼šåŒºåˆ†åœºæ™¯
        elif need_json or output_kind == "json":
            if stage in ("plan", "finalize") or task_type in ("plan", "finalize"):
                desired_type = ModelType.THINK.value
                strategy_reasons.append("åˆ†æé˜¶æ®µçš„ç»“æ„åŒ–è¾“å‡º")
            else:
                desired_type = ModelType.DEFAULT.value
                strategy_reasons.append("æ‰§è¡Œé˜¶æ®µçš„ç»“æ„åŒ–è¾“å‡º")

        # é«˜å¤æ‚åº¦ï¼šåªæœ‰åˆ†æä»»åŠ¡æ‰ç”¨think
        elif complexity in ("high", "complex"):
            if task_type in ("analysis", "planning", "reasoning"):
                desired_type = ModelType.THINK.value
                strategy_reasons.append("é«˜å¤æ‚åº¦åˆ†æä»»åŠ¡")
            else:
                desired_type = ModelType.DEFAULT.value
                strategy_reasons.append("é«˜å¤æ‚åº¦æ‰§è¡Œä»»åŠ¡")
        else:
            strategy_reasons.append("é»˜è®¤æ‰§è¡Œä»»åŠ¡")

        logger.info(f"ğŸ¯ [ModelSelection] é€‰æ‹©ç­–ç•¥: {desired_type} æ¨¡å‹ï¼ŒåŸå› : {'; '.join(strategy_reasons)}")

        # æŸ¥è¯¢ DB ä¸­æ´»è·ƒä¸”å¥åº·çš„æ¨¡å‹ï¼Œä¼˜å…ˆå½“å‰ç”¨æˆ·çš„æœåŠ¡å™¨
        with get_db_session() as db:
            # å¦‚æœuser_idä¸ºNoneæˆ–"system"ï¼Œç›´æ¥æŸ¥è¯¢å…¨å±€å¥åº·æ¨¡å‹ï¼Œé¿å…UUIDè½¬æ¢é”™è¯¯
            if not user_id or user_id == "system":
                logger.info("ğŸ”„ [ModelSelection] æœªæä¾›ç”¨æˆ·IDæˆ–ç³»ç»Ÿæ¨¡å¼ï¼Œç›´æ¥æŸ¥è¯¢å…¨å±€å¥åº·æ¨¡å‹")
                models = db.query(LLMModel).join(LLMModel.server).filter(
                    LLMModel.is_active == True,
                    LLMModel.is_healthy == True,
                    LLMModel.model_type == desired_type,
                    LLMModel.server.has(is_active=True, is_healthy=True)
                ).order_by(LLMModel.priority.asc(), LLMModel.id.asc()).all()

                user_models_count = 0  # æ— ç”¨æˆ·IDæ—¶æ²¡æœ‰ä¸“å±æ¨¡å‹
            else:
                # å…ˆæ‰¾è¯¥ç”¨æˆ·çš„å¥åº·æœåŠ¡å™¨ä¸Šçš„å¥åº·æ¨¡å‹
                models = db.query(LLMModel).join(LLMModel.server).filter(
                    LLMModel.is_active == True,
                    LLMModel.is_healthy == True,
                    LLMModel.model_type == desired_type,
                    LLMModel.server.has(is_active=True, is_healthy=True, user_id=user_id)
                ).order_by(LLMModel.priority.asc(), LLMModel.id.asc()).all()

                # è®°å½•åˆå§‹æŸ¥è¯¢ç»“æœ
                user_models_count = len(models)

            if models and user_id and user_id != "system":
                logger.info(f"ğŸ” [ModelSelection] ç”¨æˆ·ä¸“å±æ¨¡å‹æ‰¾åˆ° {user_models_count} ä¸ª")
            elif not models and user_id and user_id != "system":
                logger.info("ğŸ”„ [ModelSelection] ç”¨æˆ·ä¸“å±æ¨¡å‹æœªæ‰¾åˆ°ï¼Œå›é€€åˆ°å…¨å±€å¥åº·æ¨¡å‹")

            # è‹¥è¯¥ç”¨æˆ·æ— å¯ç”¨æ¨¡å‹ï¼Œå›é€€åˆ°ä»»æ„å¥åº·æœåŠ¡å™¨
            if not models:
                models = db.query(LLMModel).join(LLMModel.server).filter(
                    LLMModel.is_active == True,
                    LLMModel.is_healthy == True,
                    LLMModel.model_type == desired_type,
                    LLMModel.server.has(is_active=True, is_healthy=True)
                ).order_by(LLMModel.priority.asc(), LLMModel.id.asc()).all()

                global_models_count = len(models)
                if models:
                    logger.info(f"ğŸŒ [ModelSelection] å…¨å±€å¥åº·æ¨¡å‹æ‰¾åˆ° {global_models_count} ä¸ª")
                else:
                    logger.warning(f"âš ï¸ [ModelSelection] æŒ‡å®šç±»å‹({desired_type})å¥åº·æ¨¡å‹æœªæ‰¾åˆ°ï¼Œè¿›ä¸€æ­¥å›é€€")

            if not models:
                logger.warning(f"ğŸš¨ [ModelSelection] æ²¡æœ‰å¥åº·çš„{desired_type}æ¨¡å‹ï¼Œå›é€€åˆ°ä»»æ„æ´»è·ƒæ¨¡å‹")
                models = db.query(LLMModel).join(LLMModel.server).filter(
                    LLMModel.is_active == True,
                    LLMModel.server.has(is_active=True)
                ).order_by(LLMModel.priority.asc(), LLMModel.id.asc()).all()

                fallback_count = len(models)
                if models:
                    logger.warning(f"ğŸ†˜ [ModelSelection] å›é€€æ¨¡å‹æ‰¾åˆ° {fallback_count} ä¸ª")
                else:
                    logger.error("ğŸ’¥ [ModelSelection] æ— ä»»ä½•å¯ç”¨æ¨¡å‹!")

            if not models:
                logger.error(f"ğŸ’¥ [ModelSelection] æ¨¡å‹é€‰æ‹©å¤±è´¥ï¼Œcontext: {context}")
                return {
                    "model": None,
                    "provider": None,
                    "confidence": 0.0,
                    "reasoning": "æ²¡æœ‰å¯ç”¨çš„LLMæ¨¡å‹",
                    "fallback_used": True,
                    "selection_context": context
                }

            m = models[0]
            s = m.server

            # è¯¦ç»†çš„é€‰æ‹©ç»“æœæ—¥å¿—
            selection_info = {
                "model_id": m.id,
                "model_name": m.name,
                "model_type": m.model_type,
                "server_id": s.id,
                "server_name": s.name,
                "provider_type": s.provider_type,
                "is_healthy": m.is_healthy,
                "is_user_owned": s.user_id == user_id,
                "priority": m.priority
            }

            confidence = 0.9 if m.model_type == ModelType.THINK.value else 0.8
            reasoning = f"é€‰æ‹©{m.model_type}æ¨¡å‹: {m.name} @ {s.name}"

            logger.info(f"âœ… [ModelSelection] æ¨¡å‹é€‰æ‹©å®Œæˆ: {selection_info}, confidence={confidence}")

            result = {
                "model_id": m.id,
                "server_id": s.id,
                "model": m.name,
                "provider": s.provider_type,
                "model_type": m.model_type,
                "server_name": s.name,
                "confidence": confidence,
                "reasoning": reasoning,
                "selection_context": context,
                "selection_info": selection_info,
                "fallback_used": user_models_count == 0
            }

            return result
    
    async def get_user_available_models(
        self,
        user_id: str,
        model_type: Optional[str] = None,
        provider_name: Optional[str] = None
    ) -> Dict[str, Any]:
        """è·å–ç”¨æˆ·å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
        await self.initialize()
        with get_db_session() as db:
            q = db.query(LLMModel).join(LLMModel.server).filter(
                LLMModel.is_active == True,
                LLMModel.server.has(is_active=True)
            )
            if provider_name:
                q = q.filter(LLMModel.provider_name == provider_name)
            models = q.order_by(LLMModel.priority.asc()).all()
            available = {m.name: {
                "provider": m.provider_name,
                "type": m.model_type,
                "server": m.server.name,
                "healthy": m.is_healthy
            } for m in models}
            return {
                "available_models": available,
                "total_count": len(available),
                "user_id": user_id
            }
    
    async def get_user_preferences(self, user_id: str) -> Optional[Dict[str, Any]]:
        """è·å–ç”¨æˆ·LLMåå¥½"""
        # ç®€å•å®ç°ï¼Œå®é™…åº”ä»æ•°æ®åº“è·å–
        return {
            "preferred_provider": "anthropic",
            "max_cost_per_request": 0.05,
            "preferred_capabilities": ["reasoning", "analysis"]
        }
    
    def record_usage_feedback(
        self,
        user_id: str,
        model: str,
        provider: str,
        success: bool,
        satisfaction_score: float,
        actual_cost: Optional[float] = None,
        actual_latency: Optional[int] = None,
        agent_id: Optional[str] = None,
        task_type: Optional[str] = None
    ):
        """è®°å½•ç”¨æˆ·ä½¿ç”¨åé¦ˆ"""
        logger.info(f"è®°å½•ç”¨æˆ·åé¦ˆ: {user_id}, æ¨¡å‹: {model}, æ»¡æ„åº¦: {satisfaction_score}")
        # å®é™…å®ç°åº”å­˜å‚¨åˆ°æ•°æ®åº“
    
    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        return {
            "status": "healthy",
            "healthy": True,
            "manager_type": "pure_database_driven",
            "available_models": len(self.available_models),
            "initialized": self.is_initialized
        }
    
    def get_service_info(self) -> Dict[str, Any]:
        """è·å–æœåŠ¡ä¿¡æ¯"""
        with get_db_session() as db:
            total_models = db.query(LLMModel).count()
            return {
                "service_type": "pure_database_llm_manager",
                "version": "1.1.0",
                "capabilities": ["model_selection", "user_preferences", "usage_tracking"],
                "supported_providers": ["anthropic", "openai", "custom"],
                "total_models": total_models
            }


# å…¨å±€ç®¡ç†å™¨å®ä¾‹
_pure_llm_manager = None

def get_pure_llm_manager() -> PureDatabaseLLMManager:
    """è·å–çº¯æ•°æ®åº“LLMç®¡ç†å™¨å®ä¾‹"""
    global _pure_llm_manager
    if _pure_llm_manager is None:
        _pure_llm_manager = PureDatabaseLLMManager()
    return _pure_llm_manager


# ä¾¿æ·æ¥å£å‡½æ•°
async def select_model_for_user(
    user_id: str,
    task_type: str,
    complexity: str = "medium",
    constraints: Optional[Dict[str, Any]] = None,
    agent_id: Optional[str] = None
) -> Dict[str, Any]:
    """ä¸ºç”¨æˆ·é€‰æ‹©æ¨¡å‹"""
    manager = get_pure_llm_manager()
    return await manager.select_best_model_for_user(
        user_id, task_type, complexity, constraints, agent_id
    )


async def ask_agent(
    user_id: str,
    question: str,
    agent_type: str = "general",
    context: Optional[str] = None,
    task_type: str = "general",
    complexity: str = "medium"
) -> str:
    """Agentå‹å¥½çš„é—®ç­”æ¥å£"""
    try:
        # å»¶è¿Ÿå¯¼å…¥ä»¥é¿å…å¾ªç¯å¯¼å…¥
        from .model_executor import get_model_executor
        
        # è·å–æ¨¡å‹æ‰§è¡Œå™¨
        executor = get_model_executor()
        
        # æ„å»ºä»»åŠ¡éœ€æ±‚
        task_requirement = TaskRequirement(
            complexity=complexity,
            domain=task_type,
            context_length=len(question) + (len(context) if context else 0),
            response_format="json",
            quality_level="high" if complexity in ["high", "complex"] else "medium"
        )
        
        # æ„å»ºå®Œæ•´çš„æç¤ºè¯
        full_prompt = question
        if context:
            full_prompt = f"ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š{context}\n\né—®é¢˜ï¼š{question}"
        
        # æ‰§è¡Œæ¨¡å‹è°ƒç”¨ï¼ˆé»˜è®¤å¯ç”¨JSONç»“æ„åŒ–è¾“å‡ºï¼‰
        result = await executor.execute_with_auto_selection(
            user_id=user_id,
            prompt=full_prompt,
            task_requirement=task_requirement,
            response_format={"type": "json_object"}
        )
        
        if result.get("success"):
            return result.get("result", "")
        else:
            logger.error(f"LLMè°ƒç”¨å¤±è´¥: {result.get('error', 'Unknown error')}")
            return ""
            
    except Exception as e:
        logger.error(f"ask_agentè°ƒç”¨å¤±è´¥: {e}")
        return ""
