"""
Infrastructureå±‚ - Celeryä»»åŠ¡å®šä¹‰

åŸºäºDDDæ¶æ„çš„Celeryä»»åŠ¡å®šä¹‰ï¼Œä½¿ç”¨æ–°çš„TaskExecutionServiceèƒ½åŠ›
"""

import asyncio
import logging
import os
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple
from uuid import UUID

from celery import Task as CeleryTask
from celery.schedules import crontab
from sqlalchemy.orm import Session

from app.db.session import SessionLocal
from app.core.config import settings
from app.core.container import container
from app.models.task import Task, TaskExecution, TaskStatus
from app.models.report_history import ReportHistory
from app.services.application.placeholder.placeholder_service import (
    PlaceholderApplicationService as PlaceholderProcessingSystem,
)
from app.services.infrastructure.notification.notification_service import NotificationService
from app.services.infrastructure.storage.hybrid_storage_service import (
    get_hybrid_storage_service,
)
from app.services.infrastructure.task_queue.celery_config import celery_app
from app.services.infrastructure.agents.messaging import (
    TaskMessageOrchestrator,
    PromptConfigManager
)
from app.services.infrastructure.task_queue.progress_recorder import TaskProgressRecorder
from app.services.infrastructure.websocket.pipeline_notifications import (
    PipelineTaskStatus,
)
from app.utils.time_context import TimeContextManager
from app.utils.json_utils import convert_for_json

logger = logging.getLogger(__name__)

def run_async(coro):
    """
    åœ¨åŒæ­¥ä¸Šä¸‹æ–‡ä¸­å®‰å…¨åœ°æ‰§è¡Œå¼‚æ­¥ä»£ç ã€‚

    è§„åˆ™ï¼š
    - è‹¥å½“å‰çº¿ç¨‹æ²¡æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯ï¼šç›´æ¥ asyncio.run(coro)
    - è‹¥å·²æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯ï¼ˆå¦‚ Celery/WSGI ç¯å¢ƒï¼‰ï¼šåœ¨æ–°çº¿ç¨‹åˆ›å»ºç‹¬ç«‹äº‹ä»¶å¾ªç¯æ‰§è¡Œå¹¶è¿”å›ç»“æœ
    """
    try:
        asyncio.get_running_loop()
    except RuntimeError:
        # å½“å‰çº¿ç¨‹æ²¡æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯
        return asyncio.run(coro)
    else:
        # å½“å‰çº¿ç¨‹å·²å­˜åœ¨äº‹ä»¶å¾ªç¯ï¼Œè½¬ç§»åˆ°æ–°çº¿ç¨‹æ‰§è¡Œ
        import threading
        from queue import Queue

        result_queue: "Queue[Tuple[bool, Any]]" = Queue(maxsize=1)

        def _runner():
            try:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                res = loop.run_until_complete(coro)
                result_queue.put((True, res))
            except Exception as exc:  # noqa: BLE001
                result_queue.put((False, exc))
            finally:
                try:
                    loop.close()
                except Exception:
                    pass

        t = threading.Thread(target=_runner, daemon=True)
        t.start()
        ok, payload = result_queue.get()
        if ok:
            return payload
        raise payload

class DatabaseTask(CeleryTask):
    """å¸¦æ•°æ®åº“ä¼šè¯çš„åŸºç¡€ä»»åŠ¡ç±»"""
    
    def __call__(self, *args, **kwargs):
        """æ‰§è¡Œä»»åŠ¡æ—¶è‡ªåŠ¨ç®¡ç†æ•°æ®åº“ä¼šè¯"""
        with SessionLocal() as db:
            return self.run_with_db(db, *args, **kwargs)
    
    def run_with_db(self, db: Session, *args, **kwargs):
        """å­ç±»éœ€è¦å®ç°çš„æ–¹æ³•"""
        return self.run(db, *args, **kwargs)

@celery_app.task(bind=True, base=DatabaseTask, name='tasks.infrastructure.execute_report_task')
def execute_report_task(self, db: Session, task_id: int, execution_context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    æ‰§è¡ŒæŠ¥å‘Šç”Ÿæˆä»»åŠ¡ - ä½¿ç”¨æ–°çš„TaskExecutionService

    Args:
        task_id: ä»»åŠ¡ID
        execution_context: æ‰§è¡Œä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰

    Returns:
        Dict: æ‰§è¡Œç»“æœ
    """
    task_execution_id = None
    notification_service = NotificationService()

    # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«æ’¤é”€çš„è¾…åŠ©å‡½æ•°
    def check_if_cancelled():
        """æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«æ’¤é”€"""
        try:
            # æ–¹æ³•1: æ£€æŸ¥Celeryçš„æ’¤é”€çŠ¶æ€
            from celery.result import AsyncResult
            result = AsyncResult(self.request.id)
            if result.state == 'REVOKED':
                logger.info(f"Task {task_id} detected as REVOKED via Celery state")
                raise Exception("ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ")

            # æ–¹æ³•2: æ£€æŸ¥æ•°æ®åº“ä¸­çš„æ‰§è¡ŒçŠ¶æ€
            if task_execution_id:
                exec_record = db.query(TaskExecution).filter(TaskExecution.id == task_execution_id).first()
                if exec_record and exec_record.execution_status == TaskStatus.CANCELLED:
                    logger.info(f"Task {task_id} detected as CANCELLED in database")
                    raise Exception("ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ")
        except Exception as e:
            if "å–æ¶ˆ" in str(e) or "cancelled" in str(e).lower():
                raise

    try:
        # 1. è·å–ä»»åŠ¡ä¿¡æ¯
        task = db.query(Task).filter(Task.id == task_id).first()
        if not task:
            raise ValueError(f"Task {task_id} not found")
        
        if not task.is_active:
            logger.info(f"Task {task_id} is not active, skipping execution")
            return {"status": "skipped", "reason": "task_inactive"}
        
        # 2. åˆ›å»ºä»»åŠ¡æ‰§è¡Œè®°å½•
        task_execution = TaskExecution(
            task_id=task_id,
            execution_status=TaskStatus.PROCESSING,
            workflow_type=task.workflow_type,
            started_at=datetime.utcnow(),
            celery_task_id=self.request.id,
            execution_context=execution_context or {},
            progress_percentage=0
        )
        db.add(task_execution)
        db.commit()
        task_execution_id = task_execution.id

        progress_recorder = TaskProgressRecorder(
            db=db,
            task=task,
            task_execution=task_execution,
        )
        # âœ… åˆå§‹åŒ–æ¶ˆæ¯ç¼–æ’å™¨
        msg_orchestrator = TaskMessageOrchestrator()
        config = PromptConfigManager()

        progress_recorder.start(msg_orchestrator.task_started())

        # å®šä¹‰è¿›åº¦æ›´æ–°å‡½æ•°
        def update_progress(
            percentage: int,
            message: str = "",
            *,
            stage: Optional[str] = None,
            pipeline_status: PipelineTaskStatus = PipelineTaskStatus.ANALYZING,
            status: str = "running",
            placeholder: Optional[str] = None,
            details: Optional[Dict[str, Any]] = None,
            error: Optional[str] = None,
            record_only: bool = False,
        ):
            progress_recorder.update(
                percentage,
                message,
                stage=stage,
                pipeline_status=pipeline_status,
                status=status,
                placeholder=placeholder,
                details=details,
                error=error,
                record_only=record_only,
            )

        # åˆå§‹åŒ–é˜¶æ®µ
        update_progress(
            5,
            "ä»»åŠ¡åˆå§‹åŒ–å®Œæˆ",
            stage="initialization",
            pipeline_status=PipelineTaskStatus.SCANNING,
        )

        # æ£€æŸ¥æ˜¯å¦è¢«å–æ¶ˆ
        check_if_cancelled()

        # 3. æ›´æ–°ä»»åŠ¡çŠ¶æ€
        task.status = TaskStatus.PROCESSING
        task.execution_count += 1
        task.last_execution_at = datetime.utcnow()
        db.commit()

        # 4. ğŸ†• åˆå§‹åŒ– Schema Contextï¼ˆä¸€æ¬¡æ€§è·å–æ‰€æœ‰è¡¨ç»“æ„ï¼‰
        schema_context_retriever = None
        try:
            from app.services.infrastructure.agents.context_retriever import (
                create_schema_context_retriever
            )
            from app.services.infrastructure.agents.tools.table_detector import (
                create_table_detector
            )
            from app.models.data_source import DataSource

            logger.info(msg_orchestrator.schema_init_log(str(task.data_source_id)))

            update_progress(
                8,
                "æ­£åœ¨åˆå§‹åŒ–æ•°æ®è¡¨ç»“æ„ä¸Šä¸‹æ–‡...",
                stage="schema_initialization",
                pipeline_status=PipelineTaskStatus.SCANNING,
            )

            # ğŸ†• è¡¨æ£€æµ‹ä¼˜åŒ–ï¼šæ£€æµ‹æ˜¯å¦ä¸ºå•è¡¨åœºæ™¯
            target_tables_for_optimization = None
            table_detection_enabled = True  # å¯ä»¥é€šè¿‡é…ç½®æ§åˆ¶
            existing_placeholders = []  # åˆå§‹åŒ–å˜é‡ï¼Œé˜²æ­¢ä½œç”¨åŸŸé—®é¢˜

            if table_detection_enabled:
                logger.info("ğŸ” å¼€å§‹è¡¨æ£€æµ‹ä¼˜åŒ–æµç¨‹...")
                try:
                    # è·å–æ¨¡æ¿çš„å ä½ç¬¦
                    from app.crud import template_placeholder as crud_template_placeholder
                    existing_placeholders = crud_template_placeholder.get_by_template(db, str(task.template_id)) or []

                    if existing_placeholders and len(existing_placeholders) > 0:
                        # ä½¿ç”¨è¡¨æ£€æµ‹å™¨åˆ†æ
                        detector = create_table_detector()
                        detection_result = detector.detect_from_placeholders(
                            placeholders=existing_placeholders,
                            template_content=None  # å¯é€‰ï¼šä¼ é€’æ¨¡æ¿å†…å®¹è¾…åŠ©åˆ†æ
                        )

                        logger.info(f"ğŸ“Š è¡¨æ£€æµ‹ç»“æœ: {detection_result.recommendation}")
                        logger.info(
                            f"   - å•è¡¨åœºæ™¯: {detection_result.is_single_table}\n"
                            f"   - ä¸»è¦è¡¨: {detection_result.primary_table}\n"
                            f"   - æ¶‰åŠè¡¨: {detection_result.all_tables}\n"
                            f"   - ç½®ä¿¡åº¦: {detection_result.confidence:.2%}"
                        )

                        # ğŸ”¥ å•è¡¨ä¼˜åŒ–å†³ç­–
                        if detection_result.is_single_table and detection_result.primary_table:
                            target_tables_for_optimization = [detection_result.primary_table]
                            logger.info(
                                f"âœ… å¯ç”¨å•è¡¨ä¼˜åŒ–æ¨¡å¼: åªåŠ è½½è¡¨ '{detection_result.primary_table}' çš„ schema"
                            )
                            update_progress(
                                8,
                                f"æ£€æµ‹åˆ°å•è¡¨åœºæ™¯ï¼Œå¯ç”¨ä¼˜åŒ–æ¨¡å¼ï¼ˆè¡¨ï¼š{detection_result.primary_table}ï¼‰",
                                stage="schema_initialization",
                                pipeline_status=PipelineTaskStatus.SCANNING,
                            )
                        else:
                            logger.info(
                                f"âš ï¸ å¤šè¡¨åœºæ™¯æˆ–æ£€æµ‹ç½®ä¿¡åº¦ä¸è¶³ï¼Œä½¿ç”¨å®Œæ•´SchemaåŠ è½½æ¨¡å¼"
                            )
                            update_progress(
                                8,
                                "æ£€æµ‹åˆ°å¤šè¡¨åœºæ™¯ï¼ŒåŠ è½½å®Œæ•´Schema",
                                stage="schema_initialization",
                                pipeline_status=PipelineTaskStatus.SCANNING,
                            )
                    else:
                        logger.info("âš ï¸ æš‚æ— å ä½ç¬¦æ•°æ®ï¼Œè·³è¿‡è¡¨æ£€æµ‹ä¼˜åŒ–")
                except Exception as detection_error:
                    logger.warning(f"âš ï¸ è¡¨æ£€æµ‹å¤±è´¥ï¼Œé™çº§åˆ°å®Œæ•´SchemaåŠ è½½: {detection_error}", exc_info=True)
                    target_tables_for_optimization = None

            # è·å–æ•°æ®æºé…ç½®
            data_source = db.query(DataSource).filter(DataSource.id == task.data_source_id).first()
            if not data_source:
                raise RuntimeError(f"æ•°æ®æº {task.data_source_id} ä¸å­˜åœ¨")

            # æ„å»ºè¿æ¥é…ç½®
            connection_config = data_source.connection_config or {}
            if not connection_config:
                raise RuntimeError(f"æ•°æ®æº {task.data_source_id} ç¼ºå°‘è¿æ¥é…ç½®")

            # ğŸ†• å¯ç”¨é˜¶æ®µæ„ŸçŸ¥çš„æ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç† + å•è¡¨ä¼˜åŒ–
            schema_context_retriever = create_schema_context_retriever(
                data_source_id=str(task.data_source_id),
                connection_config=connection_config,
                container=container,
                top_k=10,  # Task æ‰¹é‡åˆ†æï¼Œå¤šç¼“å­˜ä¸€äº›è¡¨
                inject_as="system",
                enable_stage_aware=True,  # ğŸ”¥ å¯ç”¨é˜¶æ®µæ„ŸçŸ¥
                target_tables=target_tables_for_optimization  # ğŸ”¥ å•è¡¨ä¼˜åŒ–å‚æ•°
            )

            # é¢„åŠ è½½æ‰€æœ‰è¡¨ç»“æ„ï¼ˆç¼“å­˜ï¼‰
            run_async(schema_context_retriever.initialize())

            table_count = len(schema_context_retriever.schema_cache)

            # ğŸ†• å•è¡¨ä¼˜åŒ–æ•ˆæœç»Ÿè®¡
            optimization_info = ""
            if target_tables_for_optimization:
                # ä¼°ç®—èŠ‚çœçš„ tokenï¼ˆå‡è®¾æ¯å¼ è¡¨çº¦ 500 tokensï¼Œæ¯ä¸ªå ä½ç¬¦è°ƒç”¨ 2 æ¬¡ Agentï¼‰
                estimated_full_schema_tokens = table_count * 500
                estimated_optimized_tokens = len(target_tables_for_optimization) * 500
                estimated_saved_tokens_per_call = max(0, estimated_full_schema_tokens - estimated_optimized_tokens)
                total_placeholders = len(existing_placeholders) if existing_placeholders else 0
                estimated_total_saved = estimated_saved_tokens_per_call * 2 * total_placeholders  # 2æ¬¡è°ƒç”¨/å ä½ç¬¦

                optimization_info = (
                    f" | å•è¡¨ä¼˜åŒ–å·²å¯ç”¨ âœ…\n"
                    f"   é¢„è®¡èŠ‚çœ: ~{estimated_total_saved:,} tokens "
                    f"({estimated_saved_tokens_per_call} tokens/è°ƒç”¨ Ã— 2æ¬¡/å ä½ç¬¦ Ã— {total_placeholders}å ä½ç¬¦)"
                )
                logger.info(
                    f"ğŸ“Š å•è¡¨ä¼˜åŒ–æ•ˆæœä¼°ç®—:\n"
                    f"   - å®Œæ•´Schema: {estimated_full_schema_tokens} tokens\n"
                    f"   - ä¼˜åŒ–åSchema: {estimated_optimized_tokens} tokens\n"
                    f"   - æ¯æ¬¡è°ƒç”¨èŠ‚çœ: {estimated_saved_tokens_per_call} tokens\n"
                    f"   - æ€»è®¡èŠ‚çœ: ~{estimated_total_saved:,} tokens"
                )

            logger.info(msg_orchestrator.schema_init_completed(table_count) + optimization_info)

            update_progress(
                9,
                msg_orchestrator.schema_init_progress(table_count),
                stage="schema_initialization",
                pipeline_status=PipelineTaskStatus.SCANNING,
            )

        except Exception as e:
            logger.warning(msg_orchestrator.schema_init_failed(e), exc_info=True)
            # ä¸è¦è®©æ•´ä¸ªä»»åŠ¡å¤±è´¥ï¼Œå…è®¸é™çº§è¿è¡Œï¼ˆAgent å¯èƒ½ä¼šä½¿ç”¨æ—§çš„ schema å·¥å…·æˆ–çŒœæµ‹è¡¨ç»“æ„ï¼‰
            logger.info(msg_orchestrator.schema_init_fallback())

            # åˆ›å»ºä¸€ä¸ªç©ºçš„ schema_context_retriever ä»¥é¿å…åç»­ä»£ç å‡ºé”™
            schema_context_retriever = None

            update_progress(
                9,
                "æ•°æ®è¡¨ç»“æ„åˆå§‹åŒ–å¤±è´¥ï¼Œå°†é™çº§è¿è¡Œ",
                stage="schema_initialization",
                pipeline_status=PipelineTaskStatus.SCANNING,
                error=str(e)
            )

        # 5. åˆå§‹åŒ–æ—¶é—´ä¸Šä¸‹æ–‡å’Œ PlaceholderProcessingSystem
        # Initialize placeholder processing system with schema context
        system = PlaceholderProcessingSystem(
            user_id=str(task.owner_id),
            context_retriever=schema_context_retriever  # ğŸ”¥ ä¼ å…¥ context
        )

        # ğŸ†• è·å–é˜¶æ®µæ„ŸçŸ¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å’Œå·¥å…·è®°å½•å™¨
        state_manager = getattr(schema_context_retriever, 'state_manager', None)
        tool_recorder = None
        if state_manager:
            from app.services.infrastructure.agents.tool_wrapper import ToolResultRecorder
            from app.services.infrastructure.agents.context_manager import ExecutionStage
            tool_recorder = ToolResultRecorder(state_manager)
            logger.info("âœ… ä»»åŠ¡å·²å¯ç”¨é˜¶æ®µæ„ŸçŸ¥ä¸Šä¸‹æ–‡ç®¡ç†å’Œå·¥å…·ç»“æœè®°å½•")

        time_ctx_mgr = TimeContextManager()
        
        # 5. å‡†å¤‡æ‰§è¡Œå‚æ•°
        execution_params = {
            "task_id": task_id,
            "template_id": str(task.template_id),
            "data_source_id": str(task.data_source_id),
            "report_period": task.report_period.value if task.report_period else "monthly",
            "user_id": str(task.owner_id),
            "execution_id": str(task_execution.execution_id),
            "recipients": task.recipients or [],
            "schedule": task.schedule
        }
        
        logger.info(f"Starting task execution for task {task_id} with Agents pipeline")

        # 5. ç”Ÿæˆæ—¶é—´çª—å£ï¼ˆåŸºäºä»»åŠ¡æŠ¥å‘Šå‘¨æœŸï¼‰
        update_progress(
            10,
            "æ­£åœ¨ç”Ÿæˆæ—¶é—´ä¸Šä¸‹æ–‡...",
            stage="time_context",
            pipeline_status=PipelineTaskStatus.SCANNING,
        )
        time_ctx = time_ctx_mgr.generate_time_context(
            report_period=task.report_period.value if task.report_period else "monthly",
            execution_time=datetime.utcnow(),
            schedule=task.schedule,
        )
        time_window = {
            "start": f"{time_ctx.get('period_start_date')} 00:00:00",
            "end": f"{time_ctx.get('period_end_date')} 23:59:59",
        }

        # 6. è¿è¡ŒReActæµæ°´çº¿ï¼ˆç”ŸæˆSQLâ†’æ³¨å…¥æ—¶é—´â†’æ‰§è¡Œâ†’è‡ªä¿®æ­£ï¼‰
        update_progress(
            15,
            "æ­£åœ¨åˆå§‹åŒ–Agentç³»ç»Ÿ...",
            stage="agent_initialization",
        )

        # æ£€æŸ¥æ˜¯å¦è¢«å–æ¶ˆ
        check_if_cancelled()

        run_async(system.initialize())
        events = []

        update_progress(
            20,
            "æ­£åœ¨æ£€æŸ¥å ä½ç¬¦çŠ¶æ€...",
            stage="placeholder_precheck",
        )

        # æ£€æŸ¥æ˜¯å¦è¢«å–æ¶ˆ
        check_if_cancelled()

        # æ™ºèƒ½å¢é‡å ä½ç¬¦è§£æç­–ç•¥
        placeholders_need_analysis = []
        placeholders_ready = []

        try:
            from app.crud import template_placeholder as crud_template_placeholder
            from app.models.template import Template
            from app.services.domain.template.services.template_domain_service import TemplateParser
            import re

            # è·å–æ¨¡æ¿å†…å®¹
            template = db.query(Template).filter(Template.id == task.template_id).first()
            template_content = template.content if template else ""

            # è·å–æ•°æ®åº“ä¸­å·²æœ‰çš„å ä½ç¬¦
            existing_placeholders = crud_template_placeholder.get_by_template(db, str(task.template_id))
            existing_placeholder_names = {ph.placeholder_name for ph in existing_placeholders or []}

            # ä»æ¨¡æ¿å†…å®¹ä¸­æå–å ä½ç¬¦
            content_placeholders = set()
            if template_content:
                # æå– {{...}} æ ¼å¼çš„å ä½ç¬¦
                placeholder_pattern = r'\{\{([^}]+)\}\}'
                matches = re.findall(placeholder_pattern, template_content)
                content_placeholders = {match.strip() for match in matches}

            total_content_placeholders = len(content_placeholders)
            total_existing_placeholders = len(existing_placeholders or [])

            logger.info(msg_orchestrator.placeholder_status_summary(total_content_placeholders, total_existing_placeholders))

            # æ‰¾å‡ºéœ€è¦æ–°å»ºçš„å ä½ç¬¦ï¼ˆåœ¨å†…å®¹ä¸­ä½†ä¸åœ¨æ•°æ®åº“ä¸­ï¼‰
            new_placeholders_to_create = content_placeholders - existing_placeholder_names

            # åˆ›å»ºæ–°å‘ç°çš„å ä½ç¬¦è®°å½•
            if new_placeholders_to_create:
                update_progress(
                    22,
                    msg_orchestrator.placeholders_creating(len(new_placeholders_to_create)),
                    stage="placeholder_precheck",
                    details={"new_placeholders": len(new_placeholders_to_create)},
                )
                logger.info(msg_orchestrator.placeholders_creating_log(len(new_placeholders_to_create)))

                from app.models.template_placeholder import TemplatePlaceholder
                import uuid
                for placeholder_name in new_placeholders_to_create:
                    new_placeholder = TemplatePlaceholder(
                        id=uuid.uuid4(),
                        template_id=task.template_id,
                        placeholder_name=placeholder_name,
                        placeholder_text=placeholder_name,  # ä½¿ç”¨å ä½ç¬¦åç§°ä½œä¸ºé»˜è®¤æ–‡æœ¬
                        placeholder_type="text",  # é»˜è®¤ç±»å‹ï¼Œåç»­åˆ†ææ—¶ä¼šæ›´æ–°
                        content_type="data",  # é»˜è®¤ä¸ºæ•°æ®ç±»å‹
                        agent_analyzed=False,  # å°šæœªåˆ†æ
                        generated_sql=None,
                        sql_validated=False,
                        execution_order=0,  # é»˜è®¤é¡ºåº
                        cache_ttl_hours=24,  # é»˜è®¤ç¼“å­˜24å°æ—¶
                        is_required=True,  # é»˜è®¤ä¸ºå¿…éœ€
                        is_active=True,  # é»˜è®¤æ¿€æ´»
                        confidence_score=0.0,  # åˆå§‹ç½®ä¿¡åº¦
                        created_at=datetime.utcnow()
                    )
                    db.add(new_placeholder)
                db.commit()

                # é‡æ–°è·å–æ‰€æœ‰å ä½ç¬¦
                existing_placeholders = crud_template_placeholder.get_by_template(db, str(task.template_id))

            required_fields: set[str] = set()

            # æ£€æŸ¥æ‰€æœ‰å ä½ç¬¦çš„åˆ†æçŠ¶æ€
            for ph in existing_placeholders or []:
                # æ£€æŸ¥å ä½ç¬¦æ˜¯å¦éœ€è¦é‡æ–°åˆ†æ
                needs_analysis = (
                    not ph.generated_sql or  # æ²¡æœ‰ç”Ÿæˆçš„SQL
                    not ph.sql_validated or  # SQLæœªéªŒè¯é€šè¿‡
                    ph.generated_sql.strip() == ""  # SQLä¸ºç©º
                )

                if needs_analysis:
                    placeholders_need_analysis.append(ph)
                    logger.info(f"Placeholder '{ph.placeholder_name}' needs analysis: no_sql={not ph.generated_sql}, not_validated={not ph.sql_validated}")
                else:
                    placeholders_ready.append(ph)
                    logger.info(f"Placeholder '{ph.placeholder_name}' is ready with valid SQL")

                # æ”¶é›†æ‰€æœ‰required_fields
                rf = getattr(ph, 'required_fields', None)
                if isinstance(rf, list):
                    for f in rf:
                        if isinstance(f, str):
                            required_fields.add(f)
                elif isinstance(rf, dict):
                    for key in ('columns', 'fields', 'required_fields'):
                        val = rf.get(key)
                        if isinstance(val, list):
                            for f in val:
                                if isinstance(f, str):
                                    required_fields.add(f)
                            break

                # Fallback to parsing_metadata if present
                pm = getattr(ph, 'parsing_metadata', None)
                if isinstance(pm, dict):
                    meta_rf = pm.get('required_fields') or pm.get('metadata', {}).get('required_fields')
                    if isinstance(meta_rf, list):
                        for f in meta_rf:
                            if isinstance(f, str):
                                required_fields.add(f)

            required_fields = sorted(required_fields)

            total_placeholders = len(existing_placeholders) if existing_placeholders else 0
            if placeholders_need_analysis:
                update_progress(
                    25,
                    msg_orchestrator.placeholder_needs_analysis(len(placeholders_need_analysis), total_placeholders),
                    stage="placeholder_analysis",
                    details={
                        "pending": len(placeholders_need_analysis),
                        "total": total_placeholders,
                    },
                )
                logger.info(f"Found {len(placeholders_need_analysis)} placeholders needing analysis, {len(placeholders_ready)} ready")
            else:
                if total_placeholders == 0:
                    update_progress(
                        35,
                        "æ¨¡æ¿æ— å ä½ç¬¦ï¼Œè·³è¿‡åˆ†æé˜¶æ®µ...",
                        stage="placeholder_analysis",
                        details={"total": 0},
                    )
                    logger.info(f"Template has no placeholders, skipping analysis phase")
                else:
                    update_progress(
                        35,
                        f"æ‰€æœ‰ {len(placeholders_ready)} ä¸ªå ä½ç¬¦å·²å°±ç»ªï¼Œè·³è¿‡åˆ†æé˜¶æ®µ...",
                        stage="placeholder_analysis",
                        details={
                            "ready": len(placeholders_ready),
                            "total": total_placeholders,
                        },
                    )
                    logger.info(f"All {len(placeholders_ready)} placeholders are ready, skipping analysis")

        except Exception as e:
            logger.warning(f"Failed to load/parse placeholders: {e}")
            required_fields = []

        success_criteria = {
            "min_rows": 1,
            "max_rows": 100000,
            "required_fields": required_fields,
            "quality_threshold": 0.6,
        }
        # æ ¹æ®æ˜¯å¦éœ€è¦åˆ†æå†³å®šæ‰§è¡Œè·¯å¾„
        if placeholders_need_analysis:
            # ğŸ†• è®¾ç½®é˜¶æ®µä¸ºPLANNING - å‡†å¤‡ç”ŸæˆSQL
            if state_manager:
                state_manager.set_stage(ExecutionStage.PLANNING)
                logger.info("ğŸ¯ è®¾ç½®Agenté˜¶æ®µä¸º PLANNING - å‡†å¤‡æ‰¹é‡ç”ŸæˆSQL")

            # ä½¿ç”¨PlaceholderApplicationServiceå•ä¸ªå¤„ç†æ¯ä¸ªå ä½ç¬¦
            update_progress(
                30,
                msg_orchestrator.placeholder_analysis_start(len(placeholders_need_analysis)),
                stage="placeholder_analysis",
                details={
                    "pending": len(placeholders_need_analysis),
                    "total": len(existing_placeholders or []),
                },
            )

            async def _process_placeholders_individually():
                """
                å•ä¸ªå¾ªç¯å¤„ç†å ä½ç¬¦ + æ‰¹é‡æŒä¹…åŒ–ï¼ˆæ–¹æ¡ˆ1ä¼˜åŒ–ï¼‰

                ä¼˜åŒ–ç­–ç•¥:
                - ä¿æŒä¸²è¡Œå¤„ç†ç¡®ä¿è´¨é‡ç¨³å®š
                - æ¯5ä¸ªå ä½ç¬¦æ‰¹é‡æäº¤ä¸€æ¬¡ï¼Œå‡å°‘æ•°æ®åº“å‹åŠ›
                - æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼ˆå®šæœŸä¿å­˜è¿›åº¦ï¼‰
                """
                processed_count = 0
                total_count = len(placeholders_need_analysis)
                batch_updates = []  # ğŸ‘ˆ æ”¶é›†æ‰¹é‡æ›´æ–°
                BATCH_SIZE = 5  # ğŸ‘ˆ æ‰¹é‡å¤§å°é…ç½®

                for ph in placeholders_need_analysis:
                    try:
                        # æ£€æŸ¥æ˜¯å¦è¢«å–æ¶ˆ
                        check_if_cancelled()

                        update_progress(
                            30 + int(30 * processed_count / total_count),
                            msg_orchestrator.placeholder_analysis_progress(ph.placeholder_name, processed_count + 1, total_count),
                            stage="placeholder_analysis",
                            placeholder=ph.placeholder_name,
                            details={
                                "current": processed_count + 1,
                                "total": total_count,
                            },
                        )

                        # ğŸ‘‡ æ„å»ºçœŸå®çš„ä»»åŠ¡ä¸Šä¸‹æ–‡
                        real_task_context = {
                            "task_id": task_id,
                            "task_name": task.name,
                            "template_id": str(task.template_id),
                            "user_id": str(task.owner_id),
                            "report_period": task.report_period.value if task.report_period else "monthly",
                            "schedule": task.schedule,  # çœŸå® cron è¡¨è¾¾å¼
                            "time_window": time_window,  # çœŸå®æ—¶é—´çª—å£
                            "time_context": time_ctx,  # å®Œæ•´æ—¶é—´ä¸Šä¸‹æ–‡
                            "execution_trigger": execution_context.get("trigger", "scheduled") if execution_context else "scheduled",
                            "execution_id": str(task_execution.execution_id),
                        }

                        # ğŸ†• é€‰æ‹©å ä½ç¬¦åˆ†ææ–¹æ³•ï¼šç›´æ¥è°ƒç”¨æˆ–ä½¿ç”¨ Celery ä»»åŠ¡
                        use_celery_task = getattr(settings, 'USE_CELERY_PLACEHOLDER_ANALYSIS', False)
                        
                        if use_celery_task:
                            # ä½¿ç”¨æ–°çš„ Celery å ä½ç¬¦åˆ†æä»»åŠ¡
                            from app.services.infrastructure.task_queue.placeholder_tasks import analyze_single_placeholder_task
                            
                            logger.info(msg_orchestrator.placeholder_analysis_celery(ph.placeholder_name))
                            
                            # è§¦å‘ Celery ä»»åŠ¡
                            celery_task = analyze_single_placeholder_task.delay(
                                placeholder_name=ph.placeholder_name,
                                placeholder_text=ph.placeholder_text,
                                template_id=str(task.template_id),
                                data_source_id=str(task.data_source_id),
                                user_id=str(task.owner_id),
                                template_context=real_task_context,
                                time_window=real_task_context.get("time_window"),
                                time_column=real_task_context.get("time_column"),
                                data_range=real_task_context.get("data_range", "day"),
                                requirements=real_task_context.get("requirements"),
                                execute_sql=False,  # ä»»åŠ¡æ‰§è¡Œé˜¶æ®µä¸æ‰§è¡ŒSQLï¼Œåªç”Ÿæˆ
                                row_limit=1000,
                                **{k: v for k, v in real_task_context.items() if k not in [
                                    "template_context", "time_window", "time_column", "data_range", "requirements"
                                ]}
                            )
                            
                            # ç­‰å¾…ä»»åŠ¡å®Œæˆ
                            celery_result = celery_task.get(timeout=300)  # 5åˆ†é’Ÿè¶…æ—¶
                            
                            if celery_result.get("success"):
                                analysis_result = celery_result.get("analysis_result", {})
                                sql_result = {
                                    "success": True,
                                    "sql": analysis_result.get("generated_sql", {}).get("sql", ""),
                                    "validated": analysis_result.get("generated_sql", {}).get("validated", True),
                                    "confidence": analysis_result.get("confidence_score", 0.9),
                                    "auto_fixed": analysis_result.get("generated_sql", {}).get("auto_fixed", False),
                                    "warning": analysis_result.get("generated_sql", {}).get("warning")
                                }
                                logger.info(f"âœ… Celery ä»»åŠ¡åˆ†ææˆåŠŸ: {ph.placeholder_name}")
                            else:
                                error_msg = celery_result.get("error", "Celery ä»»åŠ¡åˆ†æå¤±è´¥")
                                sql_result = {
                                    "success": False,
                                    "error": error_msg
                                }
                                logger.error(f"âŒ Celery ä»»åŠ¡åˆ†æå¤±è´¥: {ph.placeholder_name}, é”™è¯¯: {error_msg}")
                        else:
                            # ä½¿ç”¨æˆç†Ÿçš„å•å ä½ç¬¦åˆ†æèƒ½åŠ›ï¼ˆé¿å…å¾ªç¯é—®é¢˜ï¼‰
                            async def _analyze_placeholder_async(placeholder_name, placeholder_text, template_id, data_source_id, template_context, user_id):
                                from app.api.endpoints.placeholders import PlaceholderOrchestrationService
                                orchestration_service = PlaceholderOrchestrationService()
                                
                                # è°ƒç”¨æˆç†Ÿçš„å•å ä½ç¬¦åˆ†æï¼Œç»“æœè‡ªåŠ¨ä¿å­˜åˆ°æ•°æ®åº“
                                analysis_result = await orchestration_service.analyze_placeholder_with_full_pipeline(
                                    placeholder_name=placeholder_name,
                                    placeholder_text=placeholder_text,
                                    template_id=template_id,
                                    data_source_id=data_source_id,
                                    template_context=template_context,
                                    user_id=user_id
                                )
                                
                                # è½¬æ¢ä¸ºå½“å‰ä»»åŠ¡æœŸæœ›çš„æ ¼å¼ï¼ˆç”¨äºåç»­ETLæ­¥éª¤ï¼‰
                                if analysis_result.get("status") == "success":
                                    generated_sql = analysis_result.get("generated_sql", {})
                                    return {
                                        "success": True,
                                        "sql": generated_sql.get("sql", ""),
                                        "validated": generated_sql.get("validated", True),
                                        "confidence": analysis_result.get("confidence_score", 0.9),
                                        "auto_fixed": generated_sql.get("auto_fixed", False),
                                        "warning": generated_sql.get("warning")
                                    }
                                else:
                                    return {
                                        "success": False,
                                        "error": analysis_result.get("error", "å ä½ç¬¦åˆ†æå¤±è´¥")
                                    }

                            template_id_for_analysis: Optional[str] = None
                            if getattr(task, "template_id", None):
                                template_id_for_analysis = str(task.template_id)
                            elif getattr(ph, "template_id", None):
                                template_id_for_analysis = str(ph.template_id)

                            if not template_id_for_analysis:
                                logger.warning(
                                    "å ä½ç¬¦ %s ç¼ºå°‘ template_idï¼Œè·³è¿‡åˆ†æé˜¶æ®µ",
                                    ph.placeholder_name,
                                )
                                sql_result = {
                                    "success": False,
                                    "error": "ç¼ºå°‘æ¨¡æ¿ä¸Šä¸‹æ–‡ï¼Œæ— æ³•åˆ†æå ä½ç¬¦"
                                }
                            else:
                                sql_result = run_async(_analyze_placeholder_async(
                                    placeholder_name=ph.placeholder_name,
                                    placeholder_text=ph.placeholder_text,
                                    template_id=template_id_for_analysis,
                                    data_source_id=str(task.data_source_id),
                                    template_context=real_task_context,
                                    user_id=str(task.owner_id)
                                ))

                        if sql_result.get("success"):
                            # ğŸ” SQL LLM è¾“å‡ºè¿‡æ»¤ï¼šæ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆSQLè€Œéä¸­æ–‡è¯´æ˜
                            raw_sql = sql_result.get("sql", "").strip()
                            
                            def _is_valid_sql_structure(sql: str) -> bool:
                                """æ£€æŸ¥SQLæ˜¯å¦ä¸ºæœ‰æ•ˆçš„SQLç»“æ„ï¼Œè€Œéä¸­æ–‡è¯´æ˜ä¸²"""
                                if not sql:
                                    return False
                                
                                # æ£€æŸ¥SQLèµ·å§‹å…³é”®å­—
                                sql_upper = sql[:20].upper()
                                valid_starters = ("SELECT", "WITH", "INSERT", "UPDATE", "DELETE", "CREATE", "ALTER", "DROP", "EXPLAIN")
                                if not any(sql_upper.startswith(s) for s in valid_starters):
                                    return False
                                
                                # æ£€æŸ¥æ˜¯å¦åŒ…å«å¤§é‡ä¸­æ–‡ä¸”æ— SQLç»“æ„å…³é”®å­—
                                chinese_chars = sum(1 for ch in sql[:200] if '\u4e00' <= ch <= '\u9fff')
                                if chinese_chars > 10:  # å¦‚æœå‰200å­—ç¬¦ä¸­æœ‰è¶…è¿‡10ä¸ªä¸­æ–‡
                                    sql_upper_full = sql.upper()
                                    # å¿…é¡»æœ‰SQLç»“æ„å…³é”®å­—
                                    sql_structure_keywords = (" FROM ", " WHERE ", " JOIN ", " INTO ", " VALUES ", " SET ", " WHERE", "FROM", "JOIN")
                                    if not any(kw in sql_upper_full for kw in sql_structure_keywords):
                                        return False
                                
                                return True
                            
                            if not _is_valid_sql_structure(raw_sql):
                                logger.error(
                                    msg_orchestrator.sql_rejected_chinese(ph.placeholder_name, raw_sql[:100])
                                )
                                sql_result = {
                                    "success": False,
                                    "error": "LLMè¾“å‡ºä¸ºä¸­æ–‡è¯´æ˜æˆ–éSQLæ–‡æœ¬ï¼Œè€Œéæœ‰æ•ˆSQL",
                                    "sql": raw_sql
                                }
                            
                            if sql_result.get("success"):
                                # ğŸ‘‡ æ›´æ–°å ä½ç¬¦SQLï¼ˆä¸ç«‹å³æäº¤ï¼‰
                                ph.generated_sql = sql_result["sql"]
                                # åªæœ‰å½“SQLçœŸæ­£éªŒè¯é€šè¿‡æ—¶æ‰æ ‡è®°ä¸ºå·²éªŒè¯
                                ph.sql_validated = sql_result.get("validated", True)
                                ph.agent_analyzed = True
                                ph.analyzed_at = datetime.utcnow()

                                # å¦‚æœSQLè¢«è‡ªåŠ¨ä¿®å¤ï¼Œè®°å½•åˆ°metadata
                                if sql_result.get("auto_fixed"):
                                    ph.agent_config = ph.agent_config or {}
                                    ph.agent_config["auto_fixed"] = True
                                    ph.agent_config["auto_fix_warning"] = sql_result.get("warning")

                            # ğŸ†• è®°å½•SQLç”Ÿæˆç»“æœï¼ˆä½œä¸ºéªŒè¯æˆåŠŸï¼‰
                            if tool_recorder:
                                tool_recorder.record_sql_validation(
                                    tool_name="sql_generation",
                                    result={
                                        "valid": ph.sql_validated,
                                        "sql": sql_result["sql"],
                                        "auto_fixed": sql_result.get("auto_fixed", False),
                                        "confidence": sql_result.get("confidence", 0.9)
                                    }
                                )

                            batch_updates.append(ph)  # ğŸ‘ˆ æ·»åŠ åˆ°æ‰¹æ¬¡

                            events.append({
                                "type": "placeholder_sql_generated",
                                "placeholder_name": ph.placeholder_name,
                                "sql": sql_result["sql"],
                                "confidence": sql_result.get("confidence", 0.0),
                                "validated": ph.sql_validated,
                                "auto_fixed": sql_result.get("auto_fixed", False),
                                "timestamp": datetime.utcnow().isoformat()
                            })

                            validation_status = "âœ… éªŒè¯é€šè¿‡" if ph.sql_validated else "âš ï¸ æœªéªŒè¯"
                            auto_fix_info = " (è‡ªåŠ¨ä¿®å¤)" if sql_result.get("auto_fixed") else ""
                            logger.info(msg_orchestrator.sql_generation_success_batch(
                                ph.placeholder_name,
                                len(batch_updates),
                                BATCH_SIZE,
                                auto_fix_info,
                                validation_status
                            ))

                            # ğŸ‘‡ è¾¾åˆ°æ‰¹é‡å¤§å°æ—¶æäº¤
                            if len(batch_updates) >= BATCH_SIZE:
                                db.commit()
                                logger.info(msg_orchestrator.sql_generation_batch_commit(len(batch_updates)))
                                batch_updates.clear()

                        else:
                            error_msg = sql_result.get("error", "SQLç”Ÿæˆå¤±è´¥")
                            logger.error(msg_orchestrator.sql_generation_failed(ph.placeholder_name, error_msg))

                            # ğŸ†• åˆ‡æ¢åˆ°ERROR_RECOVERYé˜¶æ®µå¹¶è®°å½•é”™è¯¯
                            if state_manager:
                                state_manager.set_stage(ExecutionStage.ERROR_RECOVERY)
                                from app.services.infrastructure.agents.context_manager import ContextType, ContextItem
                                state_manager.add_context(
                                    key=f"sql_generation_error_{ph.placeholder_name}",
                                    item=ContextItem(
                                        type=ContextType.ERROR_INFO,
                                        content=f"å ä½ç¬¦ {ph.placeholder_name} SQLç”Ÿæˆå¤±è´¥: {error_msg}",
                                        metadata={"placeholder": ph.placeholder_name},
                                        relevance_score=1.0
                                    )
                                )
                                logger.warning("âš ï¸ åˆ‡æ¢åˆ° ERROR_RECOVERY é˜¶æ®µ")

                            events.append({
                                "type": "placeholder_sql_failed",
                                "placeholder_name": ph.placeholder_name,
                                "error": error_msg,
                                "timestamp": datetime.utcnow().isoformat()
                            })

                            update_progress(
                                task_execution.progress_percentage or 30,
                                msg_orchestrator.sql_generation_failed_progress(ph.placeholder_name),
                                stage="placeholder_analysis",
                                status="failed",
                                placeholder=ph.placeholder_name,
                                details={
                                    "current": processed_count + 1,
                                    "total": total_count,
                                },
                                error=error_msg,
                                record_only=True,
                            )

                    except Exception as e:
                        # è®°å½•å®Œæ•´å †æ ˆï¼Œä¾¿äºå®šä½ä¾‹å¦‚ KeyError('template_id') çš„æ¥æº
                        logger.error(
                            msg_orchestrator.placeholder_exception(ph.placeholder_name, str(e)),
                            exc_info=True
                        )
                        events.append({
                            "type": "placeholder_processing_error",
                            "placeholder_name": ph.placeholder_name,
                            "error": str(e),
                            "timestamp": datetime.utcnow().isoformat()
                        })

                        update_progress(
                            task_execution.progress_percentage or 30,
                            msg_orchestrator.placeholder_exception_progress(ph.placeholder_name),
                            stage="placeholder_analysis",
                            status="failed",
                            placeholder=ph.placeholder_name,
                            details={
                                "current": processed_count + 1,
                                "total": total_count,
                            },
                            error=str(e),
                            record_only=True,
                        )

                    processed_count += 1

                # ğŸ‘‡ æäº¤å‰©ä½™çš„å ä½ç¬¦
                if batch_updates:
                    db.commit()
                    logger.info(msg_orchestrator.sql_generation_batch_commit(len(batch_updates)))
                    batch_updates.clear()

                return processed_count

            processed_count = run_async(_process_placeholders_individually())
            update_progress(
                65,
                f"å ä½ç¬¦åˆ†æå®Œæˆï¼ŒæˆåŠŸå¤„ç† {processed_count} ä¸ªå ä½ç¬¦",
                stage="placeholder_analysis",
                details={"processed": processed_count},
            )
        else:
            # æ‰€æœ‰å ä½ç¬¦å·²å°±ç»ªï¼Œç›´æ¥æ‰§è¡ŒETL
            update_progress(
                40,
                "å ä½ç¬¦å·²å°±ç»ªï¼Œç›´æ¥æ‰§è¡ŒETL...",
                stage="placeholder_analysis",
                details={"ready": len(placeholders_ready)},
            )
            # è®°å½•è·³è¿‡åˆ†æçš„äº‹ä»¶
            events.append({
                "type": "analysis_skipped",
                "message": "æ‰€æœ‰å ä½ç¬¦å·²å‡†å¤‡å°±ç»ªï¼Œè·³è¿‡åˆ†æé˜¶æ®µ",
                "timestamp": datetime.utcnow().isoformat(),
                "placeholders_ready": len(placeholders_ready)
            })

        # 7. æ‰§è¡ŒçœŸå®çš„ETLæ•°æ®å¤„ç†æµç¨‹
        # ğŸ†• åˆ‡æ¢åˆ°EXECUTIONé˜¶æ®µ
        if state_manager:
            state_manager.set_stage(ExecutionStage.EXECUTION)
            logger.info("ğŸ¯ åˆ‡æ¢åˆ° EXECUTION é˜¶æ®µ - å¼€å§‹æ‰§è¡ŒSQLæŸ¥è¯¢")

        update_progress(
            70,
            "å¼€å§‹ETLæ•°æ®å¤„ç†...",
            stage="etl_processing",
        )

        # æ£€æŸ¥æ˜¯å¦è¢«å–æ¶ˆ
        check_if_cancelled()

        try:
            # é‡æ–°åŠ è½½æœ€æ–°çš„å ä½ç¬¦æ•°æ®ï¼ˆå¯èƒ½åœ¨Agentåˆ†æåæœ‰æ›´æ–°ï¼‰
            placeholders = crud_template_placeholder.get_by_template(db, str(task.template_id))
            etl_results: Dict[str, Dict[str, Any]] = {}
            etl_stats = {
                "processed": 0,
                "success": 0,
                "failed": 0,
                "skipped": 0,
            }
            # ç›‘æ§æŒ‡æ ‡ï¼šSQLæ‰§è¡Œã€æ¨¡å‹è°ƒç”¨ã€å›¾è¡¨ç”Ÿæˆ
            metrics = {
                "sql_execution": {"total": 0, "success": 0, "failed": 0},
                "model_call": {"total": 0, "success": 0, "failed": 0},
                "chart_generation": {"total": 0, "generated": 0, "failed": 0},
            }

            def _set_etl_result(
                name: str,
                *,
                success: bool,
                value: Any = None,
                error: Optional[str] = None,
                metadata: Optional[Dict[str, Any]] = None,
                skipped: bool = False,
            ) -> Dict[str, Any]:
                entry = {
                    "success": success,
                    "value": value,
                    "error": error,
                    "metadata": metadata or {},
                    "skipped": skipped,
                }
                etl_results[name] = entry
                etl_stats["processed"] += 1
                if skipped:
                    etl_stats["skipped"] += 1
                elif success:
                    etl_stats["success"] += 1
                else:
                    etl_stats["failed"] += 1
                return entry

            # å¯¼å…¥SQLå ä½ç¬¦æ›¿æ¢å™¨
            from app.utils.sql_placeholder_utils import SqlPlaceholderReplacer
            sql_replacer = SqlPlaceholderReplacer()

            update_progress(
                75,
                "æ­£åœ¨å¤„ç†SQLå ä½ç¬¦æ›¿æ¢å’Œæ‰§è¡ŒæŸ¥è¯¢...",
                stage="etl_processing",
            )

            # å¯¹æ¯ä¸ªæœ‰æ•ˆçš„å ä½ç¬¦è¿›è¡Œå•ä¸ªå¤„ç†
            total_placeholders_count = len(placeholders or [])
            for i, ph in enumerate(placeholders or []):
                # åªè¦æœ‰ç”Ÿæˆçš„SQLå°±å°è¯•æ‰§è¡Œï¼Œä¸è¦æ±‚å¿…é¡»éªŒè¯é€šè¿‡
                # sql_validated åº”è¯¥åœ¨æ‰§è¡ŒæˆåŠŸåè®¾ç½®ï¼Œè€Œä¸æ˜¯ä½œä¸ºæ‰§è¡Œçš„å‰ææ¡ä»¶
                if not ph.generated_sql or (ph.generated_sql and ph.generated_sql.strip() == ""):
                    logger.warning(f"è·³è¿‡å ä½ç¬¦ {ph.placeholder_name}: æ— æœ‰æ•ˆSQL")
                    _set_etl_result(
                        ph.placeholder_name,
                        success=False,
                        error="æ— æœ‰æ•ˆSQL",
                        metadata={"reason": "missing_sql"},
                        skipped=True,
                    )
                    update_progress(
                        task_execution.progress_percentage or 75,
                        f"è·³è¿‡å ä½ç¬¦ {ph.placeholder_name}: æ— æœ‰æ•ˆSQL",
                        stage="etl_processing",
                        status="failed",
                        placeholder=ph.placeholder_name,
                        details={
                            "current": i + 1,
                            "total": total_placeholders_count,
                        },
                        error="æ— æœ‰æ•ˆSQL",
                        record_only=True,
                    )
                    continue

                # å¦‚æœSQLæœªéªŒè¯ï¼Œè®°å½•æ—¥å¿—ä½†ç»§ç»­æ‰§è¡Œ
                if not ph.sql_validated:
                    logger.info(f"å ä½ç¬¦ {ph.placeholder_name} SQLæœªéªŒè¯ï¼Œå°†å°è¯•æ‰§è¡Œå¹¶åœ¨æˆåŠŸåæ ‡è®°ä¸ºå·²éªŒè¯")

                try:
                    # 1. é¦–å…ˆè¿›è¡ŒSQLå ä½ç¬¦æ›¿æ¢ï¼ˆæ—¶é—´å‚æ•°ç­‰ï¼‰
                    final_sql = ph.generated_sql
                    sql_placeholders = sql_replacer.extract_placeholders(ph.generated_sql)

                    if sql_placeholders:
                        logger.info(f"å ä½ç¬¦ {ph.placeholder_name} éœ€è¦SQLå‚æ•°æ›¿æ¢: {sql_placeholders}")
                        # æ„å»ºæ—¶é—´ä¸Šä¸‹æ–‡
                        time_context = {
                            "data_start_time": time_window.get("start", ""),
                            "data_end_time": time_window.get("end", ""),
                            "execution_time": datetime.now().strftime("%Y-%m-%d")
                        }
                        final_sql = sql_replacer.replace_time_placeholders(
                            ph.generated_sql,
                            time_context
                        )
                        logger.info(f"æ›¿æ¢åSQL: {final_sql[:100]}...")

                    # 2. è·å–æ•°æ®æºé…ç½®ï¼ˆä¸Agentåˆ†æé˜¶æ®µä¿æŒä¸€è‡´ï¼‰
                    from app.crud.crud_data_source import crud_data_source
                    from app.models.data_source import DataSourceType
                    from app.core.data_source_utils import DataSourcePasswordManager

                    data_source = crud_data_source.get(db, id=str(task.data_source_id))
                    if not data_source:
                        raise ValueError(f"æ•°æ®æºä¸å­˜åœ¨: {task.data_source_id}")

                    # æ„å»ºæ•°æ®æºé…ç½®å­—å…¸ï¼ˆå‚è€ƒ_get_data_source_infoçš„å®ç°ï¼‰
                    data_source_config = {}
                    if data_source.source_type == DataSourceType.doris:
                        data_source_config = {
                            "source_type": "doris",
                            "name": data_source.name,
                            "database": getattr(data_source, "doris_database", "default"),
                            "fe_hosts": list(getattr(data_source, "doris_fe_hosts", []) or ["localhost"]),
                            "be_hosts": list(getattr(data_source, "doris_be_hosts", []) or ["localhost"]),
                            "http_port": getattr(data_source, "doris_http_port", 8030),
                            "query_port": getattr(data_source, "doris_query_port", 9030),
                            "username": getattr(data_source, "doris_username", "root"),
                            "password": DataSourcePasswordManager.get_password(data_source.doris_password) if getattr(data_source, "doris_password", None) else "",
                            "timeout": 30
                        }
                    elif data_source.source_type == DataSourceType.sql:
                        from app.core.security_utils import decrypt_data
                        conn_str = data_source.connection_string
                        try:
                            if conn_str:
                                conn_str = decrypt_data(conn_str)
                        except Exception:
                            pass
                        data_source_config = {
                            "source_type": "sql",
                            "name": data_source.name,
                            "connection_string": conn_str,
                            "database": getattr(data_source, "database_name", None),
                            "host": getattr(data_source, "host", None),
                            "port": getattr(data_source, "port", None),
                            "username": getattr(data_source, "username", None),
                            "password": getattr(data_source, "password", None),
                        }

                    logger.info(f"æ•°æ®æºé…ç½®: {data_source.source_type}, database: {data_source_config.get('database')}")

                    # 2.5 SQLåˆ—éªŒè¯å’Œè‡ªåŠ¨ä¿®å¤
                    validation_passed = True
                    try:
                        # å°è¯•å¯¼å…¥åˆ—éªŒè¯å·¥å…·
                        from app.services.infrastructure.agents.tools.column_validator import (
                            SQLColumnValidatorTool,
                            SQLColumnAutoFixTool
                        )

                        # è·å–è¡¨ç»“æ„ä¿¡æ¯
                        table_columns = {}
                        if hasattr(ph, 'agent_config') and ph.agent_config:
                            schema_context = ph.agent_config.get('schema_context', {})
                            table_columns = schema_context.get('table_columns', {})

                        # åªæœ‰åœ¨æœ‰è¡¨ç»“æ„ä¿¡æ¯æ—¶æ‰è¿›è¡ŒéªŒè¯
                        if table_columns:
                            logger.info(f"ğŸ” å¼€å§‹éªŒè¯ SQL åˆ—: {ph.placeholder_name}")

                            validator = SQLColumnValidatorTool()

                            async def _validate_columns_async():
                                return await validator.execute({
                                    "sql": final_sql,
                                    "schema_context": {"table_columns": table_columns}
                                })

                            validation_result = run_async(_validate_columns_async())

                            if validation_result.get("success") and not validation_result.get("valid"):
                                # å‘ç°åˆ—é”™è¯¯
                                invalid_columns = validation_result.get("invalid_columns", [])
                                suggestions = validation_result.get("suggestions", {})

                                logger.warning(
                                    f"âš ï¸ SQL åˆ—éªŒè¯å¤±è´¥: {ph.placeholder_name}\n"
                                    f"   æ— æ•ˆåˆ—: {invalid_columns}\n"
                                    f"   å»ºè®®: {suggestions}"
                                )

                                # å°è¯•è‡ªåŠ¨ä¿®å¤
                                if suggestions:
                                    logger.info(f"ğŸ”§ å°è¯•è‡ªåŠ¨ä¿®å¤ SQL: {ph.placeholder_name}")

                                    fixer = SQLColumnAutoFixTool()

                                    async def _fix_columns_async():
                                        return await fixer.execute({
                                            "sql": final_sql,
                                            "suggestions": suggestions
                                        })

                                    fix_result = run_async(_fix_columns_async())

                                    if fix_result.get("success"):
                                        fixed_sql = fix_result.get("fixed_sql")
                                        changes = fix_result.get("changes", [])

                                        logger.info(
                                            f"âœ… SQL è‡ªåŠ¨ä¿®å¤æˆåŠŸ: {ph.placeholder_name}\n"
                                            f"   ä¿®æ”¹: {changes}"
                                        )

                                        # æ›´æ–° SQL
                                        final_sql = fixed_sql

                                        # æ›´æ–°æ•°æ®åº“ä¸­çš„ SQLï¼ˆä¿å­˜ä¿®å¤åçš„ç‰ˆæœ¬ï¼Œä¿ç•™å ä½ç¬¦ï¼‰
                                        # éœ€è¦å°†å·²æ›¿æ¢çš„æ—¶é—´å€¼è¿˜åŸä¸ºå ä½ç¬¦æ ¼å¼
                                        saved_sql = fixed_sql
                                        if sql_placeholders and time_context:
                                            # å°†æ—¶é—´å€¼è¿˜åŸä¸ºå ä½ç¬¦
                                            for placeholder in sql_placeholders:
                                                if placeholder in ['start_date', 'end_date']:
                                                    time_key = 'data_start_time' if placeholder == 'start_date' else 'data_end_time'
                                                    time_value = time_context.get(time_key, '')
                                                    if time_value:
                                                        # è¿˜åŸä¸ºå ä½ç¬¦æ ¼å¼
                                                        saved_sql = saved_sql.replace(f"'{time_value}'", f"{{{{{placeholder}}}}}")

                                        ph.generated_sql = saved_sql

                                        # æ ‡è®°ä¸ºéœ€è¦äººå·¥å®¡æ ¸ï¼ˆè™½ç„¶å·²è‡ªåŠ¨ä¿®å¤ï¼‰
                                        if not hasattr(ph, 'agent_config') or not ph.agent_config:
                                            ph.agent_config = {}
                                        ph.agent_config['auto_fixed'] = True
                                        ph.agent_config['auto_fix_details'] = {
                                            "changes": changes,
                                            "original_errors": validation_result.get("errors", [])
                                        }

                                        db.commit()
                                        logger.info(f"ğŸ’¾ å·²ä¿å­˜ä¿®å¤åçš„ SQL: {ph.placeholder_name}")
                                    else:
                                        # è‡ªåŠ¨ä¿®å¤å¤±è´¥
                                        logger.error(f"âŒ SQL è‡ªåŠ¨ä¿®å¤å¤±è´¥: {ph.placeholder_name}")
                                        validation_passed = False
                                else:
                                    # æ²¡æœ‰ä¿®å¤å»ºè®®
                                    logger.error(f"âŒ æ— æ³•è‡ªåŠ¨ä¿®å¤ï¼Œç¼ºå°‘åˆ—åå»ºè®®: {ph.placeholder_name}")
                                    validation_passed = False

                                # å¦‚æœè‡ªåŠ¨ä¿®å¤å¤±è´¥ï¼Œè®°å½•é”™è¯¯å¹¶è·³è¿‡æ‰§è¡Œ
                                if not validation_passed:
                                    error_msg = "\n".join(validation_result.get("errors", ["åˆ—éªŒè¯å¤±è´¥"]))
                                    _set_etl_result(
                                        ph.placeholder_name,
                                        success=False,
                                        error=error_msg,
                                        metadata={"reason": "column_validation_failed"},
                                    )

                                    update_progress(
                                        task_execution.progress_percentage or 75,
                                        f"å ä½ç¬¦ {ph.placeholder_name} SQL åˆ—éªŒè¯å¤±è´¥",
                                        stage="etl_processing",
                                        status="failed",
                                        placeholder=ph.placeholder_name,
                                        details={
                                            "current": i + 1,
                                            "total": total_placeholders_count,
                                        },
                                        error=error_msg,
                                        record_only=True,
                                    )
                                    continue
                            else:
                                logger.info(f"âœ… SQL åˆ—éªŒè¯é€šè¿‡: {ph.placeholder_name}")

                        else:
                            logger.debug(f"â­ï¸ è·³è¿‡åˆ—éªŒè¯ï¼ˆæ— è¡¨ç»“æ„ä¿¡æ¯ï¼‰: {ph.placeholder_name}")

                    except ImportError:
                        logger.warning("åˆ—éªŒè¯å·¥å…·æœªå®‰è£…ï¼Œè·³è¿‡éªŒè¯")
                    except Exception as val_error:
                        logger.warning(f"åˆ—éªŒè¯è¿‡ç¨‹å¼‚å¸¸ï¼Œç»§ç»­æ‰§è¡Œ: {val_error}")

                    # 3. ä½¿ç”¨connectorç›´æ¥æ‰§è¡ŒæŸ¥è¯¢ï¼ˆä¸Agentä¿æŒä¸€è‡´ï¼‰
                    from app.services.data.connectors.connector_factory import create_connector_from_config

                    async def _execute_query_async():
                        connector = create_connector_from_config(
                            source_type=data_source.source_type,
                            name=data_source.name,
                            config=data_source_config
                        )
                        try:
                            await connector.connect()
                            result = await connector.execute_query(final_sql)
                            return result
                        finally:
                            await connector.disconnect()

                    # ğŸ“Š è®°å½•SQLæ‰§è¡ŒæŒ‡æ ‡
                    metrics["sql_execution"]["total"] += 1
                    
                    query_result = run_async(_execute_query_async())

                    # 4. è§£åŒ…æŸ¥è¯¢ç»“æœï¼Œæå–å®é™…æ•°æ®å€¼
                    # DorisQueryResult æ²¡æœ‰ success å±æ€§ï¼Œåªè¦æ²¡æŠ›å¼‚å¸¸å°±æ˜¯æˆåŠŸ
                    if hasattr(query_result, 'data') and query_result.data is not None and not query_result.data.empty:
                        metrics["sql_execution"]["success"] += 1
                        # å°†DataFrameè½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
                        result_data = query_result.data.to_dict('records')

                        # è½¬æ¢ Decimal ç±»å‹ä¸º floatï¼Œç¡®ä¿ JSON å¯åºåˆ—åŒ–
                        from app.utils.json_utils import convert_decimals
                        result_data = convert_decimals(result_data)

                        # æ™ºèƒ½è§£åŒ…ï¼šå•è¡Œå•åˆ—è¿”å›å€¼ï¼Œå¤šè¡Œè¿”å›åˆ—è¡¨
                        actual_value = None
                        if result_data:
                            if len(result_data) == 1 and len(result_data[0]) == 1:
                                # å•è¡Œå•åˆ—ï¼šè¿”å›å€¼æœ¬èº«
                                actual_value = list(result_data[0].values())[0]
                            elif len(result_data) == 1:
                                # å•è¡Œå¤šåˆ—ï¼šè¿”å›è¡Œå­—å…¸
                                actual_value = result_data[0]
                            else:
                                # å¤šè¡Œï¼šè¿”å›å®Œæ•´åˆ—è¡¨ï¼ˆç”¨äºå›¾è¡¨ï¼‰
                                actual_value = result_data

                        logger.info(f"âœ… å ä½ç¬¦ {ph.placeholder_name} æŸ¥è¯¢æˆåŠŸï¼Œç»“æœç±»å‹: {type(actual_value)}, å€¼: {str(actual_value)[:100]}")

                        # ğŸ†• è®°å½•SQLæ‰§è¡Œç»“æœ
                        if tool_recorder:
                            tool_recorder.record_sql_execution(
                                tool_name=f"sql_execution_{ph.placeholder_name}",
                                result={
                                    "success": True,
                                    "row_count": len(result_data),
                                    "rows": result_data[:3] if len(result_data) > 3 else result_data  # åªè®°å½•å‰3è¡Œ
                                }
                            )

                        # å­˜å‚¨å®é™…çš„æ•°æ®å€¼
                        _set_etl_result(
                            ph.placeholder_name,
                            success=True,
                            value=actual_value,
                            metadata={
                                "reason": "query_success",
                                "row_count": len(result_data),
                            },
                        )
                    else:
                        # æŸ¥è¯¢æˆåŠŸä½†æ— æ•°æ® - ğŸ†• è¿›è¡Œæ„å›¾éªŒè¯
                        logger.warning(f"âš ï¸ å ä½ç¬¦ {ph.placeholder_name} æŸ¥è¯¢æˆåŠŸä½†æ— æ•°æ®è¿”å›")

                        # ğŸ”¥ æ–°å¢ï¼šéªŒè¯SQLæ˜¯å¦ç¬¦åˆå ä½ç¬¦æ„å›¾
                        try:
                            intent_validation = run_async(system._validate_sql_result_intent(
                                sql=final_sql,
                                placeholder_text=ph.placeholder_text or ph.placeholder_name,
                                placeholder_name=ph.placeholder_name,
                                result_data=None,
                                row_count=0
                            ))

                            if not intent_validation.get("matches_intent") and intent_validation.get("requires_regeneration"):
                                logger.error(f"âŒ [æ„å›¾éªŒè¯å¤±è´¥] å ä½ç¬¦ {ph.placeholder_name} çš„SQLä¸ç¬¦åˆä¸šåŠ¡æ„å›¾")
                                logger.error(f"   å‘ç°çš„é—®é¢˜: {intent_validation.get('issues', [])}")
                                logger.info(f"ğŸ’¡ æ”¹è¿›å»ºè®®: {intent_validation.get('recommendations', [])}")

                                # æ ‡è®°ä¸ºå¤±è´¥ï¼Œè€Œä¸æ˜¯æˆåŠŸä½†æ•°æ®ä¸ºç©º
                                _set_etl_result(
                                    ph.placeholder_name,
                                    success=False,
                                    error=f"SQLä¸ç¬¦åˆå ä½ç¬¦æ„å›¾: {'; '.join(intent_validation.get('issues', []))}",
                                    metadata={
                                        "reason": "intent_validation_failed",
                                        "row_count": 0,
                                        "intent_issues": intent_validation.get("issues", []),
                                        "recommendations": intent_validation.get("recommendations", []),
                                        "sql": final_sql  # ä¿å­˜åŸSQLç”¨äºè°ƒè¯•
                                    },
                                )

                                # ğŸ”¥ TODO: åœ¨æœªæ¥ç‰ˆæœ¬ä¸­ï¼Œè¿™é‡Œå¯ä»¥è§¦å‘è‡ªåŠ¨é‡æ–°ç”ŸæˆSQL
                                # regeneration_result = run_async(system._regenerate_sql_with_feedback(
                                #     placeholder=ph,
                                #     feedback=intent_validation.get("recommendations", [])
                                # ))

                                logger.warning(f"âš ï¸ å ä½ç¬¦ {ph.placeholder_name} éœ€è¦äººå·¥å®¡æŸ¥å’Œä¿®æ­£")
                            else:
                                # æ„å›¾éªŒè¯é€šè¿‡ï¼Œåªæ˜¯æ•°æ®çœŸçš„ä¸ºç©ºï¼ˆæ­£å¸¸æƒ…å†µï¼‰
                                logger.info(f"âœ… [æ„å›¾éªŒè¯é€šè¿‡] å ä½ç¬¦ {ph.placeholder_name} SQLæ­£ç¡®ï¼Œæ•°æ®ç¡®å®ä¸ºç©º")
                                _set_etl_result(
                                    ph.placeholder_name,
                                    success=True,
                                    value=None,
                                    metadata={
                                        "reason": "query_success_empty",
                                        "row_count": 0,
                                        "intent_validated": True
                                    },
                                )
                        except Exception as validation_error:
                            logger.error(f"æ„å›¾éªŒè¯è¿‡ç¨‹å¼‚å¸¸: {validation_error}")
                            # é™çº§å¤„ç†ï¼šå³ä½¿éªŒè¯å¤±è´¥ä¹Ÿè¿”å›ç©ºç»“æœ
                            _set_etl_result(
                                ph.placeholder_name,
                                success=True,
                                value=None,
                                metadata={
                                    "reason": "query_success_empty",
                                    "row_count": 0,
                                    "validation_error": str(validation_error)
                                },
                            )

                    # æ›´æ–°è¿›åº¦
                    progress_increment = 10 / total_placeholders_count if total_placeholders_count else 0
                    current_progress = 75 + (i + 1) * progress_increment
                    update_progress(
                        int(current_progress),
                        f"å·²å¤„ç† {i + 1}/{total_placeholders_count} ä¸ªå ä½ç¬¦",
                        stage="etl_processing",
                        placeholder=ph.placeholder_name,
                        details={
                            "current": i + 1,
                            "total": total_placeholders_count,
                        },
                    )

                except Exception as e:
                    logger.error(f"Failed to execute SQL for placeholder {ph.placeholder_name}: {e}")
                    metrics["sql_execution"]["failed"] += 1
                    _set_etl_result(
                        ph.placeholder_name,
                        success=False,
                        error=str(e),
                        metadata={"reason": "execution_error"},
                    )

                    update_progress(
                        task_execution.progress_percentage or int(current_progress),
                        f"æ‰§è¡Œå ä½ç¬¦ {ph.placeholder_name} SQL å¤±è´¥",
                        stage="etl_processing",
                        status="failed",
                        placeholder=ph.placeholder_name,
                        details={
                            "current": i + 1,
                            "total": total_placeholders_count,
                        },
                        error=str(e),
                        record_only=True,
                    )

            update_progress(
                85,
                "ETLæ•°æ®å¤„ç†å®Œæˆ",
                stage="etl_processing",
            )

            # æ„å»ºæ‰§è¡Œç»“æœ
            # ç»Ÿè®¡æˆåŠŸçš„å ä½ç¬¦ï¼ˆä¸æ˜¯ERRORå¼€å¤´çš„ï¼‰
            successful_placeholders = [k for k, v in etl_results.items() if v.get("success")]
            failed_placeholders = {k: v for k, v in etl_results.items() if not v.get("success") and not v.get("skipped")}
            skipped_placeholders = {k: v for k, v in etl_results.items() if v.get("skipped")}

            # è®¡ç®—æˆåŠŸç‡æŒ‡æ ‡
            placeholder_total = len(etl_results) if etl_results else 1
            placeholder_success_rate = len(successful_placeholders) / placeholder_total if placeholder_total > 0 else 0
            
            sql_exec_total = metrics["sql_execution"]["total"]
            sql_exec_success_rate = metrics["sql_execution"]["success"] / sql_exec_total if sql_exec_total > 0 else 0
            
            model_call_total = metrics["model_call"]["total"]
            model_call_success_rate = metrics["model_call"]["success"] / model_call_total if model_call_total > 0 else 0
            
            chart_gen_total = metrics["chart_generation"]["total"]
            chart_gen_rate = metrics["chart_generation"]["generated"] / chart_gen_total if chart_gen_total > 0 else 0
            
            # åˆ¤æ–­ ETL æ˜¯å¦æˆåŠŸï¼šè‡³å°‘æœ‰ä¸€ä¸ªæˆåŠŸå ä½ç¬¦ä¸”æ²¡æœ‰ä»»ä½•å¤±è´¥å ä½ç¬¦
            etl_success_flag = len(successful_placeholders) > 0 and not failed_placeholders
            if not etl_success_flag:
                failure_reason_parts = []
                if len(successful_placeholders) == 0:
                    failure_reason_parts.append("æ²¡æœ‰æˆåŠŸå ä½ç¬¦")
                if failed_placeholders:
                    failure_reason_parts.append(f"å­˜åœ¨{len(failed_placeholders)}ä¸ªå¤±è´¥å ä½ç¬¦: {list(failed_placeholders.keys())[:5]}")  # æœ€å¤šæ˜¾ç¤º5ä¸ª
                logger.warning(f"âš ï¸ ETLæ‰§è¡Œè¢«æ ‡è®°ä¸ºå¤±è´¥: {'; '.join(failure_reason_parts)}")
            
            execution_result = {
                "success": etl_success_flag,
                "events": events,
                "etl_results": etl_results,
                "time_window": time_window,
                "placeholders_processed": len(etl_results),
                "placeholders_success": len(successful_placeholders),
                "placeholders_failed": list(failed_placeholders.keys()),
                "placeholders_skipped": list(skipped_placeholders.keys()),
                "stats": etl_stats,
                "metrics": {
                    "placeholder_success_rate": placeholder_success_rate,
                    "sql_execution": {
                        "total": sql_exec_total,
                        "success": metrics["sql_execution"]["success"],
                        "failed": metrics["sql_execution"]["failed"],
                        "success_rate": sql_exec_success_rate,
                    },
                    "model_call": {
                        "total": model_call_total,
                        "success": metrics["model_call"]["success"],
                        "failed": metrics["model_call"]["failed"],
                        "success_rate": model_call_success_rate,
                    },
                    "chart_generation": {
                        "total": chart_gen_total,
                        "generated": metrics["chart_generation"]["generated"],
                        "failed": metrics["chart_generation"]["failed"],
                        "generation_rate": chart_gen_rate,
                    },
                },
            }

            logger.info(
                "ğŸ“Š ETLå¤„ç†å®Œæˆ: æˆåŠŸ %s, å¤±è´¥ %s, è·³è¿‡ %s / æ€»è®¡ %s",
                etl_stats["success"],
                etl_stats["failed"],
                etl_stats["skipped"],
                etl_stats["processed"],
            )

        except Exception as e:
            logger.error(f"ETL processing failed: {e}")
            execution_result = {
                "success": False,
                "events": events,
                "error": str(e),
                "time_window": time_window,
            }

        placeholder_render_data: Dict[str, Any] = {}
        placeholder_errors: Dict[str, str] = {}
        etl_result_entries = execution_result.get("etl_results") or {}
        for placeholder_name, result_entry in etl_result_entries.items():
            if isinstance(result_entry, dict) and "success" in result_entry:
                if result_entry.get("success"):
                    value = result_entry.get("value")
                    placeholder_render_data[placeholder_name] = value
                else:
                    # ä»…å°†é skipped çš„å ä½ç¬¦è®¡å…¥é”™è¯¯
                    if not result_entry.get("skipped"):
                        placeholder_errors[placeholder_name] = result_entry.get("error") or "æœªçŸ¥é”™è¯¯"
                    placeholder_render_data[placeholder_name] = None
            else:
                placeholder_render_data[placeholder_name] = result_entry

        execution_result["placeholder_errors"] = placeholder_errors
        execution_result["render_placeholder_data"] = placeholder_render_data

        # æ•°æ®è´¨é‡é—¸é—¨ï¼šæ£€æŸ¥å ä½ç¬¦æ•°æ®ä¸­æ˜¯å¦åŒ…å«é”™è¯¯å…³é”®è¯
        def _check_data_quality_gate(data: Dict[str, Any]) -> Tuple[bool, List[str]]:
            """
            æ•°æ®è´¨é‡é—¸é—¨ï¼šæ£€æŸ¥å ä½ç¬¦æ•°æ®ä¸­æ˜¯å¦åŒ…å«é”™è¯¯å…³é”®è¯
            è¿”å› (æ˜¯å¦é€šè¿‡, é”™è¯¯åˆ—è¡¨)
            """
            error_keywords = ["ERROR:", "æ— æœ‰æ•ˆSQL", "æ‰§è¡Œå¤±è´¥", "éªŒè¯å¤±è´¥", "SQL éªŒè¯å¤±è´¥", "å ä½ç¬¦åˆ†æå¤±è´¥"]
            quality_issues = []
            
            for name, value in data.items():
                if value is None:
                    continue
                str_value = str(value)
                # æ£€æŸ¥æ˜¯å¦ä¸ºé”™è¯¯æ–‡æœ¬ï¼ˆé•¿åº¦é™åˆ¶ï¼šé¿å…æ£€æŸ¥è¿‡å¤§çš„æ•°æ®ï¼‰
                if len(str_value) < 500:  # åªæ£€æŸ¥è¾ƒçŸ­çš„æ•°æ®å€¼ï¼ˆé€šå¸¸æ˜¯é”™è¯¯æ¶ˆæ¯ï¼‰
                    for keyword in error_keywords:
                        if keyword in str_value:
                            quality_issues.append(f"{name}: åŒ…å«é”™è¯¯å…³é”®è¯ '{keyword}'")
                            break  # æ¯ä¸ªå ä½ç¬¦åªè®°å½•ä¸€æ¬¡
            
            return len(quality_issues) == 0, quality_issues

        # 8. ç”Ÿæˆæ–‡æ¡£ï¼ˆä½¿ç”¨æ¨¡æ¿ + doc_assemblerï¼‰
        update_progress(
            87,
            "æ­£åœ¨ç”ŸæˆæŠ¥å‘Šæ–‡æ¡£...",
            stage="document_generation",
            pipeline_status=PipelineTaskStatus.ASSEMBLING,
        )

        # æ£€æŸ¥æ˜¯å¦è¢«å–æ¶ˆ
        check_if_cancelled()

        tpl_meta = None  # åˆå§‹åŒ–æ¨¡æ¿å…ƒæ•°æ®ï¼Œç”¨äºåç»­æ¸…ç†
        report_generation_error: Optional[str] = None
        etl_phase_success = execution_result.get("success", False)
        
        # ğŸ†• æ–‡æ¡£ç”Ÿæˆå®¹é”™ç­–ç•¥ï¼ˆé€šè¿‡ settings é…ç½®ï¼‰
        failed_placeholders = execution_result.get("placeholders_failed", [])
        skipped_placeholders = execution_result.get("placeholders_skipped", [])
        successful_placeholders_count = execution_result.get("placeholders_success", 0)
        
        # è¯¦ç»†è®°å½• ETL é˜¶æ®µçŠ¶æ€ï¼ˆç”¨äºè¯Šæ–­ï¼‰
        logger.info(
            f"ğŸ“Š ETLé˜¶æ®µçŠ¶æ€æ£€æŸ¥: success={etl_phase_success}, "
            f"æˆåŠŸå ä½ç¬¦={successful_placeholders_count}, "
            f"å¤±è´¥å ä½ç¬¦={len(failed_placeholders)}, "
            f"è·³è¿‡å ä½ç¬¦={len(skipped_placeholders)}"
        )
        if failed_placeholders:
            logger.warning(f"âš ï¸ å¤±è´¥å ä½ç¬¦åˆ—è¡¨: {failed_placeholders}")
        
        # æ•°æ®è´¨é‡é—¸é—¨æ£€æŸ¥
        quality_passed, quality_issues = _check_data_quality_gate(placeholder_render_data)
        if not quality_passed:
            logger.error(f"âŒ æ•°æ®è´¨é‡æ£€æŸ¥å¤±è´¥: {quality_issues}")
            etl_phase_success = False  # å°†è´¨é‡æ£€æŸ¥å¤±è´¥è§†ä¸º ETL å¤±è´¥
        else:
            logger.info("âœ… æ•°æ®è´¨é‡æ£€æŸ¥é€šè¿‡")

        max_failed_allowed = getattr(settings, "REPORT_MAX_FAILED_PLACEHOLDERS_FOR_DOC", 0)
        allow_quality_issues = getattr(settings, "REPORT_ALLOW_QUALITY_ISSUES", False)

        # å…è®¸åœ¨æœ‰é™å¤±è´¥ä¸‹ç»§ç»­ç”Ÿæˆï¼ˆéœ€è‡³å°‘æœ‰éƒ¨åˆ†æˆåŠŸæ•°æ®ï¼‰
        tolerance_passed = (
            len(failed_placeholders) <= max_failed_allowed and successful_placeholders_count > 0
        )

        # è´¨é‡é—¸é—¨å¯é…ç½®æ”¾è¡Œ
        quality_gate_passed = quality_passed or allow_quality_issues

        should_generate_document = etl_phase_success or (tolerance_passed and quality_gate_passed)

        # ğŸ†• è‹¥åœ¨å®¹é”™æ¡ä»¶ä¸‹ç»§ç»­ç”Ÿæˆæ–‡æ¡£ï¼Œåˆ™ä¸ºå¤±è´¥å ä½ç¬¦æ³¨å…¥å‹å¥½å ä½æ–‡æœ¬ï¼Œé¿å…æ–‡æ¡£å‡ºç°ç©ºç™½
        if should_generate_document and not etl_phase_success and tolerance_passed:
            for failed_name in failed_placeholders:
                # ä»…åœ¨å½“å‰æ¸²æŸ“æ•°æ®ä¸ºç©ºæ—¶æ³¨å…¥å ä½æ–‡æœ¬
                if placeholder_render_data.get(failed_name) in (None, ""):
                    # æ³¨æ„ï¼šé¿å…è§¦å‘è´¨é‡é—¸é—¨çš„å…³é”®è¯ï¼ˆä¸è¦åŒ…å« â€œERROR/å¤±è´¥/éªŒè¯å¤±è´¥/æ— æœ‰æ•ˆSQLâ€ ç­‰å­—æ ·ï¼‰
                    placeholder_render_data[failed_name] = f"ã€å ä½æç¤ºï¼šæ•°æ®æš‚ä¸å¯ç”¨ï¼Œç³»ç»Ÿå°†åœ¨åç»­è‡ªåŠ¨è¡¥å……ï¼ˆ{failed_name}ï¼‰ã€‘"

        if not should_generate_document:
            # æ„å»ºè¯¦ç»†çš„å¤±è´¥åŸå› 
            failure_reasons = []
            # ä»…ç»Ÿè®¡çœŸå®å¤±è´¥ï¼ˆä¸åŒ…å« skippedï¼‰
            failed_count = len(failed_placeholders)
            skipped_count = len(skipped_placeholders)
            if failed_count:
                failure_reasons.append(f"å ä½ç¬¦é”™è¯¯(ä¸å«è·³è¿‡): {failed_count}ä¸ª")
            if skipped_count:
                failure_reasons.append(f"è·³è¿‡: {skipped_count}ä¸ª")
            if not quality_passed and not allow_quality_issues:
                failure_reasons.append(f"æ•°æ®è´¨é‡æ£€æŸ¥å¤±è´¥: {', '.join(quality_issues[:3])}")  # æœ€å¤šæ˜¾ç¤ºå‰3ä¸ª

            report_generation_error = "ETLé˜¶æ®µå­˜åœ¨å¤±è´¥ï¼Œå ä½ç¬¦æ•°æ®ä¸å®Œæ•´ï¼Œå·²è·³è¿‡æ–‡æ¡£ç”Ÿæˆ"
            if failure_reasons:
                report_generation_error += f" ({'; '.join(failure_reasons)})"
            
            logger.warning(report_generation_error)
            execution_result["report"] = {
                "error": report_generation_error,
                "generation_mode": "skipped_due_to_etl_failure",
                "placeholder_errors": placeholder_errors,
                "quality_issues": quality_issues if not quality_passed else [],
            }
        else:
            try:
                from app.services.infrastructure.document.template_path_resolver import resolve_docx_template_path, cleanup_template_temp_dir
                from app.services.infrastructure.document.word_template_service import WordTemplateService
                from io import BytesIO
                from app.services.infrastructure.storage.hybrid_storage_service import get_hybrid_storage_service

                if not getattr(task, "template_id", None):
                    report_generation_error = "ä»»åŠ¡æœªé…ç½®æ¨¡æ¿ï¼Œè·³è¿‡æ–‡æ¡£ç”Ÿæˆ"
                    logger.warning(report_generation_error)
                    execution_result["report"] = {
                        "error": report_generation_error,
                        "generation_mode": "skipped"
                    }
                else:
                    tpl_meta = resolve_docx_template_path(db, str(task.template_id))
                    safe_tmp_dir = os.path.join(os.path.expanduser('~'), ".autoreportai", "tmp")
                    os.makedirs(safe_tmp_dir, exist_ok=True)
                    docx_out = os.path.join(
                        safe_tmp_dir,
                        f"report_{task.id}_{int(datetime.utcnow().timestamp())}.docx",
                    )

                    word_service = WordTemplateService()
                    assemble_res = run_async(
                        word_service.process_document_template(
                            template_path=tpl_meta["path"],
                            placeholder_data=placeholder_render_data,
                            output_path=docx_out,
                            container=container,
                            use_agent_charts=True,
                            use_agent_optimization=True,
                            user_id=str(task.owner_id),
                        )
                    )

                    if not assemble_res.get("success"):
                        report_generation_error = assemble_res.get("error") or "æ–‡æ¡£å¤„ç†å¤±è´¥"
                        logger.error("æ–‡æ¡£ç”Ÿæˆå¤±è´¥: %s", report_generation_error)
                        execution_result["report"] = {
                            "error": report_generation_error,
                            "generation_mode": assemble_res.get("generation_mode", "word_template_service"),
                        }
                    else:
                        payload_bytes: Optional[bytes] = None
                        document_bytes = assemble_res.get("document_bytes")
                        if document_bytes:
                            payload_bytes = document_bytes
                        else:
                            output_path = assemble_res.get("output_path") or docx_out
                            if os.path.exists(output_path):
                                with open(output_path, "rb") as f:
                                    payload_bytes = f.read()

                        if not payload_bytes:
                            raise ValueError("æ¨¡æ¿ç»„è£…æˆåŠŸä½†æœªç”Ÿæˆå¯ç”¨çš„æ–‡æ¡£å†…å®¹")

                        from app.models.user import User

                        user = db.query(User).filter(User.id == task.owner_id).first()
                        tenant_id = getattr(user, "tenant_id", str(task.owner_id)) if user else str(task.owner_id)

                        import re

                        slug = re.sub(r"[^\w\-]+", "-", (task.name or f"task_{task.id}")).strip("-")[:50]
                        ts = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
                        object_name = f"reports/{tenant_id}/{slug}/report_{ts}.docx"
                        friendly_name = assemble_res.get("friendly_file_name") or f"{slug}_{ts}.docx"

                        storage = get_hybrid_storage_service()
                        update_progress(
                            92,
                            "æ­£åœ¨ä¸Šä¼ æ–‡æ¡£åˆ°å­˜å‚¨...",
                            stage="document_generation",
                            pipeline_status=PipelineTaskStatus.ASSEMBLING,
                        )
                        upload_result = storage.upload_with_key(
                            BytesIO(payload_bytes),
                            object_name,
                            content_type="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                        )

                        execution_result["report"] = {
                            "storage_path": upload_result.get("file_path"),
                            "backend": upload_result.get("backend"),
                            "friendly_name": friendly_name,
                            "generation_mode": assemble_res.get("generation_mode", "word_template_service"),
                            "size": upload_result.get("size", len(payload_bytes)),  # ä¿å­˜æ–‡ä»¶å¤§å°
                        }
                        logger.info(f"âœ… æŠ¥å‘Šç”Ÿæˆå¹¶å­˜å‚¨å®Œæˆ: {upload_result.get('file_path')}, å¤§å°: {upload_result.get('size', len(payload_bytes))} bytes")
                        update_progress(
                            95,
                            "æ–‡æ¡£ç”Ÿæˆå®Œæˆ",
                            stage="document_generation",
                            pipeline_status=PipelineTaskStatus.ASSEMBLING,
                        )
                        logger.info("âœ… æŠ¥å‘Šç”Ÿæˆå¹¶å­˜å‚¨å®Œæˆ: %s", upload_result.get("file_path"))
            except Exception as e:
                report_generation_error = str(e)
                logger.error(f"Document assembly failed: {e}")
                
                # ğŸ”§ ä¿®å¤ï¼šæ£€æŸ¥æ˜¯å¦å·²æœ‰æˆåŠŸçš„ report æ•°æ®ï¼ˆåŒ…å« storage_pathï¼‰
                # å¦‚æœæŠ¥å‘Šå·²ç»æˆåŠŸç”Ÿæˆå¹¶å­˜å‚¨ï¼Œä¸åº”è¯¥è¦†ç›–å®ƒï¼Œåªè®°å½•é”™è¯¯ä¿¡æ¯
                existing_report = execution_result.get("report") or {}
                if existing_report.get("storage_path"):
                    # æŠ¥å‘Šå·²æˆåŠŸç”Ÿæˆï¼Œåªæ›´æ–°é”™è¯¯ä¿¡æ¯ï¼Œä¸è¦†ç›–å·²æœ‰çš„æ•°æ®
                    logger.warning(
                        f"âš ï¸ æ¸…ç†é˜¶æ®µå‘ç”Ÿå¼‚å¸¸ï¼Œä½†æŠ¥å‘Šå·²æˆåŠŸç”Ÿæˆ: {existing_report.get('storage_path')}, "
                        f"æ¸…ç†é”™è¯¯: {report_generation_error}"
                    )
                    existing_report["cleanup_error"] = report_generation_error
                    # ä¿ç•™ storage_path ç­‰å…³é”®ä¿¡æ¯ï¼Œä¸è¦†ç›–
                    execution_result["report"] = existing_report
                else:
                    # æŠ¥å‘ŠæœªæˆåŠŸç”Ÿæˆï¼Œè®¾ç½®å®Œæ•´çš„é”™è¯¯ä¿¡æ¯
                    existing_mode = existing_report.get("generation_mode")
                    execution_result["report"] = {
                        "error": report_generation_error,
                        "generation_mode": existing_mode or "assembly_error"
                    }
            finally:
                # æ¸…ç†æ¨¡æ¿ä¸´æ—¶æ–‡ä»¶
                if tpl_meta:
                    try:
                        cleanup_template_temp_dir(tpl_meta)
                        logger.info("âœ… æ¨¡æ¿ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†")
                    except Exception as cleanup_error:
                        logger.warning(f"æ¸…ç†æ¨¡æ¿ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {cleanup_error}")
        
        # ğŸ”§ ä¿®å¤ï¼šåŒºåˆ† None å’Œç©ºå­—å…¸ï¼Œé¿å…è¦†ç›–å·²æœ‰çš„ report æ•°æ®
        report_info = execution_result.get("report")
        if report_info is None:
            report_info = {}
            execution_result["report"] = report_info

        report_generated = bool(report_info.get("storage_path"))
        if report_generated:
            report_generation_error = None
            report_info.pop("error", None)
        else:
            if not report_info.get("error"):
                report_info["error"] = report_generation_error or "æŠ¥å‘Šæ–‡æ¡£æœªç”Ÿæˆ"

        etl_success = etl_phase_success
        execution_result["etl_success"] = etl_success
        
        # æœ€ç»ˆæˆåŠŸåˆ¤æ–­ï¼šåªè¦æŠ¥å‘ŠæˆåŠŸç”Ÿæˆï¼Œä»»åŠ¡å°±è®¤ä¸ºæˆåŠŸ
        # ETLéƒ¨åˆ†å¤±è´¥ä¸å½±å“æœ€ç»ˆçŠ¶æ€ï¼Œåªä½œä¸ºè­¦å‘Šè®°å½•
        overall_success = report_generated
        execution_result["success"] = overall_success
        report_info["generated"] = report_generated
        
        # è¯¦ç»†è®°å½•æœ€ç»ˆçŠ¶æ€åˆ¤æ–­ï¼ˆç”¨äºè¯Šæ–­ï¼‰
        logger.info(
            f"ğŸ¯ æœ€ç»ˆçŠ¶æ€åˆ¤æ–­: etl_success={etl_success}, "
            f"report_generated={report_generated}, "
            f"overall_success={overall_success}"
        )
        
        # ETLéƒ¨åˆ†å¤±è´¥ä¸å½±å“æœ€ç»ˆçŠ¶æ€ï¼Œä½†éœ€è¦è®°å½•è­¦å‘Š
        if not etl_success and report_generated:
            logger.warning(
                f"âš ï¸ ETLé˜¶æ®µæœªå®Œå…¨æˆåŠŸï¼ˆå­˜åœ¨å¤±è´¥å ä½ç¬¦ï¼‰ï¼Œä½†æŠ¥å‘Šå·²æˆåŠŸç”Ÿæˆï¼Œä»»åŠ¡æ ‡è®°ä¸ºæˆåŠŸ"
            )
        
        # åªæœ‰åœ¨æŠ¥å‘Šç”Ÿæˆå¤±è´¥æ—¶ï¼Œæ‰æ ‡è®°ä¸ºå¤±è´¥
        if not report_generated:
            logger.error("âŒ ä»»åŠ¡å¤±è´¥åŸå› : æŠ¥å‘Šæ–‡æ¡£æœªç”Ÿæˆ")
        
        # 7. æ›´æ–°æ‰§è¡Œç»“æœ
        final_status = TaskStatus.COMPLETED if overall_success else TaskStatus.FAILED
        task_execution.execution_status = final_status
        task_execution.completed_at = datetime.utcnow()
        task_execution.total_duration = int((task_execution.completed_at - task_execution.started_at).total_seconds())
        task_execution.progress_percentage = 100

        owner_id = task.owner_id
        if isinstance(owner_id, str):
            owner_id = UUID(owner_id)

        history_metadata: Dict[str, Any] = {
            "execution_id": str(task_execution.execution_id),
            "generation_mode": report_info.get("generation_mode"),
            "storage_backend": report_info.get("backend"),
            "placeholders": {
                "processed": execution_result.get("placeholders_processed"),
                "success": execution_result.get("placeholders_success"),
            },
            "etl_success": etl_success,
            "report_generated": report_generated,
            "time_window": time_window,
        }
        if report_info.get("error"):
            history_metadata["error"] = report_info.get("error")

        # è½¬æ¢ history_metadata ä¸­çš„ Decimal å¯¹è±¡
        history_metadata = convert_for_json(history_metadata)

        report_history_record = ReportHistory(
            task_id=task.id,
            user_id=owner_id,
            status="completed" if final_status == TaskStatus.COMPLETED else "failed",
            file_path=report_info.get("storage_path"),
            file_size=report_info.get("size", 0),
            error_message=report_info.get("error") if not overall_success else None,
            result=None,
            processing_metadata=history_metadata,
        )
        db.add(report_history_record)
        db.flush()
        report_info["history_id"] = report_history_record.id

        # è½¬æ¢ execution_result ä¸­çš„æ‰€æœ‰ Decimal å¯¹è±¡ä¸º floatï¼Œç¡®ä¿ JSON å¯åºåˆ—åŒ–
        execution_result = convert_for_json(execution_result)
        task_execution.execution_result = execution_result
        
        # æ›´æ–°ä»»åŠ¡ç»Ÿè®¡
        task.status = final_status
        if final_status == TaskStatus.COMPLETED:
            task.success_count += 1
        else:
            task.failure_count += 1
        task.last_execution_duration = task_execution.total_duration
        
        # æ›´æ–°å¹³å‡æ‰§è¡Œæ—¶é—´ï¼ˆä»…åœ¨æˆåŠŸæ—¶æ›´æ–°ï¼‰
        if final_status == TaskStatus.COMPLETED:
            if task.average_execution_time == 0:
                task.average_execution_time = task_execution.total_duration
            else:
                task.average_execution_time = (task.average_execution_time + task_execution.total_duration) / 2
        
        db.commit()
        
        if overall_success:
            update_progress(
                97,
                "æ­£åœ¨å‘é€é€šçŸ¥...",
                stage="notification",
                pipeline_status=PipelineTaskStatus.ASSEMBLING,
            )
            if task.recipients:
                try:
                    # ç”Ÿæˆä¸‹è½½URLï¼ˆè‹¥æœ‰reportï¼‰
                    download_url = None
                    try:
                        if execution_result.get("report", {}).get("storage_path"):
                            storage = get_hybrid_storage_service()
                            download_url = storage.get_download_url(execution_result["report"]["storage_path"], expires=86400)
                    except Exception as e:
                        logger.warning(f"Failed to generate download URL: {e}")

                    # ä½¿ç”¨DeliveryService å‘é€é‚®ä»¶ï¼ˆè‹¥å¯ç”¨ï¼‰æˆ–é€šçŸ¥æœåŠ¡
                    from app.services.infrastructure.delivery.delivery_service import create_delivery_service, DeliveryRequest, DeliveryMethod, StorageConfig, EmailConfig, NotificationConfig
                    delivery_service = create_delivery_service(str(task.owner_id))
                    # å‹å¥½åç§°: ä»»åŠ¡å+æ—¶é—´
                    friendly_name = execution_result.get("report", {}).get("friendly_name") or f"report_{task.id}.docx"
                    ts_email = datetime.utcnow().strftime('%Y%m%d_%H%M%S')
                    email_config = EmailConfig(
                        recipients=task.recipients,
                        subject=f"æŠ¥å‘Šç”Ÿæˆå®Œæˆ - {task.name} - {ts_email}",
                        body=(
                            f"æŠ¥å‘Šå·²ç”Ÿæˆ: {friendly_name}\n\n"
                            f"ä¸‹è½½é“¾æ¥: {download_url if download_url else 'è¯·ç™»å½•ç³»ç»ŸæŸ¥çœ‹'}\n"
                            f"ä»»åŠ¡: {task.name}\næ—¶é—´çª—å£: {time_window['start']} - {time_window['end']}\n"
                        ),
                        attach_files=False
                    )
                    req = DeliveryRequest(
                        task_id=str(task_id),
                        user_id=str(task.owner_id),
                        files=[],
                        delivery_method=DeliveryMethod.EMAIL_ONLY,
                        storage_config=StorageConfig(bucket_name="reports", path_prefix=f"reports/{task.owner_id}", public_access=False, retention_days=90),
                        email_config=email_config,
                        notification_config=NotificationConfig(channels=["system"], message="æŠ¥å‘Šå·²ç”Ÿæˆ", priority="normal"),
                        metadata={"report_path": execution_result.get("report", {}).get("storage_path")}
                    )
                    # åœ¨åŒæ­¥ä»»åŠ¡ä¸­æ‰§è¡Œå¼‚æ­¥æŠ•é€’
                    run_async(delivery_service.deliver_report(req))
                except Exception as e:
                    logger.error(f"Failed to send success notification for task {task_id}: {e}")
        
        final_message = "ä»»åŠ¡æ‰§è¡Œå®Œæˆ" if overall_success else f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {report_info.get('error')}"
        final_pipeline_status = PipelineTaskStatus.COMPLETED if overall_success else PipelineTaskStatus.FAILED
        update_progress(
            100,
            final_message,
            stage="completion",
            pipeline_status=final_pipeline_status,
            status="success" if overall_success else "failed",
            error=report_info.get("error") if not overall_success else None,
        )

        if overall_success:
            # æŠ¥å‘ŠæˆåŠŸç”Ÿæˆï¼Œä»»åŠ¡æˆåŠŸ
            complete_message = "ä»»åŠ¡æ‰§è¡Œå®Œæˆ"
            if not etl_success:
                # å¦‚æœETLæœ‰éƒ¨åˆ†å¤±è´¥ï¼Œåœ¨æ¶ˆæ¯ä¸­æåŠï¼ˆä½†ä¸å½±å“æˆåŠŸçŠ¶æ€ï¼‰
                complete_message += "ï¼ˆETLé˜¶æ®µæœ‰éƒ¨åˆ†å ä½ç¬¦å¤±è´¥ï¼Œä½†ä¸å½±å“æŠ¥å‘Šç”Ÿæˆï¼‰"
            
            progress_recorder.complete(
                complete_message,
                result={
                    "task_id": task_id,
                    "execution_id": str(task_execution.execution_id),
                },
            )
            logger.info(f"Task {task_id} completed successfully in {task_execution.total_duration}s")
        else:
            # æŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼Œä»»åŠ¡å¤±è´¥
            failure_reasons = []
            if not report_generated:
                failure_reasons.append("æŠ¥å‘Šç”Ÿæˆå¤±è´¥")
                if report_info.get("error"):
                    failure_reasons.append(f"åŸå› : {report_info.get('error')}")
            
            if not failure_reasons:
                failure_reasons.append("æœªçŸ¥é”™è¯¯")
            
            error_message = "ä»»åŠ¡æ‰§è¡Œå¤±è´¥: " + "ï¼Œ".join(failure_reasons)
            
            progress_recorder.fail(
                message=error_message,
                stage="document_generation",
                error_details={"error": report_info.get("error") or "æŠ¥å‘Šæ–‡æ¡£æœªç”Ÿæˆ"},
            )
            logger.warning(f"Task {task_id} completed with failures in {task_execution.total_duration}s: {error_message}")

        return {
            "status": "completed" if overall_success else "failed",
            "task_id": task_id,
            "execution_id": str(task_execution.execution_id),
            "execution_time": task_execution.total_duration,
            "result": execution_result
        }
        
    except Exception as e:
        error_message = str(e)
        is_cancelled = "å–æ¶ˆ" in error_message or "cancelled" in error_message.lower()

        if is_cancelled:
            logger.info(f"Task {task_id} was cancelled: {error_message}")
        else:
            logger.error(f"Task {task_id} failed: {error_message}", exc_info=True)

        if 'progress_recorder' in locals():
            try:
                progress_recorder.fail(
                    message="ä»»åŠ¡æ‰§è¡Œå¤±è´¥" if not is_cancelled else "ä»»åŠ¡å·²å–æ¶ˆ",
                    stage="cancelled" if is_cancelled else "failure",
                    error_details={"error": error_message},
                )
            except Exception as notify_error:
                logger.warning(f"Failed to record failure progress for task {task_id}: {notify_error}")

        # æ›´æ–°å¤±è´¥/å–æ¶ˆçŠ¶æ€
        if task_execution_id:
            task_execution = db.query(TaskExecution).filter(TaskExecution.id == task_execution_id).first()
            if task_execution:
                # å¦‚æœå·²ç»æ ‡è®°ä¸ºCANCELLEDï¼Œä¿æŒè¯¥çŠ¶æ€
                if task_execution.execution_status != TaskStatus.CANCELLED:
                    task_execution.execution_status = TaskStatus.CANCELLED if is_cancelled else TaskStatus.FAILED
                task_execution.completed_at = datetime.utcnow()
                task_execution.error_details = error_message
                task_execution.total_duration = int((task_execution.completed_at - task_execution.started_at).total_seconds()) if task_execution.started_at else 0

        # æ›´æ–°ä»»åŠ¡ç»Ÿè®¡
        task = db.query(Task).filter(Task.id == task_id).first()
        if task:
            task.status = TaskStatus.CANCELLED if is_cancelled else TaskStatus.FAILED
            if not is_cancelled:  # åªæœ‰å¤±è´¥æ‰å¢åŠ å¤±è´¥è®¡æ•°ï¼Œå–æ¶ˆä¸ç®—å¤±è´¥
                task.failure_count += 1
            
        db.commit()
        
        # å‘é€å¤±è´¥é€šçŸ¥ (ä¸å‘é€å–æ¶ˆé€šçŸ¥)
        if task and task.recipients and not is_cancelled:
            try:
                notification_service.send_task_completion_notification(
                    task_id=task_id,
                    task_name=task.name,
                    recipients=task.recipients,
                    execution_result={"error": str(e)},
                    success=False
                )
            except Exception as notification_error:
                logger.error(f"Failed to send failure notification for task {task_id}: {notification_error}")
        
        # å¯¹äºå–æ¶ˆæ“ä½œï¼Œè¿”å›å–æ¶ˆçŠ¶æ€è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
        if is_cancelled:
            return {
                "status": "cancelled",
                "task_id": task_id,
                "message": "ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ",
                "execution_id": str(task_execution.execution_id) if task_execution else None
            }

        # å¯¹äºå¤±è´¥çš„ä»»åŠ¡ï¼Œé‡æ–°æŠ›å‡ºå¼‚å¸¸è®©Celeryå¤„ç†é‡è¯•
        raise

@celery_app.task(bind=True, name='tasks.infrastructure.validate_placeholders_task')
def validate_placeholders_task(self, template_id: str, data_source_id: str, user_id: str) -> Dict[str, Any]:
    """
    éªŒè¯æ¨¡æ¿å ä½ç¬¦ä»»åŠ¡
    
    Args:
        template_id: æ¨¡æ¿ID
        data_source_id: æ•°æ®æºID
        user_id: ç”¨æˆ·ID
    
    Returns:
        Dict: éªŒè¯ç»“æœ
    """
    try:
        # è¿ç§»è¯´æ˜ï¼šæ—§çš„å ä½ç¬¦éªŒè¯æµç¨‹å·²å¼ƒç”¨ã€‚
        # æ–°æ¶æ„åœ¨æ‰§è¡Œé˜¶æ®µç”± Agents è¿›è¡Œ SQL ç”Ÿæˆâ†’æ³¨å…¥â†’æ‰§è¡Œçš„è‡ªéªŒè¯ï¼ˆReActï¼‰ã€‚
        logger.info(f"Placeholder validation (legacy) skipped for template {template_id}; replaced by Agents pipeline")
        return {
            "status": "migrated",
            "template_id": template_id,
            "message": "Validation is handled by Agents pipeline during execution",
        }
        
    except Exception as e:
        logger.error(f"Placeholder validation failed for template {template_id}: {str(e)}", exc_info=True)
        raise

@celery_app.task(bind=True, base=DatabaseTask, name='tasks.infrastructure.scheduled_task_runner')
def scheduled_task_runner(self, db: Session, task_id: int) -> Dict[str, Any]:
    """
    å®šæ—¶ä»»åŠ¡æ‰§è¡Œå™¨ - ç”±è°ƒåº¦å™¨è§¦å‘
    
    Args:
        task_id: ä»»åŠ¡ID
    
    Returns:
        Dict: æ‰§è¡Œç»“æœ
    """
    try:
        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦åº”è¯¥æ‰§è¡Œ
        task = db.query(Task).filter(Task.id == task_id).first()
        if not task or not task.is_active:
            return {"status": "skipped", "reason": "task_inactive_or_not_found"}
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ­£åœ¨è¿›è¡Œçš„æ‰§è¡Œ
        ongoing_execution = db.query(TaskExecution).filter(
            TaskExecution.task_id == task_id,
            TaskExecution.execution_status.in_([TaskStatus.PENDING, TaskStatus.PROCESSING])
        ).first()
        
        if ongoing_execution:
            logger.warning(f"Task {task_id} has ongoing execution, skipping")
            return {"status": "skipped", "reason": "execution_in_progress"}
        
        # æ„å»ºæ‰§è¡Œä¸Šä¸‹æ–‡ï¼ˆåŒ…å«è°ƒåº¦ä¿¡æ¯ï¼‰
        execution_context = {
            "trigger": "scheduled",
            "schedule": task.schedule,
            "triggered_at": datetime.utcnow().isoformat()
        }
        
        # å§”æ‰˜ç»™ä¸»æ‰§è¡Œä»»åŠ¡
        result = execute_report_task.delay(task_id, execution_context)
        
        logger.info(f"Scheduled task {task_id} delegated to execution task {result.id}")
        
        return {
            "status": "delegated",
            "task_id": task_id,
            "execution_task_id": result.id
        }
        
    except Exception as e:
        logger.error(f"Scheduled task runner failed for task {task_id}: {str(e)}", exc_info=True)
        raise

@celery_app.task(name='tasks.infrastructure.cleanup_old_executions')
def cleanup_old_executions(days_to_keep: int = 30) -> Dict[str, Any]:
    """
    æ¸…ç†æ—§çš„ä»»åŠ¡æ‰§è¡Œè®°å½•
    
    Args:
        days_to_keep: ä¿ç•™å¤©æ•°ï¼Œé»˜è®¤30å¤©
    
    Returns:
        Dict: æ¸…ç†ç»“æœ
    """
    try:
        with SessionLocal() as db:
            cutoff_date = datetime.utcnow() - timedelta(days=days_to_keep)
            
            # åˆ é™¤æ—§çš„æ‰§è¡Œè®°å½•
            deleted_count = db.query(TaskExecution).filter(
                TaskExecution.created_at < cutoff_date,
                TaskExecution.execution_status.in_([TaskStatus.COMPLETED, TaskStatus.FAILED, TaskStatus.CANCELLED])
            ).delete()
            
            db.commit()
            
            logger.info(f"Cleaned up {deleted_count} old task executions")
            
            return {
                "status": "completed",
                "deleted_count": deleted_count,
                "cutoff_date": cutoff_date.isoformat()
            }
            
    except Exception as e:
        logger.error(f"Cleanup task failed: {str(e)}", exc_info=True)
        raise

# æ³¨å†Œå‘¨æœŸæ€§ä»»åŠ¡
@celery_app.on_after_configure.connect
def setup_periodic_tasks(sender, **kwargs):
    """è®¾ç½®å‘¨æœŸæ€§ä»»åŠ¡"""
    
    # æ¯å¤©å‡Œæ™¨2ç‚¹æ¸…ç†æ—§çš„æ‰§è¡Œè®°å½•
    sender.add_periodic_task(
        crontab(hour=2, minute=0),
        cleanup_old_executions.s(),
        name='cleanup_old_executions_daily',
    )
    
    logger.info("âœ… Periodic tasks configured")

logger.info("âœ… Task infrastructure layer loaded")
