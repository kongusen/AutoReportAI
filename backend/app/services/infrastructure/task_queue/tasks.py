"""
Infrastructureå±‚ - Celeryä»»åŠ¡å®šä¹‰

åŸºäºDDDæ¶æ„çš„Celeryä»»åŠ¡å®šä¹‰ï¼Œä½¿ç”¨æ–°çš„TaskExecutionServiceèƒ½åŠ›
"""

import asyncio
import logging
import os
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional
from uuid import UUID

from celery import Task as CeleryTask
from celery.schedules import crontab
from sqlalchemy.orm import Session

from app.db.session import SessionLocal
from app.core.config import settings
from app.core.container import container
from app.models.task import Task, TaskExecution, TaskStatus
from app.models.report_history import ReportHistory
from app.services.application.placeholder.placeholder_service import (
    PlaceholderApplicationService as PlaceholderProcessingSystem,
)
from app.services.infrastructure.notification.notification_service import NotificationService
from app.services.infrastructure.storage.hybrid_storage_service import (
    get_hybrid_storage_service,
)
from app.services.infrastructure.task_queue.celery_config import celery_app
from app.services.infrastructure.task_queue.progress_recorder import TaskProgressRecorder
from app.services.infrastructure.websocket.pipeline_notifications import (
    PipelineTaskStatus,
)
from app.utils.time_context import TimeContextManager
from app.utils.json_utils import convert_for_json

logger = logging.getLogger(__name__)

def run_async(coro):
    """
    åœ¨åŒæ­¥ä¸Šä¸‹æ–‡ä¸­å®‰å…¨åœ°æ‰§è¡Œå¼‚æ­¥ä»£ç 

    å¤„ç† Celery worker ä¸­å¯èƒ½å·²å­˜åœ¨çš„äº‹ä»¶å¾ªç¯é—®é¢˜
    """
    try:
        # å°è¯•è·å–å½“å‰è¿è¡Œçš„äº‹ä»¶å¾ªç¯
        loop = asyncio.get_running_loop()
    except RuntimeError:
        # æ²¡æœ‰è¿è¡Œçš„äº‹ä»¶å¾ªç¯ï¼Œå¯ä»¥ä½¿ç”¨ asyncio.run()
        return asyncio.run(coro)
    else:
        # å·²ç»æœ‰è¿è¡Œçš„äº‹ä»¶å¾ªç¯ï¼Œéœ€è¦ä½¿ç”¨ nest_asyncio æˆ–åˆ›å»ºæ–°çº¿ç¨‹
        # ä½¿ç”¨ nest_asyncio å…è®¸åµŒå¥—äº‹ä»¶å¾ªç¯
        try:
            import nest_asyncio
            nest_asyncio.apply()
            return asyncio.run(coro)
        except ImportError:
            # å¦‚æœ nest_asyncio ä¸å¯ç”¨ï¼Œä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œ
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor() as pool:
                future = pool.submit(asyncio.run, coro)
                return future.result()

class DatabaseTask(CeleryTask):
    """å¸¦æ•°æ®åº“ä¼šè¯çš„åŸºç¡€ä»»åŠ¡ç±»"""
    
    def __call__(self, *args, **kwargs):
        """æ‰§è¡Œä»»åŠ¡æ—¶è‡ªåŠ¨ç®¡ç†æ•°æ®åº“ä¼šè¯"""
        with SessionLocal() as db:
            return self.run_with_db(db, *args, **kwargs)
    
    def run_with_db(self, db: Session, *args, **kwargs):
        """å­ç±»éœ€è¦å®ç°çš„æ–¹æ³•"""
        return self.run(db, *args, **kwargs)

@celery_app.task(bind=True, base=DatabaseTask, name='tasks.infrastructure.execute_report_task')
def execute_report_task(self, db: Session, task_id: int, execution_context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    æ‰§è¡ŒæŠ¥å‘Šç”Ÿæˆä»»åŠ¡ - ä½¿ç”¨æ–°çš„TaskExecutionService

    Args:
        task_id: ä»»åŠ¡ID
        execution_context: æ‰§è¡Œä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰

    Returns:
        Dict: æ‰§è¡Œç»“æœ
    """
    task_execution_id = None
    notification_service = NotificationService()

    # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«æ’¤é”€çš„è¾…åŠ©å‡½æ•°
    def check_if_cancelled():
        """æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«æ’¤é”€"""
        try:
            # æ–¹æ³•1: æ£€æŸ¥Celeryçš„æ’¤é”€çŠ¶æ€
            from celery.result import AsyncResult
            result = AsyncResult(self.request.id)
            if result.state == 'REVOKED':
                logger.info(f"Task {task_id} detected as REVOKED via Celery state")
                raise Exception("ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ")

            # æ–¹æ³•2: æ£€æŸ¥æ•°æ®åº“ä¸­çš„æ‰§è¡ŒçŠ¶æ€
            if task_execution_id:
                exec_record = db.query(TaskExecution).filter(TaskExecution.id == task_execution_id).first()
                if exec_record and exec_record.execution_status == TaskStatus.CANCELLED:
                    logger.info(f"Task {task_id} detected as CANCELLED in database")
                    raise Exception("ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ")
        except Exception as e:
            if "å–æ¶ˆ" in str(e) or "cancelled" in str(e).lower():
                raise

    try:
        # 1. è·å–ä»»åŠ¡ä¿¡æ¯
        task = db.query(Task).filter(Task.id == task_id).first()
        if not task:
            raise ValueError(f"Task {task_id} not found")
        
        if not task.is_active:
            logger.info(f"Task {task_id} is not active, skipping execution")
            return {"status": "skipped", "reason": "task_inactive"}
        
        # 2. åˆ›å»ºä»»åŠ¡æ‰§è¡Œè®°å½•
        task_execution = TaskExecution(
            task_id=task_id,
            execution_status=TaskStatus.PROCESSING,
            workflow_type=task.workflow_type,
            started_at=datetime.utcnow(),
            celery_task_id=self.request.id,
            execution_context=execution_context or {},
            progress_percentage=0
        )
        db.add(task_execution)
        db.commit()
        task_execution_id = task_execution.id

        progress_recorder = TaskProgressRecorder(
            db=db,
            task=task,
            task_execution=task_execution,
        )
        progress_recorder.start("ä»»åŠ¡å¼€å§‹")

        # å®šä¹‰è¿›åº¦æ›´æ–°å‡½æ•°
        def update_progress(
            percentage: int,
            message: str = "",
            *,
            stage: Optional[str] = None,
            pipeline_status: PipelineTaskStatus = PipelineTaskStatus.ANALYZING,
            status: str = "running",
            placeholder: Optional[str] = None,
            details: Optional[Dict[str, Any]] = None,
            error: Optional[str] = None,
            record_only: bool = False,
        ):
            progress_recorder.update(
                percentage,
                message,
                stage=stage,
                pipeline_status=pipeline_status,
                status=status,
                placeholder=placeholder,
                details=details,
                error=error,
                record_only=record_only,
            )

        # åˆå§‹åŒ–é˜¶æ®µ
        update_progress(
            5,
            "ä»»åŠ¡åˆå§‹åŒ–å®Œæˆ",
            stage="initialization",
            pipeline_status=PipelineTaskStatus.SCANNING,
        )

        # æ£€æŸ¥æ˜¯å¦è¢«å–æ¶ˆ
        check_if_cancelled()

        # 3. æ›´æ–°ä»»åŠ¡çŠ¶æ€
        task.status = TaskStatus.PROCESSING
        task.execution_count += 1
        task.last_execution_at = datetime.utcnow()
        db.commit()

        # 4. ğŸ†• åˆå§‹åŒ– Schema Contextï¼ˆä¸€æ¬¡æ€§è·å–æ‰€æœ‰è¡¨ç»“æ„ï¼‰
        schema_context_retriever = None
        try:
            from app.services.infrastructure.agents.context_retriever import (
                create_schema_context_retriever
            )
            from app.models.data_source import DataSource

            logger.info(f"ğŸ“‹ åˆå§‹åŒ– Schema Context for data_source={task.data_source_id}")

            update_progress(
                8,
                "æ­£åœ¨åˆå§‹åŒ–æ•°æ®è¡¨ç»“æ„ä¸Šä¸‹æ–‡...",
                stage="schema_initialization",
                pipeline_status=PipelineTaskStatus.SCANNING,
            )

            # è·å–æ•°æ®æºé…ç½®
            data_source = db.query(DataSource).filter(DataSource.id == task.data_source_id).first()
            if not data_source:
                raise RuntimeError(f"æ•°æ®æº {task.data_source_id} ä¸å­˜åœ¨")

            # æ„å»ºè¿æ¥é…ç½®
            connection_config = data_source.connection_config or {}
            if not connection_config:
                raise RuntimeError(f"æ•°æ®æº {task.data_source_id} ç¼ºå°‘è¿æ¥é…ç½®")

            # ğŸ†• å¯ç”¨é˜¶æ®µæ„ŸçŸ¥çš„æ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç†
            schema_context_retriever = create_schema_context_retriever(
                data_source_id=str(task.data_source_id),
                connection_config=connection_config,
                container=container,
                top_k=10,  # Task æ‰¹é‡åˆ†æï¼Œå¤šç¼“å­˜ä¸€äº›è¡¨
                inject_as="system",
                enable_stage_aware=True  # ğŸ”¥ å¯ç”¨é˜¶æ®µæ„ŸçŸ¥
            )

            # é¢„åŠ è½½æ‰€æœ‰è¡¨ç»“æ„ï¼ˆç¼“å­˜ï¼‰
            run_async(schema_context_retriever.initialize())

            table_count = len(schema_context_retriever.schema_cache)
            logger.info(f"âœ… Schema Context åˆå§‹åŒ–å®Œæˆï¼Œç¼“å­˜äº† {table_count} ä¸ªè¡¨")

            update_progress(
                9,
                f"æ•°æ®è¡¨ç»“æ„ç¼“å­˜å®Œæˆï¼ˆ{table_count} ä¸ªè¡¨ï¼‰",
                stage="schema_initialization",
                pipeline_status=PipelineTaskStatus.SCANNING,
            )

        except Exception as e:
            logger.warning(f"âš ï¸ Schema Context åˆå§‹åŒ–å¤±è´¥: {e}", exc_info=True)
            # ä¸è¦è®©æ•´ä¸ªä»»åŠ¡å¤±è´¥ï¼Œå…è®¸é™çº§è¿è¡Œï¼ˆAgent å¯èƒ½ä¼šä½¿ç”¨æ—§çš„ schema å·¥å…·æˆ–çŒœæµ‹è¡¨ç»“æ„ï¼‰
            logger.info("ğŸ’¡ å°†åœ¨æ²¡æœ‰ Schema Context çš„æƒ…å†µä¸‹ç»§ç»­æ‰§è¡Œï¼ˆå¯èƒ½éœ€è¦ Agent è°ƒç”¨å…¶ä»–å·¥å…·è·å–è¡¨ç»“æ„ï¼‰")

            # åˆ›å»ºä¸€ä¸ªç©ºçš„ schema_context_retriever ä»¥é¿å…åç»­ä»£ç å‡ºé”™
            schema_context_retriever = None

            update_progress(
                9,
                "æ•°æ®è¡¨ç»“æ„åˆå§‹åŒ–å¤±è´¥ï¼Œå°†é™çº§è¿è¡Œ",
                stage="schema_initialization",
                pipeline_status=PipelineTaskStatus.SCANNING,
                error=str(e)
            )

        # 5. åˆå§‹åŒ–æ—¶é—´ä¸Šä¸‹æ–‡å’Œ PlaceholderProcessingSystem
        # Initialize placeholder processing system with schema context
        system = PlaceholderProcessingSystem(
            user_id=str(task.owner_id),
            context_retriever=schema_context_retriever  # ğŸ”¥ ä¼ å…¥ context
        )

        # ğŸ†• è·å–é˜¶æ®µæ„ŸçŸ¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å’Œå·¥å…·è®°å½•å™¨
        state_manager = getattr(schema_context_retriever, 'state_manager', None)
        tool_recorder = None
        if state_manager:
            from app.services.infrastructure.agents.tool_wrapper import ToolResultRecorder
            from app.services.infrastructure.agents.context_manager import ExecutionStage
            tool_recorder = ToolResultRecorder(state_manager)
            logger.info("âœ… ä»»åŠ¡å·²å¯ç”¨é˜¶æ®µæ„ŸçŸ¥ä¸Šä¸‹æ–‡ç®¡ç†å’Œå·¥å…·ç»“æœè®°å½•")

        time_ctx_mgr = TimeContextManager()
        
        # 5. å‡†å¤‡æ‰§è¡Œå‚æ•°
        execution_params = {
            "task_id": task_id,
            "template_id": str(task.template_id),
            "data_source_id": str(task.data_source_id),
            "report_period": task.report_period.value if task.report_period else "monthly",
            "user_id": str(task.owner_id),
            "execution_id": str(task_execution.execution_id),
            "recipients": task.recipients or [],
            "schedule": task.schedule
        }
        
        logger.info(f"Starting task execution for task {task_id} with Agents pipeline")

        # 5. ç”Ÿæˆæ—¶é—´çª—å£ï¼ˆåŸºäºä»»åŠ¡æŠ¥å‘Šå‘¨æœŸï¼‰
        update_progress(
            10,
            "æ­£åœ¨ç”Ÿæˆæ—¶é—´ä¸Šä¸‹æ–‡...",
            stage="time_context",
            pipeline_status=PipelineTaskStatus.SCANNING,
        )
        time_ctx = time_ctx_mgr.generate_time_context(
            report_period=task.report_period.value if task.report_period else "monthly",
            execution_time=datetime.utcnow(),
            schedule=task.schedule,
        )
        time_window = {
            "start": f"{time_ctx.get('period_start_date')} 00:00:00",
            "end": f"{time_ctx.get('period_end_date')} 23:59:59",
        }

        # 6. è¿è¡ŒReActæµæ°´çº¿ï¼ˆç”ŸæˆSQLâ†’æ³¨å…¥æ—¶é—´â†’æ‰§è¡Œâ†’è‡ªä¿®æ­£ï¼‰
        update_progress(
            15,
            "æ­£åœ¨åˆå§‹åŒ–Agentç³»ç»Ÿ...",
            stage="agent_initialization",
        )

        # æ£€æŸ¥æ˜¯å¦è¢«å–æ¶ˆ
        check_if_cancelled()

        run_async(system.initialize())
        events = []

        update_progress(
            20,
            "æ­£åœ¨æ£€æŸ¥å ä½ç¬¦çŠ¶æ€...",
            stage="placeholder_precheck",
        )

        # æ£€æŸ¥æ˜¯å¦è¢«å–æ¶ˆ
        check_if_cancelled()

        # æ™ºèƒ½å¢é‡å ä½ç¬¦è§£æç­–ç•¥
        placeholders_need_analysis = []
        placeholders_ready = []

        try:
            from app.crud import template_placeholder as crud_template_placeholder
            from app.models.template import Template
            from app.services.domain.template.services.template_domain_service import TemplateParser
            import re

            # è·å–æ¨¡æ¿å†…å®¹
            template = db.query(Template).filter(Template.id == task.template_id).first()
            template_content = template.content if template else ""

            # è·å–æ•°æ®åº“ä¸­å·²æœ‰çš„å ä½ç¬¦
            existing_placeholders = crud_template_placeholder.get_by_template(db, str(task.template_id))
            existing_placeholder_names = {ph.placeholder_name for ph in existing_placeholders or []}

            # ä»æ¨¡æ¿å†…å®¹ä¸­æå–å ä½ç¬¦
            content_placeholders = set()
            if template_content:
                # æå– {{...}} æ ¼å¼çš„å ä½ç¬¦
                placeholder_pattern = r'\{\{([^}]+)\}\}'
                matches = re.findall(placeholder_pattern, template_content)
                content_placeholders = {match.strip() for match in matches}

            total_content_placeholders = len(content_placeholders)
            total_existing_placeholders = len(existing_placeholders or [])

            logger.info(f"æ¨¡æ¿å†…å®¹ä¸­å‘ç° {total_content_placeholders} ä¸ªå ä½ç¬¦ï¼Œæ•°æ®åº“ä¸­å·²æœ‰ {total_existing_placeholders} ä¸ªå ä½ç¬¦è®°å½•")

            # æ‰¾å‡ºéœ€è¦æ–°å»ºçš„å ä½ç¬¦ï¼ˆåœ¨å†…å®¹ä¸­ä½†ä¸åœ¨æ•°æ®åº“ä¸­ï¼‰
            new_placeholders_to_create = content_placeholders - existing_placeholder_names

            # åˆ›å»ºæ–°å‘ç°çš„å ä½ç¬¦è®°å½•
            if new_placeholders_to_create:
                update_progress(
                    22,
                    f"å‘ç° {len(new_placeholders_to_create)} ä¸ªæ–°å ä½ç¬¦ï¼Œæ­£åœ¨åˆ›å»ºè®°å½•...",
                    stage="placeholder_precheck",
                    details={"new_placeholders": len(new_placeholders_to_create)},
                )
                logger.info(f"Creating {len(new_placeholders_to_create)} new placeholder records")

                from app.models.template_placeholder import TemplatePlaceholder
                import uuid
                for placeholder_name in new_placeholders_to_create:
                    new_placeholder = TemplatePlaceholder(
                        id=uuid.uuid4(),
                        template_id=task.template_id,
                        placeholder_name=placeholder_name,
                        placeholder_text=placeholder_name,  # ä½¿ç”¨å ä½ç¬¦åç§°ä½œä¸ºé»˜è®¤æ–‡æœ¬
                        placeholder_type="text",  # é»˜è®¤ç±»å‹ï¼Œåç»­åˆ†ææ—¶ä¼šæ›´æ–°
                        content_type="data",  # é»˜è®¤ä¸ºæ•°æ®ç±»å‹
                        agent_analyzed=False,  # å°šæœªåˆ†æ
                        generated_sql=None,
                        sql_validated=False,
                        execution_order=0,  # é»˜è®¤é¡ºåº
                        cache_ttl_hours=24,  # é»˜è®¤ç¼“å­˜24å°æ—¶
                        is_required=True,  # é»˜è®¤ä¸ºå¿…éœ€
                        is_active=True,  # é»˜è®¤æ¿€æ´»
                        confidence_score=0.0,  # åˆå§‹ç½®ä¿¡åº¦
                        created_at=datetime.utcnow()
                    )
                    db.add(new_placeholder)
                db.commit()

                # é‡æ–°è·å–æ‰€æœ‰å ä½ç¬¦
                existing_placeholders = crud_template_placeholder.get_by_template(db, str(task.template_id))

            required_fields: set[str] = set()

            # æ£€æŸ¥æ‰€æœ‰å ä½ç¬¦çš„åˆ†æçŠ¶æ€
            for ph in existing_placeholders or []:
                # æ£€æŸ¥å ä½ç¬¦æ˜¯å¦éœ€è¦é‡æ–°åˆ†æ
                needs_analysis = (
                    not ph.generated_sql or  # æ²¡æœ‰ç”Ÿæˆçš„SQL
                    not ph.sql_validated or  # SQLæœªéªŒè¯é€šè¿‡
                    ph.generated_sql.strip() == ""  # SQLä¸ºç©º
                )

                if needs_analysis:
                    placeholders_need_analysis.append(ph)
                    logger.info(f"Placeholder '{ph.placeholder_name}' needs analysis: no_sql={not ph.generated_sql}, not_validated={not ph.sql_validated}")
                else:
                    placeholders_ready.append(ph)
                    logger.info(f"Placeholder '{ph.placeholder_name}' is ready with valid SQL")

                # æ”¶é›†æ‰€æœ‰required_fields
                rf = getattr(ph, 'required_fields', None)
                if isinstance(rf, list):
                    for f in rf:
                        if isinstance(f, str):
                            required_fields.add(f)
                elif isinstance(rf, dict):
                    for key in ('columns', 'fields', 'required_fields'):
                        val = rf.get(key)
                        if isinstance(val, list):
                            for f in val:
                                if isinstance(f, str):
                                    required_fields.add(f)
                            break

                # Fallback to parsing_metadata if present
                pm = getattr(ph, 'parsing_metadata', None)
                if isinstance(pm, dict):
                    meta_rf = pm.get('required_fields') or pm.get('metadata', {}).get('required_fields')
                    if isinstance(meta_rf, list):
                        for f in meta_rf:
                            if isinstance(f, str):
                                required_fields.add(f)

            required_fields = sorted(required_fields)

            total_placeholders = len(existing_placeholders) if existing_placeholders else 0
            if placeholders_need_analysis:
                update_progress(
                    25,
                    f"éœ€è¦åˆ†æ {len(placeholders_need_analysis)} ä¸ªå ä½ç¬¦ï¼ˆå…± {total_placeholders} ä¸ªï¼‰...",
                    stage="placeholder_analysis",
                    details={
                        "pending": len(placeholders_need_analysis),
                        "total": total_placeholders,
                    },
                )
                logger.info(f"Found {len(placeholders_need_analysis)} placeholders needing analysis, {len(placeholders_ready)} ready")
            else:
                if total_placeholders == 0:
                    update_progress(
                        35,
                        "æ¨¡æ¿æ— å ä½ç¬¦ï¼Œè·³è¿‡åˆ†æé˜¶æ®µ...",
                        stage="placeholder_analysis",
                        details={"total": 0},
                    )
                    logger.info(f"Template has no placeholders, skipping analysis phase")
                else:
                    update_progress(
                        35,
                        f"æ‰€æœ‰ {len(placeholders_ready)} ä¸ªå ä½ç¬¦å·²å°±ç»ªï¼Œè·³è¿‡åˆ†æé˜¶æ®µ...",
                        stage="placeholder_analysis",
                        details={
                            "ready": len(placeholders_ready),
                            "total": total_placeholders,
                        },
                    )
                    logger.info(f"All {len(placeholders_ready)} placeholders are ready, skipping analysis")

        except Exception as e:
            logger.warning(f"Failed to load/parse placeholders: {e}")
            required_fields = []

        success_criteria = {
            "min_rows": 1,
            "max_rows": 100000,
            "required_fields": required_fields,
            "quality_threshold": 0.6,
        }
        # æ ¹æ®æ˜¯å¦éœ€è¦åˆ†æå†³å®šæ‰§è¡Œè·¯å¾„
        if placeholders_need_analysis:
            # ğŸ†• è®¾ç½®é˜¶æ®µä¸ºPLANNING - å‡†å¤‡ç”ŸæˆSQL
            if state_manager:
                state_manager.set_stage(ExecutionStage.PLANNING)
                logger.info("ğŸ¯ è®¾ç½®Agenté˜¶æ®µä¸º PLANNING - å‡†å¤‡æ‰¹é‡ç”ŸæˆSQL")

            # ä½¿ç”¨PlaceholderApplicationServiceå•ä¸ªå¤„ç†æ¯ä¸ªå ä½ç¬¦
            update_progress(
                30,
                f"æ­£åœ¨é€ä¸ªåˆ†æ {len(placeholders_need_analysis)} ä¸ªå ä½ç¬¦...",
                stage="placeholder_analysis",
                details={
                    "pending": len(placeholders_need_analysis),
                    "total": len(existing_placeholders or []),
                },
            )

            async def _process_placeholders_individually():
                """
                å•ä¸ªå¾ªç¯å¤„ç†å ä½ç¬¦ + æ‰¹é‡æŒä¹…åŒ–ï¼ˆæ–¹æ¡ˆ1ä¼˜åŒ–ï¼‰

                ä¼˜åŒ–ç­–ç•¥:
                - ä¿æŒä¸²è¡Œå¤„ç†ç¡®ä¿è´¨é‡ç¨³å®š
                - æ¯5ä¸ªå ä½ç¬¦æ‰¹é‡æäº¤ä¸€æ¬¡ï¼Œå‡å°‘æ•°æ®åº“å‹åŠ›
                - æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼ˆå®šæœŸä¿å­˜è¿›åº¦ï¼‰
                """
                processed_count = 0
                total_count = len(placeholders_need_analysis)
                batch_updates = []  # ğŸ‘ˆ æ”¶é›†æ‰¹é‡æ›´æ–°
                BATCH_SIZE = 5  # ğŸ‘ˆ æ‰¹é‡å¤§å°é…ç½®

                for ph in placeholders_need_analysis:
                    try:
                        # æ£€æŸ¥æ˜¯å¦è¢«å–æ¶ˆ
                        check_if_cancelled()

                        update_progress(
                            30 + int(30 * processed_count / total_count),
                            f"æ­£åœ¨åˆ†æå ä½ç¬¦: {ph.placeholder_name} ({processed_count + 1}/{total_count})",
                            stage="placeholder_analysis",
                            placeholder=ph.placeholder_name,
                            details={
                                "current": processed_count + 1,
                                "total": total_count,
                            },
                        )

                        # ğŸ‘‡ æ„å»ºçœŸå®çš„ä»»åŠ¡ä¸Šä¸‹æ–‡
                        real_task_context = {
                            "task_id": task_id,
                            "task_name": task.name,
                            "template_id": str(task.template_id),
                            "user_id": str(task.owner_id),
                            "report_period": task.report_period.value if task.report_period else "monthly",
                            "schedule": task.schedule,  # çœŸå® cron è¡¨è¾¾å¼
                            "time_window": time_window,  # çœŸå®æ—¶é—´çª—å£
                            "time_context": time_ctx,  # å®Œæ•´æ—¶é—´ä¸Šä¸‹æ–‡
                            "execution_trigger": execution_context.get("trigger", "scheduled") if execution_context else "scheduled",
                            "execution_id": str(task_execution.execution_id),
                        }

                        # ğŸ†• é€‰æ‹©å ä½ç¬¦åˆ†ææ–¹æ³•ï¼šç›´æ¥è°ƒç”¨æˆ–ä½¿ç”¨ Celery ä»»åŠ¡
                        use_celery_task = getattr(settings, 'USE_CELERY_PLACEHOLDER_ANALYSIS', False)
                        
                        if use_celery_task:
                            # ä½¿ç”¨æ–°çš„ Celery å ä½ç¬¦åˆ†æä»»åŠ¡
                            from app.services.infrastructure.task_queue.placeholder_tasks import analyze_single_placeholder_task
                            
                            logger.info(f"ğŸ”„ ä½¿ç”¨ Celery ä»»åŠ¡åˆ†æå ä½ç¬¦: {ph.placeholder_name}")
                            
                            # è§¦å‘ Celery ä»»åŠ¡
                            celery_task = analyze_single_placeholder_task.delay(
                                placeholder_name=ph.placeholder_name,
                                placeholder_text=ph.placeholder_text,
                                template_id=str(task.template_id),
                                data_source_id=str(task.data_source_id),
                                user_id=str(task.owner_id),
                                template_context=real_task_context.get("template_context"),
                                time_window=real_task_context.get("time_window"),
                                time_column=real_task_context.get("time_column"),
                                data_range=real_task_context.get("data_range", "day"),
                                requirements=real_task_context.get("requirements"),
                                execute_sql=False,  # ä»»åŠ¡æ‰§è¡Œé˜¶æ®µä¸æ‰§è¡ŒSQLï¼Œåªç”Ÿæˆ
                                row_limit=1000,
                                **{k: v for k, v in real_task_context.items() if k not in [
                                    "template_context", "time_window", "time_column", "data_range", "requirements"
                                ]}
                            )
                            
                            # ç­‰å¾…ä»»åŠ¡å®Œæˆ
                            celery_result = celery_task.get(timeout=300)  # 5åˆ†é’Ÿè¶…æ—¶
                            
                            if celery_result.get("success"):
                                analysis_result = celery_result.get("analysis_result", {})
                                sql_result = {
                                    "success": True,
                                    "sql": analysis_result.get("generated_sql", {}).get("sql", ""),
                                    "validated": analysis_result.get("generated_sql", {}).get("validated", True),
                                    "confidence": analysis_result.get("confidence_score", 0.9),
                                    "auto_fixed": analysis_result.get("generated_sql", {}).get("auto_fixed", False),
                                    "warning": analysis_result.get("generated_sql", {}).get("warning")
                                }
                                logger.info(f"âœ… Celery ä»»åŠ¡åˆ†ææˆåŠŸ: {ph.placeholder_name}")
                            else:
                                error_msg = celery_result.get("error", "Celery ä»»åŠ¡åˆ†æå¤±è´¥")
                                sql_result = {
                                    "success": False,
                                    "error": error_msg
                                }
                                logger.error(f"âŒ Celery ä»»åŠ¡åˆ†æå¤±è´¥: {ph.placeholder_name}, é”™è¯¯: {error_msg}")
                        else:
                            # ä½¿ç”¨æˆç†Ÿçš„å•å ä½ç¬¦åˆ†æèƒ½åŠ›ï¼ˆé¿å…å¾ªç¯é—®é¢˜ï¼‰
                            async def _analyze_placeholder_async(placeholder_name, placeholder_text, template_id, data_source_id, template_context, user_id):
                                from app.api.endpoints.placeholders import PlaceholderOrchestrationService
                                orchestration_service = PlaceholderOrchestrationService()
                                
                                # è°ƒç”¨æˆç†Ÿçš„å•å ä½ç¬¦åˆ†æï¼Œç»“æœè‡ªåŠ¨ä¿å­˜åˆ°æ•°æ®åº“
                                analysis_result = await orchestration_service.analyze_placeholder_with_full_pipeline(
                                    placeholder_name=placeholder_name,
                                    placeholder_text=placeholder_text,
                                    template_id=template_id,
                                    data_source_id=data_source_id,
                                    template_context=template_context,
                                    user_id=user_id,
                                    **template_context
                                )
                                
                                # è½¬æ¢ä¸ºå½“å‰ä»»åŠ¡æœŸæœ›çš„æ ¼å¼ï¼ˆç”¨äºåç»­ETLæ­¥éª¤ï¼‰
                                if analysis_result.get("status") == "success":
                                    generated_sql = analysis_result.get("generated_sql", {})
                                    return {
                                        "success": True,
                                        "sql": generated_sql.get("sql", ""),
                                        "validated": generated_sql.get("validated", True),
                                        "confidence": analysis_result.get("confidence_score", 0.9),
                                        "auto_fixed": generated_sql.get("auto_fixed", False),
                                        "warning": generated_sql.get("warning")
                                    }
                                else:
                                    return {
                                        "success": False,
                                        "error": analysis_result.get("error", "å ä½ç¬¦åˆ†æå¤±è´¥")
                                    }

                            sql_result = run_async(_analyze_placeholder_async(
                                placeholder_name=ph.placeholder_name,
                                placeholder_text=ph.placeholder_text,
                                template_id=str(tpl_meta['id']),
                                data_source_id=data_source_id,
                                template_context=real_task_context,
                                user_id=str(task.owner_id)
                            ))

                        if sql_result.get("success"):
                            # ğŸ‘‡ æ›´æ–°å ä½ç¬¦SQLï¼ˆä¸ç«‹å³æäº¤ï¼‰
                            ph.generated_sql = sql_result["sql"]
                            # åªæœ‰å½“SQLçœŸæ­£éªŒè¯é€šè¿‡æ—¶æ‰æ ‡è®°ä¸ºå·²éªŒè¯
                            ph.sql_validated = sql_result.get("validated", True)
                            ph.agent_analyzed = True
                            ph.analyzed_at = datetime.utcnow()

                            # å¦‚æœSQLè¢«è‡ªåŠ¨ä¿®å¤ï¼Œè®°å½•åˆ°metadata
                            if sql_result.get("auto_fixed"):
                                ph.agent_config = ph.agent_config or {}
                                ph.agent_config["auto_fixed"] = True
                                ph.agent_config["auto_fix_warning"] = sql_result.get("warning")

                            # ğŸ†• è®°å½•SQLç”Ÿæˆç»“æœï¼ˆä½œä¸ºéªŒè¯æˆåŠŸï¼‰
                            if tool_recorder:
                                tool_recorder.record_sql_validation(
                                    tool_name="sql_generation",
                                    result={
                                        "valid": ph.sql_validated,
                                        "sql": sql_result["sql"],
                                        "auto_fixed": sql_result.get("auto_fixed", False),
                                        "confidence": sql_result.get("confidence", 0.9)
                                    }
                                )

                            batch_updates.append(ph)  # ğŸ‘ˆ æ·»åŠ åˆ°æ‰¹æ¬¡

                            events.append({
                                "type": "placeholder_sql_generated",
                                "placeholder_name": ph.placeholder_name,
                                "sql": sql_result["sql"],
                                "confidence": sql_result.get("confidence", 0.0),
                                "validated": ph.sql_validated,
                                "auto_fixed": sql_result.get("auto_fixed", False),
                                "timestamp": datetime.utcnow().isoformat()
                            })

                            validation_status = "âœ… éªŒè¯é€šè¿‡" if ph.sql_validated else "âš ï¸ æœªéªŒè¯"
                            auto_fix_info = " (è‡ªåŠ¨ä¿®å¤)" if sql_result.get("auto_fixed") else ""
                            logger.info(f"âœ… å ä½ç¬¦ {ph.placeholder_name} SQLç”ŸæˆæˆåŠŸ{auto_fix_info} {validation_status} (æ‰¹æ¬¡: {len(batch_updates)}/{BATCH_SIZE})")

                            # ğŸ‘‡ è¾¾åˆ°æ‰¹é‡å¤§å°æ—¶æäº¤
                            if len(batch_updates) >= BATCH_SIZE:
                                db.commit()
                                logger.info(f"ğŸ“¦ æ‰¹é‡æäº¤ {len(batch_updates)} ä¸ªå ä½ç¬¦åˆ°æ•°æ®åº“")
                                batch_updates.clear()

                        else:
                            error_msg = sql_result.get("error", "SQLç”Ÿæˆå¤±è´¥")
                            logger.error(f"âŒ å ä½ç¬¦ {ph.placeholder_name} SQLç”Ÿæˆå¤±è´¥: {error_msg}")

                            # ğŸ†• åˆ‡æ¢åˆ°ERROR_RECOVERYé˜¶æ®µå¹¶è®°å½•é”™è¯¯
                            if state_manager:
                                state_manager.set_stage(ExecutionStage.ERROR_RECOVERY)
                                from app.services.infrastructure.agents.context_manager import ContextType, ContextItem
                                state_manager.add_context(
                                    key=f"sql_generation_error_{ph.placeholder_name}",
                                    item=ContextItem(
                                        type=ContextType.ERROR_INFO,
                                        content=f"å ä½ç¬¦ {ph.placeholder_name} SQLç”Ÿæˆå¤±è´¥: {error_msg}",
                                        metadata={"placeholder": ph.placeholder_name},
                                        relevance_score=1.0
                                    )
                                )
                                logger.warning("âš ï¸ åˆ‡æ¢åˆ° ERROR_RECOVERY é˜¶æ®µ")

                            events.append({
                                "type": "placeholder_sql_failed",
                                "placeholder_name": ph.placeholder_name,
                                "error": error_msg,
                                "timestamp": datetime.utcnow().isoformat()
                            })

                            update_progress(
                                task_execution.progress_percentage or 30,
                                f"å ä½ç¬¦ {ph.placeholder_name} SQLç”Ÿæˆå¤±è´¥",
                                stage="placeholder_analysis",
                                status="failed",
                                placeholder=ph.placeholder_name,
                                details={
                                    "current": processed_count + 1,
                                    "total": total_count,
                                },
                                error=error_msg,
                                record_only=True,
                            )

                    except Exception as e:
                        logger.error(f"âŒ å¤„ç†å ä½ç¬¦ {ph.placeholder_name} æ—¶å¼‚å¸¸: {e}")
                        events.append({
                            "type": "placeholder_processing_error",
                            "placeholder_name": ph.placeholder_name,
                            "error": str(e),
                            "timestamp": datetime.utcnow().isoformat()
                        })

                        update_progress(
                            task_execution.progress_percentage or 30,
                            f"å ä½ç¬¦ {ph.placeholder_name} å¤„ç†å¼‚å¸¸",
                            stage="placeholder_analysis",
                            status="failed",
                            placeholder=ph.placeholder_name,
                            details={
                                "current": processed_count + 1,
                                "total": total_count,
                            },
                            error=str(e),
                            record_only=True,
                        )

                    processed_count += 1

                # ğŸ‘‡ æäº¤å‰©ä½™çš„å ä½ç¬¦
                if batch_updates:
                    db.commit()
                    logger.info(f"ğŸ“¦ æœ€ç»ˆæ‰¹é‡æäº¤ {len(batch_updates)} ä¸ªå ä½ç¬¦åˆ°æ•°æ®åº“")
                    batch_updates.clear()

                return processed_count

            processed_count = run_async(_process_placeholders_individually())
            update_progress(
                65,
                f"å ä½ç¬¦åˆ†æå®Œæˆï¼ŒæˆåŠŸå¤„ç† {processed_count} ä¸ªå ä½ç¬¦",
                stage="placeholder_analysis",
                details={"processed": processed_count},
            )
        else:
            # æ‰€æœ‰å ä½ç¬¦å·²å°±ç»ªï¼Œç›´æ¥æ‰§è¡ŒETL
            update_progress(
                40,
                "å ä½ç¬¦å·²å°±ç»ªï¼Œç›´æ¥æ‰§è¡ŒETL...",
                stage="placeholder_analysis",
                details={"ready": len(placeholders_ready)},
            )
            # è®°å½•è·³è¿‡åˆ†æçš„äº‹ä»¶
            events.append({
                "type": "analysis_skipped",
                "message": "æ‰€æœ‰å ä½ç¬¦å·²å‡†å¤‡å°±ç»ªï¼Œè·³è¿‡åˆ†æé˜¶æ®µ",
                "timestamp": datetime.utcnow().isoformat(),
                "placeholders_ready": len(placeholders_ready)
            })

        # 7. æ‰§è¡ŒçœŸå®çš„ETLæ•°æ®å¤„ç†æµç¨‹
        # ğŸ†• åˆ‡æ¢åˆ°EXECUTIONé˜¶æ®µ
        if state_manager:
            state_manager.set_stage(ExecutionStage.EXECUTION)
            logger.info("ğŸ¯ åˆ‡æ¢åˆ° EXECUTION é˜¶æ®µ - å¼€å§‹æ‰§è¡ŒSQLæŸ¥è¯¢")

        update_progress(
            70,
            "å¼€å§‹ETLæ•°æ®å¤„ç†...",
            stage="etl_processing",
        )

        # æ£€æŸ¥æ˜¯å¦è¢«å–æ¶ˆ
        check_if_cancelled()

        try:
            # é‡æ–°åŠ è½½æœ€æ–°çš„å ä½ç¬¦æ•°æ®ï¼ˆå¯èƒ½åœ¨Agentåˆ†æåæœ‰æ›´æ–°ï¼‰
            placeholders = crud_template_placeholder.get_by_template(db, str(task.template_id))
            etl_results = {}

            # å¯¼å…¥SQLå ä½ç¬¦æ›¿æ¢å™¨
            from app.utils.sql_placeholder_utils import SqlPlaceholderReplacer
            sql_replacer = SqlPlaceholderReplacer()

            update_progress(
                75,
                "æ­£åœ¨å¤„ç†SQLå ä½ç¬¦æ›¿æ¢å’Œæ‰§è¡ŒæŸ¥è¯¢...",
                stage="etl_processing",
            )

            # å¯¹æ¯ä¸ªæœ‰æ•ˆçš„å ä½ç¬¦è¿›è¡Œå•ä¸ªå¤„ç†
            total_placeholders_count = len(placeholders or [])
            for i, ph in enumerate(placeholders or []):
                # åªè¦æœ‰ç”Ÿæˆçš„SQLå°±å°è¯•æ‰§è¡Œï¼Œä¸è¦æ±‚å¿…é¡»éªŒè¯é€šè¿‡
                # sql_validated åº”è¯¥åœ¨æ‰§è¡ŒæˆåŠŸåè®¾ç½®ï¼Œè€Œä¸æ˜¯ä½œä¸ºæ‰§è¡Œçš„å‰ææ¡ä»¶
                if not ph.generated_sql or (ph.generated_sql and ph.generated_sql.strip() == ""):
                    logger.warning(f"è·³è¿‡å ä½ç¬¦ {ph.placeholder_name}: æ— æœ‰æ•ˆSQL")
                    etl_results[ph.placeholder_name] = {
                        "success": False,
                        "error": "æ— æœ‰æ•ˆSQL",
                        "data": [],
                        "metadata": {},
                        "execution_time": 0,
                        "row_count": 0
                    }
                    update_progress(
                        task_execution.progress_percentage or 75,
                        f"è·³è¿‡å ä½ç¬¦ {ph.placeholder_name}: æ— æœ‰æ•ˆSQL",
                        stage="etl_processing",
                        status="failed",
                        placeholder=ph.placeholder_name,
                        details={
                            "current": i + 1,
                            "total": total_placeholders_count,
                        },
                        error="æ— æœ‰æ•ˆSQL",
                        record_only=True,
                    )
                    continue

                # å¦‚æœSQLæœªéªŒè¯ï¼Œè®°å½•æ—¥å¿—ä½†ç»§ç»­æ‰§è¡Œ
                if not ph.sql_validated:
                    logger.info(f"å ä½ç¬¦ {ph.placeholder_name} SQLæœªéªŒè¯ï¼Œå°†å°è¯•æ‰§è¡Œå¹¶åœ¨æˆåŠŸåæ ‡è®°ä¸ºå·²éªŒè¯")

                try:
                    # 1. é¦–å…ˆè¿›è¡ŒSQLå ä½ç¬¦æ›¿æ¢ï¼ˆæ—¶é—´å‚æ•°ç­‰ï¼‰
                    final_sql = ph.generated_sql
                    sql_placeholders = sql_replacer.extract_placeholders(ph.generated_sql)

                    if sql_placeholders:
                        logger.info(f"å ä½ç¬¦ {ph.placeholder_name} éœ€è¦SQLå‚æ•°æ›¿æ¢: {sql_placeholders}")
                        # æ„å»ºæ—¶é—´ä¸Šä¸‹æ–‡
                        time_context = {
                            "data_start_time": time_window.get("start", ""),
                            "data_end_time": time_window.get("end", ""),
                            "execution_time": datetime.now().strftime("%Y-%m-%d")
                        }
                        final_sql = sql_replacer.replace_time_placeholders(
                            ph.generated_sql,
                            time_context
                        )
                        logger.info(f"æ›¿æ¢åSQL: {final_sql[:100]}...")

                    # 2. è·å–æ•°æ®æºé…ç½®ï¼ˆä¸Agentåˆ†æé˜¶æ®µä¿æŒä¸€è‡´ï¼‰
                    from app.crud.crud_data_source import crud_data_source
                    from app.models.data_source import DataSourceType
                    from app.core.data_source_utils import DataSourcePasswordManager

                    data_source = crud_data_source.get(db, id=str(task.data_source_id))
                    if not data_source:
                        raise ValueError(f"æ•°æ®æºä¸å­˜åœ¨: {task.data_source_id}")

                    # æ„å»ºæ•°æ®æºé…ç½®å­—å…¸ï¼ˆå‚è€ƒ_get_data_source_infoçš„å®ç°ï¼‰
                    data_source_config = {}
                    if data_source.source_type == DataSourceType.doris:
                        data_source_config = {
                            "source_type": "doris",
                            "name": data_source.name,
                            "database": getattr(data_source, "doris_database", "default"),
                            "fe_hosts": list(getattr(data_source, "doris_fe_hosts", []) or ["localhost"]),
                            "be_hosts": list(getattr(data_source, "doris_be_hosts", []) or ["localhost"]),
                            "http_port": getattr(data_source, "doris_http_port", 8030),
                            "query_port": getattr(data_source, "doris_query_port", 9030),
                            "username": getattr(data_source, "doris_username", "root"),
                            "password": DataSourcePasswordManager.get_password(data_source.doris_password) if getattr(data_source, "doris_password", None) else "",
                            "timeout": 30
                        }
                    elif data_source.source_type == DataSourceType.sql:
                        from app.core.security_utils import decrypt_data
                        conn_str = data_source.connection_string
                        try:
                            if conn_str:
                                conn_str = decrypt_data(conn_str)
                        except Exception:
                            pass
                        data_source_config = {
                            "source_type": "sql",
                            "name": data_source.name,
                            "connection_string": conn_str,
                            "database": getattr(data_source, "database_name", None),
                            "host": getattr(data_source, "host", None),
                            "port": getattr(data_source, "port", None),
                            "username": getattr(data_source, "username", None),
                            "password": getattr(data_source, "password", None),
                        }

                    logger.info(f"æ•°æ®æºé…ç½®: {data_source.source_type}, database: {data_source_config.get('database')}")

                    # 2.5 SQLåˆ—éªŒè¯å’Œè‡ªåŠ¨ä¿®å¤
                    validation_passed = True
                    try:
                        # å°è¯•å¯¼å…¥åˆ—éªŒè¯å·¥å…·
                        from app.services.infrastructure.agents.tools.column_validator import (
                            SQLColumnValidatorTool,
                            SQLColumnAutoFixTool
                        )

                        # è·å–è¡¨ç»“æ„ä¿¡æ¯
                        table_columns = {}
                        if hasattr(ph, 'agent_config') and ph.agent_config:
                            schema_context = ph.agent_config.get('schema_context', {})
                            table_columns = schema_context.get('table_columns', {})

                        # åªæœ‰åœ¨æœ‰è¡¨ç»“æ„ä¿¡æ¯æ—¶æ‰è¿›è¡ŒéªŒè¯
                        if table_columns:
                            logger.info(f"ğŸ” å¼€å§‹éªŒè¯ SQL åˆ—: {ph.placeholder_name}")

                            validator = SQLColumnValidatorTool()

                            async def _validate_columns_async():
                                return await validator.execute({
                                    "sql": final_sql,
                                    "schema_context": {"table_columns": table_columns}
                                })

                            validation_result = run_async(_validate_columns_async())

                            if validation_result.get("success") and not validation_result.get("valid"):
                                # å‘ç°åˆ—é”™è¯¯
                                invalid_columns = validation_result.get("invalid_columns", [])
                                suggestions = validation_result.get("suggestions", {})

                                logger.warning(
                                    f"âš ï¸ SQL åˆ—éªŒè¯å¤±è´¥: {ph.placeholder_name}\n"
                                    f"   æ— æ•ˆåˆ—: {invalid_columns}\n"
                                    f"   å»ºè®®: {suggestions}"
                                )

                                # å°è¯•è‡ªåŠ¨ä¿®å¤
                                if suggestions:
                                    logger.info(f"ğŸ”§ å°è¯•è‡ªåŠ¨ä¿®å¤ SQL: {ph.placeholder_name}")

                                    fixer = SQLColumnAutoFixTool()

                                    async def _fix_columns_async():
                                        return await fixer.execute({
                                            "sql": final_sql,
                                            "suggestions": suggestions
                                        })

                                    fix_result = run_async(_fix_columns_async())

                                    if fix_result.get("success"):
                                        fixed_sql = fix_result.get("fixed_sql")
                                        changes = fix_result.get("changes", [])

                                        logger.info(
                                            f"âœ… SQL è‡ªåŠ¨ä¿®å¤æˆåŠŸ: {ph.placeholder_name}\n"
                                            f"   ä¿®æ”¹: {changes}"
                                        )

                                        # æ›´æ–° SQL
                                        final_sql = fixed_sql

                                        # æ›´æ–°æ•°æ®åº“ä¸­çš„ SQLï¼ˆä¿å­˜ä¿®å¤åçš„ç‰ˆæœ¬ï¼Œä¿ç•™å ä½ç¬¦ï¼‰
                                        # éœ€è¦å°†å·²æ›¿æ¢çš„æ—¶é—´å€¼è¿˜åŸä¸ºå ä½ç¬¦æ ¼å¼
                                        saved_sql = fixed_sql
                                        if sql_placeholders and time_context:
                                            # å°†æ—¶é—´å€¼è¿˜åŸä¸ºå ä½ç¬¦
                                            for placeholder in sql_placeholders:
                                                if placeholder in ['start_date', 'end_date']:
                                                    time_key = 'data_start_time' if placeholder == 'start_date' else 'data_end_time'
                                                    time_value = time_context.get(time_key, '')
                                                    if time_value:
                                                        # è¿˜åŸä¸ºå ä½ç¬¦æ ¼å¼
                                                        saved_sql = saved_sql.replace(f"'{time_value}'", f"{{{{{placeholder}}}}}")

                                        ph.generated_sql = saved_sql

                                        # æ ‡è®°ä¸ºéœ€è¦äººå·¥å®¡æ ¸ï¼ˆè™½ç„¶å·²è‡ªåŠ¨ä¿®å¤ï¼‰
                                        if not hasattr(ph, 'agent_config') or not ph.agent_config:
                                            ph.agent_config = {}
                                        ph.agent_config['auto_fixed'] = True
                                        ph.agent_config['auto_fix_details'] = {
                                            "changes": changes,
                                            "original_errors": validation_result.get("errors", [])
                                        }

                                        db.commit()
                                        logger.info(f"ğŸ’¾ å·²ä¿å­˜ä¿®å¤åçš„ SQL: {ph.placeholder_name}")
                                    else:
                                        # è‡ªåŠ¨ä¿®å¤å¤±è´¥
                                        logger.error(f"âŒ SQL è‡ªåŠ¨ä¿®å¤å¤±è´¥: {ph.placeholder_name}")
                                        validation_passed = False
                                else:
                                    # æ²¡æœ‰ä¿®å¤å»ºè®®
                                    logger.error(f"âŒ æ— æ³•è‡ªåŠ¨ä¿®å¤ï¼Œç¼ºå°‘åˆ—åå»ºè®®: {ph.placeholder_name}")
                                    validation_passed = False

                                # å¦‚æœè‡ªåŠ¨ä¿®å¤å¤±è´¥ï¼Œè®°å½•é”™è¯¯å¹¶è·³è¿‡æ‰§è¡Œ
                                if not validation_passed:
                                    error_msg = "\n".join(validation_result.get("errors", ["åˆ—éªŒè¯å¤±è´¥"]))
                                    etl_results[ph.placeholder_name] = f"ERROR: {error_msg}"

                                    update_progress(
                                        task_execution.progress_percentage or 75,
                                        f"å ä½ç¬¦ {ph.placeholder_name} SQL åˆ—éªŒè¯å¤±è´¥",
                                        stage="etl_processing",
                                        status="failed",
                                        placeholder=ph.placeholder_name,
                                        details={
                                            "current": i + 1,
                                            "total": total_placeholders_count,
                                        },
                                        error=error_msg,
                                        record_only=True,
                                    )
                                    continue
                            else:
                                logger.info(f"âœ… SQL åˆ—éªŒè¯é€šè¿‡: {ph.placeholder_name}")

                        else:
                            logger.debug(f"â­ï¸ è·³è¿‡åˆ—éªŒè¯ï¼ˆæ— è¡¨ç»“æ„ä¿¡æ¯ï¼‰: {ph.placeholder_name}")

                    except ImportError:
                        logger.warning("åˆ—éªŒè¯å·¥å…·æœªå®‰è£…ï¼Œè·³è¿‡éªŒè¯")
                    except Exception as val_error:
                        logger.warning(f"åˆ—éªŒè¯è¿‡ç¨‹å¼‚å¸¸ï¼Œç»§ç»­æ‰§è¡Œ: {val_error}")

                    # 3. ä½¿ç”¨connectorç›´æ¥æ‰§è¡ŒæŸ¥è¯¢ï¼ˆä¸Agentä¿æŒä¸€è‡´ï¼‰
                    from app.services.data.connectors.connector_factory import create_connector_from_config

                    async def _execute_query_async():
                        connector = create_connector_from_config(
                            source_type=data_source.source_type,
                            name=data_source.name,
                            config=data_source_config
                        )
                        try:
                            await connector.connect()
                            result = await connector.execute_query(final_sql)
                            return result
                        finally:
                            await connector.disconnect()

                    query_result = run_async(_execute_query_async())

                    # 4. è§£åŒ…æŸ¥è¯¢ç»“æœï¼Œæå–å®é™…æ•°æ®å€¼
                    # DorisQueryResult æ²¡æœ‰ success å±æ€§ï¼Œåªè¦æ²¡æŠ›å¼‚å¸¸å°±æ˜¯æˆåŠŸ
                    if hasattr(query_result, 'data') and query_result.data is not None and not query_result.data.empty:
                        # å°†DataFrameè½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
                        result_data = query_result.data.to_dict('records')

                        # è½¬æ¢ Decimal ç±»å‹ä¸º floatï¼Œç¡®ä¿ JSON å¯åºåˆ—åŒ–
                        from app.utils.json_utils import convert_decimals
                        result_data = convert_decimals(result_data)

                        # æ™ºèƒ½è§£åŒ…ï¼šå•è¡Œå•åˆ—è¿”å›å€¼ï¼Œå¤šè¡Œè¿”å›åˆ—è¡¨
                        actual_value = None
                        if result_data:
                            if len(result_data) == 1 and len(result_data[0]) == 1:
                                # å•è¡Œå•åˆ—ï¼šè¿”å›å€¼æœ¬èº«
                                actual_value = list(result_data[0].values())[0]
                            elif len(result_data) == 1:
                                # å•è¡Œå¤šåˆ—ï¼šè¿”å›è¡Œå­—å…¸
                                actual_value = result_data[0]
                            else:
                                # å¤šè¡Œï¼šè¿”å›å®Œæ•´åˆ—è¡¨ï¼ˆç”¨äºå›¾è¡¨ï¼‰
                                actual_value = result_data

                        logger.info(f"âœ… å ä½ç¬¦ {ph.placeholder_name} æŸ¥è¯¢æˆåŠŸï¼Œç»“æœç±»å‹: {type(actual_value)}, å€¼: {str(actual_value)[:100]}")

                        # ğŸ†• è®°å½•SQLæ‰§è¡Œç»“æœ
                        if tool_recorder:
                            tool_recorder.record_sql_execution(
                                tool_name=f"sql_execution_{ph.placeholder_name}",
                                result={
                                    "success": True,
                                    "row_count": len(result_data),
                                    "rows": result_data[:3] if len(result_data) > 3 else result_data  # åªè®°å½•å‰3è¡Œ
                                }
                            )

                        # å­˜å‚¨å®é™…çš„æ•°æ®å€¼
                        etl_results[ph.placeholder_name] = actual_value
                    else:
                        # æŸ¥è¯¢æˆåŠŸä½†æ— æ•°æ®
                        logger.warning(f"âš ï¸ å ä½ç¬¦ {ph.placeholder_name} æŸ¥è¯¢æˆåŠŸä½†æ— æ•°æ®è¿”å›")
                        etl_results[ph.placeholder_name] = None

                    # æ›´æ–°è¿›åº¦
                    progress_increment = 10 / total_placeholders_count if total_placeholders_count else 0
                    current_progress = 75 + (i + 1) * progress_increment
                    update_progress(
                        int(current_progress),
                        f"å·²å¤„ç† {i + 1}/{total_placeholders_count} ä¸ªå ä½ç¬¦",
                        stage="etl_processing",
                        placeholder=ph.placeholder_name,
                        details={
                            "current": i + 1,
                            "total": total_placeholders_count,
                        },
                    )

                except Exception as e:
                    logger.error(f"Failed to execute SQL for placeholder {ph.placeholder_name}: {e}")
                    etl_results[ph.placeholder_name] = f"ERROR: {str(e)}"

                    update_progress(
                        task_execution.progress_percentage or int(current_progress),
                        f"æ‰§è¡Œå ä½ç¬¦ {ph.placeholder_name} SQL å¤±è´¥",
                        stage="etl_processing",
                        status="failed",
                        placeholder=ph.placeholder_name,
                        details={
                            "current": i + 1,
                            "total": total_placeholders_count,
                        },
                        error=str(e),
                        record_only=True,
                    )

            update_progress(
                85,
                "ETLæ•°æ®å¤„ç†å®Œæˆ",
                stage="etl_processing",
            )

            # æ„å»ºæ‰§è¡Œç»“æœ
            # ç»Ÿè®¡æˆåŠŸçš„å ä½ç¬¦ï¼ˆä¸æ˜¯ERRORå¼€å¤´çš„ï¼‰
            successful_placeholders = [k for k, v in etl_results.items() if not str(v).startswith("ERROR")]

            execution_result = {
                "success": len(successful_placeholders) > 0,
                "events": events,
                "etl_results": etl_results,
                "time_window": time_window,
                "placeholders_processed": len(etl_results),
                "placeholders_success": len(successful_placeholders)
            }

            logger.info(f"ğŸ“Š ETLå¤„ç†å®Œæˆ: {len(successful_placeholders)}/{len(etl_results)} ä¸ªå ä½ç¬¦æˆåŠŸ")

        except Exception as e:
            logger.error(f"ETL processing failed: {e}")
            execution_result = {
                "success": False,
                "events": events,
                "error": str(e),
                "time_window": time_window,
            }

        # 8. ç”Ÿæˆæ–‡æ¡£ï¼ˆä½¿ç”¨æ¨¡æ¿ + doc_assemblerï¼‰
        update_progress(
            87,
            "æ­£åœ¨ç”ŸæˆæŠ¥å‘Šæ–‡æ¡£...",
            stage="document_generation",
            pipeline_status=PipelineTaskStatus.ASSEMBLING,
        )

        # æ£€æŸ¥æ˜¯å¦è¢«å–æ¶ˆ
        check_if_cancelled()

        tpl_meta = None  # åˆå§‹åŒ–æ¨¡æ¿å…ƒæ•°æ®ï¼Œç”¨äºåç»­æ¸…ç†
        report_generation_error: Optional[str] = None
        try:
            from app.services.infrastructure.document.template_path_resolver import resolve_docx_template_path, cleanup_template_temp_dir
            from app.services.infrastructure.document.word_template_service import WordTemplateService
            from io import BytesIO
            from app.services.infrastructure.storage.hybrid_storage_service import get_hybrid_storage_service

            # ä»…åœ¨æœ‰æ¨¡æ¿æ–‡ä»¶æ—¶ç”Ÿæˆ
            if getattr(task, 'template_id', None):
                tpl_meta = resolve_docx_template_path(db, str(task.template_id))

                # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ç›´æ¥å­˜å‚¨æ¨¡å¼ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
                use_direct_storage = getattr(settings, 'USE_DIRECT_STORAGE', True)

                if use_direct_storage:
                    # æ–°æ¨¡å¼ï¼šä½¿ç”¨WordTemplateServiceç›´æ¥å¤„ç†
                    word_service = WordTemplateService()

                    # å‡†å¤‡ä»»åŠ¡ä¿¡æ¯ç”¨äºå­˜å‚¨é”®ç”Ÿæˆ
                    from app.models.user import User
                    user = db.query(User).filter(User.id == task.owner_id).first()
                    tenant_id = getattr(user, 'tenant_id', str(task.owner_id)) if user else str(task.owner_id)

                    safe_tmp_dir = os.path.join(os.path.expanduser('~'), ".autoreportai", "tmp")
                    os.makedirs(safe_tmp_dir, exist_ok=True)
                    docx_out = os.path.join(safe_tmp_dir, f"report_{task.id}_{int(datetime.utcnow().timestamp())}.docx")

                    # ä½¿ç”¨WordTemplateServiceå¤„ç†æ–‡æ¡£
                    assemble_res = run_async(word_service.process_document_template(
                        template_path=tpl_meta['path'],
                        placeholder_data=etl_results,
                        output_path=docx_out,
                        container=container,
                        use_agent_charts=True,
                        use_agent_optimization=True,
                        user_id=str(task.owner_id)
                    ))

                    if assemble_res.get('success'):
                        # ä¸Šä¼ åˆ°å­˜å‚¨
                        storage = get_hybrid_storage_service()
                        with open(docx_out, 'rb') as f:
                            file_bytes = f.read()

                        ts = datetime.utcnow().strftime('%Y%m%d_%H%M%S')
                        # ä½¿ç”¨ä»»åŠ¡åç§°ä½œä¸ºè·¯å¾„å’Œæ–‡ä»¶å
                        import re
                        safe_task_name = re.sub(r'[<>:"/\\|?*]', '_', task.name or f'task_{task.id}')
                        object_key = f"reports/{tenant_id}/{safe_task_name}/report_{ts}.docx"
                        upload_result = storage.upload_with_key(BytesIO(file_bytes), object_key, content_type="application/vnd.openxmlformats-officedocument.wordprocessingml.document")
                        storage_path = upload_result.get("file_path")

                        execution_result["report"] = {
                            "storage_path": storage_path,
                            "backend": upload_result.get("backend"),
                            "size": upload_result.get("size", len(file_bytes)),
                            "content_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                            "generation_mode": "word_template_service"
                        }
                        logger.info(f"âœ… æŠ¥å‘Šç”Ÿæˆå¹¶å­˜å‚¨å®Œæˆ: {storage_path}")
                    else:
                        report_generation_error = assemble_res.get('error') or "æ–‡æ¡£å¤„ç†å¤±è´¥"
                        logger.error(f"æ–‡æ¡£ç”Ÿæˆå¤±è´¥: {report_generation_error}")
                        execution_result["report"] = {
                            "error": report_generation_error,
                            "generation_mode": "word_template_service"
                        }

                else:
                    # ä¼ ç»Ÿæ¨¡å¼ï¼šæœ¬åœ°ç”Ÿæˆåä¸Šä¼ 
                    word_service_traditional = WordTemplateService()
                    safe_tmp_dir = os.path.join(os.path.expanduser('~'), ".autoreportai", "tmp")
                    os.makedirs(safe_tmp_dir, exist_ok=True)
                    docx_out = os.path.join(safe_tmp_dir, f"report_{task.id}_{int(datetime.utcnow().timestamp())}.docx")
                    assemble_res = run_async(word_service_traditional.process_document_template(
                        template_path=tpl_meta['path'],
                        placeholder_data=etl_results,
                        output_path=docx_out,
                        container=container,
                        use_agent_charts=True,
                        use_agent_optimization=True,
                        user_id=str(task.owner_id)
                    ))

                    if assemble_res.get('success') and assemble_res.get('output_path'):
                        # ä¸Šä¼ åˆ°å­˜å‚¨
                        storage = get_hybrid_storage_service()
                        with open(assemble_res['output_path'], 'rb') as f:
                            file_bytes = f.read()
                        # é‡‡ç”¨å¯¹è±¡é”®: reports/{tenant_id}/{task_name}/report_{timestamp}.docx
                        ts = datetime.utcnow().strftime('%Y%m%d_%H%M%S')
                        # è·å–ç§Ÿæˆ·ï¼ˆè‹¥æ— ç§Ÿæˆ·å­—æ®µï¼Œåˆ™ä½¿ç”¨ç”¨æˆ·IDä»£æ›¿ï¼‰
                        from app.models.user import User
                        user = db.query(User).filter(User.id == task.owner_id).first()
                        tenant_id = getattr(user, 'tenant_id', str(task.owner_id)) if user else str(task.owner_id)
                        # ä½¿ç”¨ä»»åŠ¡åç§°ä½œä¸ºè·¯å¾„å’Œæ–‡ä»¶å
                        import re
                        slug = re.sub(r'[^\w\-]+', '-', (task.name or f'task_{task.id}')).strip('-')[:50]
                        object_name = f"reports/{tenant_id}/{slug}/report_{ts}.docx"
                        update_progress(
                            92,
                            "æ­£åœ¨ä¸Šä¼ æ–‡æ¡£åˆ°å­˜å‚¨...",
                            stage="document_generation",
                            pipeline_status=PipelineTaskStatus.ASSEMBLING,
                        )
                        upload = storage.upload_with_key(BytesIO(file_bytes), object_name, content_type="application/vnd.openxmlformats-officedocument.wordprocessingml.document")
                        execution_result["report"] = {
                            "storage_path": upload.get("file_path"),
                            "backend": upload.get("backend"),
                            "friendly_name": f"{slug}_{ts}.docx",
                            "generation_mode": "traditional_upload"
                        }
                        update_progress(
                            95,
                            "æ–‡æ¡£ç”Ÿæˆå®Œæˆ",
                            stage="document_generation",
                            pipeline_status=PipelineTaskStatus.ASSEMBLING,
                        )
                    else:
                        report_generation_error = assemble_res.get('error') or "ä¼ ç»Ÿæ¨¡å¼ç”Ÿæˆå¤±è´¥"
                        logger.error(f"ä¼ ç»Ÿæ¨¡å¼æ–‡æ¡£ç”Ÿæˆå¤±è´¥: {report_generation_error}")
                        execution_result["report"] = {
                            "error": report_generation_error,
                            "generation_mode": "traditional_upload"
                        }
            else:
                report_generation_error = "ä»»åŠ¡æœªé…ç½®æ¨¡æ¿ï¼Œè·³è¿‡æ–‡æ¡£ç”Ÿæˆ"
                logger.warning(report_generation_error)
                execution_result["report"] = {
                    "error": report_generation_error,
                    "generation_mode": "skipped"
                }
        except Exception as e:
            report_generation_error = str(e)
            logger.error(f"Document assembly failed: {e}")
            existing_mode = (execution_result.get("report") or {}).get("generation_mode")
            execution_result["report"] = {
                "error": report_generation_error,
                "generation_mode": existing_mode or "assembly_error"
            }
        finally:
            # æ¸…ç†æ¨¡æ¿ä¸´æ—¶æ–‡ä»¶
            if tpl_meta:
                try:
                    cleanup_template_temp_dir(tpl_meta)
                    logger.info("âœ… æ¨¡æ¿ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†")
                except Exception as cleanup_error:
                    logger.warning(f"æ¸…ç†æ¨¡æ¿ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {cleanup_error}")
        
        report_info = execution_result.get("report") or {}
        if not report_info:
            report_info = {}
            execution_result["report"] = report_info

        report_generated = bool(report_info.get("storage_path"))
        if report_generated:
            report_generation_error = None
            report_info.pop("error", None)
        else:
            if not report_info.get("error"):
                report_info["error"] = report_generation_error or "æŠ¥å‘Šæ–‡æ¡£æœªç”Ÿæˆ"

        etl_success = execution_result.get("success", False)
        execution_result["etl_success"] = etl_success
        overall_success = etl_success and report_generated
        execution_result["success"] = overall_success
        report_info["generated"] = report_generated
        
        # 7. æ›´æ–°æ‰§è¡Œç»“æœ
        final_status = TaskStatus.COMPLETED if overall_success else TaskStatus.FAILED
        task_execution.execution_status = final_status
        task_execution.completed_at = datetime.utcnow()
        task_execution.total_duration = int((task_execution.completed_at - task_execution.started_at).total_seconds())
        task_execution.progress_percentage = 100

        owner_id = task.owner_id
        if isinstance(owner_id, str):
            owner_id = UUID(owner_id)

        history_metadata: Dict[str, Any] = {
            "execution_id": str(task_execution.execution_id),
            "generation_mode": report_info.get("generation_mode"),
            "storage_backend": report_info.get("backend"),
            "placeholders": {
                "processed": execution_result.get("placeholders_processed"),
                "success": execution_result.get("placeholders_success"),
            },
            "etl_success": etl_success,
            "report_generated": report_generated,
            "time_window": time_window,
        }
        if report_info.get("error"):
            history_metadata["error"] = report_info.get("error")

        # è½¬æ¢ history_metadata ä¸­çš„ Decimal å¯¹è±¡
        history_metadata = convert_for_json(history_metadata)

        report_history_record = ReportHistory(
            task_id=task.id,
            user_id=owner_id,
            status="completed" if final_status == TaskStatus.COMPLETED else "failed",
            file_path=report_info.get("storage_path"),
            file_size=report_info.get("size", 0),
            error_message=report_info.get("error") if not overall_success else None,
            result=None,
            processing_metadata=history_metadata,
        )
        db.add(report_history_record)
        db.flush()
        report_info["history_id"] = report_history_record.id

        # è½¬æ¢ execution_result ä¸­çš„æ‰€æœ‰ Decimal å¯¹è±¡ä¸º floatï¼Œç¡®ä¿ JSON å¯åºåˆ—åŒ–
        execution_result = convert_for_json(execution_result)
        task_execution.execution_result = execution_result
        
        # æ›´æ–°ä»»åŠ¡ç»Ÿè®¡
        task.status = final_status
        if final_status == TaskStatus.COMPLETED:
            task.success_count += 1
        else:
            task.failure_count += 1
        task.last_execution_duration = task_execution.total_duration
        
        # æ›´æ–°å¹³å‡æ‰§è¡Œæ—¶é—´ï¼ˆä»…åœ¨æˆåŠŸæ—¶æ›´æ–°ï¼‰
        if final_status == TaskStatus.COMPLETED:
            if task.average_execution_time == 0:
                task.average_execution_time = task_execution.total_duration
            else:
                task.average_execution_time = (task.average_execution_time + task_execution.total_duration) / 2
        
        db.commit()
        
        if overall_success:
            update_progress(
                97,
                "æ­£åœ¨å‘é€é€šçŸ¥...",
                stage="notification",
                pipeline_status=PipelineTaskStatus.ASSEMBLING,
            )
            if task.recipients:
                try:
                    # ç”Ÿæˆä¸‹è½½URLï¼ˆè‹¥æœ‰reportï¼‰
                    download_url = None
                    try:
                        if execution_result.get("report", {}).get("storage_path"):
                            storage = get_hybrid_storage_service()
                            download_url = storage.get_download_url(execution_result["report"]["storage_path"], expires=86400)
                    except Exception as e:
                        logger.warning(f"Failed to generate download URL: {e}")

                    # ä½¿ç”¨DeliveryService å‘é€é‚®ä»¶ï¼ˆè‹¥å¯ç”¨ï¼‰æˆ–é€šçŸ¥æœåŠ¡
                    from app.services.infrastructure.delivery.delivery_service import create_delivery_service, DeliveryRequest, DeliveryMethod, StorageConfig, EmailConfig, NotificationConfig
                    delivery_service = create_delivery_service(str(task.owner_id))
                    # å‹å¥½åç§°: ä»»åŠ¡å+æ—¶é—´
                    friendly_name = execution_result.get("report", {}).get("friendly_name") or f"report_{task.id}.docx"
                    ts_email = datetime.utcnow().strftime('%Y%m%d_%H%M%S')
                    email_config = EmailConfig(
                        recipients=task.recipients,
                        subject=f"æŠ¥å‘Šç”Ÿæˆå®Œæˆ - {task.name} - {ts_email}",
                        body=(
                            f"æŠ¥å‘Šå·²ç”Ÿæˆ: {friendly_name}\n\n"
                            f"ä¸‹è½½é“¾æ¥: {download_url if download_url else 'è¯·ç™»å½•ç³»ç»ŸæŸ¥çœ‹'}\n"
                            f"ä»»åŠ¡: {task.name}\næ—¶é—´çª—å£: {time_window['start']} - {time_window['end']}\n"
                        ),
                        attach_files=False
                    )
                    req = DeliveryRequest(
                        task_id=str(task_id),
                        user_id=str(task.owner_id),
                        files=[],
                        delivery_method=DeliveryMethod.EMAIL_ONLY,
                        storage_config=StorageConfig(bucket_name="reports", path_prefix=f"reports/{task.owner_id}", public_access=False, retention_days=90),
                        email_config=email_config,
                        notification_config=NotificationConfig(channels=["system"], message="æŠ¥å‘Šå·²ç”Ÿæˆ", priority="normal"),
                        metadata={"report_path": execution_result.get("report", {}).get("storage_path")}
                    )
                    # åœ¨åŒæ­¥ä»»åŠ¡ä¸­æ‰§è¡Œå¼‚æ­¥æŠ•é€’
                    run_async(delivery_service.deliver_report(req))
                except Exception as e:
                    logger.error(f"Failed to send success notification for task {task_id}: {e}")
        
        final_message = "ä»»åŠ¡æ‰§è¡Œå®Œæˆ" if overall_success else f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {report_info.get('error')}"
        final_pipeline_status = PipelineTaskStatus.COMPLETED if overall_success else PipelineTaskStatus.FAILED
        update_progress(
            100,
            final_message,
            stage="completion",
            pipeline_status=final_pipeline_status,
            status="success" if overall_success else "failed",
            error=report_info.get("error") if not overall_success else None,
        )

        if overall_success:
            progress_recorder.complete(
                "ä»»åŠ¡æ‰§è¡Œå®Œæˆ",
                result={
                    "task_id": task_id,
                    "execution_id": str(task_execution.execution_id),
                },
            )
            logger.info(f"Task {task_id} completed successfully in {task_execution.total_duration}s")
        else:
            progress_recorder.fail(
                message="ä»»åŠ¡æ‰§è¡Œå¤±è´¥: æŠ¥å‘Šç”Ÿæˆå¤±è´¥",
                stage="document_generation",
                error_details={"error": report_info.get("error")},
            )
            logger.warning(f"Task {task_id} completed with failures in {task_execution.total_duration}s: {report_info.get('error')}")

        return {
            "status": "completed" if overall_success else "failed",
            "task_id": task_id,
            "execution_id": str(task_execution.execution_id),
            "execution_time": task_execution.total_duration,
            "result": execution_result
        }
        
    except Exception as e:
        error_message = str(e)
        is_cancelled = "å–æ¶ˆ" in error_message or "cancelled" in error_message.lower()

        if is_cancelled:
            logger.info(f"Task {task_id} was cancelled: {error_message}")
        else:
            logger.error(f"Task {task_id} failed: {error_message}", exc_info=True)

        if 'progress_recorder' in locals():
            try:
                progress_recorder.fail(
                    message="ä»»åŠ¡æ‰§è¡Œå¤±è´¥" if not is_cancelled else "ä»»åŠ¡å·²å–æ¶ˆ",
                    stage="cancelled" if is_cancelled else "failure",
                    error_details={"error": error_message},
                )
            except Exception as notify_error:
                logger.warning(f"Failed to record failure progress for task {task_id}: {notify_error}")

        # æ›´æ–°å¤±è´¥/å–æ¶ˆçŠ¶æ€
        if task_execution_id:
            task_execution = db.query(TaskExecution).filter(TaskExecution.id == task_execution_id).first()
            if task_execution:
                # å¦‚æœå·²ç»æ ‡è®°ä¸ºCANCELLEDï¼Œä¿æŒè¯¥çŠ¶æ€
                if task_execution.execution_status != TaskStatus.CANCELLED:
                    task_execution.execution_status = TaskStatus.CANCELLED if is_cancelled else TaskStatus.FAILED
                task_execution.completed_at = datetime.utcnow()
                task_execution.error_details = error_message
                task_execution.total_duration = int((task_execution.completed_at - task_execution.started_at).total_seconds()) if task_execution.started_at else 0

        # æ›´æ–°ä»»åŠ¡ç»Ÿè®¡
        task = db.query(Task).filter(Task.id == task_id).first()
        if task:
            task.status = TaskStatus.CANCELLED if is_cancelled else TaskStatus.FAILED
            if not is_cancelled:  # åªæœ‰å¤±è´¥æ‰å¢åŠ å¤±è´¥è®¡æ•°ï¼Œå–æ¶ˆä¸ç®—å¤±è´¥
                task.failure_count += 1
            
        db.commit()
        
        # å‘é€å¤±è´¥é€šçŸ¥ (ä¸å‘é€å–æ¶ˆé€šçŸ¥)
        if task and task.recipients and not is_cancelled:
            try:
                notification_service.send_task_completion_notification(
                    task_id=task_id,
                    task_name=task.name,
                    recipients=task.recipients,
                    execution_result={"error": str(e)},
                    success=False
                )
            except Exception as notification_error:
                logger.error(f"Failed to send failure notification for task {task_id}: {notification_error}")
        
        # å¯¹äºå–æ¶ˆæ“ä½œï¼Œè¿”å›å–æ¶ˆçŠ¶æ€è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
        if is_cancelled:
            return {
                "status": "cancelled",
                "task_id": task_id,
                "message": "ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ",
                "execution_id": str(task_execution.execution_id) if task_execution else None
            }

        # å¯¹äºå¤±è´¥çš„ä»»åŠ¡ï¼Œé‡æ–°æŠ›å‡ºå¼‚å¸¸è®©Celeryå¤„ç†é‡è¯•
        raise

@celery_app.task(bind=True, name='tasks.infrastructure.validate_placeholders_task')
def validate_placeholders_task(self, template_id: str, data_source_id: str, user_id: str) -> Dict[str, Any]:
    """
    éªŒè¯æ¨¡æ¿å ä½ç¬¦ä»»åŠ¡
    
    Args:
        template_id: æ¨¡æ¿ID
        data_source_id: æ•°æ®æºID
        user_id: ç”¨æˆ·ID
    
    Returns:
        Dict: éªŒè¯ç»“æœ
    """
    try:
        # è¿ç§»è¯´æ˜ï¼šæ—§çš„å ä½ç¬¦éªŒè¯æµç¨‹å·²å¼ƒç”¨ã€‚
        # æ–°æ¶æ„åœ¨æ‰§è¡Œé˜¶æ®µç”± Agents è¿›è¡Œ SQL ç”Ÿæˆâ†’æ³¨å…¥â†’æ‰§è¡Œçš„è‡ªéªŒè¯ï¼ˆReActï¼‰ã€‚
        logger.info(f"Placeholder validation (legacy) skipped for template {template_id}; replaced by Agents pipeline")
        return {
            "status": "migrated",
            "template_id": template_id,
            "message": "Validation is handled by Agents pipeline during execution",
        }
        
    except Exception as e:
        logger.error(f"Placeholder validation failed for template {template_id}: {str(e)}", exc_info=True)
        raise

@celery_app.task(bind=True, base=DatabaseTask, name='tasks.infrastructure.scheduled_task_runner')
def scheduled_task_runner(self, db: Session, task_id: int) -> Dict[str, Any]:
    """
    å®šæ—¶ä»»åŠ¡æ‰§è¡Œå™¨ - ç”±è°ƒåº¦å™¨è§¦å‘
    
    Args:
        task_id: ä»»åŠ¡ID
    
    Returns:
        Dict: æ‰§è¡Œç»“æœ
    """
    try:
        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦åº”è¯¥æ‰§è¡Œ
        task = db.query(Task).filter(Task.id == task_id).first()
        if not task or not task.is_active:
            return {"status": "skipped", "reason": "task_inactive_or_not_found"}
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ­£åœ¨è¿›è¡Œçš„æ‰§è¡Œ
        ongoing_execution = db.query(TaskExecution).filter(
            TaskExecution.task_id == task_id,
            TaskExecution.execution_status.in_([TaskStatus.PENDING, TaskStatus.PROCESSING])
        ).first()
        
        if ongoing_execution:
            logger.warning(f"Task {task_id} has ongoing execution, skipping")
            return {"status": "skipped", "reason": "execution_in_progress"}
        
        # æ„å»ºæ‰§è¡Œä¸Šä¸‹æ–‡ï¼ˆåŒ…å«è°ƒåº¦ä¿¡æ¯ï¼‰
        execution_context = {
            "trigger": "scheduled",
            "schedule": task.schedule,
            "triggered_at": datetime.utcnow().isoformat()
        }
        
        # å§”æ‰˜ç»™ä¸»æ‰§è¡Œä»»åŠ¡
        result = execute_report_task.delay(task_id, execution_context)
        
        logger.info(f"Scheduled task {task_id} delegated to execution task {result.id}")
        
        return {
            "status": "delegated",
            "task_id": task_id,
            "execution_task_id": result.id
        }
        
    except Exception as e:
        logger.error(f"Scheduled task runner failed for task {task_id}: {str(e)}", exc_info=True)
        raise

@celery_app.task(name='tasks.infrastructure.cleanup_old_executions')
def cleanup_old_executions(days_to_keep: int = 30) -> Dict[str, Any]:
    """
    æ¸…ç†æ—§çš„ä»»åŠ¡æ‰§è¡Œè®°å½•
    
    Args:
        days_to_keep: ä¿ç•™å¤©æ•°ï¼Œé»˜è®¤30å¤©
    
    Returns:
        Dict: æ¸…ç†ç»“æœ
    """
    try:
        with SessionLocal() as db:
            cutoff_date = datetime.utcnow() - timedelta(days=days_to_keep)
            
            # åˆ é™¤æ—§çš„æ‰§è¡Œè®°å½•
            deleted_count = db.query(TaskExecution).filter(
                TaskExecution.created_at < cutoff_date,
                TaskExecution.execution_status.in_([TaskStatus.COMPLETED, TaskStatus.FAILED, TaskStatus.CANCELLED])
            ).delete()
            
            db.commit()
            
            logger.info(f"Cleaned up {deleted_count} old task executions")
            
            return {
                "status": "completed",
                "deleted_count": deleted_count,
                "cutoff_date": cutoff_date.isoformat()
            }
            
    except Exception as e:
        logger.error(f"Cleanup task failed: {str(e)}", exc_info=True)
        raise

# æ³¨å†Œå‘¨æœŸæ€§ä»»åŠ¡
@celery_app.on_after_configure.connect
def setup_periodic_tasks(sender, **kwargs):
    """è®¾ç½®å‘¨æœŸæ€§ä»»åŠ¡"""
    
    # æ¯å¤©å‡Œæ™¨2ç‚¹æ¸…ç†æ—§çš„æ‰§è¡Œè®°å½•
    sender.add_periodic_task(
        crontab(hour=2, minute=0),
        cleanup_old_executions.s(),
        name='cleanup_old_executions_daily',
    )
    
    logger.info("âœ… Periodic tasks configured")

logger.info("âœ… Task infrastructure layer loaded")
