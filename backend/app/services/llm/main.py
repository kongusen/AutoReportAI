#!/usr/bin/env python3
"""
ç‹¬ç«‹LLMæœåŠ¡å™¨ - ä¸“é—¨æä¾›å¤§è¯­è¨€æ¨¡å‹æ¨ç†æœåŠ¡

æä¾›ç»Ÿä¸€çš„LLM APIæ¥å£ï¼Œæ”¯æŒå¤šç§æ¨¡å‹æä¾›å•†ï¼š
- OpenAI (GPTç³»åˆ—)
- Anthropic (Claudeç³»åˆ—) 
- Google (Geminiç³»åˆ—)
- æœ¬åœ°æ¨¡å‹ (Ollamaç­‰)

ä¸»è¦åŠŸèƒ½ï¼š
1. ç»Ÿä¸€çš„LLMè°ƒç”¨æ¥å£
2. æ™ºèƒ½è´Ÿè½½å‡è¡¡å’Œæ•…éšœè½¬ç§»
3. Tokenä½¿ç”¨é‡ç»Ÿè®¡å’Œæˆæœ¬ä¼°ç®—
4. è¯·æ±‚ç¼“å­˜å’Œæ€§èƒ½ä¼˜åŒ–
5. ç”¨æˆ·è®¤è¯å’Œæƒé™ç®¡ç†
"""

import asyncio
import logging
import os
from contextlib import asynccontextmanager
from datetime import datetime
from typing import Any, Dict, List, Optional

import uvicorn
from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.responses import JSONResponse

# LLMæœåŠ¡æ ¸å¿ƒç»„ä»¶
from core.llm_manager import LLMManager, get_llm_manager
from core.models import (
    LLMRequest, 
    LLMResponse, 
    LLMProvider,
    ProviderConfig,
    UsageStats,
    HealthStatus,
    ErrorResponse
)
from core.auth import authenticate_request, get_current_user
from core.cache import LLMCacheManager, get_cache_manager
from core.metrics import MetricsCollector, get_metrics_collector

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('llm-server.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å¯åŠ¨æ—¶åˆå§‹åŒ–
    logger.info("ğŸš€ å¯åŠ¨ç‹¬ç«‹LLMæœåŠ¡å™¨...")
    
    # åˆå§‹åŒ–LLMç®¡ç†å™¨
    llm_manager = get_llm_manager()
    await llm_manager.initialize()
    logger.info(f"âœ… åˆå§‹åŒ– {len(llm_manager.get_available_providers())} ä¸ªLLMæä¾›å•†")
    
    # åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
    cache_manager = get_cache_manager()
    await cache_manager.initialize()
    logger.info("âœ… ç¼“å­˜ç³»ç»Ÿå·²åˆå§‹åŒ–")
    
    # åˆå§‹åŒ–æŒ‡æ ‡æ”¶é›†å™¨
    metrics_collector = get_metrics_collector()
    await metrics_collector.start()
    logger.info("âœ… æŒ‡æ ‡æ”¶é›†ç³»ç»Ÿå·²å¯åŠ¨")
    
    logger.info("ğŸ‰ LLMæœåŠ¡å™¨å¯åŠ¨å®Œæˆï¼")
    
    yield
    
    # å…³é—­æ—¶æ¸…ç†
    logger.info("ğŸ›‘ æ­£åœ¨å…³é—­LLMæœåŠ¡å™¨...")
    await metrics_collector.stop()
    await cache_manager.cleanup()
    await llm_manager.cleanup()
    logger.info("âœ… LLMæœåŠ¡å™¨å·²å®‰å…¨å…³é—­")


# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="ç‹¬ç«‹LLMæœåŠ¡å™¨",
    description="ä¸“é—¨æä¾›å¤§è¯­è¨€æ¨¡å‹æ¨ç†æœåŠ¡çš„ç‹¬ç«‹æœåŠ¡å™¨",
    version="1.0.0",
    lifespan=lifespan,
    docs_url="/docs",
    redoc_url="/redoc"
)

# æ·»åŠ ä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒéœ€è¦é™åˆ¶
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.add_middleware(GZipMiddleware, minimum_size=1000)


# === æ ¸å¿ƒAPIç«¯ç‚¹ ===

@app.post("/v1/chat/completions", response_model=LLMResponse)
async def chat_completions(
    request: LLMRequest,
    background_tasks: BackgroundTasks,
    user_id: str = Depends(get_current_user),
    llm_manager: LLMManager = Depends(get_llm_manager),
    cache_manager: LLMCacheManager = Depends(get_cache_manager),
    metrics: MetricsCollector = Depends(get_metrics_collector)
):
    """
    ç»Ÿä¸€çš„èŠå¤©å®Œæˆæ¥å£ - å…¼å®¹OpenAI APIæ ¼å¼
    
    æ”¯æŒå¤šä¸ªLLMæä¾›å•†çš„ç»Ÿä¸€è°ƒç”¨æ¥å£ï¼Œè‡ªåŠ¨å¤„ç†ï¼š
    - æä¾›å•†é€‰æ‹©å’Œæ•…éšœè½¬ç§»
    - è¯·æ±‚ç¼“å­˜å’Œå»é‡
    - Tokenä½¿ç”¨ç»Ÿè®¡
    - æ€§èƒ½ç›‘æ§
    """
    start_time = datetime.utcnow()
    
    try:
        # è®°å½•è¯·æ±‚å¼€å§‹
        await metrics.record_request_start(user_id, request.model or "default")
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = cache_manager.generate_cache_key(request, user_id)
        cached_response = await cache_manager.get_cached_response(cache_key)
        
        if cached_response:
            logger.info(f"ç¼“å­˜å‘½ä¸­: {cache_key}")
            await metrics.record_cache_hit(user_id)
            return cached_response
        
        # è°ƒç”¨LLMæœåŠ¡
        response = await llm_manager.chat_completion(request, user_id)
        
        # ç¼“å­˜å“åº”
        if request.cache_enabled:
            background_tasks.add_task(
                cache_manager.cache_response, 
                cache_key, 
                response, 
                ttl=request.cache_ttl
            )
        
        # è®°å½•æˆåŠŸæŒ‡æ ‡
        duration = (datetime.utcnow() - start_time).total_seconds()
        await metrics.record_successful_request(
            user_id, 
            response.provider, 
            response.model,
            response.usage.get("total_tokens", 0),
            duration,
            response.cost_estimate or 0.0
        )
        
        return response
        
    except Exception as e:
        # è®°å½•å¤±è´¥æŒ‡æ ‡
        duration = (datetime.utcnow() - start_time).total_seconds()
        await metrics.record_failed_request(user_id, str(e), duration)
        
        logger.error(f"LLMè¯·æ±‚å¤±è´¥: {e}")
        raise HTTPException(
            status_code=500,
            detail=ErrorResponse(
                error=str(e),
                error_type=type(e).__name__,
                error_code="LLM_REQUEST_FAILED"
            ).dict()
        )


@app.post("/v1/embeddings")
async def create_embeddings(
    request: Dict[str, Any],
    user_id: str = Depends(get_current_user),
    llm_manager: LLMManager = Depends(get_llm_manager)
):
    """åˆ›å»ºæ–‡æœ¬åµŒå…¥å‘é‡"""
    try:
        result = await llm_manager.create_embeddings(
            texts=request.get("input", []),
            model=request.get("model", "text-embedding-ada-002"),
            user_id=user_id
        )
        return result
    except Exception as e:
        logger.error(f"åµŒå…¥åˆ›å»ºå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# === ç®¡ç†APIç«¯ç‚¹ ===

@app.get("/v1/providers", response_model=List[LLMProvider])
async def list_providers(
    llm_manager: LLMManager = Depends(get_llm_manager)
):
    """è·å–æ‰€æœ‰å¯ç”¨çš„LLMæä¾›å•†åˆ—è¡¨"""
    return await llm_manager.get_provider_list()


@app.get("/v1/providers/{provider_name}/models")
async def list_provider_models(
    provider_name: str,
    llm_manager: LLMManager = Depends(get_llm_manager)
):
    """è·å–æŒ‡å®šæä¾›å•†çš„å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    try:
        models = await llm_manager.get_provider_models(provider_name)
        return {"models": models}
    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e))


@app.post("/v1/providers/{provider_name}/config")
async def update_provider_config(
    provider_name: str,
    config: ProviderConfig,
    user_id: str = Depends(authenticate_request),  # éœ€è¦ç®¡ç†å‘˜æƒé™
    llm_manager: LLMManager = Depends(get_llm_manager)
):
    """æ›´æ–°æä¾›å•†é…ç½®"""
    try:
        await llm_manager.update_provider_config(provider_name, config)
        return {"message": f"æä¾›å•† {provider_name} é…ç½®å·²æ›´æ–°"}
    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e))


# === ç›‘æ§å’Œç»Ÿè®¡API ===

@app.get("/v1/health", response_model=HealthStatus)
async def health_check(
    llm_manager: LLMManager = Depends(get_llm_manager)
):
    """ç³»ç»Ÿå¥åº·æ£€æŸ¥"""
    return await llm_manager.health_check()


@app.get("/v1/usage/{user_id}", response_model=UsageStats)
async def get_user_usage(
    user_id: str,
    hours: int = 24,
    current_user: str = Depends(get_current_user),
    metrics: MetricsCollector = Depends(get_metrics_collector)
):
    """è·å–ç”¨æˆ·ä½¿ç”¨ç»Ÿè®¡"""
    # æƒé™æ£€æŸ¥ï¼šç”¨æˆ·åªèƒ½æŸ¥çœ‹è‡ªå·±çš„ç»Ÿè®¡ï¼Œç®¡ç†å‘˜å¯ä»¥æŸ¥çœ‹æ‰€æœ‰
    if current_user != user_id and not await authenticate_request(current_user):
        raise HTTPException(status_code=403, detail="æƒé™ä¸è¶³")
    
    return await metrics.get_user_usage_stats(user_id, hours)


@app.get("/v1/stats/global")
async def get_global_stats(
    hours: int = 24,
    _: str = Depends(authenticate_request),  # éœ€è¦ç®¡ç†å‘˜æƒé™
    metrics: MetricsCollector = Depends(get_metrics_collector)
):
    """è·å–å…¨å±€ä½¿ç”¨ç»Ÿè®¡"""
    return await metrics.get_global_stats(hours)


@app.post("/v1/cache/clear")
async def clear_cache(
    user_id: Optional[str] = None,
    _: str = Depends(authenticate_request),  # éœ€è¦ç®¡ç†å‘˜æƒé™
    cache_manager: LLMCacheManager = Depends(get_cache_manager)
):
    """æ¸…ç†ç¼“å­˜"""
    if user_id:
        await cache_manager.clear_user_cache(user_id)
        return {"message": f"ç”¨æˆ· {user_id} çš„ç¼“å­˜å·²æ¸…ç†"}
    else:
        await cache_manager.clear_all_cache()
        return {"message": "æ‰€æœ‰ç¼“å­˜å·²æ¸…ç†"}


# === è°ƒè¯•å’Œå¼€å‘API ===

@app.get("/v1/debug/config")
async def get_debug_config(
    _: str = Depends(authenticate_request)  # éœ€è¦ç®¡ç†å‘˜æƒé™
):
    """è·å–è°ƒè¯•é…ç½®ä¿¡æ¯"""
    return {
        "version": "1.0.0",
        "environment": os.getenv("ENVIRONMENT", "development"),
        "log_level": logging.getLogger().level,
        "providers_count": len(get_llm_manager().get_available_providers()),
        "cache_enabled": get_cache_manager().is_enabled(),
        "metrics_enabled": get_metrics_collector().is_enabled()
    }


@app.post("/v1/debug/test-provider/{provider_name}")
async def test_provider(
    provider_name: str,
    _: str = Depends(authenticate_request),
    llm_manager: LLMManager = Depends(get_llm_manager)
):
    """æµ‹è¯•æŒ‡å®šæä¾›å•†çš„è¿æ¥"""
    try:
        result = await llm_manager.test_provider_connection(provider_name)
        return result
    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e))


# === é”™è¯¯å¤„ç† ===

@app.exception_handler(HTTPException)
async def http_exception_handler(request, exc):
    """HTTPå¼‚å¸¸å¤„ç†å™¨"""
    return JSONResponse(
        status_code=exc.status_code,
        content={"error": exc.detail, "timestamp": datetime.utcnow().isoformat()}
    )


@app.exception_handler(Exception)
async def general_exception_handler(request, exc):
    """é€šç”¨å¼‚å¸¸å¤„ç†å™¨"""
    logger.error(f"æœªå¤„ç†çš„å¼‚å¸¸: {exc}", exc_info=True)
    return JSONResponse(
        status_code=500,
        content={
            "error": "å†…éƒ¨æœåŠ¡å™¨é”™è¯¯",
            "error_type": type(exc).__name__,
            "timestamp": datetime.utcnow().isoformat()
        }
    )


# === æœåŠ¡å™¨é…ç½® ===

def get_server_config():
    """è·å–æœåŠ¡å™¨é…ç½®"""
    return {
        "host": os.getenv("LLM_SERVER_HOST", "0.0.0.0"),
        "port": int(os.getenv("LLM_SERVER_PORT", "8001")),
        "workers": int(os.getenv("LLM_SERVER_WORKERS", "1")),
        "log_level": os.getenv("LOG_LEVEL", "info"),
        "reload": os.getenv("ENVIRONMENT") == "development"
    }


if __name__ == "__main__":
    config = get_server_config()
    
    logger.info(f"ğŸš€ å¯åŠ¨ç‹¬ç«‹LLMæœåŠ¡å™¨")
    logger.info(f"ğŸ“ åœ°å€: http://{config['host']}:{config['port']}")
    logger.info(f"ğŸ“– APIæ–‡æ¡£: http://{config['host']}:{config['port']}/docs")
    
    uvicorn.run(
        "main:app",
        host=config["host"],
        port=config["port"],
        workers=config["workers"],
        log_level=config["log_level"],
        reload=config["reload"]
    )