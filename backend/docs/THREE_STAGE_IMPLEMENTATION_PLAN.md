# ä¸‰é˜¶æ®µAgentå®æ–½æ–¹æ¡ˆ

## ğŸ¯ å®æ–½æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›è¯¦ç»†çš„å®æ–½æ­¥éª¤ï¼Œå¸®åŠ©å°†å½“å‰å•ä¸€Agentæ¶æ„è½¬æ¢ä¸ºä¸‰é˜¶æ®µPipelineæ¶æ„ã€‚

---

## ğŸ”¥ ä¼˜å…ˆçº§0ï¼šä¿®å¤æ¨¡å‹è‡ªä¸»é€‰æ‹©åŠŸèƒ½ï¼ˆç«‹å³æ‰§è¡Œï¼‰

### é—®é¢˜è¯Šæ–­

å½“å‰ `model_selection.py` ä¸­çš„ `_llm_assess_complexity` æ–¹æ³•ä½¿ç”¨**æ¨¡æ‹Ÿè¯„ä¼°**ï¼Œè€Œä¸æ˜¯çœŸæ­£çš„LLMæ¨ç†ï¼š

```python
# âŒ å½“å‰å®ç°ï¼ˆæ¨¡æ‹Ÿï¼‰
async def _llm_assess_complexity(self, prompt: str) -> TaskComplexityAssessment:
    # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„LLMæœåŠ¡
    # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªç®€åŒ–çš„å®ç°

    # ç®€å•çš„å…³é”®è¯åŒ¹é…
    if any(keyword in prompt_lower for keyword in ["å¤æ‚", "å¤šè¡¨"]):
        complexity_score = 0.7
```

### è§£å†³æ–¹æ¡ˆ

åˆ›å»ºçœŸæ­£çš„LLMè¯„ä¼°æœåŠ¡ï¼š

```python
# âœ… æ–°å®ç°ï¼ˆçœŸå®LLMè¯„ä¼°ï¼‰

from typing import Optional
import json


class LLMComplexityEvaluator:
    """ä½¿ç”¨LLMè¿›è¡Œä»»åŠ¡å¤æ‚åº¦è¯„ä¼°"""

    def __init__(self, container):
        """
        Args:
            container: æœåŠ¡å®¹å™¨ï¼ŒåŒ…å«llm_adapter
        """
        self.container = container
        self.llm_adapter = None

    async def initialize(self):
        """åˆå§‹åŒ–LLMé€‚é…å™¨"""
        if not self.llm_adapter:
            from ..llm_adapter import get_llm_adapter
            self.llm_adapter = await get_llm_adapter(self.container)

    async def evaluate_complexity(
        self,
        task_description: str,
        context: Optional[Dict[str, Any]] = None
    ) -> TaskComplexityAssessment:
        """
        ä½¿ç”¨LLMè¯„ä¼°ä»»åŠ¡å¤æ‚åº¦

        Args:
            task_description: ä»»åŠ¡æè¿°
            context: ä»»åŠ¡ä¸Šä¸‹æ–‡

        Returns:
            TaskComplexityAssessment: å¤æ‚åº¦è¯„ä¼°ç»“æœ
        """
        await self.initialize()

        # æ„å»ºè¯„ä¼°æç¤º
        evaluation_prompt = self._build_evaluation_prompt(task_description, context)

        # è°ƒç”¨LLM
        messages = [
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»»åŠ¡å¤æ‚åº¦è¯„ä¼°ä¸“å®¶ã€‚ä½ éœ€è¦åˆ†æä»»åŠ¡å¹¶è¯„ä¼°å…¶å¤æ‚åº¦ã€‚"
            },
            {
                "role": "user",
                "content": evaluation_prompt
            }
        ]

        # ä½¿ç”¨ç»“æ„åŒ–è¾“å‡º
        response = await self.llm_adapter.chat_completion(
            messages=messages,
            temperature=0.0,  # ä½¿ç”¨ç¡®å®šæ€§è¾“å‡º
            response_format={"type": "json_object"}  # è¦æ±‚JSONæ ¼å¼
        )

        # è§£æå“åº”
        result = self._parse_llm_response(response)

        return result

    def _build_evaluation_prompt(
        self,
        task_description: str,
        context: Optional[Dict[str, Any]]
    ) -> str:
        """æ„å»ºè¯„ä¼°æç¤º"""

        prompt = f"""è¯·è¯„ä¼°ä»¥ä¸‹ä»»åŠ¡çš„å¤æ‚åº¦ï¼š

## ä»»åŠ¡æè¿°
{task_description}

## è¯„ä¼°ç»´åº¦

### 1. æ•°æ®æŸ¥è¯¢å¤æ‚åº¦ (0.0-0.3)
- 0.0-0.1: å•è¡¨æŸ¥è¯¢ï¼Œç®€å•æ¡ä»¶
- 0.1-0.2: å¤šè¡¨JOINï¼ŒåŸºç¡€èšåˆ
- 0.2-0.3: å¤æ‚JOINï¼Œå­æŸ¥è¯¢ï¼Œçª—å£å‡½æ•°

### 2. ä¸šåŠ¡é€»è¾‘å¤æ‚åº¦ (0.0-0.3)
- 0.0-0.1: å•ä¸€æŒ‡æ ‡ï¼Œç›´æ¥è®¡ç®—
- 0.1-0.2: å¤šä¸ªæŒ‡æ ‡ï¼Œç®€å•é€»è¾‘
- 0.2-0.3: å¤æ‚ä¸šåŠ¡è§„åˆ™ï¼Œå¤šç»´åº¦åˆ†æ

### 3. è®¡ç®—å¤æ‚åº¦ (0.0-0.2)
- 0.0-0.1: åŸºç¡€ç»Ÿè®¡ï¼ˆSUM, AVG, COUNTï¼‰
- 0.1-0.2: å¤æ‚è®¡ç®—ï¼ˆåŒæ¯”ã€ç¯æ¯”ã€è¶‹åŠ¿åˆ†æï¼‰

### 4. ä¸Šä¸‹æ–‡ç†è§£å¤æ‚åº¦ (0.0-0.2)
- 0.0-0.1: ç›´æ¥æ˜ç¡®çš„éœ€æ±‚
- 0.1-0.2: éœ€è¦æ¨ç†å’Œç†è§£éšå«éœ€æ±‚

"""

        if context:
            prompt += f"\n## ä»»åŠ¡ä¸Šä¸‹æ–‡\n{json.dumps(context, ensure_ascii=False, indent=2)}\n"

        prompt += """
## è¾“å‡ºæ ¼å¼

è¯·ä»¥JSONæ ¼å¼è¿”å›è¯„ä¼°ç»“æœï¼š

```json
{
    "complexity_score": 0.75,  // æ€»å¤æ‚åº¦è¯„åˆ† (0.0-1.0)
    "reasoning": "ä»»åŠ¡æ¶‰åŠå¤šè¡¨å…³è”æŸ¥è¯¢å’Œå¤æ‚çš„æ—¶é—´åºåˆ—åˆ†æ...",
    "factors": [
        "å¤šè¡¨JOINæŸ¥è¯¢",
        "æ—¶é—´åºåˆ—åˆ†æ",
        "å¤æ‚èšåˆè®¡ç®—"
    ],
    "confidence": 0.85,  // è¯„ä¼°ç½®ä¿¡åº¦ (0.0-1.0)
    "dimension_scores": {
        "data_query": 0.25,
        "business_logic": 0.20,
        "computation": 0.15,
        "context_understanding": 0.15
    }
}
```

è¯·æ ¹æ®ä¸Šè¿°è¯„ä¼°ç»´åº¦ï¼Œç»¼åˆåˆ†æä»»åŠ¡å¤æ‚åº¦ã€‚
"""

        return prompt

    def _parse_llm_response(self, response: str) -> TaskComplexityAssessment:
        """è§£æLLMå“åº”"""
        try:
            # å°è¯•è§£æJSON
            data = json.loads(response)

            return TaskComplexityAssessment(
                complexity_score=data.get("complexity_score", 0.5),
                reasoning=data.get("reasoning", ""),
                factors=data.get("factors", []),
                confidence=data.get("confidence", 0.8)
            )
        except json.JSONDecodeError as e:
            logger.error(f"è§£æLLMå“åº”å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤å€¼
            return TaskComplexityAssessment(
                complexity_score=0.5,
                reasoning="è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å¤æ‚åº¦",
                factors=["è§£æå¤±è´¥"],
                confidence=0.3
            )


class LLMModelSelector:
    """ä½¿ç”¨LLMè¿›è¡Œæ¨¡å‹é€‰æ‹©"""

    def __init__(self, container, user_model_resolver):
        self.container = container
        self.user_model_resolver = user_model_resolver
        self.llm_adapter = None

    async def initialize(self):
        """åˆå§‹åŒ–LLMé€‚é…å™¨"""
        if not self.llm_adapter:
            from ..llm_adapter import get_llm_adapter
            self.llm_adapter = await get_llm_adapter(self.container)

    async def select_model(
        self,
        task_description: str,
        complexity_assessment: TaskComplexityAssessment,
        user_id: str,
        task_type: str,
        available_models: List[Dict[str, Any]]
    ) -> ModelSelectionDecision:
        """
        ä½¿ç”¨LLMé€‰æ‹©æœ€åˆé€‚çš„æ¨¡å‹

        Args:
            task_description: ä»»åŠ¡æè¿°
            complexity_assessment: å¤æ‚åº¦è¯„ä¼°ç»“æœ
            user_id: ç”¨æˆ·ID
            task_type: ä»»åŠ¡ç±»å‹
            available_models: å¯ç”¨æ¨¡å‹åˆ—è¡¨

        Returns:
            ModelSelectionDecision: æ¨¡å‹é€‰æ‹©å†³ç­–
        """
        await self.initialize()

        # æ„å»ºé€‰æ‹©æç¤º
        selection_prompt = self._build_selection_prompt(
            task_description,
            complexity_assessment,
            available_models
        )

        # è°ƒç”¨LLM
        messages = [
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªAIæ¨¡å‹é€‰æ‹©ä¸“å®¶ï¼Œèƒ½å¤Ÿæ ¹æ®ä»»åŠ¡ç‰¹ç‚¹é€‰æ‹©æœ€åˆé€‚çš„æ¨¡å‹ã€‚"
            },
            {
                "role": "user",
                "content": selection_prompt
            }
        ]

        response = await self.llm_adapter.chat_completion(
            messages=messages,
            temperature=0.0,
            response_format={"type": "json_object"}
        )

        # è§£æå“åº”
        decision = self._parse_selection_response(response, available_models)

        return decision

    def _build_selection_prompt(
        self,
        task_description: str,
        complexity_assessment: TaskComplexityAssessment,
        available_models: List[Dict[str, Any]]
    ) -> str:
        """æ„å»ºæ¨¡å‹é€‰æ‹©æç¤º"""

        prompt = f"""è¯·ä¸ºä»¥ä¸‹ä»»åŠ¡é€‰æ‹©æœ€åˆé€‚çš„AIæ¨¡å‹ï¼š

## ä»»åŠ¡æè¿°
{task_description}

## å¤æ‚åº¦è¯„ä¼°
- å¤æ‚åº¦è¯„åˆ†: {complexity_assessment.complexity_score:.2f}
- è¯„ä¼°æ¨ç†: {complexity_assessment.reasoning}
- å½±å“å› ç´ : {', '.join(complexity_assessment.factors)}
- è¯„ä¼°ç½®ä¿¡åº¦: {complexity_assessment.confidence:.2f}

## å¯ç”¨æ¨¡å‹

"""

        for i, model in enumerate(available_models, 1):
            prompt += f"""
### {i}. {model['name']}
- ç±»å‹: {model['type']}
- èƒ½åŠ›: {model['capabilities']}
- æ¨ç†èƒ½åŠ›: {model['reasoning_level']}
- é€Ÿåº¦: {model['speed']}
- æˆæœ¬: {model['cost']}
- é€‚ç”¨åœºæ™¯: {model['use_cases']}
"""

        prompt += """
## é€‰æ‹©æ ‡å‡†

1. **ä»»åŠ¡åŒ¹é…åº¦**: æ¨¡å‹èƒ½åŠ›æ˜¯å¦åŒ¹é…ä»»åŠ¡éœ€æ±‚
2. **æ€§èƒ½éœ€æ±‚**: ä»»åŠ¡å¤æ‚åº¦ä¸æ¨¡å‹æ¨ç†èƒ½åŠ›çš„åŒ¹é…
3. **æˆæœ¬æ•ˆç›Š**: åœ¨æ»¡è¶³éœ€æ±‚çš„å‰æä¸‹é€‰æ‹©æ€§ä»·æ¯”æœ€é«˜çš„æ¨¡å‹
4. **é€Ÿåº¦è¦æ±‚**: è€ƒè™‘ä»»åŠ¡çš„æ—¶æ•ˆæ€§éœ€æ±‚

## è¾“å‡ºæ ¼å¼

```json
{
    "selected_model": "gpt-4",
    "model_type": "default",
    "reasoning": "ä»»åŠ¡å¤æ‚åº¦è¾ƒé«˜(0.75)ï¼Œéœ€è¦å¼ºå¤§çš„æ¨ç†èƒ½åŠ›...",
    "expected_performance": "é«˜æ€§èƒ½ï¼Œé¢„è®¡å‡†ç¡®ç‡95%+",
    "fallback_model": "gpt-3.5-turbo",
    "confidence": 0.9
}
```

è¯·æ ¹æ®ä»»åŠ¡ç‰¹ç‚¹å’Œå¯ç”¨æ¨¡å‹ï¼Œé€‰æ‹©æœ€åˆé€‚çš„æ¨¡å‹ã€‚
"""

        return prompt

    def _parse_selection_response(
        self,
        response: str,
        available_models: List[Dict[str, Any]]
    ) -> ModelSelectionDecision:
        """è§£ææ¨¡å‹é€‰æ‹©å“åº”"""
        try:
            data = json.loads(response)

            return ModelSelectionDecision(
                selected_model=data.get("selected_model", available_models[0]["name"]),
                model_type=data.get("model_type", "default"),
                reasoning=data.get("reasoning", ""),
                expected_performance=data.get("expected_performance", "æ ‡å‡†æ€§èƒ½"),
                fallback_model=data.get("fallback_model")
            )
        except json.JSONDecodeError as e:
            logger.error(f"è§£ææ¨¡å‹é€‰æ‹©å“åº”å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤é€‰æ‹©
            return ModelSelectionDecision(
                selected_model=available_models[0]["name"],
                model_type="default",
                reasoning="è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹",
                expected_performance="æ ‡å‡†æ€§èƒ½",
                fallback_model=available_models[1]["name"] if len(available_models) > 1 else None
            )
```

### é›†æˆæ­¥éª¤

#### 1. æ›´æ–° `TaskComplexityAssessmentTool`

```python
class TaskComplexityAssessmentTool(BaseTool):
    """ä»»åŠ¡å¤æ‚åº¦è¯„ä¼°å·¥å…·ï¼ˆä½¿ç”¨çœŸå®LLMï¼‰"""

    def __init__(self, container, user_model_resolver: UserModelResolver):
        super().__init__()
        self.container = container
        self.user_model_resolver = user_model_resolver
        self.evaluator = LLMComplexityEvaluator(container)
        self.name = "assess_task_complexity"
        self.description = "ä½¿ç”¨LLMè¯„ä¼°ä»»åŠ¡çš„å¤æ‚åº¦"

    async def arun(
        self,
        task_description: str,
        context: Optional[Dict[str, Any]] = None,
        **kwargs
    ) -> TaskComplexityAssessment:
        """è¯„ä¼°ä»»åŠ¡å¤æ‚åº¦"""
        try:
            logger.info(f"ğŸ” å¼€å§‹LLMè¯„ä¼°ä»»åŠ¡å¤æ‚åº¦: {task_description[:100]}...")

            # ä½¿ç”¨çœŸå®LLMè¯„ä¼°
            result = await self.evaluator.evaluate_complexity(
                task_description=task_description,
                context=context
            )

            logger.info(f"âœ… LLMè¯„ä¼°å®Œæˆ: {result.complexity_score:.2f}")
            return result

        except Exception as e:
            logger.error(f"âŒ LLMè¯„ä¼°å¤±è´¥: {e}")
            # å›é€€åˆ°è§„åˆ™åŸºç¡€è¯„ä¼°
            return self._fallback_assessment(task_description, context)

    def _fallback_assessment(
        self,
        task_description: str,
        context: Optional[Dict[str, Any]]
    ) -> TaskComplexityAssessment:
        """å›é€€è¯„ä¼°æ–¹æ³•"""
        # ä½¿ç”¨ç®€å•è§„åˆ™è¯„ä¼°
        complexity_score = 0.5

        # åŸºäºå…³é”®è¯çš„ç®€å•è¯„ä¼°
        keywords_complex = ["å¤æ‚", "å¤šè¡¨", "èšåˆ", "åˆ†æ", "JOIN"]
        keywords_simple = ["ç®€å•", "å•ä¸€", "åŸºç¡€"]

        text = task_description.lower()

        if any(kw in text for kw in keywords_complex):
            complexity_score = 0.7
        elif any(kw in text for kw in keywords_simple):
            complexity_score = 0.3

        return TaskComplexityAssessment(
            complexity_score=complexity_score,
            reasoning="LLMè¯„ä¼°å¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™åŸºç¡€è¯„ä¼°",
            factors=["è§„åˆ™è¯„ä¼°"],
            confidence=0.6
        )
```

#### 2. æ›´æ–° `ModelSelectionTool`

```python
class ModelSelectionTool(BaseTool):
    """æ¨¡å‹é€‰æ‹©å·¥å…·ï¼ˆä½¿ç”¨çœŸå®LLMï¼‰"""

    def __init__(self, container, user_model_resolver: UserModelResolver):
        super().__init__()
        self.container = container
        self.user_model_resolver = user_model_resolver
        self.selector = LLMModelSelector(container, user_model_resolver)
        self.name = "select_optimal_model"
        self.description = "ä½¿ç”¨LLMé€‰æ‹©æœ€åˆé€‚çš„æ¨¡å‹"

    async def arun(
        self,
        task_description: str,
        complexity_score: float,
        user_id: str,
        task_type: str = "placeholder_analysis",
        **kwargs
    ) -> ModelSelectionDecision:
        """é€‰æ‹©åˆé€‚çš„æ¨¡å‹"""
        try:
            logger.info(f"ğŸ¤– å¼€å§‹LLMæ¨¡å‹é€‰æ‹©: complexity={complexity_score:.2f}")

            # è·å–ç”¨æˆ·é…ç½®
            user_config = await get_user_model_config(user_id, task_type)

            # å‡†å¤‡å¯ç”¨æ¨¡å‹åˆ—è¡¨
            available_models = self._prepare_available_models(user_config)

            # åˆ›å»ºå¤æ‚åº¦è¯„ä¼°å¯¹è±¡
            complexity_assessment = TaskComplexityAssessment(
                complexity_score=complexity_score,
                reasoning=kwargs.get("complexity_reasoning", ""),
                factors=kwargs.get("complexity_factors", []),
                confidence=kwargs.get("complexity_confidence", 0.8)
            )

            # ä½¿ç”¨LLMé€‰æ‹©æ¨¡å‹
            decision = await self.selector.select_model(
                task_description=task_description,
                complexity_assessment=complexity_assessment,
                user_id=user_id,
                task_type=task_type,
                available_models=available_models
            )

            logger.info(f"âœ… LLMé€‰æ‹©å®Œæˆ: {decision.selected_model}")
            return decision

        except Exception as e:
            logger.error(f"âŒ LLMæ¨¡å‹é€‰æ‹©å¤±è´¥: {e}")
            # å›é€€åˆ°è§„åˆ™é€‰æ‹©
            return self._fallback_selection(user_config, complexity_score)

    def _prepare_available_models(
        self,
        user_config
    ) -> List[Dict[str, Any]]:
        """å‡†å¤‡å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        models = []

        if user_config.default_model:
            models.append({
                "name": user_config.default_model.model_name,
                "type": "default",
                "capabilities": "é€šç”¨ä»»åŠ¡å¤„ç†",
                "reasoning_level": "ä¸­ç­‰",
                "speed": "å¿«é€Ÿ",
                "cost": "ä½",
                "use_cases": "ç®€å•åˆ°ä¸­ç­‰å¤æ‚åº¦ä»»åŠ¡"
            })

        if user_config.think_model:
            models.append({
                "name": user_config.think_model.model_name,
                "type": "think",
                "capabilities": "æ·±åº¦æ¨ç†å’Œå¤æ‚ä»»åŠ¡å¤„ç†",
                "reasoning_level": "é«˜",
                "speed": "è¾ƒæ…¢",
                "cost": "é«˜",
                "use_cases": "å¤æ‚æ¨ç†ã€å¤šæ­¥éª¤ä»»åŠ¡"
            })

        return models

    def _fallback_selection(
        self,
        user_config,
        complexity_score: float
    ) -> ModelSelectionDecision:
        """å›é€€é€‰æ‹©æ–¹æ³•"""
        # ä½¿ç”¨è§„åˆ™é€‰æ‹©
        selected_model_config = self.user_model_resolver.select_model_for_task(
            user_config, complexity_score, "placeholder_analysis"
        )

        return ModelSelectionDecision(
            selected_model=selected_model_config.model_name,
            model_type=selected_model_config.model_type,
            reasoning="LLMé€‰æ‹©å¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™é€‰æ‹©",
            expected_performance="æ ‡å‡†æ€§èƒ½",
            fallback_model=None
        )
```

---

## ğŸ“ ä¼˜å…ˆçº§1ï¼šåˆ›å»ºä¸‰é˜¶æ®µAgentç±»

### 1.1 åˆ›å»ºåŸºç¡€ç›®å½•ç»“æ„

```bash
backend/app/services/infrastructure/agents/
â”œâ”€â”€ stages/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py              # åŸºç¡€é˜¶æ®µAgent
â”‚   â”œâ”€â”€ sql_stage.py         # SQLç”Ÿæˆé˜¶æ®µ
â”‚   â”œâ”€â”€ chart_stage.py       # å›¾è¡¨ç”Ÿæˆé˜¶æ®µ
â”‚   â””â”€â”€ document_stage.py    # æ–‡æ¡£ç”Ÿæˆé˜¶æ®µ
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ coordinator.py       # é˜¶æ®µåè°ƒå™¨
â”‚   â””â”€â”€ pipeline.py          # ä¸‰é˜¶æ®µPipeline
```

### 1.2 å®ç°åŸºç¡€é˜¶æ®µAgent

åˆ›å»º `backend/app/services/infrastructure/agents/stages/base.py`ï¼š

```python
"""
åŸºç¡€é˜¶æ®µAgent

æ‰€æœ‰é˜¶æ®µAgentçš„åŸºç±»
"""

from __future__ import annotations

import logging
from abc import ABC, abstractmethod
from typing import Any, Dict, Optional, AsyncGenerator

from ..facade import LoomAgentFacade
from ..types import AgentConfig, AgentEvent, ExecutionStage
from ..tools.model_selection import DynamicModelSwitcher


logger = logging.getLogger(__name__)


class BaseStageAgent(ABC):
    """åŸºç¡€é˜¶æ®µAgent"""

    def __init__(self, container, stage_name: str):
        """
        Args:
            container: æœåŠ¡å®¹å™¨
            stage_name: é˜¶æ®µåç§°
        """
        self.container = container
        self.stage_name = stage_name
        self.config: Optional[AgentConfig] = None
        self.facade: Optional[LoomAgentFacade] = None
        self.model_switcher = DynamicModelSwitcher(container)

        self._initialized = False

        logger.info(f"ğŸ—ï¸ [{self.stage_name}] é˜¶æ®µAgentåˆ›å»º")

    @abstractmethod
    def _create_stage_config(self) -> AgentConfig:
        """åˆ›å»ºé˜¶æ®µä¸“ç”¨é…ç½®"""
        pass

    @abstractmethod
    async def execute(self, **kwargs) -> Dict[str, Any]:
        """æ‰§è¡Œé˜¶æ®µä»»åŠ¡"""
        pass

    async def initialize(
        self,
        user_id: str,
        model_config: Optional[Dict[str, Any]] = None
    ):
        """
        åˆå§‹åŒ–é˜¶æ®µAgent

        Args:
            user_id: ç”¨æˆ·ID
            model_config: æ¨¡å‹é…ç½®ï¼ˆç”±DynamicModelSwitcherç”Ÿæˆï¼‰
        """
        if self._initialized:
            return

        try:
            logger.info(f"ğŸš€ [{self.stage_name}] å¼€å§‹åˆå§‹åŒ–")

            # åˆ›å»ºé˜¶æ®µé…ç½®
            self.config = self._create_stage_config()

            # å¦‚æœæä¾›äº†æ¨¡å‹é…ç½®ï¼Œæ›´æ–°LLMé…ç½®
            if model_config:
                self._apply_model_config(model_config)

            # åˆ›å»ºFacadeå®ä¾‹
            self.facade = LoomAgentFacade(
                container=self.container,
                config=self.config,
                enable_context_retriever=self._should_enable_context_retriever()
            )

            # åˆå§‹åŒ–Facade
            await self.facade.initialize(
                user_id=user_id,
                task_type=self._get_task_type(),
                task_complexity=model_config.get("complexity_assessment", {}).get("complexity_score", 0.5)
                    if model_config else 0.5
            )

            self._initialized = True
            logger.info(f"âœ… [{self.stage_name}] åˆå§‹åŒ–å®Œæˆ")

        except Exception as e:
            logger.error(f"âŒ [{self.stage_name}] åˆå§‹åŒ–å¤±è´¥: {e}", exc_info=True)
            raise

    def _apply_model_config(self, model_config: Dict[str, Any]):
        """åº”ç”¨æ¨¡å‹é…ç½®"""
        selected_model = model_config.get("selected_model_config", {})

        if selected_model:
            self.config.llm.model = selected_model.get("model_name", "gpt-4")
            self.config.llm.temperature = selected_model.get("temperature", 0.0)
            self.config.llm.max_tokens = selected_model.get("max_tokens")
            self.config.max_context_tokens = model_config.get("max_context_tokens", 16000)

            logger.info(
                f"ğŸ“ [{self.stage_name}] åº”ç”¨æ¨¡å‹é…ç½®: "
                f"model={self.config.llm.model}, "
                f"context_tokens={self.config.max_context_tokens}"
            )

    @abstractmethod
    def _should_enable_context_retriever(self) -> bool:
        """æ˜¯å¦å¯ç”¨ä¸Šä¸‹æ–‡æ£€ç´¢å™¨"""
        pass

    @abstractmethod
    def _get_task_type(self) -> str:
        """è·å–ä»»åŠ¡ç±»å‹"""
        pass

    def get_metrics(self) -> Dict[str, Any]:
        """è·å–é˜¶æ®µæŒ‡æ ‡"""
        if not self.facade:
            return {}

        return {
            "stage_name": self.stage_name,
            "initialized": self._initialized,
            **self.facade.get_metrics()
        }
```

### 1.3 å®ç°SQLç”Ÿæˆé˜¶æ®µAgent

åˆ›å»º `backend/app/services/infrastructure/agents/stages/sql_stage.py`ï¼š

```python
"""
SQLç”Ÿæˆé˜¶æ®µAgent

è´Ÿè´£æ ¹æ®æ¨¡æ¿ã€æ•°æ®æºç”Ÿæˆå¹¶éªŒè¯SQL
"""

from __future__ import annotations

import logging
from typing import Any, Dict, Optional, AsyncGenerator

from .base import BaseStageAgent
from ..config.agent import create_default_agent_config
from ..prompts.stages import get_stage_prompt
from ..types import AgentEvent, ExecutionStage, TaskComplexity


logger = logging.getLogger(__name__)


class SQLGenerationAgent(BaseStageAgent):
    """SQLç”Ÿæˆé˜¶æ®µAgent"""

    def __init__(self, container):
        super().__init__(container, "SQLç”Ÿæˆé˜¶æ®µ")

    def _create_stage_config(self):
        """åˆ›å»ºSQLé˜¶æ®µä¸“ç”¨é…ç½®"""
        config = create_default_agent_config()

        # SQLé˜¶æ®µåªå¯ç”¨ç›¸å…³å·¥å…·
        config.tools.enabled_tools = [
            "schema_discovery",
            "schema_retrieval",
            "schema_cache",
            "sql_generator",
            "sql_validator",
            "sql_column_checker",
            "sql_auto_fixer",
            "sql_executor",  # ç”¨äºæµ‹è¯•SQL
        ]

        # SQLé˜¶æ®µé…ç½®
        config.max_iterations = 8
        config.behavior.quality_threshold = 0.8
        config.behavior.enable_self_correction = True

        # SQLé˜¶æ®µç³»ç»Ÿæç¤º
        config.system_prompt = get_stage_prompt(ExecutionStage.SQL_GENERATION)

        return config

    def _should_enable_context_retriever(self) -> bool:
        """SQLé˜¶æ®µéœ€è¦Schemaä¸Šä¸‹æ–‡æ£€ç´¢"""
        return True

    def _get_task_type(self) -> str:
        """SQLç”Ÿæˆä»»åŠ¡ç±»å‹"""
        return "sql_generation"

    async def execute(
        self,
        placeholder: str,
        data_source_id: int,
        user_id: str,
        task_context: Optional[Dict[str, Any]] = None,
        template_context: Optional[Dict[str, Any]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        æ‰§è¡ŒSQLç”Ÿæˆé˜¶æ®µ

        Args:
            placeholder: å ä½ç¬¦æè¿°
            data_source_id: æ•°æ®æºID
            user_id: ç”¨æˆ·ID
            task_context: ä»»åŠ¡ä¸Šä¸‹æ–‡
            template_context: æ¨¡æ¿ä¸Šä¸‹æ–‡
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            Dict[str, Any]: SQLç”Ÿæˆç»“æœ
        """
        if not self._initialized:
            raise RuntimeError(f"[{self.stage_name}] Agentæœªåˆå§‹åŒ–")

        logger.info(f"ğŸ¯ [{self.stage_name}] å¼€å§‹æ‰§è¡Œ")
        logger.info(f"   å ä½ç¬¦: {placeholder[:100]}...")
        logger.info(f"   æ•°æ®æºID: {data_source_id}")

        try:
            # æ‰§è¡Œåˆ†æ
            result = await self.facade.analyze_placeholder_sync(
                placeholder=placeholder,
                data_source_id=data_source_id,
                user_id=user_id,
                task_context=task_context,
                template_context=template_context,
                complexity=TaskComplexity.MEDIUM,
                **kwargs
            )

            # æå–SQLç»“æœ
            sql_query = None
            if isinstance(result.result, str):
                sql_query = result.result
            elif isinstance(result.result, dict):
                sql_query = result.result.get("sql", result.result.get("result", ""))

            logger.info(f"âœ… [{self.stage_name}] æ‰§è¡Œå®Œæˆ")
            logger.info(f"   ç”ŸæˆSQLé•¿åº¦: {len(sql_query) if sql_query else 0}")
            logger.info(f"   è´¨é‡è¯„åˆ†: {result.quality_score:.2f}")

            return {
                "stage": "sql_generation",
                "sql": sql_query,
                "quality_score": result.quality_score,
                "reasoning": result.reasoning,
                "validation_results": result.validation_results,
                "metadata": result.metadata
            }

        except Exception as e:
            logger.error(f"âŒ [{self.stage_name}] æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            raise

    async def execute_stream(
        self,
        placeholder: str,
        data_source_id: int,
        user_id: str,
        **kwargs
    ) -> AsyncGenerator[AgentEvent, None]:
        """
        æµå¼æ‰§è¡ŒSQLç”Ÿæˆé˜¶æ®µ

        Yields:
            AgentEvent: æ‰§è¡Œäº‹ä»¶
        """
        if not self._initialized:
            raise RuntimeError(f"[{self.stage_name}] Agentæœªåˆå§‹åŒ–")

        logger.info(f"ğŸ¯ [{self.stage_name}] å¼€å§‹æµå¼æ‰§è¡Œ")

        async for event in self.facade.analyze_placeholder(
            placeholder=placeholder,
            data_source_id=data_source_id,
            user_id=user_id,
            **kwargs
        ):
            # æ·»åŠ é˜¶æ®µä¿¡æ¯
            event.data["stage_name"] = self.stage_name
            yield event
```

### 1.4 å®ç°å›¾è¡¨ç”Ÿæˆé˜¶æ®µAgent

åˆ›å»º `backend/app/services/infrastructure/agents/stages/chart_stage.py`ï¼š

```python
"""
å›¾è¡¨ç”Ÿæˆé˜¶æ®µAgent

è´Ÿè´£åŸºäºETLæ•°æ®ç”Ÿæˆå›¾è¡¨é…ç½®
"""

from __future__ import annotations

import logging
from typing import Any, Dict, Optional

from .base import BaseStageAgent
from ..config.agent import create_default_agent_config
from ..prompts.stages import get_stage_prompt
from ..types import ExecutionStage, TaskComplexity


logger = logging.getLogger(__name__)


class ChartGenerationAgent(BaseStageAgent):
    """å›¾è¡¨ç”Ÿæˆé˜¶æ®µAgent"""

    def __init__(self, container):
        super().__init__(container, "å›¾è¡¨ç”Ÿæˆé˜¶æ®µ")

    def _create_stage_config(self):
        """åˆ›å»ºå›¾è¡¨é˜¶æ®µä¸“ç”¨é…ç½®"""
        config = create_default_agent_config()

        # å›¾è¡¨é˜¶æ®µåªå¯ç”¨ç›¸å…³å·¥å…·
        config.tools.enabled_tools = [
            "chart_generator",
            "chart_analyzer",
            "data_sampler",
            "data_analyzer",
        ]

        # å›¾è¡¨é˜¶æ®µé…ç½®
        config.max_iterations = 6
        config.behavior.quality_threshold = 0.75

        # å›¾è¡¨é˜¶æ®µç³»ç»Ÿæç¤º
        config.system_prompt = get_stage_prompt(ExecutionStage.CHART_GENERATION)

        return config

    def _should_enable_context_retriever(self) -> bool:
        """å›¾è¡¨é˜¶æ®µä¸éœ€è¦Schemaä¸Šä¸‹æ–‡æ£€ç´¢"""
        return False

    def _get_task_type(self) -> str:
        """å›¾è¡¨ç”Ÿæˆä»»åŠ¡ç±»å‹"""
        return "chart_generation"

    async def execute(
        self,
        etl_data: Dict[str, Any],
        chart_placeholder: str,
        user_id: str,
        statistics: Optional[Dict[str, Any]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œå›¾è¡¨ç”Ÿæˆé˜¶æ®µ

        Args:
            etl_data: ETLå¤„ç†åçš„æ•°æ®
            chart_placeholder: å›¾è¡¨å ä½ç¬¦è¦æ±‚
            user_id: ç”¨æˆ·ID
            statistics: æ•°æ®ç»Ÿè®¡ä¿¡æ¯
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            Dict[str, Any]: å›¾è¡¨é…ç½®ç»“æœ
        """
        if not self._initialized:
            raise RuntimeError(f"[{self.stage_name}] Agentæœªåˆå§‹åŒ–")

        logger.info(f"ğŸ¯ [{self.stage_name}] å¼€å§‹æ‰§è¡Œ")
        logger.info(f"   å›¾è¡¨è¦æ±‚: {chart_placeholder[:100]}...")

        try:
            # æ„å»ºä»»åŠ¡ä¸Šä¸‹æ–‡
            task_context = {
                "etl_data": etl_data,
                "statistics": statistics or {},
                **kwargs.get("task_context", {})
            }

            # æ‰§è¡Œå›¾è¡¨ç”Ÿæˆ
            result = await self.facade.generate_chart(
                data_summary=str(statistics) if statistics else str(etl_data),
                chart_requirements=chart_placeholder,
                data_source_id=kwargs.get("data_source_id", 0),
                user_id=user_id,
                task_context=task_context
            )

            logger.info(f"âœ… [{self.stage_name}] æ‰§è¡Œå®Œæˆ")

            return {
                "stage": "chart_generation",
                "chart_config": result,
                "quality_score": result.get("quality_score", 0.0),
                "reasoning": result.get("reasoning", "")
            }

        except Exception as e:
            logger.error(f"âŒ [{self.stage_name}] æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            raise
```

### 1.5 å®ç°æ–‡æ¡£ç”Ÿæˆé˜¶æ®µAgent

åˆ›å»º `backend/app/services/infrastructure/agents/stages/document_stage.py`ï¼š

```python
"""
æ–‡æ¡£ç”Ÿæˆé˜¶æ®µAgent

è´Ÿè´£åŸºäºæ•°æ®é‡æ–°è¡¨è¾¾æ®µè½æ–‡æœ¬
"""

from __future__ import annotations

import logging
from typing import Any, Dict, Optional

from .base import BaseStageAgent
from ..config.agent import create_default_agent_config
from ..prompts.stages import get_stage_prompt
from ..types import ExecutionStage


logger = logging.getLogger(__name__)


class DocumentGenerationAgent(BaseStageAgent):
    """æ–‡æ¡£ç”Ÿæˆé˜¶æ®µAgent"""

    def __init__(self, container):
        super().__init__(container, "æ–‡æ¡£ç”Ÿæˆé˜¶æ®µ")

    def _create_stage_config(self):
        """åˆ›å»ºæ–‡æ¡£é˜¶æ®µä¸“ç”¨é…ç½®"""
        config = create_default_agent_config()

        # æ–‡æ¡£é˜¶æ®µå·¥å…·ï¼ˆç›®å‰ä½¿ç”¨LLMç›´æ¥ç”Ÿæˆï¼‰
        config.tools.enabled_tools = []

        # æ–‡æ¡£é˜¶æ®µé…ç½®
        config.max_iterations = 5
        config.behavior.quality_threshold = 0.85

        # æ–‡æ¡£é˜¶æ®µéœ€è¦æ›´é«˜çš„temperatureä»¥æ”¯æŒåˆ›é€ æ€§è¡¨è¾¾
        config.llm.temperature = 0.3

        # æ–‡æ¡£é˜¶æ®µç³»ç»Ÿæç¤º
        config.system_prompt = get_stage_prompt(ExecutionStage.ANALYSIS)  # æš‚æ—¶ä½¿ç”¨ANALYSIS

        return config

    def _should_enable_context_retriever(self) -> bool:
        """æ–‡æ¡£é˜¶æ®µä¸éœ€è¦Schemaä¸Šä¸‹æ–‡æ£€ç´¢"""
        return False

    def _get_task_type(self) -> str:
        """æ–‡æ¡£ç”Ÿæˆä»»åŠ¡ç±»å‹"""
        return "document_generation"

    async def execute(
        self,
        filled_template: str,
        paragraph_context: str,
        placeholder_data: Dict[str, Any],
        user_id: str,
        document_context: Optional[Dict[str, Any]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ–‡æ¡£ç”Ÿæˆé˜¶æ®µ

        Args:
            filled_template: å›å¡«æ•°æ®åçš„æ¨¡æ¿
            paragraph_context: å ä½ç¬¦æ‰€åœ¨æ®µè½
            placeholder_data: å ä½ç¬¦æ•°æ®
            user_id: ç”¨æˆ·ID
            document_context: æ–‡æ¡£ä¸Šä¸‹æ–‡
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            Dict[str, Any]: æ–‡æ¡£ç”Ÿæˆç»“æœ
        """
        if not self._initialized:
            raise RuntimeError(f"[{self.stage_name}] Agentæœªåˆå§‹åŒ–")

        logger.info(f"ğŸ¯ [{self.stage_name}] å¼€å§‹æ‰§è¡Œ")
        logger.info(f"   æ®µè½é•¿åº¦: {len(paragraph_context)}")

        try:
            # æ„å»ºæ–‡æ¡£ç”Ÿæˆæç¤º
            document_prompt = f"""
åŸºäºä»¥ä¸‹ä¿¡æ¯é‡æ–°è¡¨è¾¾æ®µè½ï¼š

## åŸå§‹æ®µè½
{paragraph_context}

## å ä½ç¬¦æ•°æ®
{placeholder_data}

## æ–‡æ¡£ä¸Šä¸‹æ–‡
{document_context or 'æ— '}

è¯·åŸºäºå ä½ç¬¦æ•°æ®ï¼Œä¿æŒæ®µè½çš„è¯­è¨€é£æ ¼å’Œé€»è¾‘ç»“æ„ï¼Œé‡æ–°è¡¨è¾¾è¿™ä¸ªæ®µè½ã€‚
ç¡®ä¿ï¼š
1. æ•°æ®å‡†ç¡®æ€§
2. è¯­è¨€æµç•…æ€§
3. é£æ ¼ä¸€è‡´æ€§
4. é€»è¾‘è¿è´¯æ€§
"""

            # ä½¿ç”¨LLMç”Ÿæˆæ–‡æ¡£
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å¯èƒ½éœ€è¦æ›´å¤æ‚çš„å·¥å…·
            result = await self.facade.analyze_placeholder_sync(
                placeholder=document_prompt,
                data_source_id=kwargs.get("data_source_id", 0),
                user_id=user_id,
                task_context={
                    "filled_template": filled_template,
                    "paragraph_context": paragraph_context,
                    "placeholder_data": placeholder_data,
                    "document_context": document_context or {}
                }
            )

            # æå–ç”Ÿæˆçš„æ–‡æœ¬
            generated_text = None
            if isinstance(result.result, str):
                generated_text = result.result
            elif isinstance(result.result, dict):
                generated_text = result.result.get("text", result.result.get("result", ""))

            logger.info(f"âœ… [{self.stage_name}] æ‰§è¡Œå®Œæˆ")
            logger.info(f"   ç”Ÿæˆæ–‡æœ¬é•¿åº¦: {len(generated_text) if generated_text else 0}")

            return {
                "stage": "document_generation",
                "generated_text": generated_text,
                "quality_score": result.quality_score,
                "reasoning": result.reasoning
            }

        except Exception as e:
            logger.error(f"âŒ [{self.stage_name}] æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            raise
```

---

## ğŸ“ ä¼˜å…ˆçº§2ï¼šåˆ›å»ºé˜¶æ®µåè°ƒå™¨

åˆ›å»º `backend/app/services/infrastructure/agents/pipeline/coordinator.py`ï¼š

```python
"""
é˜¶æ®µåè°ƒå™¨

ç®¡ç†ä¸‰ä¸ªé˜¶æ®µä¹‹é—´çš„æ•°æ®æµå’Œä¾èµ–å…³ç³»
"""

from __future__ import annotations

import logging
from typing import Any, Dict, List, Optional

logger = logging.getLogger(__name__)


class StageCoordinator:
    """é˜¶æ®µåè°ƒå™¨"""

    def __init__(self):
        # å®šä¹‰é˜¶æ®µä¾èµ–å…³ç³»
        self.stage_dependencies = {
            "sql_generation": [],  # SQLç”Ÿæˆæ²¡æœ‰ä¾èµ–
            "chart_generation": ["sql_generation"],  # å›¾è¡¨ä¾èµ–SQLç»“æœ
            "document_generation": ["sql_generation"]  # æ–‡æ¡£ä¾èµ–SQLç»“æœï¼ˆå¯é€‰ä¾èµ–å›¾è¡¨ï¼‰
        }

        # é˜¶æ®µæ‰§è¡Œé¡ºåº
        self.stage_order = ["sql_generation", "chart_generation", "document_generation"]

    def get_execution_order(self, requested_stages: List[str]) -> List[str]:
        """
        è·å–é˜¶æ®µæ‰§è¡Œé¡ºåº

        Args:
            requested_stages: è¯·æ±‚çš„é˜¶æ®µåˆ—è¡¨

        Returns:
            List[str]: æ’åºåçš„é˜¶æ®µåˆ—è¡¨
        """
        # æ·»åŠ æ‰€æœ‰ä¾èµ–
        all_stages = set(requested_stages)
        for stage in requested_stages:
            all_stages.update(self.stage_dependencies.get(stage, []))

        # æŒ‰é¢„å®šä¹‰é¡ºåºæ’åº
        ordered_stages = [s for s in self.stage_order if s in all_stages]

        logger.info(f"ğŸ“‹ é˜¶æ®µæ‰§è¡Œé¡ºåº: {ordered_stages}")
        return ordered_stages

    def validate_dependencies(self, stage: str, completed_stages: List[str]) -> bool:
        """
        éªŒè¯é˜¶æ®µä¾èµ–æ˜¯å¦æ»¡è¶³

        Args:
            stage: è¦æ‰§è¡Œçš„é˜¶æ®µ
            completed_stages: å·²å®Œæˆçš„é˜¶æ®µåˆ—è¡¨

        Returns:
            bool: ä¾èµ–æ˜¯å¦æ»¡è¶³
        """
        dependencies = self.stage_dependencies.get(stage, [])
        missing = [d for d in dependencies if d not in completed_stages]

        if missing:
            logger.error(f"âŒ é˜¶æ®µ {stage} ç¼ºå°‘ä¾èµ–: {missing}")
            return False

        return True

    def prepare_stage_input(
        self,
        stage: str,
        base_input: Dict[str, Any],
        stage_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        å‡†å¤‡é˜¶æ®µè¾“å…¥

        Args:
            stage: é˜¶æ®µåç§°
            base_input: åŸºç¡€è¾“å…¥
            stage_results: ä¹‹å‰é˜¶æ®µçš„ç»“æœ

        Returns:
            Dict[str, Any]: å‡†å¤‡å¥½çš„é˜¶æ®µè¾“å…¥
        """
        stage_input = base_input.copy()

        if stage == "chart_generation":
            # å›¾è¡¨ç”Ÿæˆéœ€è¦SQLç»“æœ
            sql_result = stage_results.get("sql_generation", {})
            stage_input["etl_data"] = sql_result.get("etl_data", {})
            stage_input["sql"] = sql_result.get("sql")

        elif stage == "document_generation":
            # æ–‡æ¡£ç”Ÿæˆéœ€è¦SQLå’Œå›¾è¡¨ç»“æœ
            sql_result = stage_results.get("sql_generation", {})
            chart_result = stage_results.get("chart_generation", {})

            stage_input["filled_template"] = sql_result.get("filled_template", "")
            stage_input["placeholder_data"] = sql_result.get("placeholder_data", {})

            if chart_result:
                stage_input["chart_configs"] = chart_result.get("chart_config", {})

        return stage_input

    def merge_stage_results(self, stage_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        åˆå¹¶æ‰€æœ‰é˜¶æ®µç»“æœ

        Args:
            stage_results: å„é˜¶æ®µç»“æœ

        Returns:
            Dict[str, Any]: åˆå¹¶åçš„ç»“æœ
        """
        merged = {
            "stages": {},
            "summary": {},
            "metadata": {}
        }

        # å¤åˆ¶å„é˜¶æ®µç»“æœ
        for stage, result in stage_results.items():
            merged["stages"][stage] = result

            # æå–å…³é”®ä¿¡æ¯åˆ°summary
            if stage == "sql_generation":
                merged["summary"]["sql"] = result.get("sql")
                merged["summary"]["sql_quality"] = result.get("quality_score")

            elif stage == "chart_generation":
                merged["summary"]["chart_config"] = result.get("chart_config")

            elif stage == "document_generation":
                merged["summary"]["generated_text"] = result.get("generated_text")
                merged["summary"]["text_quality"] = result.get("quality_score")

        # è®¡ç®—æ•´ä½“è´¨é‡
        quality_scores = [
            result.get("quality_score", 0.0)
            for result in stage_results.values()
            if "quality_score" in result
        ]

        if quality_scores:
            merged["summary"]["overall_quality"] = sum(quality_scores) / len(quality_scores)

        return merged
```

---

## ğŸ“ ä¼˜å…ˆçº§3ï¼šåˆ›å»ºä¸‰é˜¶æ®µPipeline

åˆ›å»º `backend/app/services/infrastructure/agents/pipeline/pipeline.py`ï¼š

```python
"""
ä¸‰é˜¶æ®µAgent Pipeline

åè°ƒSQLç”Ÿæˆã€å›¾è¡¨ç”Ÿæˆã€æ–‡æ¡£ç”Ÿæˆä¸‰ä¸ªé˜¶æ®µ
"""

from __future__ import annotations

import logging
from typing import Any, Dict, List, Optional

from .coordinator import StageCoordinator
from ..stages.sql_stage import SQLGenerationAgent
from ..stages.chart_stage import ChartGenerationAgent
from ..stages.document_stage import DocumentGenerationAgent
from ..tools.model_selection import DynamicModelSwitcher, user_model_resolver

logger = logging.getLogger(__name__)


class ThreeStageAgentPipeline:
    """ä¸‰é˜¶æ®µAgent Pipeline"""

    def __init__(self, container):
        """
        Args:
            container: æœåŠ¡å®¹å™¨
        """
        self.container = container

        # åˆ›å»ºä¸‰ä¸ªé˜¶æ®µçš„Agent
        self.sql_agent = SQLGenerationAgent(container)
        self.chart_agent = ChartGenerationAgent(container)
        self.document_agent = DocumentGenerationAgent(container)

        # æ¨¡å‹è‡ªä¸»é€‰æ‹©å™¨
        self.model_switcher = DynamicModelSwitcher(user_model_resolver)

        # é˜¶æ®µåè°ƒå™¨
        self.coordinator = StageCoordinator()

        logger.info("ğŸ—ï¸ [ThreeStageAgentPipeline] Pipelineåˆ›å»ºå®Œæˆ")

    async def execute_sql_stage(
        self,
        placeholder: str,
        data_source_id: int,
        user_id: str,
        **kwargs
    ) -> Dict[str, Any]:
        """æ‰§è¡Œé˜¶æ®µ1ï¼šSQLç”Ÿæˆä¸éªŒè¯"""
        logger.info("ğŸ¯ å¼€å§‹æ‰§è¡Œé˜¶æ®µ1ï¼šSQLç”Ÿæˆ")

        # 1. LLMè‡ªä¸»åˆ¤æ–­ä»»åŠ¡å¤æ‚åº¦å’Œæ¨¡å‹é€‰æ‹©
        model_config = await self.model_switcher.assess_and_select_model(
            task_description=f"SQLç”Ÿæˆ: {placeholder}",
            user_id=user_id,
            context=kwargs.get("task_context"),
            task_type="sql_generation"
        )

        # 2. åˆå§‹åŒ–SQL Agent
        await self.sql_agent.initialize(user_id, model_config)

        # 3. æ‰§è¡ŒSQLç”Ÿæˆ
        result = await self.sql_agent.execute(
            placeholder=placeholder,
            data_source_id=data_source_id,
            user_id=user_id,
            **kwargs
        )

        logger.info(f"âœ… é˜¶æ®µ1å®Œæˆ: SQLè´¨é‡={result.get('quality_score', 0):.2f}")
        return result

    async def execute_chart_stage(
        self,
        etl_data: Dict[str, Any],
        chart_placeholder: str,
        user_id: str,
        **kwargs
    ) -> Dict[str, Any]:
        """æ‰§è¡Œé˜¶æ®µ2ï¼šå›¾è¡¨ç”Ÿæˆ"""
        logger.info("ğŸ¯ å¼€å§‹æ‰§è¡Œé˜¶æ®µ2ï¼šå›¾è¡¨ç”Ÿæˆ")

        # 1. LLMè‡ªä¸»åˆ¤æ–­ä»»åŠ¡å¤æ‚åº¦å’Œæ¨¡å‹é€‰æ‹©
        model_config = await self.model_switcher.assess_and_select_model(
            task_description=f"å›¾è¡¨ç”Ÿæˆ: {chart_placeholder}",
            user_id=user_id,
            context={"etl_data": etl_data, **kwargs.get("task_context", {})},
            task_type="chart_generation"
        )

        # 2. åˆå§‹åŒ–Chart Agent
        await self.chart_agent.initialize(user_id, model_config)

        # 3. æ‰§è¡Œå›¾è¡¨ç”Ÿæˆ
        result = await self.chart_agent.execute(
            etl_data=etl_data,
            chart_placeholder=chart_placeholder,
            user_id=user_id,
            **kwargs
        )

        logger.info(f"âœ… é˜¶æ®µ2å®Œæˆ: å›¾è¡¨è´¨é‡={result.get('quality_score', 0):.2f}")
        return result

    async def execute_document_stage(
        self,
        filled_template: str,
        paragraph_context: str,
        placeholder_data: Dict[str, Any],
        user_id: str,
        **kwargs
    ) -> Dict[str, Any]:
        """æ‰§è¡Œé˜¶æ®µ3ï¼šæ–‡æ¡£ç”Ÿæˆ"""
        logger.info("ğŸ¯ å¼€å§‹æ‰§è¡Œé˜¶æ®µ3ï¼šæ–‡æ¡£ç”Ÿæˆ")

        # 1. LLMè‡ªä¸»åˆ¤æ–­ä»»åŠ¡å¤æ‚åº¦å’Œæ¨¡å‹é€‰æ‹©
        model_config = await self.model_switcher.assess_and_select_model(
            task_description=f"æ–‡æ¡£ç”Ÿæˆ: {paragraph_context[:100]}",
            user_id=user_id,
            context={
                "paragraph_context": paragraph_context,
                "placeholder_data": placeholder_data,
                **kwargs.get("document_context", {})
            },
            task_type="document_generation"
        )

        # 2. åˆå§‹åŒ–Document Agent
        await self.document_agent.initialize(user_id, model_config)

        # 3. æ‰§è¡Œæ–‡æ¡£ç”Ÿæˆ
        result = await self.document_agent.execute(
            filled_template=filled_template,
            paragraph_context=paragraph_context,
            placeholder_data=placeholder_data,
            user_id=user_id,
            **kwargs
        )

        logger.info(f"âœ… é˜¶æ®µ3å®Œæˆ: æ–‡æœ¬è´¨é‡={result.get('quality_score', 0):.2f}")
        return result

    async def execute_pipeline(
        self,
        placeholder: str,
        data_source_id: int,
        user_id: str,
        stages: Optional[List[str]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œå®Œæ•´çš„Pipeline

        Args:
            placeholder: å ä½ç¬¦æè¿°
            data_source_id: æ•°æ®æºID
            user_id: ç”¨æˆ·ID
            stages: è¦æ‰§è¡Œçš„é˜¶æ®µåˆ—è¡¨ï¼ˆé»˜è®¤å…¨éƒ¨æ‰§è¡Œï¼‰
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            Dict[str, Any]: Pipelineæ‰§è¡Œç»“æœ
        """
        logger.info("ğŸš€ [ThreeStageAgentPipeline] å¼€å§‹æ‰§è¡ŒPipeline")

        # ç¡®å®šæ‰§è¡Œé˜¶æ®µ
        if stages is None:
            stages = ["sql_generation", "chart_generation", "document_generation"]

        # è·å–æ‰§è¡Œé¡ºåº
        execution_order = self.coordinator.get_execution_order(stages)

        # å‡†å¤‡åŸºç¡€è¾“å…¥
        base_input = {
            "placeholder": placeholder,
            "data_source_id": data_source_id,
            "user_id": user_id,
            **kwargs
        }

        # æ‰§è¡Œå„é˜¶æ®µ
        stage_results = {}
        completed_stages = []

        for stage in execution_order:
            # éªŒè¯ä¾èµ–
            if not self.coordinator.validate_dependencies(stage, completed_stages):
                logger.error(f"âŒ é˜¶æ®µ {stage} ä¾èµ–æœªæ»¡è¶³ï¼Œè·³è¿‡")
                continue

            # å‡†å¤‡é˜¶æ®µè¾“å…¥
            stage_input = self.coordinator.prepare_stage_input(
                stage, base_input, stage_results
            )

            # æ‰§è¡Œé˜¶æ®µ
            try:
                if stage == "sql_generation":
                    result = await self.execute_sql_stage(**stage_input)
                elif stage == "chart_generation":
                    result = await self.execute_chart_stage(**stage_input)
                elif stage == "document_generation":
                    result = await self.execute_document_stage(**stage_input)
                else:
                    logger.warning(f"âš ï¸ æœªçŸ¥é˜¶æ®µ: {stage}")
                    continue

                stage_results[stage] = result
                completed_stages.append(stage)

            except Exception as e:
                logger.error(f"âŒ é˜¶æ®µ {stage} æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
                stage_results[stage] = {
                    "stage": stage,
                    "error": str(e),
                    "status": "failed"
                }

        # åˆå¹¶ç»“æœ
        merged_results = self.coordinator.merge_stage_results(stage_results)

        logger.info("âœ… [ThreeStageAgentPipeline] Pipelineæ‰§è¡Œå®Œæˆ")
        logger.info(f"   æ‰§è¡Œé˜¶æ®µ: {completed_stages}")
        logger.info(f"   æ•´ä½“è´¨é‡: {merged_results['summary'].get('overall_quality', 0):.2f}")

        return merged_results


# ä¾¿æ·å‡½æ•°

def create_three_stage_pipeline(container) -> ThreeStageAgentPipeline:
    """åˆ›å»ºä¸‰é˜¶æ®µPipeline"""
    return ThreeStageAgentPipeline(container)
```

---

## ğŸ“ ä¼˜å…ˆçº§4ï¼šæ›´æ–°Facadeæ¥å£

æ›´æ–° `facade.py` ä»¥æ”¯æŒä¸‰é˜¶æ®µPipelineï¼š

```python
# åœ¨LoomAgentFacadeç±»ä¸­æ·»åŠ ä»¥ä¸‹æ–¹æ³•

from .pipeline.pipeline import ThreeStageAgentPipeline

class LoomAgentFacade:
    # ... ç°æœ‰ä»£ç  ...

    def __init__(self, container, config=None, enable_context_retriever=True):
        # ... ç°æœ‰ä»£ç  ...

        # æ·»åŠ ä¸‰é˜¶æ®µPipeline
        self._pipeline: Optional[ThreeStageAgentPipeline] = None

    def get_pipeline(self) -> ThreeStageAgentPipeline:
        """è·å–ä¸‰é˜¶æ®µPipelineå®ä¾‹"""
        if not self._pipeline:
            self._pipeline = ThreeStageAgentPipeline(self.container)
        return self._pipeline

    async def execute_three_stage_pipeline(
        self,
        placeholder: str,
        data_source_id: int,
        user_id: str,
        stages: Optional[List[str]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œä¸‰é˜¶æ®µPipeline

        Args:
            placeholder: å ä½ç¬¦æè¿°
            data_source_id: æ•°æ®æºID
            user_id: ç”¨æˆ·ID
            stages: è¦æ‰§è¡Œçš„é˜¶æ®µåˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            Dict[str, Any]: Pipelineç»“æœ
        """
        pipeline = self.get_pipeline()

        return await pipeline.execute_pipeline(
            placeholder=placeholder,
            data_source_id=data_source_id,
            user_id=user_id,
            stages=stages,
            **kwargs
        )
```

---

## ğŸ“ æµ‹è¯•è®¡åˆ’

åˆ›å»ºæµ‹è¯•è„šæœ¬ `backend/scripts/test_three_stage_pipeline.py`ï¼š

```python
"""
æµ‹è¯•ä¸‰é˜¶æ®µPipeline
"""

import asyncio
import logging
from app.core.container import Container
from app.services.infrastructure.agents.pipeline.pipeline import create_three_stage_pipeline

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


async def test_three_stage_pipeline():
    """æµ‹è¯•å®Œæ•´çš„ä¸‰é˜¶æ®µPipeline"""

    # åˆ›å»ºå®¹å™¨
    container = Container()

    # åˆ›å»ºPipeline
    pipeline = create_three_stage_pipeline(container)

    # æµ‹è¯•æ•°æ®
    test_input = {
        "placeholder": "ç»Ÿè®¡2023å¹´å„éƒ¨é—¨çš„é”€å”®é¢ï¼Œå¹¶æŒ‰é”€å”®é¢é™åºæ’åˆ—",
        "data_source_id": 1,
        "user_id": "test_user_123",
        "task_context": {
            "template_name": "å¹´åº¦é”€å”®æŠ¥å‘Š",
            "report_type": "summary"
        }
    }

    try:
        logger.info("=" * 60)
        logger.info("å¼€å§‹æµ‹è¯•ä¸‰é˜¶æ®µPipeline")
        logger.info("=" * 60)

        # æ‰§è¡ŒPipeline
        result = await pipeline.execute_pipeline(**test_input)

        logger.info("=" * 60)
        logger.info("Pipelineæ‰§è¡Œç»“æœ")
        logger.info("=" * 60)

        # è¾“å‡ºç»“æœ
        logger.info(f"æ‰§è¡Œé˜¶æ®µ: {list(result['stages'].keys())}")
        logger.info(f"æ•´ä½“è´¨é‡: {result['summary'].get('overall_quality', 0):.2f}")

        if "sql" in result["summary"]:
            logger.info(f"\nSQLæŸ¥è¯¢:\n{result['summary']['sql']}")

        if "chart_config" in result["summary"]:
            logger.info(f"\nå›¾è¡¨é…ç½®:\n{result['summary']['chart_config']}")

        if "generated_text" in result["summary"]:
            logger.info(f"\nç”Ÿæˆæ–‡æœ¬:\n{result['summary']['generated_text']}")

        logger.info("=" * 60)
        logger.info("âœ… æµ‹è¯•å®Œæˆ")
        logger.info("=" * 60)

    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}", exc_info=True)


if __name__ == "__main__":
    asyncio.run(test_three_stage_pipeline())
```

---

## ğŸ“Š å®æ–½æ—¶é—´è¡¨

| ä¼˜å…ˆçº§ | ä»»åŠ¡ | é¢„è®¡æ—¶é—´ | ä¾èµ– |
|--------|------|----------|------|
| P0 | ä¿®å¤æ¨¡å‹è‡ªä¸»é€‰æ‹©åŠŸèƒ½ | 1å¤© | æ—  |
| P1 | åˆ›å»ºä¸‰é˜¶æ®µAgentç±» | 2å¤© | P0 |
| P2 | åˆ›å»ºé˜¶æ®µåè°ƒå™¨ | 0.5å¤© | P1 |
| P3 | åˆ›å»ºä¸‰é˜¶æ®µPipeline | 1å¤© | P1, P2 |
| P4 | æ›´æ–°Facadeæ¥å£ | 0.5å¤© | P3 |
| P5 | ç¼–å†™æµ‹è¯•ç”¨ä¾‹ | 1å¤© | P4 |
| P6 | æ–‡æ¡£å’Œç¤ºä¾‹ | 0.5å¤© | P5 |

**æ€»è®¡**: çº¦6.5å¤©

---

## ğŸ¯ æˆåŠŸæ ‡å‡†

1. âœ… æ¨¡å‹è‡ªä¸»é€‰æ‹©åŠŸèƒ½ä½¿ç”¨çœŸå®LLMè¯„ä¼°
2. âœ… ä¸‰ä¸ªé˜¶æ®µçš„Agentç‹¬ç«‹è¿è¡Œ
3. âœ… é˜¶æ®µåè°ƒå™¨æ­£ç¡®ç®¡ç†ä¾èµ–
4. âœ… Pipelineèƒ½å¤Ÿå®Œæ•´æ‰§è¡Œä¸‰ä¸ªé˜¶æ®µ
5. âœ… æ¯ä¸ªé˜¶æ®µçš„è´¨é‡è¯„åˆ†è¾¾åˆ°é¢„æœŸ
6. âœ… æ•´ä½“æ‰§è¡Œæ—¶é—´å‡å°‘30%+
7. âœ… Tokenä½¿ç”¨é‡å‡å°‘40%+

---

## ğŸ”„ åç»­ä¼˜åŒ–

1. **é˜¶æ®µç¼“å­˜**ï¼šç¼“å­˜SQLç”Ÿæˆç»“æœï¼Œé¿å…é‡å¤æ‰§è¡Œ
2. **å¹¶è¡Œæ‰§è¡Œ**ï¼šå›¾è¡¨å’Œæ–‡æ¡£é˜¶æ®µå¯ä»¥éƒ¨åˆ†å¹¶è¡Œ
3. **å¢é‡æ›´æ–°**ï¼šä»…é‡æ–°æ‰§è¡Œæœ‰å˜åŒ–çš„é˜¶æ®µ
4. **æ€§èƒ½ç›‘æ§**ï¼šæ·»åŠ è¯¦ç»†çš„æ€§èƒ½æŒ‡æ ‡
5. **A/Bæµ‹è¯•**ï¼šå¯¹æ¯”å•Agent vs ä¸‰é˜¶æ®µçš„æ•ˆæœ
