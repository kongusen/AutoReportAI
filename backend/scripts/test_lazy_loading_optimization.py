"""
æµ‹è¯•æ‡’åŠ è½½ä¼˜åŒ–çš„å®Œæ•´åŠŸèƒ½

éªŒè¯ï¼š
1. å¯åŠ¨æ—¶åªåŠ è½½è¡¨åï¼Œä¸åŠ è½½åˆ—ä¿¡æ¯
2. TTå¾ªç¯ä¸­æŒ‰éœ€åŠ è½½åˆ—ä¿¡æ¯
3. é¿å…é‡å¤æŸ¥è¯¢å·²åŠ è½½çš„è¡¨
4. æ™ºèƒ½ç­›é€‰ç›¸å…³è¡¨
5. ç¼“å­˜ç»Ÿè®¡åŠŸèƒ½
"""

import asyncio
import logging
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
backend_path = Path(__file__).parent.parent
sys.path.insert(0, str(backend_path))

from app.core.container import Container
from app.db.session import get_db_session
from app.models.data_source import DataSource
from app.services.infrastructure.agents.context_retriever import (
    create_schema_context_retriever
)
from app.services.infrastructure.agents.tools.schema import (
    create_schema_discovery_tool
)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def get_test_data_source():
    """ä»æ•°æ®åº“è·å–æµ‹è¯•ç”¨çš„æ•°æ®æºé…ç½®"""
    logger.info("\nğŸ“Š ä»æ•°æ®åº“è·å–æµ‹è¯•æ•°æ®æº...")

    with get_db_session() as db:
        # å°è¯•è·å–ç¬¬ä¸€ä¸ªå¯ç”¨çš„æ•°æ®æº
        data_source = db.query(DataSource).first()

        if not data_source:
            logger.error("âŒ æ•°æ®åº“ä¸­æ²¡æœ‰å¯ç”¨çš„æ•°æ®æº")
            return None, None

        logger.info(f"âœ… æ‰¾åˆ°æ•°æ®æº: {data_source.name} (ID: {data_source.id})")
        logger.info(f"   ç±»å‹: {data_source.source_type}")

        return str(data_source.id), data_source.connection_config


async def test_schema_discovery_tool_lazy_loading():
    """æµ‹è¯• SchemaDiscoveryTool çš„æ‡’åŠ è½½åŠŸèƒ½"""
    logger.info("\n" + "="*80)
    logger.info("æµ‹è¯• SchemaDiscoveryTool æ‡’åŠ è½½åŠŸèƒ½")
    logger.info("="*80)

    try:
        # 1. è·å–æµ‹è¯•æ•°æ®æºé…ç½®
        data_source_id, connection_config = get_test_data_source()
        if not connection_config:
            logger.error("âŒ æ— æ³•è·å–æ•°æ®æºé…ç½®ï¼Œè·³è¿‡æµ‹è¯•")
            return False

        # 2. åˆ›å»ºå®¹å™¨
        container = Container()

        # 3. åˆ›å»º SchemaDiscoveryToolï¼ˆå¯ç”¨æ‡’åŠ è½½ï¼‰
        tool = create_schema_discovery_tool(container, enable_lazy_loading=True)
        logger.info(f"âœ… åˆ›å»º SchemaDiscoveryToolï¼Œæ‡’åŠ è½½æ¨¡å¼: {tool.enable_lazy_loading}")

        # 4. æµ‹è¯•ï¼šåªå‘ç°è¡¨åï¼ˆæ‡’åŠ è½½æ¨¡å¼ï¼‰
        logger.info("\nğŸ“‹ æµ‹è¯•1: åªå‘ç°è¡¨åï¼ˆæ‡’åŠ è½½æ¨¡å¼ï¼‰")
        result = await tool.run(
            connection_config=connection_config,
            discovery_type="tables",
            max_tables=100
        )

        if result.get("success"):
            tables = result.get("tables", [])
            logger.info(f"âœ… å‘ç° {len(tables)} ä¸ªè¡¨")
            logger.info(f"   æ‡’åŠ è½½å¯ç”¨: {result['metadata'].get('lazy_loading_enabled')}")

            # æ£€æŸ¥è¡¨ä¿¡æ¯æ˜¯å¦åªåŒ…å«è¡¨åï¼Œä¸åŒ…å«åˆ—ä¿¡æ¯
            if tables:
                sample_table = tables[0]
                logger.info(f"   ç¤ºä¾‹è¡¨: {sample_table.get('table_name')}")
                logger.info(f"   åˆ—æ•°: {len(sample_table.get('columns', []))}")
                logger.info(f"   æ˜¯å¦æ‡’åŠ è½½: {sample_table.get('lazy_loaded')}")

                if len(sample_table.get('columns', [])) == 0:
                    logger.info("âœ… éªŒè¯é€šè¿‡ï¼šè¡¨ä¿¡æ¯ä¸åŒ…å«åˆ—æ•°æ®ï¼ˆæ‡’åŠ è½½ç”Ÿæ•ˆï¼‰")
                else:
                    logger.warning("âš ï¸ éªŒè¯å¤±è´¥ï¼šè¡¨ä¿¡æ¯åŒ…å«åˆ—æ•°æ®ï¼ˆæ‡’åŠ è½½æœªç”Ÿæ•ˆï¼‰")
        else:
            logger.error(f"âŒ å‘ç°å¤±è´¥: {result.get('error')}")

        # 5. è·å–ç¼“å­˜ç»Ÿè®¡
        cache_stats = tool.get_cache_stats()
        logger.info(f"\nğŸ“Š ç¼“å­˜ç»Ÿè®¡:")
        logger.info(f"   æ‡’åŠ è½½å¯ç”¨: {cache_stats['lazy_loading_enabled']}")
        logger.info(f"   ç¼“å­˜å·²åˆå§‹åŒ–: {cache_stats['cache_initialized']}")
        logger.info(f"   æ€»è¡¨æ•°: {cache_stats['total_tables']}")
        logger.info(f"   å·²åŠ è½½è¡¨æ•°: {cache_stats['loaded_tables']}")
        logger.info(f"   è¡¨ååˆ—è¡¨ï¼ˆå‰10ä¸ªï¼‰: {cache_stats['table_names']}")

        # 6. æµ‹è¯•ï¼šæŒ‰éœ€åŠ è½½ç‰¹å®šè¡¨çš„åˆ—ä¿¡æ¯
        logger.info("\nğŸ“‹ æµ‹è¯•2: æŒ‰éœ€åŠ è½½åˆ—ä¿¡æ¯")
        result = await tool.run(
            connection_config=connection_config,
            discovery_type="columns",
            tables=cache_stats['table_names'][:3]  # åªåŠ è½½å‰3ä¸ªè¡¨çš„åˆ—ä¿¡æ¯
        )

        if result.get("success"):
            columns = result.get("columns", [])
            logger.info(f"âœ… åŠ è½½ {len(columns)} ä¸ªåˆ—")

            # å†æ¬¡è·å–ç¼“å­˜ç»Ÿè®¡
            cache_stats_after = tool.get_cache_stats()
            logger.info(f"\nğŸ“Š åŠ è½½åçš„ç¼“å­˜ç»Ÿè®¡:")
            logger.info(f"   æ€»è¡¨æ•°: {cache_stats_after['total_tables']}")
            logger.info(f"   å·²åŠ è½½è¡¨æ•°: {cache_stats_after['loaded_tables']}")
            logger.info(f"   å·²åŠ è½½çš„è¡¨: {cache_stats_after['loaded_table_names']}")

            if cache_stats_after['loaded_tables'] == 3:
                logger.info("âœ… éªŒè¯é€šè¿‡ï¼šåªåŠ è½½äº†è¯·æ±‚çš„3ä¸ªè¡¨")
            else:
                logger.warning(f"âš ï¸ éªŒè¯å¤±è´¥ï¼šåŠ è½½äº† {cache_stats_after['loaded_tables']} ä¸ªè¡¨")

        # 7. æµ‹è¯•ï¼šé‡å¤åŠ è½½ç›¸åŒçš„è¡¨ï¼ˆåº”è¯¥ä½¿ç”¨ç¼“å­˜ï¼‰
        logger.info("\nğŸ“‹ æµ‹è¯•3: é‡å¤åŠ è½½ç›¸åŒçš„è¡¨ï¼ˆæµ‹è¯•ç¼“å­˜ï¼‰")
        result = await tool.run(
            connection_config=connection_config,
            discovery_type="columns",
            tables=cache_stats['table_names'][:3]  # å†æ¬¡åŠ è½½ç›¸åŒçš„è¡¨
        )

        if result.get("success"):
            logger.info("âœ… é‡å¤åŠ è½½æˆåŠŸï¼ˆåº”è¯¥ä½¿ç”¨äº†ç¼“å­˜ï¼‰")

            # æ£€æŸ¥ç¼“å­˜ç»Ÿè®¡æ˜¯å¦æœªå˜åŒ–
            cache_stats_repeat = tool.get_cache_stats()
            if cache_stats_repeat['loaded_tables'] == cache_stats_after['loaded_tables']:
                logger.info("âœ… éªŒè¯é€šè¿‡ï¼šç¼“å­˜ç”Ÿæ•ˆï¼Œæœªé‡å¤åŠ è½½")
            else:
                logger.warning("âš ï¸ éªŒè¯å¤±è´¥ï¼šå‘ç”Ÿäº†é‡å¤åŠ è½½")

        return True

    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}", exc_info=True)
        return False


async def test_schema_context_retriever_lazy_loading():
    """æµ‹è¯• SchemaContextRetriever çš„æ‡’åŠ è½½åŠŸèƒ½"""
    logger.info("\n" + "="*80)
    logger.info("æµ‹è¯• SchemaContextRetriever æ‡’åŠ è½½åŠŸèƒ½")
    logger.info("="*80)

    try:
        # 1. è·å–æµ‹è¯•æ•°æ®æºé…ç½®
        data_source_id, connection_config = get_test_data_source()
        if not connection_config:
            logger.error("âŒ æ— æ³•è·å–æ•°æ®æºé…ç½®ï¼Œè·³è¿‡æµ‹è¯•")
            return False

        # 2. åˆ›å»ºå®¹å™¨
        container = Container()

        # 3. åˆ›å»º SchemaContextRetrieverï¼ˆå¯ç”¨æ‡’åŠ è½½ï¼‰
        retriever = create_schema_context_retriever(
            data_source_id=data_source_id,
            connection_config=connection_config,
            container=container,
            top_k=5,
            enable_stage_aware=True,
            enable_lazy_loading=True
        )
        logger.info(f"âœ… åˆ›å»º SchemaContextRetrieverï¼Œæ‡’åŠ è½½æ¨¡å¼: {retriever.enable_lazy_loading}")

        # 4. åˆå§‹åŒ–ï¼ˆåªåŠ è½½è¡¨åï¼‰
        logger.info("\nğŸ“‹ æµ‹è¯•1: åˆå§‹åŒ–ï¼ˆåªåŠ è½½è¡¨åï¼‰")
        await retriever.initialize()

        # è·å–åˆå§‹åŒ–åçš„ç¼“å­˜ç»Ÿè®¡
        # æ³¨æ„ï¼šcreate_schema_context_retriever è¿”å›çš„æ˜¯ SchemaContextRetriever å®ä¾‹
        # è€Œ SchemaContextRetriever æœ‰ get_cache_stats æ–¹æ³•
        cache_stats = retriever.get_cache_stats()
        logger.info(f"\nğŸ“Š åˆå§‹åŒ–åçš„ç¼“å­˜ç»Ÿè®¡:")
        logger.info(f"   æ‡’åŠ è½½å¯ç”¨: {cache_stats['lazy_loading_enabled']}")
        logger.info(f"   æ€»è¡¨æ•°: {cache_stats['total_tables']}")
        logger.info(f"   å·²åŠ è½½è¡¨æ•°: {cache_stats['loaded_tables']}")
        logger.info(f"   ç¼“å­˜å¤§å°: {cache_stats['cache_size']}")

        if cache_stats['loaded_tables'] == 0 and cache_stats['total_tables'] > 0:
            logger.info("âœ… éªŒè¯é€šè¿‡ï¼šåˆå§‹åŒ–åªè·å–äº†è¡¨åï¼ŒæœªåŠ è½½åˆ—ä¿¡æ¯")
        else:
            logger.warning(f"âš ï¸ éªŒè¯å¤±è´¥ï¼šå·²åŠ è½½ {cache_stats['loaded_tables']} ä¸ªè¡¨çš„åˆ—ä¿¡æ¯")

        # 5. æµ‹è¯•ï¼šæ£€ç´¢ç›¸å…³è¡¨ï¼ˆè§¦å‘æŒ‰éœ€åŠ è½½ï¼‰
        logger.info("\nğŸ“‹ æµ‹è¯•2: æ£€ç´¢ç›¸å…³è¡¨ï¼ˆè§¦å‘æŒ‰éœ€åŠ è½½ï¼‰")
        query = "ç»Ÿè®¡é€€è´§ç”³è¯·çš„æ€»æ•°"
        documents = await retriever.retrieve(query=query, top_k=3)

        logger.info(f"âœ… æ£€ç´¢åˆ° {len(documents)} ä¸ªç›¸å…³è¡¨")
        for doc in documents:
            table_name = doc.metadata.get('table_name')
            relevance_score = doc.metadata.get('relevance_score', 0)
            retrieval_method = doc.metadata.get('retrieval_method', 'unknown')
            logger.info(f"   - {table_name} (ç›¸å…³æ€§: {relevance_score:.2f}, æ–¹æ³•: {retrieval_method})")

        # è·å–æ£€ç´¢åçš„ç¼“å­˜ç»Ÿè®¡
        cache_stats_after = retriever.get_cache_stats()
        logger.info(f"\nğŸ“Š æ£€ç´¢åçš„ç¼“å­˜ç»Ÿè®¡:")
        logger.info(f"   æ€»è¡¨æ•°: {cache_stats_after['total_tables']}")
        logger.info(f"   å·²åŠ è½½è¡¨æ•°: {cache_stats_after['loaded_tables']}")
        logger.info(f"   ç¼“å­˜å¤§å°: {cache_stats_after['cache_size']}")

        if cache_stats_after['loaded_tables'] > 0:
            logger.info(f"âœ… éªŒè¯é€šè¿‡ï¼šæŒ‰éœ€åŠ è½½äº† {cache_stats_after['loaded_tables']} ä¸ªè¡¨çš„åˆ—ä¿¡æ¯")
        else:
            logger.warning("âš ï¸ éªŒè¯å¤±è´¥ï¼šæœªæŒ‰éœ€åŠ è½½è¡¨ä¿¡æ¯")

        # 6. æµ‹è¯•ï¼šé‡å¤æ£€ç´¢ï¼ˆåº”è¯¥ä½¿ç”¨ç¼“å­˜ï¼‰
        logger.info("\nğŸ“‹ æµ‹è¯•3: é‡å¤æ£€ç´¢ç›¸åŒæŸ¥è¯¢ï¼ˆæµ‹è¯•ç¼“å­˜ï¼‰")
        documents_repeat = await retriever.retrieve(query=query, top_k=3)

        logger.info(f"âœ… é‡å¤æ£€ç´¢åˆ° {len(documents_repeat)} ä¸ªè¡¨")

        # æ£€æŸ¥ç¼“å­˜ç»Ÿè®¡æ˜¯å¦æœªå˜åŒ–
        cache_stats_repeat = retriever.get_cache_stats()
        if cache_stats_repeat['loaded_tables'] == cache_stats_after['loaded_tables']:
            logger.info("âœ… éªŒè¯é€šè¿‡ï¼šç¼“å­˜ç”Ÿæ•ˆï¼Œæœªé‡å¤åŠ è½½")
        else:
            logger.warning("âš ï¸ éªŒè¯å¤±è´¥ï¼šå‘ç”Ÿäº†é‡å¤åŠ è½½")

        # 7. æµ‹è¯•ï¼šä¸åŒæŸ¥è¯¢ï¼ˆå¯èƒ½åŠ è½½æ–°è¡¨ï¼‰
        logger.info("\nğŸ“‹ æµ‹è¯•4: ä¸åŒæŸ¥è¯¢ï¼ˆå¯èƒ½åŠ è½½æ–°è¡¨ï¼‰")
        new_query = "åˆ†æç”¨æˆ·çš„è®¢å•æ•°æ®"
        documents_new = await retriever.retrieve(query=new_query, top_k=3)

        logger.info(f"âœ… æ£€ç´¢åˆ° {len(documents_new)} ä¸ªè¡¨")
        for doc in documents_new:
            table_name = doc.metadata.get('table_name')
            logger.info(f"   - {table_name}")

        # æœ€ç»ˆç¼“å­˜ç»Ÿè®¡
        cache_stats_final = retriever.get_cache_stats()
        logger.info(f"\nğŸ“Š æœ€ç»ˆç¼“å­˜ç»Ÿè®¡:")
        logger.info(f"   æ€»è¡¨æ•°: {cache_stats_final['total_tables']}")
        logger.info(f"   å·²åŠ è½½è¡¨æ•°: {cache_stats_final['loaded_tables']}")
        logger.info(f"   æ™ºèƒ½æ£€ç´¢å¯ç”¨: {cache_stats_final['intelligent_retrieval_enabled']}")

        return True

    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}", exc_info=True)
        return False


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("\n" + "ğŸš€"*40)
    logger.info("å¼€å§‹æµ‹è¯•æ‡’åŠ è½½ä¼˜åŒ–åŠŸèƒ½")
    logger.info("ğŸš€"*40 + "\n")

    results = []

    # æµ‹è¯• SchemaDiscoveryTool
    logger.info("\n" + "="*80)
    logger.info("ç¬¬ä¸€éƒ¨åˆ†ï¼šæµ‹è¯• SchemaDiscoveryTool")
    logger.info("="*80)
    result1 = await test_schema_discovery_tool_lazy_loading()
    results.append(("SchemaDiscoveryTool", result1))

    # æµ‹è¯• SchemaContextRetriever
    logger.info("\n" + "="*80)
    logger.info("ç¬¬äºŒéƒ¨åˆ†ï¼šæµ‹è¯• SchemaContextRetriever")
    logger.info("="*80)
    result2 = await test_schema_context_retriever_lazy_loading()
    results.append(("SchemaContextRetriever", result2))

    # æ€»ç»“
    logger.info("\n" + "="*80)
    logger.info("æµ‹è¯•æ€»ç»“")
    logger.info("="*80)

    for name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        logger.info(f"{name}: {status}")

    all_passed = all(result for _, result in results)
    if all_passed:
        logger.info("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ‡’åŠ è½½ä¼˜åŒ–åŠŸèƒ½æ­£å¸¸å·¥ä½œã€‚")
    else:
        logger.info("\nâš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—ã€‚")

    return all_passed


if __name__ == "__main__":
    try:
        result = asyncio.run(main())
        sys.exit(0 if result else 1)
    except KeyboardInterrupt:
        logger.info("\nâš ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        logger.error(f"\nâŒ æµ‹è¯•å¼‚å¸¸: {e}", exc_info=True)
        sys.exit(1)
