"""
å®Œæ•´çš„AutoReportAI AgentçœŸå®ç¯å¢ƒæµ‹è¯•

ä½¿ç”¨çœŸå®çš„:
- venvç¯å¢ƒ
- Docker PostgreSQLæ•°æ®åº“
- çœŸå®çš„LLMæœåŠ¡å™¨é…ç½® (xiaoai)
- çœŸå®çš„Dorisæ•°æ®æº
- çœŸå®çš„ç”¨æˆ·å’Œæ¨¡æ¿æ•°æ®

å®Œæ•´æµ‹è¯•Agentçš„ä¸‰å¤§æ ¸å¿ƒåŠŸèƒ½å’Œäº”å¤§æµç¨‹
"""

import asyncio
import sys
import os
import logging
from datetime import datetime
import json
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from app.core.config import settings
from app.db.session import get_db
from app.models.data_source import DataSource
from app.models.template import Template
from app.models.llm_server import LLMServer
from app.models.user import User
from app.services.infrastructure.ai.agents.autoreport_ai_agent import (
    create_autoreport_ai_agent,
    AgentRequest,
    WorkflowType
)


class CompleteRealAgentTester:
    """å®Œæ•´çœŸå®Agentæµ‹è¯•å™¨"""
    
    def __init__(self):
        self.test_results = {}
        
    async def run_complete_test(self):
        """è¿è¡Œå®Œæ•´çš„çœŸå®ç¯å¢ƒæµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹å®Œæ•´çš„AutoReportAI AgentçœŸå®ç¯å¢ƒæµ‹è¯•")
        
        try:
            # 1. è·å–çœŸå®é…ç½®æ•°æ®
            real_config = await self.get_real_database_config()
            
            if not real_config:
                logger.error("æ— æ³•è·å–çœŸå®æ•°æ®åº“é…ç½®ï¼Œåœæ­¢æµ‹è¯•")
                return
                
            # 2. ä½¿ç”¨çœŸå®ç”¨æˆ·åˆ›å»ºAgent
            user_id = real_config["user"]["id"]
            agent = create_autoreport_ai_agent(user_id)
            
            # 3. æµ‹è¯•æ ¸å¿ƒåŠŸèƒ½1ï¼šå ä½ç¬¦â†’SQLè½¬æ¢
            await self.test_real_placeholder_to_sql(agent, real_config)
            
            # 4. æµ‹è¯•æ ¸å¿ƒåŠŸèƒ½2ï¼šä»»åŠ¡è¡¥å……æœºåˆ¶
            await self.test_real_task_supplement(agent, real_config)
            
            # 5. æµ‹è¯•æ ¸å¿ƒåŠŸèƒ½3ï¼šå¤šä¸Šä¸‹æ–‡é›†æˆ
            await self.test_real_context_integration(agent, real_config)
            
            # 6. æµ‹è¯•å®Œæ•´æµæ°´çº¿
            await self.test_complete_pipeline(agent, real_config)
            
            # 7. ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
            self.generate_complete_test_report()
            
        except Exception as e:
            logger.error(f"å®Œæ•´æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
        logger.info("âœ… å®Œæ•´çš„AutoReportAI AgentçœŸå®ç¯å¢ƒæµ‹è¯•å®Œæˆ")
    
    async def get_real_database_config(self):
        """è·å–çœŸå®æ•°æ®åº“é…ç½®"""
        try:
            db = next(get_db())
            
            # è·å–ç”¨æˆ·
            user = db.query(User).first()
            if not user:
                logger.error("æ•°æ®åº“ä¸­æ²¡æœ‰ç”¨æˆ·æ•°æ®")
                return None
                
            # è·å–LLMæœåŠ¡å™¨
            llm_server = db.query(LLMServer).filter(
                LLMServer.user_id == user.id,
                LLMServer.is_active == True
            ).first()
            
            if not llm_server:
                logger.error("æ²¡æœ‰æ‰¾åˆ°æ¿€æ´»çš„LLMæœåŠ¡å™¨")
                return None
            
            # è·å–æ•°æ®æº
            data_sources = db.query(DataSource).filter(
                DataSource.is_active == True
            ).all()
            
            if not data_sources:
                logger.error("æ²¡æœ‰æ‰¾åˆ°æ¿€æ´»çš„æ•°æ®æº")
                return None
            
            # è·å–æ¨¡æ¿
            templates = db.query(Template).filter(
                Template.is_active == True
            ).limit(2).all()
            
            config = {
                "user": {
                    "id": str(user.id),
                    "username": user.username,
                    "email": user.email
                },
                "llm_server": {
                    "id": str(llm_server.id),
                    "name": llm_server.name,
                    "provider_type": llm_server.provider_type,
                    "base_url": llm_server.base_url,
                    "has_api_key": bool(getattr(llm_server, 'api_key', None))
                },
                "data_sources": [
                    {
                        "id": str(ds.id),
                        "name": ds.name,
                        "source_type": ds.source_type.value if hasattr(ds.source_type, 'value') else str(ds.source_type),
                        "config": ds.config if hasattr(ds, 'config') else {}
                    } for ds in data_sources
                ],
                "templates": [
                    {
                        "id": str(t.id),
                        "name": t.name,
                        "description": t.description,
                        "template_type": t.template_type.value if hasattr(t.template_type, 'value') else str(t.template_type)
                    } for t in templates
                ]
            }
            
            db.close()
            
            logger.info(f"âœ… è·å–çœŸå®é…ç½®æˆåŠŸ:")
            logger.info(f"   ç”¨æˆ·: {config['user']['username']}")
            logger.info(f"   LLMæœåŠ¡å™¨: {config['llm_server']['name']} ({config['llm_server']['provider_type']})")
            logger.info(f"   æ•°æ®æºæ•°é‡: {len(config['data_sources'])}")
            logger.info(f"   æ¨¡æ¿æ•°é‡: {len(config['templates'])}")
            
            return config
            
        except Exception as e:
            logger.error(f"è·å–çœŸå®é…ç½®å¤±è´¥: {e}")
            return None
    
    async def test_real_placeholder_to_sql(self, agent, real_config):
        """æµ‹è¯•çœŸå®çš„å ä½ç¬¦â†’SQLè½¬æ¢"""
        logger.info("ğŸ”§ æµ‹è¯•æ ¸å¿ƒåŠŸèƒ½1ï¼šå ä½ç¬¦â†’SQLè½¬æ¢")
        
        try:
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ•°æ®æº
            data_source = real_config["data_sources"][0]
            
            # æ„å»ºçœŸå®çš„å ä½ç¬¦è½¬æ¢è¯·æ±‚
            request = AgentRequest(
                request_id="real_sql_conversion",
                workflow_type=WorkflowType.PLACEHOLDER_TO_SQL,
                parameters={
                    "placeholder_name": "user_total_count",
                    "placeholder_description": f"ç³»ç»Ÿä¸­{data_source['name']}æ•°æ®æºçš„ç”¨æˆ·æ€»æ•°ç»Ÿè®¡",
                    "placeholder_type": "metric",
                    "expected_data_type": "number",
                    "task_context": {
                        "task_id": "real_analysis_001",
                        "task_name": "ç”¨æˆ·æ•°æ®åˆ†æ",
                        "task_description": f"åŸºäº{data_source['name']}è¿›è¡Œç”¨æˆ·æ•°æ®ç»Ÿè®¡åˆ†æ",
                        "business_domain": "analytics",
                        "report_type": "dashboard",
                        "priority": "high"
                    },
                    "data_source_context": {
                        "source_id": data_source["id"],
                        "source_type": data_source["source_type"],
                        "database_name": "default_db",
                        "available_tables": ["users", "user_behavior", "user_profiles"],
                        "table_schemas": {
                            "users": [
                                {"name": "user_id", "type": "bigint", "comment": "ç”¨æˆ·ID"},
                                {"name": "username", "type": "varchar", "comment": "ç”¨æˆ·å"},
                                {"name": "created_at", "type": "datetime", "comment": "åˆ›å»ºæ—¶é—´"}
                            ]
                        },
                        "connection_info": {
                            "host": "localhost",
                            "port": 9030,
                            "database": "default_db"
                        }
                    }
                }
            )
            
            response = await agent.execute_request(request)
            
            self.test_results["real_placeholder_to_sql"] = {
                "success": response.status.value == "completed",
                "status": response.status.value,
                "confidence_score": response.confidence_score,
                "execution_time": response.execution_time_seconds,
                "has_sql_query": bool(response.result and response.result.get("sql_query")),
                "sql_preview": response.result.get("sql_query", "")[:200] + "..." if response.result and response.result.get("sql_query") else "",
                "validation_errors_count": len(response.result.get("validation_errors", [])) if response.result else 0
            }
            
            if response.status.value == "completed":
                logger.info(f"âœ… å ä½ç¬¦â†’SQLè½¬æ¢æˆåŠŸ")
                logger.info(f"   ä¿¡å¿ƒåº¦: {response.confidence_score:.2f}")
                if response.result and response.result.get("sql_query"):
                    logger.info(f"   ç”ŸæˆSQL: {response.result['sql_query'][:100]}...")
            else:
                logger.warning(f"âš ï¸ å ä½ç¬¦â†’SQLè½¬æ¢éƒ¨åˆ†æˆåŠŸ: {response.error_message}")
                
        except Exception as e:
            logger.error(f"âŒ å ä½ç¬¦â†’SQLè½¬æ¢æµ‹è¯•å¤±è´¥: {e}")
            self.test_results["real_placeholder_to_sql"] = {
                "success": False,
                "error": str(e)
            }
    
    async def test_real_task_supplement(self, agent, real_config):
        """æµ‹è¯•çœŸå®çš„ä»»åŠ¡è¡¥å……æœºåˆ¶"""
        logger.info("ğŸ”„ æµ‹è¯•æ ¸å¿ƒåŠŸèƒ½2ï¼šä»»åŠ¡è¡¥å……æœºåˆ¶")
        
        try:
            # ä½¿ç”¨çœŸå®æ¨¡æ¿å’Œæ•°æ®æº
            template = real_config["templates"][0] if real_config["templates"] else None
            data_source = real_config["data_sources"][0]
            
            if not template:
                logger.warning("æ²¡æœ‰å¯ç”¨æ¨¡æ¿ï¼Œè·³è¿‡ä»»åŠ¡è¡¥å……æµ‹è¯•")
                self.test_results["real_task_supplement"] = {"success": False, "error": "no_template"}
                return
            
            # æ¨¡æ‹Ÿéœ€è¦è¡¥å……çš„å ä½ç¬¦
            placeholders = [
                {
                    "placeholder_name": f"{template['name']}_key_metric",
                    "placeholder_description": f"æ¨¡æ¿{template['name']}çš„å…³é”®æŒ‡æ ‡",
                    "placeholder_type": "metric",
                    "expected_data_type": "number",
                    "current_value": None,
                    "is_empty": True
                }
            ]
            
            request = AgentRequest(
                request_id="real_task_supplement",
                workflow_type=WorkflowType.TASK_SUPPLEMENT,
                parameters={
                    "template_id": template["id"],
                    "placeholders": placeholders,
                    "task_context": {
                        "task_id": f"supplement_{template['id']}",
                        "task_name": f"è¡¥å……{template['name']}æ¨¡æ¿",
                        "task_description": template["description"],
                        "business_domain": "template_management",
                        "report_type": template["template_type"]
                    },
                    "data_source_context": {
                        "source_id": data_source["id"],
                        "source_type": data_source["source_type"],
                        "database_name": "default_db",
                        "available_tables": ["templates", "users", "data_sources"],
                        "table_schemas": {
                            "templates": [
                                {"name": "id", "type": "uuid", "comment": "æ¨¡æ¿ID"},
                                {"name": "name", "type": "varchar", "comment": "æ¨¡æ¿åç§°"}
                            ]
                        },
                        "connection_info": {"host": "localhost", "port": 9030}
                    }
                }
            )
            
            response = await agent.execute_request(request)
            
            self.test_results["real_task_supplement"] = {
                "success": response.status.value == "completed",
                "status": response.status.value,
                "execution_time": response.execution_time_seconds,
                "template_id": template["id"],
                "template_name": template["name"],
                "processed_placeholders": len(placeholders),
                "result_summary": response.result if response.result else {}
            }
            
            if response.status.value == "completed":
                logger.info(f"âœ… ä»»åŠ¡è¡¥å……æœºåˆ¶æˆåŠŸ")
                logger.info(f"   å¤„ç†æ¨¡æ¿: {template['name']}")
                logger.info(f"   å¤„ç†å ä½ç¬¦æ•°: {len(placeholders)}")
            else:
                logger.warning(f"âš ï¸ ä»»åŠ¡è¡¥å……æœºåˆ¶éƒ¨åˆ†æˆåŠŸ: {response.error_message}")
                
        except Exception as e:
            logger.error(f"âŒ ä»»åŠ¡è¡¥å……æœºåˆ¶æµ‹è¯•å¤±è´¥: {e}")
            self.test_results["real_task_supplement"] = {
                "success": False,
                "error": str(e)
            }
    
    async def test_real_context_integration(self, agent, real_config):
        """æµ‹è¯•çœŸå®çš„å¤šä¸Šä¸‹æ–‡é›†æˆ"""
        logger.info("ğŸ§  æµ‹è¯•æ ¸å¿ƒåŠŸèƒ½3ï¼šå¤šä¸Šä¸‹æ–‡é›†æˆ")
        
        try:
            contexts = [
                {
                    "context_id": "real_data_source",
                    "context_type": "data_source",
                    "priority": "critical",
                    "content": {
                        "data_sources": real_config["data_sources"],
                        "total_sources": len(real_config["data_sources"]),
                        "primary_source": real_config["data_sources"][0]["name"]
                    }
                },
                {
                    "context_id": "real_user",
                    "context_type": "user",
                    "priority": "high",
                    "content": {
                        "user_id": real_config["user"]["id"],
                        "username": real_config["user"]["username"],
                        "has_llm_config": real_config["llm_server"]["has_api_key"]
                    }
                },
                {
                    "context_id": "real_templates",
                    "context_type": "business",
                    "priority": "medium",
                    "content": {
                        "templates": real_config["templates"],
                        "template_count": len(real_config["templates"])
                    }
                }
            ]
            
            request = AgentRequest(
                request_id="real_context_integration",
                workflow_type=WorkflowType.MULTI_CONTEXT_INTEGRATION,
                parameters={
                    "contexts": contexts,
                    "target_task": f"ä¸ºç”¨æˆ·{real_config['user']['username']}è¿›è¡Œæ•°æ®åˆ†æä»»åŠ¡çš„ä¸Šä¸‹æ–‡é›†æˆ",
                    "required_context_types": ["data_source", "user"],
                    "custom_weights": {
                        "data_source": 0.4,
                        "user": 0.3,
                        "business": 0.3
                    }
                }
            )
            
            response = await agent.execute_request(request)
            
            self.test_results["real_context_integration"] = {
                "success": response.status.value == "completed",
                "status": response.status.value,
                "execution_time": response.execution_time_seconds,
                "contexts_processed": len(contexts),
                "integration_confidence": response.result.get("integration_confidence", 0.0) if response.result else 0.0,
                "used_contexts": response.result.get("used_contexts", []) if response.result else []
            }
            
            if response.status.value == "completed":
                logger.info(f"âœ… å¤šä¸Šä¸‹æ–‡é›†æˆæˆåŠŸ")
                logger.info(f"   é›†æˆä¿¡å¿ƒåº¦: {self.test_results['real_context_integration']['integration_confidence']:.2f}")
                logger.info(f"   ä½¿ç”¨çš„ä¸Šä¸‹æ–‡: {len(self.test_results['real_context_integration']['used_contexts'])}")
            else:
                logger.warning(f"âš ï¸ å¤šä¸Šä¸‹æ–‡é›†æˆéƒ¨åˆ†æˆåŠŸ: {response.error_message}")
                
        except Exception as e:
            logger.error(f"âŒ å¤šä¸Šä¸‹æ–‡é›†æˆæµ‹è¯•å¤±è´¥: {e}")
            self.test_results["real_context_integration"] = {
                "success": False,
                "error": str(e)
            }
    
    async def test_complete_pipeline(self, agent, real_config):
        """æµ‹è¯•å®Œæ•´æµæ°´çº¿"""
        logger.info("ğŸ­ æµ‹è¯•å®Œæ•´æµæ°´çº¿é›†æˆ")
        
        try:
            # æ„å»ºå®Œæ•´çš„æµæ°´çº¿é…ç½®
            pipeline_config = {
                "enable_context_integration": True,
                "enable_task_supplement": True,
                "enable_placeholder_sql": True,
                "enable_sql_testing": True,
                "enable_chart_generation": False,  # å›¾è¡¨ç”Ÿæˆå¯èƒ½éœ€è¦é¢å¤–ä¾èµ–
                
                # ä¸Šä¸‹æ–‡é›†æˆå‚æ•°
                "context_integration_params": {
                    "contexts": [
                        {
                            "context_id": "pipeline_user",
                            "context_type": "user",
                            "priority": "critical",
                            "content": real_config["user"]
                        },
                        {
                            "context_id": "pipeline_data_source",
                            "context_type": "data_source",
                            "priority": "critical",
                            "content": real_config["data_sources"][0]
                        }
                    ],
                    "target_task": f"å®Œæ•´æµæ°´çº¿æµ‹è¯• - ç”¨æˆ·{real_config['user']['username']}çš„æ•°æ®åˆ†æ",
                    "required_context_types": ["user", "data_source"]
                },
                
                # å ä½ç¬¦SQLå‚æ•°
                "placeholder_sql_params": {
                    "placeholder_name": "pipeline_test_metric",
                    "placeholder_description": f"æµæ°´çº¿æµ‹è¯•ï¼šåŸºäº{real_config['data_sources'][0]['name']}çš„æ ¸å¿ƒæŒ‡æ ‡",
                    "placeholder_type": "metric",
                    "expected_data_type": "number",
                    "task_context": {
                        "task_id": "pipeline_test",
                        "task_name": "å®Œæ•´æµæ°´çº¿æµ‹è¯•",
                        "task_description": "æµ‹è¯•å®Œæ•´çš„Agentæµæ°´çº¿åŠŸèƒ½",
                        "business_domain": "system_test",
                        "report_type": "dashboard"
                    },
                    "data_source_context": {
                        "source_id": real_config["data_sources"][0]["id"],
                        "source_type": real_config["data_sources"][0]["source_type"],
                        "database_name": "test_db",
                        "available_tables": ["users", "metrics", "analytics"],
                        "table_schemas": {
                            "users": [{"name": "id", "type": "bigint", "comment": "ID"}],
                            "metrics": [{"name": "value", "type": "decimal", "comment": "æŒ‡æ ‡å€¼"}]
                        },
                        "connection_info": {"host": "localhost", "port": 9030}
                    }
                },
                
                # SQLæµ‹è¯•å‚æ•°
                "sql_testing_params": {
                    "data_source_context": {
                        "source_id": real_config["data_sources"][0]["id"],
                        "source_type": real_config["data_sources"][0]["source_type"],
                        "available_tables": ["users", "metrics"],
                        "table_schemas": {}
                    },
                    "validation_level": "standard"
                }
            }
            
            request = AgentRequest(
                request_id="complete_pipeline_test",
                workflow_type=WorkflowType.FULL_PIPELINE,
                parameters=pipeline_config
            )
            
            response = await agent.execute_request(request)
            
            pipeline_results = response.result.get("pipeline_results", {}) if response.result else {}
            
            self.test_results["complete_pipeline"] = {
                "success": response.status.value == "completed",
                "status": response.status.value,
                "execution_time": response.execution_time_seconds,
                "steps_completed": response.result.get("steps_completed", 0) if response.result else 0,
                "overall_confidence": response.result.get("overall_confidence", 0.0) if response.result else 0.0,
                "pipeline_steps": list(pipeline_results.keys()),
                "context_integration_success": "context_integration" in pipeline_results,
                "placeholder_sql_success": "placeholder_to_sql" in pipeline_results,
                "sql_testing_success": "sql_testing" in pipeline_results
            }
            
            if response.status.value == "completed":
                logger.info(f"âœ… å®Œæ•´æµæ°´çº¿æµ‹è¯•æˆåŠŸ")
                logger.info(f"   å®Œæˆæ­¥éª¤æ•°: {self.test_results['complete_pipeline']['steps_completed']}")
                logger.info(f"   æ€»ä½“ä¿¡å¿ƒåº¦: {self.test_results['complete_pipeline']['overall_confidence']:.2f}")
                logger.info(f"   æ‰§è¡Œçš„æ­¥éª¤: {', '.join(self.test_results['complete_pipeline']['pipeline_steps'])}")
            else:
                logger.warning(f"âš ï¸ å®Œæ•´æµæ°´çº¿éƒ¨åˆ†æˆåŠŸ: {response.error_message}")
                
        except Exception as e:
            logger.error(f"âŒ å®Œæ•´æµæ°´çº¿æµ‹è¯•å¤±è´¥: {e}")
            self.test_results["complete_pipeline"] = {
                "success": False,
                "error": str(e)
            }
    
    def generate_complete_test_report(self):
        """ç”Ÿæˆå®Œæ•´æµ‹è¯•æŠ¥å‘Š"""
        logger.info("ğŸ“Š === å®Œæ•´çœŸå®ç¯å¢ƒæµ‹è¯•æŠ¥å‘Š ===")
        
        total_tests = len(self.test_results)
        successful_tests = sum(1 for result in self.test_results.values() if result.get("success", False))
        
        logger.info(f"ğŸ“ˆ æ€»ä½“ç»“æœ:")
        logger.info(f"   æ€»æµ‹è¯•æ•°: {total_tests}")
        logger.info(f"   æˆåŠŸæµ‹è¯•æ•°: {successful_tests}")
        logger.info(f"   æˆåŠŸç‡: {successful_tests/total_tests:.2%}")
        logger.info("")
        
        logger.info(f"ğŸ” è¯¦ç»†ç»“æœ:")
        test_emojis = {
            "real_placeholder_to_sql": "ğŸ”§",
            "real_task_supplement": "ğŸ”„", 
            "real_context_integration": "ğŸ§ ",
            "complete_pipeline": "ğŸ­"
        }
        
        for test_name, result in self.test_results.items():
            emoji = test_emojis.get(test_name, "ğŸ§ª")
            status = "âœ… PASS" if result.get("success", False) else "âŒ FAIL"
            logger.info(f"   {emoji} {test_name}: {status}")
            
            # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
            if result.get("success", False):
                if "confidence_score" in result:
                    logger.info(f"      ä¿¡å¿ƒåº¦: {result['confidence_score']:.2f}")
                if "execution_time" in result:
                    logger.info(f"      æ‰§è¡Œæ—¶é—´: {result['execution_time']:.3f}s")
                if "steps_completed" in result:
                    logger.info(f"      å®Œæˆæ­¥éª¤: {result['steps_completed']}")
            else:
                if result.get("error"):
                    logger.info(f"      é”™è¯¯: {result['error']}")
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_file = f"complete_real_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump({
                "test_summary": {
                    "total_tests": total_tests,
                    "successful_tests": successful_tests,
                    "success_rate": successful_tests/total_tests,
                    "test_time": datetime.now().isoformat(),
                    "environment": "complete_real_venv_docker_llm"
                },
                "detailed_results": self.test_results
            }, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“„ è¯¦ç»†æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    try:
        tester = CompleteRealAgentTester()
        await tester.run_complete_test()
    except Exception as e:
        logger.error(f"ä¸»æµ‹è¯•å‡½æ•°å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # è®¾ç½®æ•°æ®åº“è¿æ¥
    os.environ["DATABASE_URL"] = "postgresql://postgres:postgres123@localhost:5432/autoreport"
    
    # è¿è¡Œå®Œæ•´æµ‹è¯•
    asyncio.run(main())