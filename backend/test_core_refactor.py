"""
æ ¸å¿ƒé‡æ„æµ‹è¯• - ç®€åŒ–ç‰ˆæœ¬
ä¸“æ³¨æµ‹è¯•é‡æ„åçš„æ ¸å¿ƒç»„ä»¶ï¼Œé¿å…å¤æ‚ä¾èµ–
"""

import asyncio
import logging
import sys
import os
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__)))

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


async def test_api_messages():
    """æµ‹è¯•APIæ¶ˆæ¯ç³»ç»Ÿ"""
    logger.info("ğŸ§ª æµ‹è¯•APIæ¶ˆæ¯ç³»ç»Ÿ")
    
    try:
        # ç›´æ¥å¯¼å…¥æ ¸å¿ƒæ¶ˆæ¯æ¨¡å—
        from app.services.infrastructure.ai.core.api_messages import APIMessage, MessageConverter
        from app.services.infrastructure.ai.core.messages import AgentMessage, MessageType
        
        # åˆ›å»ºæµ‹è¯•æ¶ˆæ¯
        agent_msg = AgentMessage.create_progress(
            current_step="æµ‹è¯•è¿›åº¦",
            user_id="test_user", 
            task_id="test_task",
            percentage=75.0
        )
        
        # è½¬æ¢ä¸ºAPIæ¶ˆæ¯
        api_msg = agent_msg.to_api_message()
        
        # éªŒè¯è½¬æ¢
        assert api_msg.role == "assistant"
        assert "æµ‹è¯•è¿›åº¦" in api_msg.content
        
        # æµ‹è¯•æ¶ˆæ¯è½¬æ¢å™¨
        converter = MessageConverter()
        api_messages = converter.agent_messages_to_api_messages([agent_msg])
        assert len(api_messages) == 1
        
        logger.info("âœ… APIæ¶ˆæ¯ç³»ç»Ÿæµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        logger.error(f"âŒ APIæ¶ˆæ¯ç³»ç»Ÿæµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_security_checker():
    """æµ‹è¯•å®‰å…¨æ£€æŸ¥å™¨"""
    logger.info("ğŸ§ª æµ‹è¯•å®‰å…¨æ£€æŸ¥å™¨")
    
    try:
        from app.services.infrastructure.ai.core.security import SecurityChecker, SecurityLevel
        
        checker = SecurityChecker()
        
        # æµ‹è¯•å®‰å…¨æ“ä½œ
        safe_result = await checker.check_tool_execution(
            "template_analysis_tool",
            {"template_id": "safe_template"}
        )
        assert safe_result.allowed == True
        
        # æµ‹è¯•å±é™©æ“ä½œ
        dangerous_result = await checker.check_tool_execution(
            "bash_tool", 
            {"command": "rm -rf / --no-preserve-root"}
        )
        assert dangerous_result.level == SecurityLevel.FORBIDDEN
        assert dangerous_result.allowed == False
        
        # æµ‹è¯•ä¸­ç­‰é£é™©
        medium_result = await checker.check_tool_execution(
            "sql_tool",
            {"sql": "DELETE FROM users"}  # æ— WHEREæ¡ä»¶çš„åˆ é™¤
        )
        assert medium_result.level == SecurityLevel.HIGH_RISK
        
        logger.info("âœ… å®‰å…¨æ£€æŸ¥å™¨æµ‹è¯•é€šè¿‡")
        logger.info(f"å®‰å…¨ç»Ÿè®¡: {checker.get_security_statistics()}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ å®‰å…¨æ£€æŸ¥å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_enhanced_prompts():
    """æµ‹è¯•å¢å¼ºæç¤ºç³»ç»Ÿ"""
    logger.info("ğŸ§ª æµ‹è¯•å¢å¼ºæç¤ºç³»ç»Ÿ")
    
    try:
        from app.services.infrastructure.ai.core.enhanced_prompts import SimplifiedPromptManager
        
        manager = SimplifiedPromptManager()
        
        # æµ‹è¯•ç¼–æ’æç¤º
        prompt = manager.get_orchestration_prompt(
            goal="æµ‹è¯•ä»»åŠ¡ç›®æ ‡",
            available_tools=["tool1", "tool2", "tool3"],
            context={
                "iteration": 0,
                "conversation_history": []
            }
        )
        
        assert len(prompt) > 0
        assert "ä»»åŠ¡ç›®æ ‡" in prompt
        assert "tool1" in prompt
        assert "<task_analysis>" in prompt  # XMLæ ‡ç­¾
        
        # æµ‹è¯•SQLåˆ†ææç¤º
        sql_prompt = manager.get_sql_analysis_prompt(
            placeholder_name="test_placeholder",
            template_context="æµ‹è¯•æ¨¡æ¿ä¸Šä¸‹æ–‡",
            available_tables=["table1", "table2"]
        )
        
        assert len(sql_prompt) > 0
        assert "test_placeholder" in sql_prompt
        assert "table1" in sql_prompt
        
        # æ£€æŸ¥ä½¿ç”¨ç»Ÿè®¡
        stats = manager.get_usage_statistics()
        assert stats["total_prompts"] >= 2
        assert stats["avg_length"] > 0
        
        logger.info("âœ… å¢å¼ºæç¤ºç³»ç»Ÿæµ‹è¯•é€šè¿‡")
        logger.info(f"æç¤ºç»Ÿè®¡: {stats}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ å¢å¼ºæç¤ºç³»ç»Ÿæµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_streaming_json_parser():
    """æµ‹è¯•æµå¼JSONè§£æå™¨"""
    logger.info("ğŸ§ª æµ‹è¯•æµå¼JSONè§£æå™¨")
    
    try:
        from app.services.infrastructure.ai.core.api_messages import StreamingJSONParser
        
        parser = StreamingJSONParser()
        
        # æµ‹è¯•å®Œæ•´JSON
        complete_json = '{"tool": "test_tool", "params": {"key": "value"}}'
        results = parser.process_chunk(complete_json)
        
        assert len(results) == 1
        assert results[0]["tool"] == "test_tool"
        assert results[0]["params"]["key"] == "value"
        
        # æµ‹è¯•åˆ†å—JSON
        parser.reset()
        chunk1 = '{"tool": "test'
        chunk2 = '_tool", "params": {'
        chunk3 = '"key": "value"}}'
        
        results1 = parser.process_chunk(chunk1)
        results2 = parser.process_chunk(chunk2)
        results3 = parser.process_chunk(chunk3)
        
        assert len(results1) == 0  # ä¸å®Œæ•´
        assert len(results2) == 0  # ä¸å®Œæ•´  
        assert len(results3) == 1  # å®Œæ•´
        assert results3[0]["tool"] == "test_tool"
        
        # æµ‹è¯•ä¿®å¤åŠŸèƒ½
        parser.reset()
        broken_json = '{"tool": "test_tool", "params": {"key": "value"'  # ç¼ºå°‘æ‹¬å·
        fixed_result = parser._fix_incomplete_json(broken_json)
        
        if fixed_result:  # å¦‚æœä¿®å¤æˆåŠŸ
            assert "tool" in fixed_result
        
        logger.info("âœ… æµå¼JSONè§£æå™¨æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        logger.error(f"âŒ æµå¼JSONè§£æå™¨æµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_tool_context():
    """æµ‹è¯•å·¥å…·ä¸Šä¸‹æ–‡"""
    logger.info("ğŸ§ª æµ‹è¯•å·¥å…·ä¸Šä¸‹æ–‡")
    
    try:
        from app.services.infrastructure.ai.core.tools import ToolContext
        
        # åˆ›å»ºåŸºç¡€ä¸Šä¸‹æ–‡
        context = ToolContext(
            user_id="test_user",
            task_id="test_task",
            session_id="test_session",
            context_data={"key": "value"}
        )
        
        assert context.user_id == "test_user"
        assert context.task_id == "test_task"
        assert context.context_data["key"] == "value"
        
        # æµ‹è¯•é”™è¯¯è®°å½•
        context.add_error("test_error", "æµ‹è¯•é”™è¯¯ä¿¡æ¯")
        recent_errors = context.get_recent_errors(limit=1)
        assert len(recent_errors) == 1
        assert recent_errors[0]["type"] == "test_error"
        
        # æµ‹è¯•æ´å¯Ÿè®°å½•
        context.add_insight("æµ‹è¯•æ´å¯Ÿ")
        assert "æµ‹è¯•æ´å¯Ÿ" in context.learned_insights
        
        logger.info("âœ… å·¥å…·ä¸Šä¸‹æ–‡æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        logger.error(f"âŒ å·¥å…·ä¸Šä¸‹æ–‡æµ‹è¯•å¤±è´¥: {e}")
        return False


async def run_core_tests():
    """è¿è¡Œæ‰€æœ‰æ ¸å¿ƒæµ‹è¯•"""
    logger.info("ğŸš€ å¼€å§‹æ ¸å¿ƒé‡æ„æµ‹è¯•")
    
    tests = [
        ("APIæ¶ˆæ¯ç³»ç»Ÿ", test_api_messages),
        ("å®‰å…¨æ£€æŸ¥å™¨", test_security_checker), 
        ("å¢å¼ºæç¤ºç³»ç»Ÿ", test_enhanced_prompts),
        ("æµå¼JSONè§£æå™¨", test_streaming_json_parser),
        ("å·¥å…·ä¸Šä¸‹æ–‡", test_tool_context)
    ]
    
    results = {}
    passed = 0
    
    for test_name, test_func in tests:
        logger.info(f"\nğŸ“‹ æ‰§è¡Œæµ‹è¯•: {test_name}")
        start_time = datetime.now()
        
        try:
            success = await test_func()
            duration = (datetime.now() - start_time).total_seconds()
            
            results[test_name] = {
                "success": success,
                "duration": duration
            }
            
            if success:
                passed += 1
                logger.info(f"âœ… {test_name} é€šè¿‡ ({duration:.2f}s)")
            else:
                logger.error(f"âŒ {test_name} å¤±è´¥ ({duration:.2f}s)")
                
        except Exception as e:
            duration = (datetime.now() - start_time).total_seconds() 
            logger.error(f"ğŸ’¥ {test_name} å¼‚å¸¸: {e} ({duration:.2f}s)")
            results[test_name] = {
                "success": False,
                "error": str(e),
                "duration": duration
            }
    
    # è¾“å‡ºæ€»ç»“
    total = len(tests)
    success_rate = passed / total * 100
    
    logger.info("\n" + "="*60)
    logger.info("ğŸ† æ ¸å¿ƒé‡æ„æµ‹è¯•æ€»ç»“")
    logger.info("="*60)
    logger.info(f"æ€»æµ‹è¯•æ•°: {total}")
    logger.info(f"é€šè¿‡æ•°é‡: {passed}")
    logger.info(f"æˆåŠŸç‡: {success_rate:.1f}%")
    
    for test_name, result in results.items():
        status = "âœ… é€šè¿‡" if result["success"] else "âŒ å¤±è´¥"
        duration = result["duration"]
        logger.info(f"{status} {test_name} ({duration:.2f}s)")
    
    logger.info("="*60)
    
    if success_rate >= 80:
        logger.info("ğŸ‰ æ ¸å¿ƒé‡æ„éªŒè¯æˆåŠŸï¼æ–°æ¶æ„ç»„ä»¶è¿è¡Œè‰¯å¥½ã€‚")
    elif success_rate >= 60:
        logger.info("âš ï¸ æ ¸å¿ƒé‡æ„éƒ¨åˆ†æˆåŠŸï¼Œæœ‰å¾…ä¼˜åŒ–ã€‚")
    else:
        logger.info("âŒ æ ¸å¿ƒé‡æ„å­˜åœ¨é—®é¢˜ï¼Œéœ€è¦ä¿®å¤ã€‚")
    
    return results


if __name__ == "__main__":
    try:
        results = asyncio.run(run_core_tests())
    except KeyboardInterrupt:
        logger.info("æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"æµ‹è¯•è¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()