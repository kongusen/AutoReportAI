#!/usr/bin/env python3
"""
æ•°æ®åº“é©±åŠ¨çš„æ™ºèƒ½æ¨¡å‹é€‰æ‹©æµ‹è¯•

æµ‹è¯•ç”¨æˆ·é…ç½®çš„LLMæœåŠ¡å™¨å’Œæ¨¡å‹çš„æ™ºèƒ½é€‰æ‹©åŠŸèƒ½
å±•ç¤ºä¸å‰ç«¯é…ç½®é¡µé¢çš„å®Œæ•´é›†æˆ
"""

import asyncio
import logging
from datetime import datetime
from sqlalchemy.orm import Session
from uuid import uuid4

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


async def setup_test_data(db: Session):
    """è®¾ç½®æµ‹è¯•æ•°æ®"""
    print("ğŸ”§ è®¾ç½®æµ‹è¯•æ•°æ®...")
    
    from app.models.llm_server import LLMServer, LLMModel, ProviderType, ModelType
    from app.models.user_llm_preference import UserLLMPreference
    from app.models.user import User
    
    # åˆ›å»ºæµ‹è¯•ç”¨æˆ·
    test_user = User(
        id=uuid4(),
        username="test_user",
        email="test@example.com",
        hashed_password="hashed_password_here",
        is_active=True
    )
    db.add(test_user)
    
    # åˆ›å»ºæµ‹è¯•LLMæœåŠ¡å™¨
    xiaoai_server = LLMServer(
        name="xiaoai",
        description="å°è‰¾APIæœåŠ¡å™¨",
        base_url="https://xiaoai.com/api/v1/chat/completions",
        provider_type=ProviderType.OPENAI,
        api_key="sk-cFoNGtf6djfyk1mJftn5xSOr6HMvV4jtxmnO9e1nEfnsXM4S",
        is_active=True,
        is_healthy=True,
        last_health_check=datetime.utcnow()
    )
    db.add(xiaoai_server)
    db.flush()  # è·å–ID
    
    # åˆ›å»ºæœ¬åœ°æœåŠ¡å™¨
    local_server = LLMServer(
        name="local_ollama",
        description="æœ¬åœ°OllamaæœåŠ¡å™¨", 
        base_url="http://localhost:11434",
        provider_type=ProviderType.CUSTOM,
        api_key=None,
        is_active=True,
        is_healthy=True,
        last_health_check=datetime.utcnow()
    )
    db.add(local_server)
    db.flush()
    
    # ä¸ºxiaoaiæœåŠ¡å™¨æ·»åŠ æ¨¡å‹
    xiaoai_models = [
        {
            "name": "gpt-4o-mini",
            "display_name": "GPT-4o Mini",
            "model_type": ModelType.CHAT,
            "provider_name": "openai",
            "max_tokens": 4000,
            "supports_function_calls": True,
            "supports_thinking": False
        },
        {
            "name": "gpt-5-chat-latest", 
            "display_name": "GPT-5 Chat Latest",
            "model_type": ModelType.CHAT,
            "provider_name": "openai",
            "max_tokens": 8000,
            "supports_function_calls": True,
            "supports_thinking": False
        },
        {
            "name": "claude-sonnet-4-20250514",
            "display_name": "Claude Sonnet 4",
            "model_type": ModelType.CHAT,
            "provider_name": "anthropic",
            "max_tokens": 200000,
            "supports_function_calls": True,
            "supports_thinking": False
        },
        {
            "name": "claude-sonnet-4-20250514-thinking",
            "display_name": "Claude Sonnet 4 Thinking",
            "model_type": ModelType.THINK,
            "provider_name": "anthropic",
            "max_tokens": 200000,
            "supports_function_calls": True,
            "supports_thinking": True
        }
    ]
    
    for model_data in xiaoai_models:
        model = LLMModel(
            server_id=xiaoai_server.id,
            name=model_data["name"],
            display_name=model_data["display_name"],
            model_type=model_data["model_type"],
            provider_name=model_data["provider_name"],
            is_active=True,
            is_healthy=True,
            max_tokens=model_data["max_tokens"],
            supports_function_calls=model_data["supports_function_calls"],
            supports_thinking=model_data["supports_thinking"],
            last_health_check=datetime.utcnow()
        )
        db.add(model)
    
    # ä¸ºæœ¬åœ°æœåŠ¡å™¨æ·»åŠ æ¨¡å‹
    local_models = [
        {
            "name": "qwen2",
            "display_name": "Qwen2 7B",
            "model_type": ModelType.CHAT,
            "provider_name": "qwen",
            "max_tokens": 32000,
            "supports_function_calls": False,
            "supports_thinking": False
        },
        {
            "name": "llama3",
            "display_name": "Llama 3 8B",
            "model_type": ModelType.CHAT,
            "provider_name": "meta",
            "max_tokens": 8000,
            "supports_function_calls": False,
            "supports_thinking": False
        }
    ]
    
    for model_data in local_models:
        model = LLMModel(
            server_id=local_server.id,
            name=model_data["name"],
            display_name=model_data["display_name"],
            model_type=model_data["model_type"],
            provider_name=model_data["provider_name"],
            is_active=True,
            is_healthy=True,
            max_tokens=model_data["max_tokens"],
            supports_function_calls=model_data["supports_function_calls"],
            supports_thinking=model_data["supports_thinking"],
            last_health_check=datetime.utcnow()
        )
        db.add(model)
    
    # åˆ›å»ºç”¨æˆ·åå¥½è®¾ç½®
    user_preference = UserLLMPreference(
        user_id=test_user.id,
        default_llm_server_id=xiaoai_server.id,
        default_provider_name="anthropic",
        default_model_name="claude-sonnet-4-20250514",
        preferred_temperature=0.7,
        max_tokens_limit=8000,
        daily_token_quota=100000,
        monthly_cost_limit=200.0,
        enable_caching=True,
        enable_learning=True,
        provider_priorities={"anthropic": 1, "openai": 2, "qwen": 3, "meta": 4},
        model_preferences={"reasoning": "claude-sonnet-4-20250514", "coding": "gpt-4o-mini"}
    )
    db.add(user_preference)
    
    db.commit()
    
    print(f"âœ… æµ‹è¯•æ•°æ®åˆ›å»ºå®Œæˆ:")
    print(f"   - ç”¨æˆ·: {test_user.username} ({test_user.id})")
    print(f"   - LLMæœåŠ¡å™¨: {xiaoai_server.name}, {local_server.name}")
    print(f"   - æ¨¡å‹æ€»æ•°: {len(xiaoai_models) + len(local_models)}")
    print(f"   - ç”¨æˆ·åå¥½: å·²é…ç½®")
    
    return test_user.id


async def test_database_selector():
    """æµ‹è¯•æ•°æ®åº“é©±åŠ¨çš„æ™ºèƒ½é€‰æ‹©å™¨"""
    print("\nğŸ§  æµ‹è¯•æ•°æ®åº“é©±åŠ¨çš„æ™ºèƒ½é€‰æ‹©å™¨")
    print("=" * 50)
    
    from app.db.session import SessionLocal
    
    db = SessionLocal()
    
    try:
        # è®¾ç½®æµ‹è¯•æ•°æ®
        user_id = await setup_test_data(db)
        
        # æµ‹è¯•ä¸åŒåœºæ™¯çš„æ¨¡å‹é€‰æ‹©
        from app.services.infrastructure.ai.llm.database_selector import (
            get_database_selector,
            TaskType,
            TaskComplexity,
            TaskCharacteristics,
            SelectionCriteria
        )
        
        selector = get_database_selector()
        
        test_scenarios = [
            {
                "name": "é«˜è´¨é‡æ¨ç†ä»»åŠ¡ï¼ˆç”¨æˆ·åå¥½Claudeï¼‰",
                "task": TaskCharacteristics(
                    task_type=TaskType.REASONING,
                    complexity=TaskComplexity.COMPLEX,
                    estimated_tokens=5000,
                    accuracy_critical=True
                ),
                "criteria": SelectionCriteria(min_capability_score=0.8),
                "agent_id": "reasoning_agent"
            },
            {
                "name": "æˆæœ¬æ•æ„Ÿçš„ç¼–ç¨‹ä»»åŠ¡",
                "task": TaskCharacteristics(
                    task_type=TaskType.CODING,
                    complexity=TaskComplexity.MEDIUM,
                    estimated_tokens=3000,
                    cost_sensitive=True
                ),
                "criteria": SelectionCriteria(max_cost_per_request=0.05),
                "agent_id": "coding_agent"
            },
            {
                "name": "éœ€è¦æ€è€ƒæ¨¡å¼çš„ä¸“å®¶çº§åˆ†æ",
                "task": TaskCharacteristics(
                    task_type=TaskType.ANALYSIS,
                    complexity=TaskComplexity.EXPERT,
                    estimated_tokens=8000,
                    accuracy_critical=True
                ),
                "criteria": SelectionCriteria(preferred_providers=["anthropic"]),
                "agent_id": "analysis_agent"
            },
            {
                "name": "å¿«é€Ÿç®€å•é—®ç­”ï¼ˆæœ¬åœ°ä¼˜å…ˆï¼‰",
                "task": TaskCharacteristics(
                    task_type=TaskType.QA,
                    complexity=TaskComplexity.SIMPLE,
                    estimated_tokens=500,
                    speed_priority=True,
                    cost_sensitive=True
                ),
                "criteria": SelectionCriteria(max_latency_ms=3000, max_cost_per_request=0.001),
                "agent_id": "qa_agent"
            }
        ]
        
        recommendations = []
        
        for i, scenario in enumerate(test_scenarios, 1):
            print(f"\nğŸ“‹ åœºæ™¯ {i}: {scenario['name']}")
            
            try:
                recommendation = await selector.select_best_model_for_user(
                    user_id=str(user_id),
                    task_characteristics=scenario['task'],
                    criteria=scenario['criteria'],
                    agent_id=scenario['agent_id'],
                    db=db
                )
                
                recommendations.append({
                    "scenario": scenario['name'],
                    "recommendation": recommendation,
                    "agent_id": scenario['agent_id']
                })
                
                print(f"   ğŸ¯ æ¨èæ¨¡å‹: {recommendation.provider}:{recommendation.model}")
                print(f"   ğŸ“Š ç½®ä¿¡åº¦: {recommendation.confidence:.1%}")
                print(f"   ğŸ’­ æ¨èç†ç”±: {recommendation.reasoning}")
                print(f"   ğŸ’° é¢„æœŸæˆæœ¬: ${recommendation.expected_cost:.4f}")
                print(f"   âš¡ é¢„æœŸå»¶è¿Ÿ: {recommendation.expected_latency}ms")
                print(f"   ğŸ›ï¸ èƒ½åŠ›åŒ¹é…: {recommendation.capability_match_score:.2f}")
                
                if recommendation.fallback_models:
                    fallbacks = ", ".join([f"{p}:{m}" for m, p in recommendation.fallback_models[:2]])
                    print(f"   ğŸ”„ å¤‡é€‰æ–¹æ¡ˆ: {fallbacks}")
                
            except Exception as e:
                print(f"   âŒ é€‰æ‹©å¤±è´¥: {e}")
        
        # æµ‹è¯•ä½¿ç”¨åé¦ˆè®°å½•
        print(f"\nğŸ“ˆ æµ‹è¯•ä½¿ç”¨åé¦ˆè®°å½•")
        print("-" * 30)
        
        for rec_data in recommendations:
            if not rec_data.get("recommendation"):
                continue
            
            recommendation = rec_data["recommendation"]
            agent_id = rec_data["agent_id"]
            
            # æ¨¡æ‹Ÿä½¿ç”¨åé¦ˆ
            import random
            success = random.choice([True, True, True, False])  # 75% æˆåŠŸç‡
            satisfaction = random.uniform(0.8, 0.95) if success else random.uniform(0.3, 0.6)
            
            selector.record_user_feedback(
                user_id=str(user_id),
                model=recommendation.model,
                provider=recommendation.provider,
                success=success,
                satisfaction_score=satisfaction,
                actual_cost=recommendation.expected_cost * random.uniform(0.9, 1.1),
                actual_latency=int(recommendation.expected_latency * random.uniform(0.8, 1.2)),
                agent_id=agent_id,
                task_type=TaskType.REASONING
            )
            
            print(f"   ğŸ“Š {agent_id}: {recommendation.provider}:{recommendation.model}")
            print(f"      åé¦ˆ - æˆåŠŸ: {success}, æ»¡æ„åº¦: {satisfaction:.2f}")
        
        # å±•ç¤ºAgentåå¥½å­¦ä¹ ç»“æœ
        print(f"\nğŸ‘¤ Agentåå¥½å­¦ä¹ ç»“æœ")
        print("-" * 30)
        
        for agent_id in ["reasoning_agent", "coding_agent", "analysis_agent", "qa_agent"]:
            agent_prefs = selector.agent_preferences.get(agent_id, {})
            if agent_prefs:
                print(f"\nğŸ¤– {agent_id}:")
                for model_key, pref_data in agent_prefs.items():
                    print(f"   ğŸ“ˆ {model_key}: ä½¿ç”¨{pref_data['usage_count']}æ¬¡, æ»¡æ„åº¦{pref_data['avg_satisfaction']:.2f}")
        
        return {
            "status": "success",
            "user_id": str(user_id),
            "scenarios_tested": len(test_scenarios),
            "successful_recommendations": len([r for r in recommendations if r.get("recommendation")]),
            "agent_preferences": len(selector.agent_preferences)
        }
        
    except Exception as e:
        logger.error(f"æ•°æ®åº“é€‰æ‹©å™¨æµ‹è¯•å¤±è´¥: {e}", exc_info=True)
        return {"status": "error", "error": str(e)}
        
    finally:
        db.close()


async def test_llm_manager_integration():
    """æµ‹è¯•LLMç®¡ç†å™¨ä¸æ•°æ®åº“é€‰æ‹©å™¨çš„é›†æˆ"""
    print("\nğŸ”— æµ‹è¯•LLMç®¡ç†å™¨é›†æˆ")
    print("=" * 40)
    
    try:
        from app.services.infrastructure.ai.llm import get_llm_manager
        from app.db.session import SessionLocal
        from app.models.user import User
        
        llm_manager = await get_llm_manager()
        
        # è·å–æµ‹è¯•ç”¨æˆ·ID
        db = SessionLocal()
        try:
            test_user = db.query(User).filter(User.username == "test_user").first()
            if not test_user:
                print("âŒ æ‰¾ä¸åˆ°æµ‹è¯•ç”¨æˆ·")
                return {"status": "error", "error": "æµ‹è¯•ç”¨æˆ·ä¸å­˜åœ¨"}
            
            user_id = str(test_user.id)
        finally:
            db.close()
        
        # æµ‹è¯•ä¸åŒAgentçš„æ¨¡å‹é€‰æ‹©
        test_cases = [
            {
                "agent": "react_agent",
                "task_type": "reasoning",
                "complexity": "medium",
                "constraints": {"max_cost": 0.02, "cost_sensitive": True}
            },
            {
                "agent": "coding_agent", 
                "task_type": "coding",
                "complexity": "complex",
                "constraints": {"accuracy_critical": True}
            },
            {
                "agent": "analysis_agent",
                "task_type": "analysis", 
                "complexity": "expert",
                "constraints": {"preferred_providers": ["anthropic"]}
            }
        ]
        
        results = []
        
        for case in test_cases:
            print(f"\nğŸ¤– æµ‹è¯• {case['agent']}")
            
            try:
                # ä½¿ç”¨æ•°æ®åº“é€‰æ‹©å™¨
                selection = await llm_manager.select_best_model(
                    task_type=case["task_type"],
                    complexity=case["complexity"], 
                    constraints=case["constraints"],
                    agent_id=case["agent"],
                    user_id=user_id  # å…³é”®ï¼šä¼ é€’user_idå¯ç”¨æ•°æ®åº“é€‰æ‹©
                )
                
                print(f"   âœ… é€‰æ‹©æˆåŠŸ: {selection.get('provider')}:{selection.get('model')}")
                print(f"   ğŸ“Š ç½®ä¿¡åº¦: {selection.get('confidence', 0):.1%}")
                print(f"   ğŸ’­ ç†ç”±: {selection.get('reasoning', 'N/A')}")
                print(f"   ğŸ’° é¢„æœŸæˆæœ¬: ${selection.get('expected_cost', 0):.4f}")
                print(f"   ğŸ”§ æ•°æ®æº: {selection.get('source', 'unknown')}")
                
                results.append({"agent": case["agent"], "status": "success", "selection": selection})
                
            except Exception as e:
                print(f"   âŒ é€‰æ‹©å¤±è´¥: {e}")
                results.append({"agent": case["agent"], "status": "error", "error": str(e)})
        
        success_count = sum(1 for r in results if r["status"] == "success")
        
        return {
            "status": "success",
            "total_tests": len(test_cases),
            "successful_tests": success_count,
            "success_rate": success_count / len(test_cases),
            "results": results
        }
        
    except Exception as e:
        logger.error(f"LLMç®¡ç†å™¨é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        return {"status": "error", "error": str(e)}


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª æ•°æ®åº“é©±åŠ¨æ™ºèƒ½æ¨¡å‹é€‰æ‹©æµ‹è¯•")
    print("=" * 70)
    print("å±•ç¤ºç”¨æˆ·é…ç½®çš„LLMæœåŠ¡å™¨å’Œæ¨¡å‹çš„æ™ºèƒ½é€‰æ‹©")
    print("ä¸å‰ç«¯é…ç½®é¡µé¢å®Œå…¨é›†æˆ")
    print("=" * 70)
    
    try:
        # æµ‹è¯•æ•°æ®åº“é€‰æ‹©å™¨
        result1 = await test_database_selector()
        
        # æµ‹è¯•LLMç®¡ç†å™¨é›†æˆ
        result2 = await test_llm_manager_integration()
        
        # æ€»ç»“æŠ¥å‘Š
        print(f"\nğŸ† æµ‹è¯•å®Œæˆæ€»ç»“")
        print("=" * 50)
        
        if result1.get("status") == "success":
            print("âœ… æ•°æ®åº“æ™ºèƒ½é€‰æ‹©å™¨: æµ‹è¯•æˆåŠŸ")
            print(f"   - ç”¨æˆ·ID: {result1['user_id']}")
            print(f"   - æµ‹è¯•åœºæ™¯: {result1['scenarios_tested']}")
            print(f"   - æˆåŠŸæ¨è: {result1['successful_recommendations']}")
            print(f"   - Agentåå¥½: {result1['agent_preferences']}")
        else:
            print("âŒ æ•°æ®åº“æ™ºèƒ½é€‰æ‹©å™¨: æµ‹è¯•å¤±è´¥")
            print(f"   é”™è¯¯: {result1.get('error')}")
        
        if result2.get("status") == "success":
            print("âœ… LLMç®¡ç†å™¨é›†æˆ: æµ‹è¯•æˆåŠŸ")
            print(f"   - æµ‹è¯•Agent: {result2['total_tests']}")
            print(f"   - æˆåŠŸç‡: {result2['success_rate']:.1%}")
        else:
            print("âŒ LLMç®¡ç†å™¨é›†æˆ: æµ‹è¯•å¤±è´¥")
            print(f"   é”™è¯¯: {result2.get('error')}")
        
        print(f"\nğŸ’¡ æ ¸å¿ƒåŠŸèƒ½éªŒè¯:")
        print("ğŸ¯ æ•°æ®åº“é…ç½®é©±åŠ¨ - ä»ç”¨æˆ·é…ç½®çš„æœåŠ¡å™¨å’Œæ¨¡å‹ä¸­é€‰æ‹©")
        print("ğŸ‘¤ ç”¨æˆ·åå¥½é›†æˆ - åŸºäºç”¨æˆ·çš„æä¾›å•†ä¼˜å…ˆçº§å’Œæ¨¡å‹åå¥½")
        print("ğŸ“Š é…é¢çº¦æŸæ”¯æŒ - è€ƒè™‘ç”¨æˆ·çš„æˆæœ¬å’ŒTokené™åˆ¶")
        print("ğŸ§  æ™ºèƒ½æ¨èç®—æ³• - å¤šç»´åº¦è¯„åˆ†å’ŒåŠ¨æ€å­¦ä¹ ")
        print("ğŸ”„ Agentä¸ªæ€§åŒ– - ä¸åŒAgentå½¢æˆå„è‡ªçš„ä½¿ç”¨åå¥½")
        print("ğŸ“ˆ ä½¿ç”¨åé¦ˆå­¦ä¹  - æŒç»­ä¼˜åŒ–é€‰æ‹©ç­–ç•¥")
        
        print(f"\nâœ¨ ä¸å‰ç«¯é›†æˆä¼˜åŠ¿:")
        print("ğŸ–¥ï¸  å‰ç«¯é…ç½®é¡µé¢ â†’ æ•°æ®åº“å­˜å‚¨ â†’ åç«¯æ™ºèƒ½é€‰æ‹©")
        print("âš™ï¸  ç”¨æˆ·å¯è§†åŒ–é…ç½®LLMæœåŠ¡å™¨å’Œæ¨¡å‹åå¥½")
        print("ğŸ“Š å®æ—¶ç»Ÿè®¡å’Œç›‘æ§ç”¨æˆ·çš„æ¨¡å‹ä½¿ç”¨æƒ…å†µ")
        print("ğŸ›ï¸ çµæ´»çš„é…é¢ç®¡ç†å’Œæˆæœ¬æ§åˆ¶")
        
        print(f"\nğŸ‰ æ•°æ®åº“é©±åŠ¨æ™ºèƒ½é€‰æ‹©ç³»ç»Ÿæµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"ä¸»æµ‹è¯•å‡½æ•°å¤±è´¥: {e}")
        print(f"âŒ æµ‹è¯•ç³»ç»Ÿå¤±è´¥: {e}")


if __name__ == "__main__":
    asyncio.run(main())