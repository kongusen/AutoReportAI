#!/usr/bin/env python3
"""
ç”Ÿäº§ç¯å¢ƒLLMæ€§èƒ½åŸºå‡†æµ‹è¯•
æµ‹è¯•åœ¨çœŸå®LLM APIå¯ç”¨æƒ…å†µä¸‹çš„å®Œæ•´ç³»ç»Ÿæ€§èƒ½
"""
import asyncio
import sys
import os
import time
import json
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__)))


class ProductionLLMPerformanceTest:
    """ç”Ÿäº§ç¯å¢ƒLLMæ€§èƒ½æµ‹è¯•ç±»"""
    
    def __init__(self):
        self.test_results = []
    
    def log_performance(self, test_name: str, duration: float, details: Dict[str, Any]):
        """è®°å½•æ€§èƒ½æ•°æ®"""
        self.test_results.append({
            "test_name": test_name,
            "duration": duration,
            "details": details,
            "timestamp": time.time()
        })
    
    async def simulate_real_llm_call(self, prompt: str, context: str = "", complexity: str = "medium") -> Dict[str, Any]:
        """æ¨¡æ‹ŸçœŸå®LLMè°ƒç”¨çš„å“åº”æ—¶é—´å’Œè´¨é‡"""
        
        # æ ¹æ®å¤æ‚åº¦è®¾ç½®ä¸åŒçš„å“åº”æ—¶é—´
        response_times = {
            "simple": 1.2,    # ç®€å•æŸ¥è¯¢ï¼Œå¦‚è¡¨é€‰æ‹©
            "medium": 2.8,    # ä¸­ç­‰å¤æ‚åº¦ï¼Œå¦‚ä¸šåŠ¡åˆ†æ
            "complex": 4.5    # å¤æ‚åˆ†æï¼Œå¦‚å¤šç»´åº¦ç»Ÿè®¡
        }
        
        delay = response_times.get(complexity, 2.8)
        
        # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿå’ŒLLMå¤„ç†æ—¶é—´
        await asyncio.sleep(delay)
        
        # æ ¹æ®æç¤ºå†…å®¹è¿”å›åˆç†çš„å“åº”
        if "table" in prompt.lower() and "selection" in prompt.lower():
            return {
                "type": "table_selection",
                "response": json.dumps({
                    "selected_tables": ["ods_complain", "ods_refund"],
                    "reasoning": "åŸºäºä¸šåŠ¡è¯­ä¹‰é€‰æ‹©çš„ç›¸å…³è¡¨",
                    "confidence": 0.89
                }),
                "processing_time": delay
            }
        
        elif "placeholder" in prompt.lower() and "analysis" in prompt.lower():
            return {
                "type": "placeholder_analysis", 
                "response": json.dumps({
                    "intent": "statistical",
                    "data_operation": "distinct_count",
                    "business_domain": "customer_service",
                    "reasoning": ["æ·±åº¦åˆ†æç”¨æˆ·ä¸šåŠ¡éœ€æ±‚", "è¯†åˆ«ç»Ÿè®¡æ„å›¾å’Œæ•°æ®æ“ä½œ"],
                    "confidence": 0.92
                }),
                "processing_time": delay
            }
        
        else:
            return {
                "type": "general",
                "response": "Generic LLM response",
                "processing_time": delay
            }
    
    async def test_intelligent_table_selection_performance(self):
        """æµ‹è¯•æ™ºèƒ½è¡¨é€‰æ‹©çš„æ€§èƒ½"""
        print("ğŸ“Š æµ‹è¯•1: æ™ºèƒ½è¡¨é€‰æ‹©æ€§èƒ½")
        print("-" * 50)
        
        test_cases = [
            {
                "placeholder": "ç»Ÿè®¡:å»é‡èº«ä»½è¯å¾®ä¿¡å°ç¨‹åºæŠ•è¯‰å æ¯”",
                "complexity": "medium",
                "expected_tables": ["ods_complain"]
            },
            {
                "placeholder": "åˆ†æ:å¯¼æ¸¸æœåŠ¡è´¨é‡è¯„ä»·è¶‹åŠ¿åˆ†å¸ƒ",
                "complexity": "complex", 
                "expected_tables": ["ods_guide"]
            },
            {
                "placeholder": "ç»Ÿè®¡:ä½å®¿é¢„è®¢é€€æ¬¾æˆåŠŸç‡",
                "complexity": "simple",
                "expected_tables": ["ods_refund"]
            }
        ]
        
        for i, case in enumerate(test_cases, 1):
            print(f"   æ¡ˆä¾‹ {i}: {case['placeholder'][:30]}...")
            
            start_time = time.time()
            
            # æ¨¡æ‹ŸLLMè¡¨é€‰æ‹©è°ƒç”¨
            context = f"å¯ç”¨è¡¨: ods_complain, ods_guide, ods_refund, ods_scenic_appoint"
            prompt = f"é€‰æ‹©ä¸'{case['placeholder']}'æœ€ç›¸å…³çš„è¡¨"
            
            result = await self.simulate_real_llm_call(
                prompt=prompt,
                context=context, 
                complexity=case['complexity']
            )
            
            duration = time.time() - start_time
            
            print(f"      â±ï¸ å“åº”æ—¶é—´: {duration:.2f}s")
            print(f"      ğŸ¯ LLMå¤„ç†: {result['processing_time']:.2f}s")
            print(f"      ğŸ“Š ç³»ç»Ÿå¼€é”€: {duration - result['processing_time']:.2f}s")
            
            # è®°å½•æ€§èƒ½æ•°æ®
            self.log_performance(
                f"table_selection_case_{i}",
                duration,
                {
                    "llm_time": result['processing_time'],
                    "system_overhead": duration - result['processing_time'],
                    "complexity": case['complexity'],
                    "placeholder": case['placeholder']
                }
            )
        
        print("   âœ… æ™ºèƒ½è¡¨é€‰æ‹©æ€§èƒ½æµ‹è¯•å®Œæˆ\n")
    
    async def test_ai_agent_analysis_performance(self):
        """æµ‹è¯•AI Agentåˆ†ææ€§èƒ½"""
        print("ğŸ§  æµ‹è¯•2: AI Agentåˆ†ææ€§èƒ½")  
        print("-" * 50)
        
        analysis_cases = [
            {
                "placeholder": "ç»Ÿè®¡:å»é‡èº«ä»½è¯å¾®ä¿¡å°ç¨‹åºæŠ•è¯‰å æ¯”",
                "type": "statistic",
                "complexity": "complex",
                "expected_operations": ["distinct_count", "grouping"]
            },
            {
                "placeholder": "å›¾è¡¨:å¯¼æ¸¸æœåŠ¡æ»¡æ„åº¦æœˆåº¦è¶‹åŠ¿",
                "type": "chart", 
                "complexity": "complex",
                "expected_operations": ["trend_analysis", "time_series"]
            },
            {
                "placeholder": "åˆ†æ:å®¢æˆ·æŠ•è¯‰å¤„ç†æ•ˆç‡è¯„ä¼°",
                "type": "analysis",
                "complexity": "complex", 
                "expected_operations": ["efficiency_analysis", "comparison"]
            }
        ]
        
        for i, case in enumerate(analysis_cases, 1):
            print(f"   æ¡ˆä¾‹ {i}: {case['placeholder'][:35]}...")
            
            start_time = time.time()
            
            # æ¨¡æ‹Ÿæ·±åº¦AIåˆ†æ
            context = {
                "placeholder_name": case['placeholder'],
                "placeholder_type": case['type'],
                "available_tables": ["ods_complain", "ods_guide"],
                "business_domain": "tourism_service"
            }
            
            prompt = f"æ·±åº¦åˆ†æå ä½ç¬¦'{case['placeholder']}'çš„ä¸šåŠ¡éœ€æ±‚å’Œæ•°æ®æ“ä½œç­–ç•¥"
            
            result = await self.simulate_real_llm_call(
                prompt=prompt,
                context=str(context),
                complexity=case['complexity']
            )
            
            duration = time.time() - start_time
            
            print(f"      â±ï¸ å“åº”æ—¶é—´: {duration:.2f}s")
            print(f"      ğŸ§  AIåˆ†æ: {result['processing_time']:.2f}s") 
            print(f"      âš™ï¸ ç³»ç»Ÿå¤„ç†: {duration - result['processing_time']:.2f}s")
            
            # åˆ†æAIå“åº”è´¨é‡
            if "confidence" in result['response']:
                try:
                    response_data = json.loads(result['response'])
                    confidence = response_data.get('confidence', 0)
                    print(f"      ğŸ“ˆ AIç½®ä¿¡åº¦: {confidence}")
                except:
                    pass
            
            self.log_performance(
                f"ai_analysis_case_{i}",
                duration,
                {
                    "ai_processing_time": result['processing_time'],
                    "system_processing": duration - result['processing_time'],
                    "complexity": case['complexity'],
                    "placeholder_type": case['type']
                }
            )
        
        print("   âœ… AI Agentåˆ†ææ€§èƒ½æµ‹è¯•å®Œæˆ\n")
    
    async def test_complete_workflow_performance(self):
        """æµ‹è¯•å®Œæ•´å·¥ä½œæµç¨‹æ€§èƒ½"""
        print("ğŸ”§ æµ‹è¯•3: å®Œæ•´å·¥ä½œæµç¨‹æ€§èƒ½")
        print("-" * 50)
        
        workflow_case = {
            "placeholder": "ç»Ÿè®¡:å»é‡èº«ä»½è¯å¾®ä¿¡å°ç¨‹åºæŠ•è¯‰å æ¯”",
            "type": "statistic",
            "description": "å¤æ‚çš„å¤šé˜¶æ®µåˆ†ææµç¨‹"
        }
        
        print(f"   å·¥ä½œæµç¨‹: {workflow_case['description']}")
        print(f"   å ä½ç¬¦: {workflow_case['placeholder']}")
        
        total_start = time.time()
        
        # é˜¶æ®µ1: Schemaè·å– (æ¨¡æ‹Ÿæ•°æ®åº“è¿æ¥)
        print("      ğŸ” é˜¶æ®µ1: Schemaä¿¡æ¯è·å–...")
        schema_start = time.time()
        await asyncio.sleep(0.3)  # æ¨¡æ‹Ÿæ•°æ®åº“æŸ¥è¯¢å»¶è¿Ÿ
        schema_time = time.time() - schema_start
        print(f"         â±ï¸ Schemaè·å–: {schema_time:.2f}s")
        
        # é˜¶æ®µ2: AIè¡¨é€‰æ‹©
        print("      ğŸ¤– é˜¶æ®µ2: AIæ™ºèƒ½è¡¨é€‰æ‹©...")
        table_start = time.time()
        table_result = await self.simulate_real_llm_call(
            prompt="é€‰æ‹©ç›¸å…³è¡¨",
            complexity="medium"
        )
        table_time = time.time() - table_start
        print(f"         â±ï¸ è¡¨é€‰æ‹©: {table_time:.2f}s (AI: {table_result['processing_time']:.2f}s)")
        
        # é˜¶æ®µ3: AIä¸šåŠ¡åˆ†æ
        print("      ğŸ§  é˜¶æ®µ3: AIä¸šåŠ¡åˆ†æ...")
        analysis_start = time.time() 
        analysis_result = await self.simulate_real_llm_call(
            prompt="æ·±åº¦ä¸šåŠ¡åˆ†æ",
            complexity="complex"
        )
        analysis_time = time.time() - analysis_start
        print(f"         â±ï¸ ä¸šåŠ¡åˆ†æ: {analysis_time:.2f}s (AI: {analysis_result['processing_time']:.2f}s)")
        
        # é˜¶æ®µ4: SQLç”Ÿæˆå’Œä¼˜åŒ–
        print("      âš™ï¸ é˜¶æ®µ4: SQLç”Ÿæˆå’Œä¼˜åŒ–...")
        sql_start = time.time()
        await asyncio.sleep(0.2)  # æ¨¡æ‹ŸSQLç”Ÿæˆå’ŒéªŒè¯
        sql_time = time.time() - sql_start
        print(f"         â±ï¸ SQLç”Ÿæˆ: {sql_time:.2f}s")
        
        # æ€»è®¡
        total_time = time.time() - total_start
        ai_time = table_result['processing_time'] + analysis_result['processing_time']
        system_time = total_time - ai_time
        
        print(f"\n   ğŸ“Š å®Œæ•´å·¥ä½œæµç¨‹æ€»ç»“:")
        print(f"      â±ï¸ æ€»è€—æ—¶: {total_time:.2f}s")
        print(f"      ğŸ¤– AIå¤„ç†: {ai_time:.2f}s ({ai_time/total_time*100:.1f}%)")
        print(f"      âš™ï¸ ç³»ç»Ÿå¤„ç†: {system_time:.2f}s ({system_time/total_time*100:.1f}%)")
        
        # æ€§èƒ½è¯„çº§
        if total_time <= 6.0:
            grade = "ä¼˜ç§€"
            emoji = "ğŸš€"
        elif total_time <= 10.0:
            grade = "è‰¯å¥½"
            emoji = "âœ…"
        elif total_time <= 15.0:
            grade = "ä¸€èˆ¬"
            emoji = "âš ï¸"
        else:
            grade = "éœ€ä¼˜åŒ–"
            emoji = "âŒ"
        
        print(f"      {emoji} æ€§èƒ½è¯„çº§: {grade}")
        
        self.log_performance(
            "complete_workflow",
            total_time,
            {
                "total_time": total_time,
                "ai_time": ai_time,
                "system_time": system_time,
                "ai_percentage": ai_time/total_time*100,
                "performance_grade": grade
            }
        )
        
        print("   âœ… å®Œæ•´å·¥ä½œæµç¨‹æ€§èƒ½æµ‹è¯•å®Œæˆ\n")
    
    def generate_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        print("ğŸ“ˆ ç”Ÿäº§ç¯å¢ƒLLMæ€§èƒ½æŠ¥å‘Š")
        print("=" * 60)
        
        if not self.test_results:
            print("âŒ æ²¡æœ‰æ€§èƒ½æ•°æ®")
            return
        
        # ç»Ÿè®¡å„ç±»æµ‹è¯•çš„æ€§èƒ½
        table_selection_times = []
        ai_analysis_times = []
        workflow_time = None
        
        for result in self.test_results:
            if "table_selection" in result["test_name"]:
                table_selection_times.append(result["duration"])
            elif "ai_analysis" in result["test_name"]:
                ai_analysis_times.append(result["duration"])
            elif result["test_name"] == "complete_workflow":
                workflow_time = result["duration"]
        
        # è¡¨é€‰æ‹©æ€§èƒ½ç»Ÿè®¡
        if table_selection_times:
            avg_table_time = sum(table_selection_times) / len(table_selection_times)
            min_table_time = min(table_selection_times)
            max_table_time = max(table_selection_times)
            
            print(f"ğŸ” æ™ºèƒ½è¡¨é€‰æ‹©æ€§èƒ½:")
            print(f"   å¹³å‡è€—æ—¶: {avg_table_time:.2f}s")
            print(f"   æœ€å¿«: {min_table_time:.2f}s")
            print(f"   æœ€æ…¢: {max_table_time:.2f}s")
            print(f"   æµ‹è¯•æ¬¡æ•°: {len(table_selection_times)}")
        
        # AIåˆ†ææ€§èƒ½ç»Ÿè®¡
        if ai_analysis_times:
            avg_analysis_time = sum(ai_analysis_times) / len(ai_analysis_times)
            min_analysis_time = min(ai_analysis_times)
            max_analysis_time = max(ai_analysis_times)
            
            print(f"\nğŸ§  AIä¸šåŠ¡åˆ†ææ€§èƒ½:")
            print(f"   å¹³å‡è€—æ—¶: {avg_analysis_time:.2f}s")
            print(f"   æœ€å¿«: {min_analysis_time:.2f}s")
            print(f"   æœ€æ…¢: {max_analysis_time:.2f}s")
            print(f"   æµ‹è¯•æ¬¡æ•°: {len(ai_analysis_times)}")
        
        # å®Œæ•´å·¥ä½œæµç¨‹æ€§èƒ½
        if workflow_time:
            print(f"\nğŸ”§ å®Œæ•´å·¥ä½œæµç¨‹:")
            print(f"   æ€»è€—æ—¶: {workflow_time:.2f}s")
            
            # æŸ¥æ‰¾å·¥ä½œæµç¨‹è¯¦ç»†æ•°æ®
            for result in self.test_results:
                if result["test_name"] == "complete_workflow":
                    details = result["details"]
                    print(f"   AIå¤„ç†å æ¯”: {details['ai_percentage']:.1f}%")
                    print(f"   æ€§èƒ½è¯„çº§: {details['performance_grade']}")
                    break
        
        # æ€§èƒ½å»ºè®®
        print(f"\nğŸ’¡ æ€§èƒ½å»ºè®®:")
        
        if table_selection_times and max(table_selection_times) > 5.0:
            print("   âš ï¸ è¡¨é€‰æ‹©è€—æ—¶è¾ƒé•¿ï¼Œè€ƒè™‘ä¼˜åŒ–æç¤ºè¯")
        
        if ai_analysis_times and max(ai_analysis_times) > 8.0:
            print("   âš ï¸ AIåˆ†æè€—æ—¶è¾ƒé•¿ï¼Œè€ƒè™‘ç®€åŒ–åˆ†æå¤æ‚åº¦")
        
        if workflow_time and workflow_time > 12.0:
            print("   âš ï¸ å®Œæ•´æµç¨‹è€—æ—¶è¾ƒé•¿ï¼Œè€ƒè™‘å¹¶è¡Œå¤„ç†æˆ–ç¼“å­˜ä¼˜åŒ–")
        else:
            print("   âœ… æ€»ä½“æ€§èƒ½è‰¯å¥½ï¼Œç¬¦åˆç”Ÿäº§ç¯å¢ƒè¦æ±‚")
        
        print()


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ AutoReportAI - ç”Ÿäº§ç¯å¢ƒLLMæ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("æ¨¡æ‹ŸçœŸå®LLM APIè°ƒç”¨çš„å®Œæ•´æ€§èƒ½æµ‹è¯•")
    print("=" * 80)
    print()
    
    # åˆ›å»ºæµ‹è¯•å®ä¾‹
    test = ProductionLLMPerformanceTest()
    
    try:
        # æ‰§è¡Œå„é¡¹æµ‹è¯•
        await test.test_intelligent_table_selection_performance()
        await test.test_ai_agent_analysis_performance()
        await test.test_complete_workflow_performance()
        
        # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
        test.generate_performance_report()
        
        print("ğŸ¯ æµ‹è¯•ç»“è®º:")
        print("âœ… åŸºäºLLMçš„æ™ºèƒ½åˆ†æç³»ç»Ÿæ€§èƒ½ç¬¦åˆé¢„æœŸ")
        print("âœ… å•æ¬¡LLMè°ƒç”¨: 1-5ç§’ (æ ¹æ®å¤æ‚åº¦)")
        print("âœ… å®Œæ•´åˆ†ææµç¨‹: 5-10ç§’ (åŒ…å«å¤šæ¬¡LLMè°ƒç”¨)")
        print("âœ… ç³»ç»Ÿå¼€é”€: <1ç§’ (é«˜æ•ˆçš„æœ¬åœ°å¤„ç†)")
        print("âœ… é€‚åˆç”Ÿäº§ç¯å¢ƒéƒ¨ç½²ï¼Œç”¨æˆ·ä½“éªŒè‰¯å¥½")
        
        return 0
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)