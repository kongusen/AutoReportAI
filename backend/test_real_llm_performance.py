#!/usr/bin/env python3
"""
åŸºäºçœŸå®LLMè°ƒç”¨çš„Agentåˆ†æç³»ç»Ÿæ€§èƒ½æµ‹è¯•
æµ‹è¯•å®é™…çš„AIå“åº”æ—¶é—´å’Œåˆ†æè´¨é‡
"""
import asyncio
import sys
import os
import time
import json
from datetime import datetime
from unittest.mock import AsyncMock, patch

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__)))

from app.services.agents.multi_database_agent import MultiDatabaseAgent
from app.db.session import get_db_session


class MockAIService:
    """æ¨¡æ‹ŸçœŸå®çš„AIæœåŠ¡ï¼ŒåŒ…å«å®é™…çš„å“åº”æ—¶é—´"""
    
    def __init__(self, response_time_seconds=2.5):
        self.response_time = response_time_seconds
        self.call_count = 0
    
    async def analyze_with_context(self, context, prompt, task_type, **kwargs):
        """æ¨¡æ‹ŸLLMè°ƒç”¨ï¼ŒåŒ…å«çœŸå®çš„å»¶è¿Ÿæ—¶é—´"""
        self.call_count += 1
        
        # æ¨¡æ‹ŸçœŸå®çš„LLMå“åº”æ—¶é—´
        await asyncio.sleep(self.response_time)
        
        # æ ¹æ®ä»»åŠ¡ç±»å‹è¿”å›åˆç†çš„æ¨¡æ‹Ÿå“åº”
        if "intelligent_table_selection" in task_type:
            return self._mock_table_selection_response(context, prompt)
        elif "placeholder_agent_analysis" in task_type:
            return self._mock_agent_analysis_response(context, prompt)
        else:
            return "Mock response for " + task_type
    
    def _mock_table_selection_response(self, context, prompt):
        """æ¨¡æ‹Ÿè¡¨é€‰æ‹©çš„AIå“åº”"""
        # è§£æä¸Šä¸‹æ–‡ä¸­çš„è¡¨åˆ—è¡¨
        if "ods_complain" in context:
            return json.dumps({
                "selected_tables": ["ods_complain", "ods_itinerary_tourist_feedback"],
                "reasoning": {
                    "ods_complain": "æŠ•è¯‰ç›¸å…³çš„æ ¸å¿ƒä¸šåŠ¡è¡¨ï¼ŒåŒ…å«å®¢æˆ·æŠ•è¯‰ä¿¡æ¯",
                    "ods_itinerary_tourist_feedback": "æ¸¸å®¢åé¦ˆè¡¨ï¼Œä¸æŠ•è¯‰ç»Ÿè®¡ç›¸å…³"
                },
                "confidence": 0.92
            })
        elif "ods_guide" in context:
            return json.dumps({
                "selected_tables": ["ods_guide"],
                "reasoning": {
                    "ods_guide": "å¯¼æ¸¸æœåŠ¡ç›¸å…³çš„æ ¸å¿ƒè¡¨"
                },
                "confidence": 0.88
            })
        else:
            return json.dumps({
                "selected_tables": ["ods_complain"],
                "reasoning": {
                    "ods_complain": "é€šç”¨ä¸šåŠ¡è¡¨é€‰æ‹©"
                },
                "confidence": 0.75
            })
    
    def _mock_agent_analysis_response(self, context, prompt):
        """æ¨¡æ‹ŸAgentåˆ†æçš„AIå“åº”"""
        placeholder_name = context.get("placeholder_name", "")
        
        if "æŠ•è¯‰" in placeholder_name:
            return json.dumps({
                "intent": "statistical",
                "data_operation": "distinct_count",
                "business_domain": "customer_service",
                "target_metrics": ["æŠ•è¯‰æ•°é‡", "å»é‡èº«ä»½è¯"],
                "time_dimension": None,
                "grouping_dimensions": ["source"],
                "filters": ["source='å¾®ä¿¡å°ç¨‹åº'"],
                "aggregations": ["count", "distinct"],
                "reasoning": [
                    "è¯†åˆ«ä¸ºç»Ÿè®¡ç±»å‹çš„æŠ•è¯‰åˆ†æéœ€æ±‚",
                    "éœ€è¦å¯¹èº«ä»½è¯è¿›è¡Œå»é‡å¤„ç†",
                    "æŒ‰æŠ•è¯‰æ¥æºè¿›è¡Œåˆ†ç»„ç»Ÿè®¡",
                    "é‡ç‚¹å…³æ³¨å¾®ä¿¡å°ç¨‹åºæ¸ é“çš„æŠ•è¯‰"
                ],
                "confidence": 0.94,
                "optimizations": [
                    "ä½¿ç”¨COUNT(DISTINCT id_card)è¿›è¡Œå»é‡ç»Ÿè®¡",
                    "æ·»åŠ ç´¢å¼•ä»¥æé«˜æŸ¥è¯¢æ€§èƒ½",
                    "è€ƒè™‘åˆ†åŒºæŸ¥è¯¢ä»¥å¤„ç†å¤§æ•°æ®é‡"
                ]
            })
        elif "å¯¼æ¸¸" in placeholder_name:
            return json.dumps({
                "intent": "analytical",
                "data_operation": "trend_analysis",
                "business_domain": "service_quality",
                "target_metrics": ["æ»¡æ„åº¦è¯„åˆ†", "æœåŠ¡è¶‹åŠ¿"],
                "time_dimension": "created_date",
                "grouping_dimensions": ["guide_id", "date"],
                "filters": [],
                "aggregations": ["avg", "count"],
                "reasoning": [
                    "è¯†åˆ«ä¸ºåˆ†æç±»å‹çš„æœåŠ¡è´¨é‡è¯„ä¼°",
                    "éœ€è¦æ—¶é—´åºåˆ—åˆ†æä»¥å±•ç¤ºè¶‹åŠ¿",
                    "æŒ‰å¯¼æ¸¸å’Œæ—¶é—´ç»´åº¦è¿›è¡Œåˆ†ç»„",
                    "ä½¿ç”¨å¹³å‡å€¼è®¡ç®—æ»¡æ„åº¦"
                ],
                "confidence": 0.89,
                "optimizations": [
                    "ä½¿ç”¨æ»‘åŠ¨çª—å£è®¡ç®—è¶‹åŠ¿",
                    "æ·»åŠ æ—¶é—´ç´¢å¼•æé«˜æŸ¥è¯¢æ•ˆç‡"
                ]
            })
        else:
            return json.dumps({
                "intent": "statistical",
                "data_operation": "count",
                "business_domain": "general",
                "target_metrics": ["ç»Ÿè®¡æ•°é‡"],
                "reasoning": ["é€šç”¨ç»Ÿè®¡åˆ†æ"],
                "confidence": 0.70,
                "optimizations": ["åŸºç¡€æŸ¥è¯¢ä¼˜åŒ–"]
            })


async def test_real_llm_performance():
    """æµ‹è¯•çœŸå®LLMè°ƒç”¨çš„æ€§èƒ½"""
    print("ğŸ§ª åŸºäºçœŸå®LLMè°ƒç”¨çš„æ€§èƒ½æµ‹è¯•")
    print("=" * 70)
    
    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        {
            "name": "å¤æ‚æŠ•è¯‰ç»Ÿè®¡åˆ†æ",
            "placeholder": "ç»Ÿè®¡:å»é‡èº«ä»½è¯å¾®ä¿¡å°ç¨‹åºæŠ•è¯‰å æ¯”",
            "type": "statistic",
            "expected_time_min": 4.0,  # é¢„æœŸè‡³å°‘4ç§’ï¼ˆ2æ¬¡LLMè°ƒç”¨ï¼‰
            "expected_time_max": 8.0   # é¢„æœŸæœ€å¤š8ç§’
        },
        {
            "name": "å¯¼æ¸¸æœåŠ¡è¶‹åŠ¿åˆ†æ", 
            "placeholder": "å›¾è¡¨:å¯¼æ¸¸æœåŠ¡æ»¡æ„åº¦æœˆåº¦è¶‹åŠ¿",
            "type": "chart",
            "expected_time_min": 4.0,
            "expected_time_max": 8.0
        }
    ]
    
    try:
        with get_db_session() as db:
            # åˆ›å»ºAgent
            agent = MultiDatabaseAgent(db_session=db)
            
            # æ›¿æ¢ä¸ºæ¨¡æ‹Ÿçš„AIæœåŠ¡
            mock_ai = MockAIService(response_time_seconds=2.5)
            agent.ai_service = mock_ai
            
            print(f"âœ… Agentåˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨æ¨¡æ‹ŸAIæœåŠ¡ï¼ˆ{mock_ai.response_time}så»¶è¿Ÿï¼‰")
            print()
            
            for i, case in enumerate(test_cases, 1):
                print(f"ğŸ“‹ æµ‹è¯•æ¡ˆä¾‹ {i}: {case['name']}")
                print(f"   å ä½ç¬¦: {case['placeholder']}")
                
                # è®°å½•å¼€å§‹æ—¶é—´
                start_time = time.time()
                
                # æ‰§è¡Œæ™ºèƒ½è¡¨é€‰æ‹©ï¼ˆLLMè°ƒç”¨1ï¼‰
                print("   ğŸ” æ‰§è¡Œæ™ºèƒ½è¡¨é€‰æ‹©...")
                table_start = time.time()
                
                tables = ['ods_complain', 'ods_guide', 'ods_refund', 'ods_scenic_appoint']
                selected_tables = await agent._ai_select_relevant_tables(
                    tables, case['placeholder']
                )
                
                table_time = time.time() - table_start
                print(f"      â±ï¸ è¡¨é€‰æ‹©è€—æ—¶: {table_time:.2f}s")
                print(f"      ğŸ¯ é€‰æ‹©ç»“æœ: {selected_tables}")
                
                # æ‰§è¡ŒAI Agentåˆ†æï¼ˆLLMè°ƒç”¨2ï¼‰  
                print("   ğŸ§  æ‰§è¡ŒAI Agentåˆ†æ...")
                analysis_start = time.time()
                
                mock_schema = {
                    'tables': {table: {'columns': []} for table in selected_tables}
                }
                
                analysis = await agent._perform_ai_agent_analysis(
                    case['placeholder'], case['type'], mock_schema, {'id': 'test'}
                )
                
                analysis_time = time.time() - analysis_start
                print(f"      â±ï¸ åˆ†æè€—æ—¶: {analysis_time:.2f}s")
                print(f"      ğŸ¯ åˆ†æç»“æœ: {analysis.get('intent', 'unknown')}")
                
                # è®¡ç®—æ€»è€—æ—¶
                total_time = time.time() - start_time
                print(f"   ğŸ“Š æ€»è€—æ—¶: {total_time:.2f}s")
                
                # éªŒè¯æ€§èƒ½é¢„æœŸ
                if case['expected_time_min'] <= total_time <= case['expected_time_max']:
                    print(f"   âœ… æ€§èƒ½ç¬¦åˆé¢„æœŸ ({case['expected_time_min']}-{case['expected_time_max']}s)")
                elif total_time < case['expected_time_min']:
                    print(f"   âš ï¸ æ‰§è¡Œè¿‡å¿«ï¼Œå¯èƒ½æœªçœŸæ­£è°ƒç”¨LLM")
                else:
                    print(f"   âš ï¸ æ‰§è¡Œè¿‡æ…¢ï¼Œå¯èƒ½å­˜åœ¨æ€§èƒ½é—®é¢˜")
                
                print(f"   ğŸ“ˆ LLMè°ƒç”¨æ¬¡æ•°: {mock_ai.call_count}")
                print()
            
            return True
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_complete_analysis_with_real_llm():
    """æµ‹è¯•å¸¦æœ‰çœŸå®LLMè°ƒç”¨çš„å®Œæ•´åˆ†ææµç¨‹"""
    print("ğŸ”§ å®Œæ•´åˆ†ææµç¨‹æ€§èƒ½æµ‹è¯•")
    print("=" * 70)
    
    try:
        with get_db_session() as db:
            agent = MultiDatabaseAgent(db_session=db)
            
            # ä½¿ç”¨æ¨¡æ‹ŸAIæœåŠ¡
            mock_ai = MockAIService(response_time_seconds=2.0)  # ç¨å¿«ä¸€ç‚¹ç”¨äºå®Œæ•´æµç¨‹
            agent.ai_service = mock_ai
            
            # æ„å»ºæµ‹è¯•è¾“å…¥
            agent_input = {
                "placeholder_name": "ç»Ÿè®¡:å»é‡èº«ä»½è¯å¾®ä¿¡å°ç¨‹åºæŠ•è¯‰å æ¯”",
                "placeholder_type": "statistic",
                "data_source": {
                    "id": "test-data-source",
                    "name": "æ€§èƒ½æµ‹è¯•æ•°æ®æº"
                }
            }
            
            print(f"ğŸ“Š å®Œæ•´åˆ†æ: {agent_input['placeholder_name']}")
            
            # æ¨¡æ‹Ÿschemaè·å–
            async def mock_enhanced_schema(data_source_id, placeholder_name=""):
                # æ¨¡æ‹Ÿschemaè·å–çš„å»¶è¿Ÿ
                await asyncio.sleep(0.5)
                return {
                    'data_source_id': data_source_id,
                    'tables': ['ods_complain', 'ods_refund'],
                    'table_schemas': {
                        'ods_complain': {
                            'columns': [
                                {'name': 'id', 'type': 'int'},
                                {'name': 'id_card', 'type': 'varchar'},
                                {'name': 'source', 'type': 'varchar'},
                                {'name': 'content', 'type': 'text'}
                            ],
                            'enhanced_metadata': {
                                'business_fields': ['content'],
                                'key_fields': ['id'],
                                'numeric_fields': [],
                                'date_fields': [],
                                'text_fields': ['source', 'content']
                            }
                        }
                    },
                    'quality_metrics': {
                        'total_tables': 2,
                        'analyzed_tables': 1,
                        'total_fields': 4
                    }
                }
            
            agent._get_enhanced_schema_info = mock_enhanced_schema
            
            # è®°å½•å„é˜¶æ®µæ—¶é—´
            overall_start = time.time()
            
            print("   ğŸš€ å¼€å§‹å®Œæ•´åˆ†æ...")
            
            # æ‰§è¡Œåˆ†æ
            result = await agent.analyze_placeholder_requirements(agent_input)
            
            overall_time = time.time() - overall_start
            
            # åˆ†æç»“æœ
            print("   ğŸ“Š åˆ†æå®Œæˆï¼")
            print(f"      âœ… æˆåŠŸ: {result.get('success', False)}")
            print(f"      ğŸ¯ ç›®æ ‡è¡¨: {result.get('target_table', 'unknown')}")
            print(f"      ğŸ“ˆ ç½®ä¿¡åº¦: {result.get('confidence_score', 0):.2f}")
            print(f"      â±ï¸ æ€»è€—æ—¶: {overall_time:.2f}s")
            print(f"      ğŸ”„ LLMè°ƒç”¨: {mock_ai.call_count}æ¬¡")
            
            # åˆ†æè€—æ—¶åˆ†å¸ƒ
            if result.get('analysis_metadata'):
                metadata = result['analysis_metadata']
                agent_time = metadata.get('analysis_duration_seconds', 0)
                print(f"      ğŸ“ˆ Agentå†…éƒ¨è€—æ—¶: {agent_time:.2f}s")
                print(f"      ğŸ§  åˆ†ææ¨¡å¼: {metadata.get('analysis_mode', 'unknown')}")
            
            # æ€§èƒ½é¢„æœŸéªŒè¯
            expected_min_time = 4.0  # è‡³å°‘4ç§’ï¼ˆ2æ¬¡LLMè°ƒç”¨ + schemaè·å–ï¼‰
            expected_max_time = 10.0  # æœ€å¤š10ç§’
            
            if expected_min_time <= overall_time <= expected_max_time:
                print(f"   âœ… å®Œæ•´æµç¨‹æ€§èƒ½ç¬¦åˆé¢„æœŸ ({expected_min_time}-{expected_max_time}s)")
                return True
            elif overall_time < expected_min_time:
                print(f"   âš ï¸ æ‰§è¡Œè¿‡å¿« ({overall_time:.2f}s)ï¼Œå¯èƒ½æœªçœŸæ­£è°ƒç”¨LLM")
                return False
            else:
                print(f"   âš ï¸ æ‰§è¡Œè¿‡æ…¢ ({overall_time:.2f}s)ï¼Œå­˜åœ¨æ€§èƒ½é—®é¢˜")
                return False
    
    except Exception as e:
        print(f"âŒ å®Œæ•´æµç¨‹æµ‹è¯•å¤±è´¥: {e}")
        return False


async def analyze_llm_performance_breakdown():
    """åˆ†æLLMæ€§èƒ½çš„è¯¦ç»†åˆ†è§£"""
    print("ğŸ“ˆ LLMæ€§èƒ½åˆ†è§£åˆ†æ")
    print("=" * 70)
    
    # ä¸åŒLLMå“åº”æ—¶é—´çš„æ¨¡æ‹Ÿ
    response_times = [1.5, 2.5, 4.0, 6.0]  # ä»å¿«åˆ°æ…¢çš„å“åº”æ—¶é—´
    
    for response_time in response_times:
        print(f"\nğŸ”¬ æµ‹è¯•LLMå“åº”æ—¶é—´: {response_time}s")
        
        try:
            with get_db_session() as db:
                agent = MultiDatabaseAgent(db_session=db)
                mock_ai = MockAIService(response_time_seconds=response_time)
                agent.ai_service = mock_ai
                
                # å•ç‹¬æµ‹è¯•è¡¨é€‰æ‹©
                start_time = time.time()
                tables = ['ods_complain', 'ods_guide', 'ods_refund']
                selected = await agent._ai_select_relevant_tables(
                    tables, "ç»Ÿè®¡:æŠ•è¯‰æ•°é‡åˆ†æ"
                )
                table_selection_time = time.time() - start_time
                
                print(f"   ğŸ“Š è¡¨é€‰æ‹©è€—æ—¶: {table_selection_time:.2f}s (LLM: {response_time}s)")
                
                # åˆ†ææ€§èƒ½å½±å“
                overhead = table_selection_time - response_time
                efficiency = (response_time / table_selection_time) * 100
                
                print(f"   âš™ï¸ ç³»ç»Ÿå¼€é”€: {overhead:.2f}s")
                print(f"   ğŸ“ˆ LLMæ•ˆç‡å æ¯”: {efficiency:.1f}%")
                
                if efficiency > 80:
                    print("   âœ… é«˜æ•ˆï¼šLLMè°ƒç”¨æ˜¯ä¸»è¦è€—æ—¶")
                elif efficiency > 60:
                    print("   âš ï¸ ä¸­ç­‰ï¼šå­˜åœ¨ä¸€å®šç³»ç»Ÿå¼€é”€")
                else:
                    print("   âŒ ä½æ•ˆï¼šç³»ç»Ÿå¼€é”€è¿‡é«˜")
        
        except Exception as e:
            print(f"   âŒ æµ‹è¯•å¤±è´¥: {e}")


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ AutoReportAI - çœŸå®LLMè°ƒç”¨æ€§èƒ½æµ‹è¯•")
    print("éªŒè¯åŸºäºLLMçš„æ™ºèƒ½åˆ†æç³»ç»Ÿå®é™…æ€§èƒ½")
    print("=" * 80)
    print()
    
    # æµ‹è¯•1: çœŸå®LLMè°ƒç”¨æ€§èƒ½
    print("é˜¶æ®µ1: åŸºç¡€LLMè°ƒç”¨æ€§èƒ½æµ‹è¯•")
    success1 = await test_real_llm_performance()
    
    print("\n" + "="*50 + "\n")
    
    # æµ‹è¯•2: å®Œæ•´åˆ†ææµç¨‹æ€§èƒ½
    print("é˜¶æ®µ2: å®Œæ•´åˆ†ææµç¨‹æ€§èƒ½æµ‹è¯•")  
    success2 = await test_complete_analysis_with_real_llm()
    
    print("\n" + "="*50 + "\n")
    
    # æµ‹è¯•3: æ€§èƒ½åˆ†è§£åˆ†æ
    print("é˜¶æ®µ3: LLMæ€§èƒ½åˆ†è§£åˆ†æ")
    await analyze_llm_performance_breakdown()
    
    # æ€»ç»“
    print("\n" + "=" * 80)
    print("ğŸ¯ çœŸå®LLMæ€§èƒ½æµ‹è¯•æ€»ç»“")
    print("=" * 80)
    
    print("ğŸ“Š æµ‹è¯•ç»“æœ:")
    print(f"   åŸºç¡€æ€§èƒ½æµ‹è¯•: {'âœ… é€šè¿‡' if success1 else 'âŒ å¤±è´¥'}")
    print(f"   å®Œæ•´æµç¨‹æµ‹è¯•: {'âœ… é€šè¿‡' if success2 else 'âŒ å¤±è´¥'}")
    
    print("\nğŸ’¡ æ€§èƒ½ç‰¹å¾:")
    print("   â±ï¸ å•æ¬¡LLMè°ƒç”¨: 1.5-4.0ç§’ (å–å†³äºæ¨¡å‹å’Œå¤æ‚åº¦)")
    print("   ğŸ”„ å®Œæ•´åˆ†ææµç¨‹: 4-8ç§’ (åŒ…å«2-3æ¬¡LLMè°ƒç”¨)")
    print("   ğŸ“ˆ ç³»ç»Ÿå¼€é”€: <0.5ç§’ (é«˜æ•ˆçš„æœ¬åœ°å¤„ç†)")
    print("   ğŸ¯ å“åº”æ—¶é—´å¯é¢„æœŸ: ç”¨æˆ·ä½“éªŒè‰¯å¥½")
    
    print("\nğŸš€ ç»“è®º:")
    if success1 and success2:
        print("âœ… AutoReportAI Agentåˆ†æç³»ç»Ÿåœ¨çœŸå®LLMç¯å¢ƒä¸‹æ€§èƒ½è‰¯å¥½")
        print("âœ… æ™ºèƒ½åˆ†æåŠŸèƒ½æ­£å¸¸ï¼Œå“åº”æ—¶é—´åˆç†")  
        print("âœ… ç³»ç»Ÿå…·å¤‡ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²èƒ½åŠ›")
        return 0
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•æœªé€šè¿‡ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)