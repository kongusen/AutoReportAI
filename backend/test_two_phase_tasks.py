#!/usr/bin/env python3
"""
æµ‹è¯•ä¸¤æ®µå¼ä»»åŠ¡æ‰§è¡ŒåŠŸèƒ½
å±•ç¤ºæ•°æ®å‘ç°->åˆ†ææŠ¥å‘Šç”Ÿæˆçš„å®Œæ•´æµç¨‹
"""

import asyncio
import json
import logging
from datetime import datetime
from typing import Dict, Any

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def test_two_phase_task_execution():
    """æµ‹è¯•ä¸¤æ®µå¼ä»»åŠ¡æ‰§è¡Œ"""
    print("\nğŸ”„ æµ‹è¯•ä¸¤æ®µå¼ä»»åŠ¡æ‰§è¡Œ...")
    
    from app.services.agents.factory import create_agent, AgentType
    from app.services.agents.core.performance_monitor import performance_context
    from app.db.session import get_db_session
    
    try:
        with get_db_session() as db:
            print("  ğŸ“Š ç¬¬ä¸€é˜¶æ®µï¼šæ•°æ®å‘ç°å’ŒSchemaåˆ†æ...")
            
            # ç¬¬ä¸€é˜¶æ®µï¼šæ•°æ®å‘ç°
            with performance_context("phase1_data_discovery"):
                schema_agent = create_agent(AgentType.SCHEMA_ANALYSIS, db_session=db)
                
                # æ¨¡æ‹Ÿæ•°æ®æºä¿¡æ¯
                mock_data_source = {
                    "type": "postgresql",
                    "host": "localhost",
                    "port": 5432,
                    "database": "autoreport",
                    "tables": ["users", "reports", "tasks"]
                }
                
                print(f"     ğŸ” å‘ç°æ•°æ®æº: {mock_data_source['type']}")
                print(f"     ğŸ“‹ å‘ç°è¡¨æ ¼: {', '.join(mock_data_source['tables'])}")
                
                # æ¨¡æ‹ŸSchemaåˆ†æç»“æœ
                schema_analysis = {
                    "database_type": mock_data_source["type"],
                    "total_tables": len(mock_data_source["tables"]),
                    "schema_complexity": "medium",
                    "recommended_queries": [
                        "SELECT COUNT(*) FROM users",
                        "SELECT status, COUNT(*) FROM tasks GROUP BY status",
                        "SELECT created_at, COUNT(*) FROM reports GROUP BY DATE(created_at)"
                    ],
                    "data_relationships": {
                        "users -> tasks": "one-to-many",
                        "users -> reports": "one-to-many"
                    }
                }
                
                print(f"     âœ… Schemaåˆ†æå®Œæˆï¼Œå‘ç° {schema_analysis['total_tables']} ä¸ªè¡¨")
            
            print("\n  ğŸ“ˆ ç¬¬äºŒé˜¶æ®µï¼šæŠ¥å‘Šç”Ÿæˆ...")
            
            # ç¬¬äºŒé˜¶æ®µï¼šæŠ¥å‘Šç”Ÿæˆ
            with performance_context("phase2_report_generation"):
                content_agent = create_agent(AgentType.CONTENT_GENERATION, db_session=db)
                
                # æ¨¡æ‹ŸæŸ¥è¯¢æ‰§è¡Œç»“æœ
                mock_query_results = {
                    "user_count": 150,
                    "task_status_distribution": {
                        "completed": 45,
                        "in_progress": 23,
                        "pending": 12
                    },
                    "daily_reports": {
                        "2025-08-15": 8,
                        "2025-08-16": 12,
                        "2025-08-17": 15
                    }
                }
                
                print(f"     ğŸ“Š æ‰§è¡ŒæŸ¥è¯¢è·å–æ•°æ®...")
                print(f"     ğŸ‘¥ ç”¨æˆ·æ€»æ•°: {mock_query_results['user_count']}")
                print(f"     ğŸ“‹ ä»»åŠ¡åˆ†å¸ƒ: {mock_query_results['task_status_distribution']}")
                
                # ç”Ÿæˆåˆ†ææŠ¥å‘Š
                report_content = generate_analysis_report(schema_analysis, mock_query_results)
                
                print(f"     âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼Œå…± {len(report_content)} å­—ç¬¦")
            
            print("\n  ğŸ“‹ ä¸¤æ®µå¼ä»»åŠ¡æ‰§è¡Œç»“æœ:")
            print("     âœ… ç¬¬ä¸€é˜¶æ®µï¼šæ•°æ®å‘ç°å’ŒSchemaåˆ†æ - å®Œæˆ")
            print("     âœ… ç¬¬äºŒé˜¶æ®µï¼šæŸ¥è¯¢æ‰§è¡Œå’ŒæŠ¥å‘Šç”Ÿæˆ - å®Œæˆ")
            
            return {
                "phase1_result": schema_analysis,
                "phase2_result": report_content,
                "execution_status": "success"
            }
            
    except Exception as e:
        print(f"     âŒ ä¸¤æ®µå¼ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
        return {"execution_status": "failed", "error": str(e)}

def generate_analysis_report(schema_analysis: Dict[str, Any], query_results: Dict[str, Any]) -> str:
    """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
    report = f"""
# æ•°æ®åˆ†ææŠ¥å‘Š

## æ•°æ®æºæ¦‚è§ˆ
- æ•°æ®åº“ç±»å‹: {schema_analysis['database_type']}
- è¡¨æ ¼æ•°é‡: {schema_analysis['total_tables']}
- Schemaå¤æ‚åº¦: {schema_analysis['schema_complexity']}

## å…³é”®æŒ‡æ ‡
- ç”¨æˆ·æ€»æ•°: {query_results['user_count']}
- ä»»åŠ¡å®Œæˆç‡: {query_results['task_status_distribution']['completed'] / sum(query_results['task_status_distribution'].values()) * 100:.1f}%

## ä»»åŠ¡çŠ¶æ€åˆ†å¸ƒ
{json.dumps(query_results['task_status_distribution'], indent=2, ensure_ascii=False)}

## æ¯æ—¥æŠ¥å‘Šè¶‹åŠ¿
{json.dumps(query_results['daily_reports'], indent=2, ensure_ascii=False)}

## æ•°æ®å…³ç³»
{json.dumps(schema_analysis['data_relationships'], indent=2, ensure_ascii=False)}

## å»ºè®®æŸ¥è¯¢
{chr(10).join(['- ' + query for query in schema_analysis['recommended_queries']])}

---
ç”Ÿæˆæ—¶é—´: {datetime.now().isoformat()}
"""
    return report

async def test_performance_with_monitoring():
    """æµ‹è¯•æ€§èƒ½ç›‘æ§ä¸‹çš„ä»»åŠ¡æ‰§è¡Œ"""
    print("\nğŸ“Š æµ‹è¯•æ€§èƒ½ç›‘æ§...")
    
    from app.services.agents.core.performance_monitor import get_performance_monitor
    from app.services.agents.core.cache_manager import get_cache_manager
    
    try:
        monitor = get_performance_monitor()
        cache_manager = get_cache_manager()
        
        # å¯åŠ¨ç›‘æ§
        monitor.start_monitoring()
        
        # æ¨¡æ‹Ÿå¤šæ¬¡ä»»åŠ¡æ‰§è¡Œæ¥æµ‹è¯•ç¼“å­˜
        print("  ğŸ”„ æ‰§è¡Œå¤šè½®ä»»åŠ¡æµ‹è¯•ç¼“å­˜æ•ˆæœ...")
        
        results = []
        for i in range(3):
            print(f"     ç¬¬ {i+1} è½®æ‰§è¡Œ...")
            result = await test_two_phase_task_execution()
            results.append(result)
        
        # è·å–æ€§èƒ½ç»Ÿè®¡
        perf_summary = monitor.get_performance_summary()
        cache_stats = cache_manager.get_global_stats()
        
        print(f"\n  ğŸ“ˆ æ€§èƒ½ç›‘æ§ç»“æœ:")
        print(f"     æ‰§è¡Œè½®æ•°: {len(results)}")
        print(f"     ç¼“å­˜å‘½ä¸­ç‡: {cache_stats['global']['global_hit_rate']:.2%}")
        print(f"     æ€»ç¼“å­˜é¡¹ç›®: {cache_stats['global']['total_size']}")
        
        # åœæ­¢ç›‘æ§
        monitor.stop_monitoring()
        
        return {
            "rounds_executed": len(results),
            "performance_summary": perf_summary,
            "cache_statistics": cache_stats
        }
        
    except Exception as e:
        print(f"     âŒ æ€§èƒ½ç›‘æ§æµ‹è¯•å¤±è´¥: {e}")
        return {"error": str(e)}

async def test_health_monitoring_during_tasks():
    """æµ‹è¯•ä»»åŠ¡æ‰§è¡ŒæœŸé—´çš„å¥åº·ç›‘æ§"""
    print("\nğŸ¥ æµ‹è¯•å¥åº·ç›‘æ§...")
    
    from app.services.agents.core.health_monitor import get_health_monitor, perform_system_health_check
    
    try:
        monitor = get_health_monitor()
        
        # æ‰§è¡Œä»»åŠ¡å‰å¥åº·æ£€æŸ¥
        print("  ğŸ” ä»»åŠ¡æ‰§è¡Œå‰å¥åº·æ£€æŸ¥...")
        health_before = await perform_system_health_check()
        print(f"     ç³»ç»ŸçŠ¶æ€: {health_before['overall_status']}")
        
        # æ‰§è¡Œä»»åŠ¡
        print("  ğŸ”„ æ‰§è¡Œä»»åŠ¡...")
        task_result = await test_two_phase_task_execution()
        
        # æ‰§è¡Œä»»åŠ¡åå¥åº·æ£€æŸ¥
        print("  ğŸ” ä»»åŠ¡æ‰§è¡Œåå¥åº·æ£€æŸ¥...")
        health_after = await perform_system_health_check()
        print(f"     ç³»ç»ŸçŠ¶æ€: {health_after['overall_status']}")
        
        # æ¯”è¾ƒå¥åº·çŠ¶æ€
        health_comparison = {
            "before": health_before,
            "after": health_after,
            "task_result": task_result,
            "health_stable": health_before['overall_status'] == health_after['overall_status']
        }
        
        print(f"     å¥åº·çŠ¶æ€ç¨³å®š: {'âœ…' if health_comparison['health_stable'] else 'âŒ'}")
        
        return health_comparison
        
    except Exception as e:
        print(f"     âŒ å¥åº·ç›‘æ§æµ‹è¯•å¤±è´¥: {e}")
        return {"error": str(e)}

async def run_comprehensive_two_phase_test():
    """è¿è¡Œå®Œæ•´çš„ä¸¤æ®µå¼ä»»åŠ¡æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹ä¸¤æ®µå¼ä»»åŠ¡ç³»ç»Ÿå…¨é¢æµ‹è¯•")
    print("=" * 60)
    
    test_results = {}
    
    # 1. åŸºç¡€ä¸¤æ®µå¼ä»»åŠ¡æµ‹è¯•
    print("\n1ï¸âƒ£ åŸºç¡€ä¸¤æ®µå¼ä»»åŠ¡æµ‹è¯•")
    test_results["basic_task"] = await test_two_phase_task_execution()
    
    # 2. æ€§èƒ½ç›‘æ§æµ‹è¯•
    print("\n2ï¸âƒ£ æ€§èƒ½ç›‘æ§æµ‹è¯•")
    test_results["performance_monitoring"] = await test_performance_with_monitoring()
    
    # 3. å¥åº·ç›‘æ§æµ‹è¯•
    print("\n3ï¸âƒ£ å¥åº·ç›‘æ§æµ‹è¯•")
    test_results["health_monitoring"] = await test_health_monitoring_during_tasks()
    
    # è¾“å‡ºæ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ“Š ä¸¤æ®µå¼ä»»åŠ¡æµ‹è¯•æ€»ç»“")
    print("=" * 60)
    
    success_count = 0
    total_tests = len(test_results)
    
    for test_name, result in test_results.items():
        if isinstance(result, dict) and result.get("execution_status") == "success":
            success_count += 1
            status = "âœ… æˆåŠŸ"
        elif isinstance(result, dict) and "error" not in result:
            success_count += 1
            status = "âœ… æˆåŠŸ"
        else:
            status = "âŒ å¤±è´¥"
        
        print(f"{test_name:<25} {status}")
    
    print("-" * 60)
    print(f"æ€»æµ‹è¯•æ•°: {total_tests}")
    print(f"æˆåŠŸæ•°: {success_count}")
    print(f"æˆåŠŸç‡: {success_count/total_tests*100:.1f}%")
    print("=" * 60)
    
    if success_count == total_tests:
        print("ğŸ‰ ä¸¤æ®µå¼ä»»åŠ¡ç³»ç»Ÿæµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
    
    return test_results

if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    asyncio.run(run_comprehensive_two_phase_test())