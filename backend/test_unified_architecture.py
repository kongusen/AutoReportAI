"""
ç»Ÿä¸€æ¶æ„æµ‹è¯• - éªŒè¯é‡æ„åçš„AIæ ¸å¿ƒç³»ç»Ÿ
åŸºäºClaude Codeç†å¿µçš„é‡æ„æ•ˆæœéªŒè¯
"""

import asyncio
import logging
from datetime import datetime
from typing import List, Dict, Any

# å¯¼å…¥é‡æ„åçš„æ ¸å¿ƒç»„ä»¶
from app.services.infrastructure.ai.core import (
    # æ–°çš„ç»Ÿä¸€æ¶æ„
    tt, get_unified_controller, get_auto_report_ai,
    
    # å®‰å…¨ç³»ç»Ÿ
    get_security_checker, SecurityLevel,
    
    # APIæ¶ˆæ¯ç³»ç»Ÿ
    APIMessage, MessageConverter,
    
    # å·¥å…·å’Œä¸Šä¸‹æ–‡
    ToolContext, AgentMessage,
    
    # å…¼å®¹æ€§å±‚
    execute_task_unified, get_compatibility_layer,
    
    # æ—§ç³»ç»Ÿï¼ˆç”¨äºå¯¹æ¯”ï¼‰
    AgentController, AgentTask, TaskType
)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ArchitectureTestSuite:
    """
    æ¶æ„é‡æ„æµ‹è¯•å¥—ä»¶
    
    æµ‹è¯•å†…å®¹ï¼š
    1. æ–°ç»Ÿä¸€æ¶æ„çš„åŠŸèƒ½å®Œæ•´æ€§
    2. å®‰å…¨æ£€æŸ¥æœºåˆ¶çš„æœ‰æ•ˆæ€§
    3. å…¼å®¹æ€§å±‚çš„å¹³æ»‘è¿ç§»
    4. æ€§èƒ½å’Œå¯é æ€§å¯¹æ¯”
    """
    
    def __init__(self):
        self.test_results = {}
        self.performance_metrics = {}
        
    async def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        logger.info("ğŸ§ª å¼€å§‹æ¶æ„é‡æ„éªŒè¯æµ‹è¯•")
        
        tests = [
            ("åŸºç¡€æ¶ˆæ¯ç³»ç»Ÿæµ‹è¯•", self.test_message_system),
            ("å®‰å…¨æ£€æŸ¥ç³»ç»Ÿæµ‹è¯•", self.test_security_system),
            ("ç»Ÿä¸€æ§åˆ¶å™¨æµ‹è¯•", self.test_unified_controller),
            ("å…¼å®¹æ€§å±‚æµ‹è¯•", self.test_compatibility_layer),
            ("æ€§èƒ½å¯¹æ¯”æµ‹è¯•", self.test_performance_comparison),
            ("ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•", self.test_end_to_end_integration)
        ]
        
        for test_name, test_func in tests:
            logger.info(f"ğŸ” æ‰§è¡Œæµ‹è¯•: {test_name}")
            try:
                start_time = datetime.now()
                result = await test_func()
                duration = (datetime.now() - start_time).total_seconds()
                
                self.test_results[test_name] = {
                    "status": "passed" if result else "failed",
                    "duration": duration,
                    "details": result
                }
                
                logger.info(f"âœ… {test_name} {'é€šè¿‡' if result else 'å¤±è´¥'} ({duration:.2f}s)")
                
            except Exception as e:
                logger.error(f"âŒ {test_name} å¼‚å¸¸: {e}")
                self.test_results[test_name] = {
                    "status": "error",
                    "error": str(e)
                }
        
        # è¾“å‡ºæµ‹è¯•æ€»ç»“
        self.print_test_summary()
        return self.test_results
    
    async def test_message_system(self) -> bool:
        """æµ‹è¯•æ¶ˆæ¯ç³»ç»Ÿçš„åŒé‡è¡¨ç¤ºåŠŸèƒ½"""
        logger.info("æµ‹è¯• AgentMessage <-> APIMessage è½¬æ¢")
        
        try:
            # åˆ›å»ºåŸå§‹æ¶ˆæ¯
            agent_msg = AgentMessage.create_progress(
                current_step="æµ‹è¯•æ­¥éª¤",
                user_id="test_user",
                task_id="test_task",
                percentage=50.0,
                details="æµ‹è¯•è¯¦æƒ…"
            )
            
            # è½¬æ¢ä¸ºAPIæ¶ˆæ¯
            api_msg = agent_msg.to_api_message()
            
            # éªŒè¯è½¬æ¢ç»“æœ
            assert api_msg.role == "assistant"
            assert "æµ‹è¯•æ­¥éª¤" in api_msg.content
            
            # æµ‹è¯•æ¶ˆæ¯è½¬æ¢å™¨
            converter = MessageConverter()
            api_messages = converter.agent_messages_to_api_messages([agent_msg])
            
            assert len(api_messages) == 1
            assert isinstance(api_messages[0], APIMessage)
            
            logger.info("âœ… æ¶ˆæ¯ç³»ç»Ÿè½¬æ¢æ­£å¸¸")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ¶ˆæ¯ç³»ç»Ÿæµ‹è¯•å¤±è´¥: {e}")
            return False
    
    async def test_security_system(self) -> bool:
        """æµ‹è¯•å®‰å…¨æ£€æŸ¥ç³»ç»Ÿ"""
        logger.info("æµ‹è¯•å¤šå±‚å®‰å…¨æ£€æŸ¥æœºåˆ¶")
        
        try:
            security_checker = get_security_checker()
            
            # æµ‹è¯•å®‰å…¨å·¥å…·æ‰§è¡Œ
            safe_result = await security_checker.check_tool_execution(
                "template_analysis_tool",
                {"template_id": "test_template"}
            )
            assert safe_result.allowed == True
            assert safe_result.level == SecurityLevel.SAFE
            
            # æµ‹è¯•å±é™©æ“ä½œæ£€æµ‹
            dangerous_result = await security_checker.check_tool_execution(
                "sql_execution_tool",
                {"sql": "DROP DATABASE test; rm -rf /"}
            )
            assert dangerous_result.level in [SecurityLevel.FORBIDDEN, SecurityLevel.HIGH_RISK]
            
            # æµ‹è¯•ä¸­ç­‰é£é™©æ“ä½œ
            medium_risk_result = await security_checker.check_tool_execution(
                "bash_tool",
                {"command": "ls -la && grep something"}
            )
            
            logger.info("âœ… å®‰å…¨æ£€æŸ¥æœºåˆ¶æ­£å¸¸")
            logger.info(f"å®‰å…¨ç»Ÿè®¡: {security_checker.get_security_statistics()}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ å®‰å…¨ç³»ç»Ÿæµ‹è¯•å¤±è´¥: {e}")
            return False
    
    async def test_unified_controller(self) -> bool:
        """æµ‹è¯•ç»Ÿä¸€æ§åˆ¶å™¨çš„ttå‡½æ•°"""
        logger.info("æµ‹è¯•ç»Ÿä¸€æ§åˆ¶å™¨çš„æ ¸å¿ƒttå‡½æ•°")
        
        try:
            # åˆ›å»ºæµ‹è¯•ä¸Šä¸‹æ–‡
            context = ToolContext(
                user_id="test_user",
                task_id="test_task_unified",
                session_id="test_session",
                context_data={"test": "data"}
            )
            
            # æµ‹è¯•ç®€å•ä»»åŠ¡
            goal = "åˆ†ææµ‹è¯•å ä½ç¬¦å¹¶ç”Ÿæˆç›¸åº”çš„æŸ¥è¯¢"
            results = []
            
            # æ”¶é›†ttå‡½æ•°çš„è¾“å‡º
            async for message in tt(goal, context, max_iterations=2):
                results.append(message)
                logger.info(f"æ”¶åˆ°æ¶ˆæ¯: {message.type.value} - {message.get_display_text()}")
            
            # éªŒè¯ç»“æœ
            assert len(results) > 0
            
            # åº”è¯¥æœ‰è¿›åº¦æ¶ˆæ¯å’Œæœ€ç»ˆç»“æœ
            progress_messages = [r for r in results if r.type.value == "progress"]
            result_messages = [r for r in results if r.type.value == "result"]
            
            assert len(progress_messages) > 0, "åº”è¯¥æœ‰è¿›åº¦æ¶ˆæ¯"
            logger.info(f"è¿›åº¦æ¶ˆæ¯æ•°é‡: {len(progress_messages)}")
            logger.info(f"ç»“æœæ¶ˆæ¯æ•°é‡: {len(result_messages)}")
            
            # æµ‹è¯•æ§åˆ¶å™¨ç»Ÿè®¡
            controller = get_unified_controller()
            stats = controller.get_statistics()
            logger.info(f"æ§åˆ¶å™¨ç»Ÿè®¡: {stats}")
            
            logger.info("âœ… ç»Ÿä¸€æ§åˆ¶å™¨æµ‹è¯•æ­£å¸¸")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ç»Ÿä¸€æ§åˆ¶å™¨æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    async def test_compatibility_layer(self) -> bool:
        """æµ‹è¯•å…¼å®¹æ€§å±‚çš„è¿ç§»åŠŸèƒ½"""
        logger.info("æµ‹è¯•æ–°æ—§ç³»ç»Ÿå…¼å®¹æ€§")
        
        try:
            # åˆ›å»ºæ—§æ ¼å¼ä»»åŠ¡
            old_task = AgentTask(
                type=TaskType.PLACEHOLDER_ANALYSIS,
                task_id="compat_test_task",
                user_id="test_user",
                data={
                    "placeholder_name": "test_placeholder",
                    "placeholder_text": "{{test}}",
                    "template_context": "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å ä½ç¬¦"
                }
            )
            
            # ä½¿ç”¨å…¼å®¹æ€§å±‚æ‰§è¡Œ
            results = []
            async for message in execute_task_unified(old_task):
                results.append(message)
                logger.info(f"å…¼å®¹å±‚æ¶ˆæ¯: {message.type.value}")
            
            # éªŒè¯å…¼å®¹æ€§
            assert len(results) > 0
            
            # æ£€æŸ¥è¿ç§»ç»Ÿè®¡
            compat_layer = get_compatibility_layer()
            migration_stats = compat_layer.get_migration_statistics()
            logger.info(f"è¿ç§»ç»Ÿè®¡: {migration_stats}")
            
            # éªŒè¯æ–°ç³»ç»Ÿä½¿ç”¨ç‡
            assert migration_stats["total_usage"] > 0
            
            logger.info("âœ… å…¼å®¹æ€§å±‚æµ‹è¯•æ­£å¸¸")
            return True
            
        except Exception as e:
            logger.error(f"âŒ å…¼å®¹æ€§å±‚æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    async def test_performance_comparison(self) -> bool:
        """æ€§èƒ½å¯¹æ¯”æµ‹è¯•ï¼šæ–°ç³»ç»Ÿ vs æ—§ç³»ç»Ÿ"""
        logger.info("æ‰§è¡Œæ€§èƒ½å¯¹æ¯”æµ‹è¯•")
        
        try:
            # æµ‹è¯•æ•°æ®
            test_task = AgentTask(
                type=TaskType.PLACEHOLDER_ANALYSIS,
                task_id="perf_test",
                user_id="test_user",
                data={
                    "placeholder_name": "performance_test",
                    "placeholder_text": "{{perf_test}}",
                    "template_context": "æ€§èƒ½æµ‹è¯•å ä½ç¬¦"
                }
            )
            
            # æµ‹è¯•æ–°ç³»ç»Ÿæ€§èƒ½
            new_system_start = datetime.now()
            new_results = []
            async for msg in execute_task_unified(test_task):
                new_results.append(msg)
            new_system_duration = (datetime.now() - new_system_start).total_seconds()
            
            # æµ‹è¯•æ—§ç³»ç»Ÿæ€§èƒ½ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            old_system_duration = 0
            old_results = []
            try:
                old_controller = AgentController()
                old_system_start = datetime.now()
                async for msg in old_controller.execute_task(test_task):
                    old_results.append(msg)
                old_system_duration = (datetime.now() - old_system_start).total_seconds()
            except Exception as e:
                logger.warning(f"æ—§ç³»ç»Ÿæµ‹è¯•è·³è¿‡: {e}")
                old_system_duration = float('inf')  # è¡¨ç¤ºæ— æ³•æµ‹è¯•
            
            # è®°å½•æ€§èƒ½æŒ‡æ ‡
            self.performance_metrics = {
                "new_system": {
                    "duration": new_system_duration,
                    "message_count": len(new_results),
                    "avg_message_time": new_system_duration / len(new_results) if new_results else 0
                },
                "old_system": {
                    "duration": old_system_duration,
                    "message_count": len(old_results),
                    "avg_message_time": old_system_duration / len(old_results) if old_results else 0
                }
            }
            
            logger.info(f"æ–°ç³»ç»Ÿè€—æ—¶: {new_system_duration:.2f}s ({len(new_results)} æ¶ˆæ¯)")
            if old_system_duration != float('inf'):
                logger.info(f"æ—§ç³»ç»Ÿè€—æ—¶: {old_system_duration:.2f}s ({len(old_results)} æ¶ˆæ¯)")
                speedup = old_system_duration / new_system_duration if new_system_duration > 0 else 0
                logger.info(f"æ€§èƒ½æå‡: {speedup:.2f}x")
            
            logger.info("âœ… æ€§èƒ½å¯¹æ¯”æµ‹è¯•å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ€§èƒ½å¯¹æ¯”æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    async def test_end_to_end_integration(self) -> bool:
        """ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•"""
        logger.info("æ‰§è¡Œç«¯åˆ°ç«¯é›†æˆæµ‹è¯•")
        
        try:
            # è·å–AutoReportAIå®ä¾‹
            ai_system = get_auto_report_ai()
            
            # åˆ›å»ºå¤æ‚çš„æµ‹è¯•åœºæ™¯
            context = ToolContext(
                user_id="integration_test_user",
                task_id="integration_test",
                session_id="integration_session",
                context_data={
                    "template_id": "test_template",
                    "data_source_id": "test_datasource"
                }
            )
            
            # æ‰§è¡Œå¤æ‚ä»»åŠ¡
            goal = "æ‰§è¡Œå®Œæ•´çš„æ¨¡æ¿åˆ†æå·¥ä½œæµï¼ŒåŒ…æ‹¬å ä½ç¬¦è¯†åˆ«ã€SQLç”Ÿæˆå’Œæ•°æ®æŸ¥è¯¢"
            
            messages = []
            start_time = datetime.now()
            
            async for message in ai_system.process_task(goal, context):
                messages.append(message)
                logger.info(f"é›†æˆæµ‹è¯•æ¶ˆæ¯: {message.type.value} - {message.get_display_text()[:100]}")
            
            duration = (datetime.now() - start_time).total_seconds()
            
            # éªŒè¯é›†æˆç»“æœ
            assert len(messages) > 0
            
            # æ£€æŸ¥ç³»ç»Ÿç»Ÿè®¡
            system_stats = ai_system.get_system_statistics()
            logger.info(f"ç³»ç»Ÿç»Ÿè®¡: {system_stats}")
            
            # éªŒè¯å„å­ç³»ç»Ÿåä½œ
            assert system_stats["total_requests"] > 0
            
            logger.info(f"âœ… ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•å®Œæˆ ({duration:.2f}s, {len(messages)} æ¶ˆæ¯)")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def print_test_summary(self):
        """æ‰“å°æµ‹è¯•æ€»ç»“"""
        logger.info("=" * 60)
        logger.info("ğŸ† æ¶æ„é‡æ„æµ‹è¯•æ€»ç»“")
        logger.info("=" * 60)
        
        total_tests = len(self.test_results)
        passed_tests = sum(1 for r in self.test_results.values() if r.get("status") == "passed")
        failed_tests = sum(1 for r in self.test_results.values() if r.get("status") == "failed")
        error_tests = sum(1 for r in self.test_results.values() if r.get("status") == "error")
        
        logger.info(f"æ€»æµ‹è¯•æ•°: {total_tests}")
        logger.info(f"é€šè¿‡: {passed_tests} âœ…")
        logger.info(f"å¤±è´¥: {failed_tests} âŒ")
        logger.info(f"å¼‚å¸¸: {error_tests} ğŸ’¥")
        logger.info(f"æˆåŠŸç‡: {passed_tests/total_tests*100:.1f}%")
        
        # è¯¦ç»†ç»“æœ
        for test_name, result in self.test_results.items():
            status_emoji = {"passed": "âœ…", "failed": "âŒ", "error": "ğŸ’¥"}.get(result["status"], "â“")
            duration = result.get("duration", 0)
            logger.info(f"{status_emoji} {test_name}: {result['status']} ({duration:.2f}s)")
        
        # æ€§èƒ½æŒ‡æ ‡
        if self.performance_metrics:
            logger.info("\nğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
            for system, metrics in self.performance_metrics.items():
                logger.info(f"{system}: {metrics['duration']:.2f}s ({metrics['message_count']} æ¶ˆæ¯)")
        
        logger.info("=" * 60)


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨AutoReportAIæ¶æ„é‡æ„éªŒè¯")
    
    test_suite = ArchitectureTestSuite()
    results = await test_suite.run_all_tests()
    
    # è¯„ä¼°é‡æ„æˆåŠŸåº¦
    passed_count = sum(1 for r in results.values() if r.get("status") == "passed")
    total_count = len(results)
    success_rate = passed_count / total_count * 100
    
    print(f"\nğŸ¯ é‡æ„éªŒè¯ç»“æœ: {success_rate:.1f}% æˆåŠŸç‡")
    
    if success_rate >= 80:
        print("ğŸ‰ é‡æ„åŸºæœ¬æˆåŠŸï¼æ–°æ¶æ„è¿è¡Œè‰¯å¥½ã€‚")
    elif success_rate >= 60:
        print("âš ï¸ é‡æ„éƒ¨åˆ†æˆåŠŸï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚")
    else:
        print("âŒ é‡æ„å­˜åœ¨é‡å¤§é—®é¢˜ï¼Œéœ€è¦ä¿®å¤ã€‚")
    
    return results


if __name__ == "__main__":
    asyncio.run(main())