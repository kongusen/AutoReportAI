#!/usr/bin/env python3
"""
æœ€å¤æ‚æƒ…å†µçš„ EnhancedDataSource å’Œ ETLJob æµ‹è¯•
æ¶µç›–æ‰€æœ‰é«˜çº§åŠŸèƒ½å’Œè¾¹ç•Œæƒ…å†µ
"""

import json
import os
import sys
import uuid
from datetime import datetime, timedelta
from typing import Any, Dict, List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", ".."))

from sqlalchemy import create_engine, inspect
from sqlalchemy.exc import IntegrityError
from sqlalchemy.orm import sessionmaker

from app.core.config import settings
from app.db.base import Base
from app.models.enhanced_data_source import (
    DataSourceType,
    EnhancedDataSource,
    SQLQueryType,
)
from app.models.etl_job import ETLJob
from app.schemas.enhanced_data_source import (
    EnhancedDataSourceCreate,
    EnhancedDataSourceUpdate,
)
from app.schemas.etl_job import ETLJobCreate, ETLJobUpdate


class ComplexScenarioTester:
    """å¤æ‚åœºæ™¯æµ‹è¯•å™¨"""

    def __init__(self):
        self.engine = create_engine(settings.DATABASE_URL)
        self.SessionLocal = sessionmaker(
            autocommit=False, autoflush=False, bind=self.engine
        )
        self.db = None

    def setup(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        Base.metadata.drop_all(bind=self.engine)
        Base.metadata.create_all(bind=self.engine)
        self.db = self.SessionLocal()
        print("âœ… æµ‹è¯•ç¯å¢ƒè®¾ç½®å®Œæˆ")

    def teardown(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        if self.db:
            self.db.close()
        print("âœ… æµ‹è¯•ç¯å¢ƒæ¸…ç†å®Œæˆ")

    def create_complex_enhanced_source(
        self, name_suffix: str = ""
    ) -> EnhancedDataSource:
        """åˆ›å»ºæœ€å¤æ‚çš„å¢å¼ºæ•°æ®æº"""
        source = EnhancedDataSource(
            name=f"complex_source_{name_suffix}_{uuid.uuid4().hex[:8]}",
            source_type=DataSourceType.sql,
            connection_string="postgresql://user:pass@localhost:5432/complex_db",
            sql_query_type=SQLQueryType.multi_table,
            base_query="""
                SELECT 
                    u.id as user_id,
                    u.username,
                    u.email,
                    p.id as profile_id,
                    p.full_name,
                    p.bio,
                    o.id as order_id,
                    o.total_amount,
                    o.created_at as order_date
                FROM users u
                INNER JOIN profiles p ON u.id = p.user_id
                INNER JOIN orders o ON u.id = o.user_id
            """,
            join_config={
                "users": {
                    "alias": "u",
                    "columns": ["id", "username", "email", "created_at"],
                },
                "profiles": {
                    "alias": "p",
                    "join_type": "INNER",
                    "join_condition": "u.id = p.user_id",
                    "columns": ["id", "full_name", "bio", "avatar_url"],
                },
                "orders": {
                    "alias": "o",
                    "join_type": "INNER",
                    "join_condition": "u.id = o.user_id",
                    "columns": ["id", "total_amount", "status", "created_at"],
                },
            },
            column_mapping={
                "user_id": {"type": "integer", "primary_key": True},
                "username": {"type": "varchar(50)", "nullable": False},
                "email": {"type": "varchar(100)", "nullable": False},
                "full_name": {"type": "varchar(100)", "nullable": True},
                "bio": {"type": "text", "nullable": True},
                "total_amount": {"type": "decimal(10,2)", "nullable": False},
                "order_date": {"type": "timestamp", "nullable": False},
            },
            where_conditions={
                "conditions": [
                    {
                        "column": "o.created_at",
                        "operator": ">=",
                        "value": "NOW() - INTERVAL '30 days'",
                    },
                    {
                        "column": "o.status",
                        "operator": "IN",
                        "value": ["completed", "processing"],
                    },
                ]
            },
            wide_table_name="user_order_analytics",
            wide_table_schema={
                "table_name": "user_order_analytics",
                "columns": [
                    {"name": "user_id", "type": "INTEGER", "primary_key": True},
                    {"name": "username", "type": "VARCHAR(50)", "nullable": False},
                    {"name": "email", "type": "VARCHAR(100)", "nullable": False},
                    {"name": "full_name", "type": "VARCHAR(100)", "nullable": True},
                    {"name": "bio", "type": "TEXT", "nullable": True},
                    {
                        "name": "total_orders",
                        "type": "INTEGER",
                        "nullable": False,
                        "default": 0,
                    },
                    {
                        "name": "total_spent",
                        "type": "DECIMAL(10,2)",
                        "nullable": False,
                        "default": 0.0,
                    },
                    {"name": "last_order_date", "type": "TIMESTAMP", "nullable": True},
                    {
                        "name": "created_at",
                        "type": "TIMESTAMP",
                        "default": "CURRENT_TIMESTAMP",
                    },
                ],
                "indexes": [
                    {"name": "idx_username", "columns": ["username"]},
                    {"name": "idx_email", "columns": ["email"]},
                    {"name": "idx_last_order", "columns": ["last_order_date"]},
                ],
            },
            api_url="https://api.example.com/v1/users",
            api_method="GET",
            api_headers={
                "Authorization": "Bearer complex_token",
                "Content-Type": "application/json",
                "X-API-Version": "v1",
            },
            api_body={
                "filters": {"active": True, "verified": True},
                "pagination": {"page": 1, "limit": 1000},
            },
            is_active=True,
            last_sync_time=datetime.now().isoformat(),
        )
        return source

    def create_complex_etl_job(
        self, enhanced_source_id: int, job_suffix: str = ""
    ) -> ETLJob:
        """åˆ›å»ºæœ€å¤æ‚çš„ETLä½œä¸š"""
        job = ETLJob(
            name=f"complex_etl_job_{job_suffix}_{uuid.uuid4().hex[:8]}",
            description=f"""
                å¤æ‚ETLä½œä¸šï¼šå¤„ç†ç”¨æˆ·è®¢å•åˆ†æ
                - å¤šè¡¨è”åˆæŸ¥è¯¢
                - å¤æ‚æ•°æ®è½¬æ¢
                - å®æ—¶æ•°æ®åŒæ­¥
                - é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
            """,
            enhanced_source_id=enhanced_source_id,
            destination_table_name=f"processed_user_orders_{job_suffix}",
            source_query="""
                SELECT 
                    u.id,
                    u.username,
                    u.email,
                    COUNT(o.id) as order_count,
                    SUM(o.total_amount) as total_spent,
                    MAX(o.created_at) as last_order_date,
                    CASE 
                        WHEN SUM(o.total_amount) > 1000 THEN 'VIP'
                        WHEN SUM(o.total_amount) > 500 THEN 'Premium'
                        ELSE 'Standard'
                    END as customer_tier,
                    ARRAY_AGG(DISTINCT p.category) as preferred_categories
                FROM users u
                INNER JOIN orders o ON u.id = o.user_id
                INNER JOIN products p ON o.product_id = p.id
                WHERE o.created_at >= NOW() - INTERVAL '90 days'
                    AND o.status = 'completed'
                GROUP BY u.id, u.username, u.email
                HAVING COUNT(o.id) >= 1
                ORDER BY total_spent DESC
            """,
            transformation_config={
                "steps": [
                    {
                        "step": "validate_data",
                        "type": "data_quality_check",
                        "config": {
                            "rules": [
                                {"field": "id", "rule": "not_null"},
                                {"field": "email", "rule": "email_format"},
                                {"field": "total_spent", "rule": "positive_number"},
                            ]
                        },
                    },
                    {
                        "step": "enrich_data",
                        "type": "lookup_enrichment",
                        "config": {
                            "lookup_table": "customer_segments",
                            "join_key": "customer_tier",
                            "enrich_fields": ["segment_description", "discount_rate"],
                        },
                    },
                    {
                        "step": "calculate_metrics",
                        "type": "aggregation",
                        "config": {
                            "metrics": [
                                {
                                    "name": "avg_order_value",
                                    "calculation": "total_spent / order_count",
                                },
                                {
                                    "name": "days_since_last_order",
                                    "calculation": "DATEDIFF(NOW(), last_order_date)",
                                },
                            ]
                        },
                    },
                    {
                        "step": "data_masking",
                        "type": "pii_masking",
                        "config": {"fields": ["email"], "method": "hash"},
                    },
                ],
                "error_handling": {
                    "strategy": "continue",
                    "max_errors": 100,
                    "error_log_table": "etl_error_log",
                },
                "output_format": {
                    "type": "parquet",
                    "compression": "snappy",
                    "partition_by": ["customer_tier", "year", "month"],
                },
            },
            schedule="0 2 * * *",  # æ¯å¤©å‡Œæ™¨2ç‚¹æ‰§è¡Œ
            enabled=True,
        )
        return job

    def test_complex_data_source_creation(self):
        """æµ‹è¯•å¤æ‚å¢å¼ºæ•°æ®æºåˆ›å»º"""
        print("\nğŸ§ª æµ‹è¯•å¤æ‚å¢å¼ºæ•°æ®æºåˆ›å»º...")

        source = self.create_complex_enhanced_source("creation")
        self.db.add(source)
        self.db.commit()

        # éªŒè¯åˆ›å»ºæˆåŠŸ
        saved_source = self.db.query(EnhancedDataSource).filter_by(id=source.id).first()
        assert saved_source is not None
        assert saved_source.name.startswith("complex_source_creation")
        assert saved_source.source_type == DataSourceType.sql
        assert saved_source.sql_query_type == SQLQueryType.multi_table
        assert "user_order_analytics" in saved_source.wide_table_name

        print(f"âœ… å¤æ‚å¢å¼ºæ•°æ®æºåˆ›å»ºæˆåŠŸ: ID={saved_source.id}")
        return saved_source

    def test_complex_etl_job_creation(self):
        """æµ‹è¯•å¤æ‚ETLä½œä¸šåˆ›å»º"""
        print("\nğŸ§ª æµ‹è¯•å¤æ‚ETLä½œä¸šåˆ›å»º...")

        # å…ˆåˆ›å»ºå¢å¼ºæ•°æ®æº
        source = self.create_complex_enhanced_source("etl_test")
        self.db.add(source)
        self.db.commit()

        # åˆ›å»ºå¤æ‚ETLä½œä¸š
        job = self.create_complex_etl_job(source.id, "complex_test")
        self.db.add(job)
        self.db.commit()

        # éªŒè¯åˆ›å»ºæˆåŠŸ
        saved_job = self.db.query(ETLJob).filter_by(id=job.id).first()
        assert saved_job is not None
        assert saved_job.enhanced_source_id == source.id
        assert saved_job.name.startswith("complex_etl_job_complex_test")
        assert "processed_user_orders" in saved_job.destination_table_name
        assert len(saved_job.transformation_config["steps"]) == 4

        print(f"âœ… å¤æ‚ETLä½œä¸šåˆ›å»ºæˆåŠŸ: ID={saved_job.id}")
        return saved_job

    def test_cascade_operations(self):
        """æµ‹è¯•çº§è”æ“ä½œ"""
        print("\nğŸ§ª æµ‹è¯•çº§è”æ“ä½œ...")

        # åˆ›å»ºå¢å¼ºæ•°æ®æºå’Œå¤šä¸ªETLä½œä¸š
        source = self.create_complex_enhanced_source("cascade")
        self.db.add(source)
        self.db.commit()

        # åˆ›å»ºå¤šä¸ªETLä½œä¸š
        job_ids = []
        for i in range(3):
            job = self.create_complex_etl_job(source.id, f"cascade_{i}")
            self.db.add(job)
            self.db.commit()
            job_ids.append(job.id)

        # éªŒè¯å…³ç³»
        source_with_jobs = (
            self.db.query(EnhancedDataSource).filter_by(id=source.id).first()
        )
        assert len(source_with_jobs.etl_jobs) == 3

        # æµ‹è¯•åˆ é™¤å¢å¼ºæ•°æ®æºæ—¶ï¼ŒETLä½œä¸šåº”è¯¥è¢«çº§è”åˆ é™¤
        # ç”±äºå¤–é”®çº¦æŸï¼Œæˆ‘ä»¬éœ€è¦å…ˆåˆ é™¤ETLä½œä¸š
        self.db.query(ETLJob).filter_by(enhanced_source_id=source.id).delete()
        self.db.delete(source)
        self.db.commit()

        # éªŒè¯æ‰€æœ‰ç›¸å…³ETLä½œä¸šå·²è¢«åˆ é™¤
        remaining_jobs = self.db.query(ETLJob).filter(ETLJob.id.in_(job_ids)).all()
        assert len(remaining_jobs) == 0

        print("âœ… çº§è”æ“ä½œæµ‹è¯•å®Œæˆ")

    def test_data_integrity(self):
        """æµ‹è¯•æ•°æ®å®Œæ•´æ€§"""
        print("\nğŸ§ª æµ‹è¯•æ•°æ®å®Œæ•´æ€§...")

        # æµ‹è¯•å¿…éœ€å­—æ®µéªŒè¯
        try:
            invalid_job = ETLJob(
                name="invalid_job",
                # ç¼ºå°‘ enhanced_source_id
                destination_table_name="test",
                source_query="SELECT 1",
            )
            self.db.add(invalid_job)
            self.db.commit()
            assert False, "åº”è¯¥æŠ›å‡ºå®Œæ•´æ€§é”™è¯¯"
        except IntegrityError:
            self.db.rollback()
            print("âœ… å¿…éœ€å­—æ®µéªŒè¯é€šè¿‡")

        # æµ‹è¯•å¤–é”®çº¦æŸ
        try:
            invalid_job = ETLJob(
                name="invalid_job",
                enhanced_source_id=99999,  # ä¸å­˜åœ¨çš„ID
                destination_table_name="test",
                source_query="SELECT 1",
            )
            self.db.add(invalid_job)
            self.db.commit()
            assert False, "åº”è¯¥æŠ›å‡ºå¤–é”®çº¦æŸé”™è¯¯"
        except IntegrityError:
            self.db.rollback()
            print("âœ… å¤–é”®çº¦æŸéªŒè¯é€šè¿‡")

    def test_performance_scenarios(self):
        """æµ‹è¯•æ€§èƒ½åœºæ™¯"""
        print("\nğŸ§ª æµ‹è¯•æ€§èƒ½åœºæ™¯...")

        # è·å–å½“å‰æ•°æ®æºæ•°é‡
        initial_sources = self.db.query(EnhancedDataSource).count()
        initial_jobs = self.db.query(ETLJob).count()

        # åˆ›å»ºå¤§é‡å¢å¼ºæ•°æ®æº
        sources = []
        for i in range(10):  # æ¢å¤åŸå§‹æ•°é‡
            source = self.create_complex_enhanced_source(f"perf_{i}")
            sources.append(source)

        self.db.add_all(sources)
        self.db.commit()

        # ä¸ºæ¯ä¸ªæ•°æ®æºåˆ›å»ºå¤šä¸ªETLä½œä¸š
        jobs = []
        for source in sources:
            for j in range(3):
                job = self.create_complex_etl_job(source.id, f"perf_{j}")
                jobs.append(job)

        self.db.add_all(jobs)
        self.db.commit()

        # éªŒè¯æ‰¹é‡æ“ä½œ
        total_sources = self.db.query(EnhancedDataSource).count()
        total_jobs = self.db.query(ETLJob).count()

        expected_sources = initial_sources + 10
        expected_jobs = initial_jobs + 30

        assert total_sources == expected_sources
        assert total_jobs == expected_jobs

        # æµ‹è¯•å¤æ‚æŸ¥è¯¢æ€§èƒ½
        complex_query = (
            self.db.query(EnhancedDataSource)
            .filter(
                EnhancedDataSource.source_type == DataSourceType.sql,
                EnhancedDataSource.is_active == True,
            )
            .all()
        )

        assert len(complex_query) >= 10

        print(f"âœ… æ€§èƒ½åœºæ™¯æµ‹è¯•å®Œæˆ: {total_sources} æ•°æ®æº, {total_jobs} ETLä½œä¸š")

    def test_schema_validation(self):
        """æµ‹è¯•SchemaéªŒè¯"""
        print("\nğŸ§ª æµ‹è¯•SchemaéªŒè¯...")

        # æµ‹è¯•EnhancedDataSourceCreate
        valid_source_data = {
            "name": "test_schema_source",
            "source_type": "sql",
            "connection_string": "postgresql://test:test@localhost/test",
            "sql_query_type": "multi_table",
        }

        source_schema = EnhancedDataSourceCreate(**valid_source_data)
        assert source_schema.name == "test_schema_source"

        # æµ‹è¯•ETLJobCreate
        valid_job_data = {
            "name": "test_schema_job",
            "enhanced_source_id": 1,
            "destination_table_name": "test_table",
            "source_query": "SELECT * FROM test",
            "transformation_config": {"steps": [{"type": "filter"}]},
            "schedule": "0 1 * * *",
            "enabled": True,
        }

        job_schema = ETLJobCreate(**valid_job_data)
        assert job_schema.enhanced_source_id == 1
        assert job_schema.schedule == "0 1 * * *"

        print("âœ… SchemaéªŒè¯æµ‹è¯•å®Œæˆ")

    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰å¤æ‚åœºæ™¯æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹æœ€å¤æ‚æƒ…å†µçš„ EnhancedDataSource å’Œ ETLJob æµ‹è¯•")
        print("=" * 80)

        self.setup()

        try:
            # è¿è¡Œæ‰€æœ‰æµ‹è¯•
            source = self.test_complex_data_source_creation()
            job = self.test_complex_etl_job_creation()
            self.test_cascade_operations()
            self.test_data_integrity()
            self.test_performance_scenarios()
            self.test_schema_validation()

            print("\n" + "=" * 80)
            print("ğŸ‰ æ‰€æœ‰å¤æ‚åœºæ™¯æµ‹è¯•é€šè¿‡ï¼")
            print("âœ… EnhancedDataSource å’Œ ETLJob çš„å¤æ‚å…³ç³»å®Œå…¨éªŒè¯")
            print("âœ… ç³»ç»Ÿå·²å‡†å¤‡å¥½å¤„ç†æœ€å¤æ‚çš„ä¸šåŠ¡åœºæ™¯")

            return {
                "enhanced_source_id": source.id,
                "etl_job_id": job.id,
                "test_status": "PASSED",
            }

        finally:
            self.teardown()


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    tester = ComplexScenarioTester()
    return tester.run_all_tests()


if __name__ == "__main__":
    result = main()
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœ: {result}")
