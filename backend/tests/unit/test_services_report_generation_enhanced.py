"""
Enhanced comprehensive unit tests for report_generation service module
"""

import pytest
from unittest.mock import Mock, patch, AsyncMock, MagicMock, mock_open
from datetime import datetime
import os
import tempfile
import json
import uuid
import time

from app.services.report_generation.generator import (
    ReportGenerationService,
    ReportGenerationStatus,
)
from app.services.report_generation.composer import (
    ReportCompositionService,
)
from app.services.report_generation.quality_checker import (
    ReportQualityChecker,
    QualityCheckResult,
    QualityMetrics,
    QualityIssue,
    QualityIssueType,
    QualitySeverity,
    LanguageAnalyzer,
    DataConsistencyValidator,
)


class TestReportGenerationServiceEnhanced:
    """Enhanced tests for ReportGenerationService class"""

    def setup_method(self):
        """Setup test fixtures"""
        self.mock_db = Mock()
        self.service = ReportGenerationService(self.mock_db)

    @patch('app.services.report_generation.generator.crud')
    @patch('app.services.report_generation.generator.os.path.exists')
    @patch('app.services.report_generation.generator.os.makedirs')
    def test_generate_report_complete_workflow(self, mock_makedirs, mock_exists, mock_crud):
        """Test complete report generation workflow"""
        # Setup comprehensive mocks
        mock_task = Mock()
        mock_task.name = "comprehensive_test_task"
        mock_task.id = 1
        
        mock_template = Mock()
        mock_template.file_path = "/path/to/template.docx"
        mock_template.name = "Test Template"
        
        mock_data_source = Mock()
        mock_data_source.name = "Test Data Source"
        mock_data_source.id = 1
        
        mock_crud.task.get.return_value = mock_task
        mock_crud.template.get.return_value = mock_template
        mock_crud.data_source.get.return_value = mock_data_source
        mock_exists.return_value = True
        
        # Mock template parser with complex placeholders
        complex_placeholders = [
            {
                "name": "total_complaints",
                "type": "scalar",
                "description": "æ€»æŠ•è¯‰ä»¶æ•°"
            },
            {
                "name": "region_chart",
                "type": "chart",
                "description": "åœ°åŒºåˆ†å¸ƒå›¾"
            },
            {
                "name": "trend_analysis",
                "type": "analysis",
                "description": "è¶‹åŠ¿åˆ†æ"
            }
        ]
        self.service.template_parser.parse = Mock(return_value={
            "placeholders": complex_placeholders
        })
        
        # Mock tool dispatcher with different results
        def mock_dispatch(data_source_id, placeholder_type, placeholder_description):
            if placeholder_type == "scalar":
                return "1,234"
            elif placeholder_type == "chart":
                return "base64_chart_data_here"
            elif placeholder_type == "analysis":
                return "è¶‹åŠ¿åˆ†ææ˜¾ç¤ºæŠ•è¯‰é‡å‘ˆä¸‹é™è¶‹åŠ¿"
            return "default_value"
        
        self.service.tool_dispatcher.dispatch = Mock(side_effect=mock_dispatch)
        
        # Mock file operations
        template_content = """
        æŠ¥å‘Šæ‘˜è¦ï¼š
        æ€»æŠ•è¯‰ä»¶æ•°ï¼š{{total_complaints}}
        åœ°åŒºåˆ†å¸ƒï¼š[chart:region_chart]
        è¶‹åŠ¿åˆ†æï¼š[analysis:trend_analysis]
        """
        
        with patch('builtins.open', mock_open(read_data=template_content)):
            # Mock word generator
            self.service.word_generator.generate_report_from_content = Mock()
            
            result = self.service.generate_report(
                task_id=1,
                template_id=1,
                data_source_id=1,
                output_dir="test_output"
            )
        
        # Verify comprehensive results
        assert result["status"] == ReportGenerationStatus.COMPLETED
        assert result["task_id"] == 1
        assert result["template_id"] == 1
        assert result["data_source_id"] == 1
        assert result["placeholders_processed"] == 3
        assert result["output_path"] is not None
        assert result["generation_id"] is not None
        assert result["duration_seconds"] is not None
        
        # Verify UUID format
        uuid.UUID(result["generation_id"])  # Should not raise exception

    @patch('app.services.report_generation.generator.crud')
    def test_generate_report_error_scenarios(self, mock_crud):
        """Test various error scenarios in report generation"""
        # Test missing task
        mock_crud.task.get.return_value = None
        mock_crud.template.get.return_value = Mock()
        mock_crud.data_source.get.return_value = Mock()
        
        with pytest.raises(ValueError, match="Task not found"):
            self.service.generate_report(1, 1, 1)
        
        # Test missing template
        mock_crud.task.get.return_value = Mock()
        mock_crud.template.get.return_value = None
        mock_crud.data_source.get.return_value = Mock()
        
        with pytest.raises(ValueError, match="Template not found"):
            self.service.generate_report(1, 1, 1)
        
        # Test missing data source
        mock_crud.task.get.return_value = Mock()
        mock_crud.template.get.return_value = Mock()
        mock_crud.data_source.get.return_value = None
        
        with pytest.raises(ValueError, match="Data source not found"):
            self.service.generate_report(1, 1, 1)

    @pytest.mark.asyncio
    @patch('app.services.report_generation.generator.crud')
    async def test_preview_report_data_comprehensive(self, mock_crud):
        """Test comprehensive report data preview"""
        # Setup mocks
        mock_template = Mock()
        mock_template.name = "Comprehensive Template"
        mock_template.file_path = "/path/to/template.docx"
        
        mock_data_source = Mock()
        mock_data_source.name = "Comprehensive Data Source"
        
        mock_crud.template.get.return_value = mock_template
        mock_crud.data_source.get.return_value = mock_data_source
        
        # Mock complex template structure
        complex_placeholders = [
            {
                "name": "summary_stats",
                "type": "scalar",
                "description": "æ±‡æ€»ç»Ÿè®¡æ•°æ®"
            },
            {
                "name": "regional_breakdown",
                "type": "table",
                "description": "åœ°åŒºåˆ†è§£æ•°æ®"
            },
            {
                "name": "trend_chart",
                "type": "chart",
                "description": "è¶‹åŠ¿å›¾è¡¨"
            }
        ]
        self.service.template_parser.parse = Mock(return_value={
            "placeholders": complex_placeholders
        })
        
        # Mock comprehensive sample data
        import pandas as pd
        sample_data = pd.DataFrame({
            "region": ["æ˜†æ˜", "å¤§ç†", "ä¸½æ±Ÿ", "è¥¿åŒç‰ˆçº³", "é¦™æ ¼é‡Œæ‹‰"],
            "complaint_count": [120, 95, 150, 80, 65],
            "resolution_rate": [0.85, 0.92, 0.78, 0.88, 0.91],
            "date": pd.date_range("2024-01-01", periods=5)
        })
        
        self.service.data_retrieval.fetch_data = AsyncMock(return_value=sample_data)
        
        # Mock AI interpretation
        def mock_ai_interpret(task_type, description, df_columns):
            return {
                "interpretation": f"AIè§£é‡Šï¼š{description}",
                "suggested_columns": df_columns[:2],
                "confidence": 0.85
            }
        
        self.service.ai_service.interpret_description_for_tool = Mock(
            side_effect=mock_ai_interpret
        )
        
        result = await self.service.preview_report_data(
            template_id=1,
            data_source_id=1,
            limit=3
        )
        
        # Verify comprehensive preview results
        assert result["template_id"] == 1
        assert result["data_source_id"] == 1
        assert result["template_name"] == "Comprehensive Template"
        assert result["data_source_name"] == "Comprehensive Data Source"
        assert len(result["placeholders"]) == 3
        assert result["sample_data_shape"]["rows"] == 3  # Limited by limit parameter
        assert result["sample_data_shape"]["columns"] == 4
        
        # Verify placeholder analysis
        for placeholder in result["placeholders"]:
            assert "name" in placeholder
            assert "type" in placeholder
            assert "description" in placeholder
            assert "available_columns" in placeholder
            assert "sample_values" in placeholder
            assert "ai_interpretation" in placeholder

    def test_validate_report_configuration_comprehensive(self):
        """Test comprehensive report configuration validation"""
        with patch('app.services.report_generation.generator.crud') as mock_crud, \
             patch('app.services.report_generation.generator.os.path.exists') as mock_exists:
            
            # Setup valid configuration
            mock_template = Mock()
            mock_template.file_path = "/valid/template.docx"
            
            mock_data_source = Mock()
            mock_data_source.source_type.value = "sql"
            mock_data_source.connection_string = "valid_connection"
            
            mock_crud.template.get.return_value = mock_template
            mock_crud.data_source.get.return_value = mock_data_source
            mock_exists.return_value = True
            
            # Mock template parser
            self.service.template_parser.parse = Mock(return_value={
                "placeholders": [{"name": "test", "type": "scalar"}]
            })
            
            # Mock AI service health check
            self.service.ai_service.health_check = Mock(return_value={
                "status": "healthy"
            })
            
            result = self.service.validate_report_configuration(1, 1)
            
            assert result["valid"] is True
            assert len(result["errors"]) == 0
            assert len(result["warnings"]) == 0

    def test_validate_report_configuration_various_data_sources(self):
        """Test validation with various data source types"""
        test_cases = [
            # CSV data source
            {
                "source_type": "csv",
                "file_path": "/valid/data.csv",
                "should_be_valid": True
            },
            # API data source
            {
                "source_type": "api",
                "api_url": "https://api.example.com/data",
                "should_be_valid": True
            },
            # Invalid CSV (missing file)
            {
                "source_type": "csv",
                "file_path": None,
                "should_be_valid": False
            },
            # Invalid API (missing URL)
            {
                "source_type": "api",
                "api_url": None,
                "should_be_valid": False
            }
        ]
        
        for case in test_cases:
            with patch('app.services.report_generation.generator.crud') as mock_crud, \
                 patch('app.services.report_generation.generator.os.path.exists') as mock_exists:
                
                mock_template = Mock()
                mock_template.file_path = "/valid/template.docx"
                
                mock_data_source = Mock()
                mock_data_source.source_type.value = case["source_type"]
                
                # Set appropriate attributes based on source type
                if case["source_type"] == "csv":
                    mock_data_source.file_path = case.get("file_path")
                elif case["source_type"] == "api":
                    mock_data_source.api_url = case.get("api_url")
                elif case["source_type"] == "sql":
                    mock_data_source.connection_string = case.get("connection_string")
                
                mock_crud.template.get.return_value = mock_template
                mock_crud.data_source.get.return_value = mock_data_source
                mock_exists.return_value = True
                
                self.service.template_parser.parse = Mock(return_value={
                    "placeholders": [{"name": "test", "type": "scalar"}]
                })
                self.service.ai_service.health_check = Mock(return_value={
                    "status": "healthy"
                })
                
                result = self.service.validate_report_configuration(1, 1)
                
                if case["should_be_valid"]:
                    assert result["valid"] is True or len(result["errors"]) == 0
                else:
                    assert result["valid"] is False or len(result["errors"]) > 0


class TestReportCompositionServiceEnhanced:
    """Enhanced tests for ReportCompositionService class"""

    def setup_method(self):
        """Setup test fixtures"""
        self.service = ReportCompositionService()

    def test_compose_report_complex_content(self):
        """Test composition with complex content types"""
        template_content = """
        # æŠ•è¯‰åˆ†ææŠ¥å‘Š
        
        ## æ€»ä½“æƒ…å†µ
        æœ¬æœŸå…±å¤„ç†æŠ•è¯‰ {{total_count}} ä»¶ï¼Œå®Œæˆç‡è¾¾åˆ° {{completion_rate}}ã€‚
        
        ## åœ°åŒºåˆ†å¸ƒ
        {{region_chart}}
        
        ## è¶‹åŠ¿åˆ†æ
        {{trend_description}}
        
        ## è¯¦ç»†æ•°æ®
        {{data_table}}
        """
        
        results = {
            "{{total_count}}": "1,234",
            "{{completion_rate}}": "95.6%",
            "{{region_chart}}": "iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mP8/5+hHgAHggJ/PchI7wAAAABJRU5ErkJggg==",
            "{{trend_description}}": "æŠ•è¯‰é‡å‘ˆç°ç¨³å®šä¸‹é™è¶‹åŠ¿ï¼Œå®¢æˆ·æ»¡æ„åº¦æŒç»­æå‡ã€‚",
            "{{data_table}}": "<table><tr><th>åœ°åŒº</th><th>æŠ•è¯‰é‡</th></tr><tr><td>æ˜†æ˜</td><td>456</td></tr></table>"
        }
        
        composed = self.service.compose_report(template_content, results)
        
        # Verify all placeholders are replaced
        for placeholder in results.keys():
            assert placeholder not in composed
        
        # Verify content is properly inserted
        assert "1,234" in composed
        assert "95.6%" in composed
        assert "æŠ•è¯‰é‡å‘ˆç°ç¨³å®šä¸‹é™è¶‹åŠ¿" in composed
        assert "<table>" in composed
        
        # Verify base64 image is converted to img tag
        assert '<img src="data:image/png;base64,' in composed

    def test_is_base64_detection_comprehensive(self):
        """Test comprehensive base64 detection"""
        test_cases = [
            # Valid base64 images
            ("iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mP8/5+hHgAHggJ/PchI7wAAAABJRU5ErkJggg==", True),
            ("data:image/png;base64,iVBORw0KGgo=", False),  # Has prefix, not pure base64
            ("SGVsbG8gV29ybGQ=", True),  # Simple base64
            
            # Invalid cases
            ("regular text", False),
            ("", False),
            (None, False),
            (123, False),
            ([], False),
            
            # Edge cases
            ("a", False),  # Too short
            ("====", True),  # Only padding
        ]
        
        for test_input, expected in test_cases:
            result = self.service.is_base64(test_input)
            assert result == expected, f"Failed for input: {test_input}"

    def test_compose_report_edge_cases(self):
        """Test composition with edge cases"""
        # Empty template
        assert self.service.compose_report("", {}) == ""
        
        # Template with no placeholders
        template = "è¿™æ˜¯ä¸€ä¸ªæ²¡æœ‰å ä½ç¬¦çš„æ¨¡æ¿ã€‚"
        assert self.service.compose_report(template, {}) == template
        
        # Empty results
        template = "æ¨¡æ¿åŒ…å«{{placeholder}}ä½†æ²¡æœ‰æ›¿æ¢å€¼ã€‚"
        composed = self.service.compose_report(template, {})
        assert "{{placeholder}}" in composed  # Should remain unchanged
        
        # Partial replacements
        template = "{{replaced}}å’Œ{{not_replaced}}"
        results = {"{{replaced}}": "å·²æ›¿æ¢"}
        composed = self.service.compose_report(template, results)
        assert "å·²æ›¿æ¢" in composed
        assert "{{not_replaced}}" in composed

    def test_compose_report_special_characters(self):
        """Test composition with special characters and encoding"""
        template = "ç‰¹æ®Šå­—ç¬¦æµ‹è¯•ï¼š{{emoji}}ï¼Œ{{chinese}}ï¼Œ{{symbols}}"
        results = {
            "{{emoji}}": "ğŸ˜ŠğŸ“ŠğŸ¯",
            "{{chinese}}": "ä¸­æ–‡æµ‹è¯•å†…å®¹",
            "{{symbols}}": "!@#$%^&*()_+-=[]{}|;:,.<>?"
        }
        
        composed = self.service.compose_report(template, results)
        
        assert "ğŸ˜ŠğŸ“ŠğŸ¯" in composed
        assert "ä¸­æ–‡æµ‹è¯•å†…å®¹" in composed
        assert "!@#$%^&*()_+-=[]{}|;:,.<>?" in composed


class TestLanguageAnalyzerEnhanced:
    """Enhanced tests for LanguageAnalyzer class"""

    def setup_method(self):
        """Setup test fixtures"""
        self.analyzer = LanguageAnalyzer()

    def test_analyze_text_comprehensive(self):
        """Test comprehensive text analysis"""
        complex_text = """
        è¿™æ˜¯ç¬¬ä¸€æ®µæ–‡å­—ï¼ŒåŒ…å«äº†åŸºæœ¬çš„å¥å­ç»“æ„ã€‚è¿™ä¸ªæ®µè½ç”¨æ¥æµ‹è¯•åŸºç¡€åŠŸèƒ½ï¼
        
        ç¬¬äºŒæ®µåŒ…å«äº†æ›´å¤æ‚çš„å¥å­ç»“æ„ï¼Œè™½ç„¶å†…å®¹è¾ƒé•¿ï¼Œä½†æ˜¯åº”è¯¥èƒ½å¤Ÿæ­£ç¡®å¤„ç†ï¼Ÿ
        è¿™é‡Œè¿˜æœ‰ä¸€äº›çŸ­å¥ã€‚ä»¥åŠä¸€äº›éå¸¸éå¸¸é•¿çš„å¥å­ï¼Œç”¨æ¥æµ‹è¯•ç³»ç»Ÿå¯¹äºé•¿å¥å­çš„å¤„ç†èƒ½åŠ›ï¼Œ
        çœ‹çœ‹æ˜¯å¦èƒ½å¤Ÿæ­£ç¡®è¯†åˆ«å’Œåˆ†æè¿™ç§å¤æ‚çš„è¯­è¨€ç»“æ„ã€‚
        
        ç¬¬ä¸‰æ®µæµ‹è¯•ç‰¹æ®Šæƒ…å†µï¼šåŒ…å«æ•°å­—123ï¼Œè‹±æ–‡wordsï¼Œä»¥åŠå„ç§æ ‡ç‚¹ç¬¦å·ï¼@#$%ã€‚
        """
        
        result = self.analyzer.analyze_text(complex_text)
        
        # Verify comprehensive analysis results
        assert result["word_count"] > 0
        assert result["sentence_count"] >= 5
        assert result["paragraph_count"] >= 3
        assert result["avg_sentence_length"] > 0
        assert 0 <= result["readability_score"] <= 100
        assert isinstance(result["fluency_issues"], list)
        assert result["complex_sentences"] >= 0

    def test_sentence_splitting_various_punctuation(self):
        """Test sentence splitting with various Chinese punctuation"""
        test_cases = [
            ("ç¬¬ä¸€å¥ã€‚ç¬¬äºŒå¥ï¼ç¬¬ä¸‰å¥ï¼Ÿ", 3),
            ("å¥å­ä¸€ï¼›å¥å­äºŒï¼šå¥å­ä¸‰ã€‚", 3),
            ("é—®é¢˜ä¸€ï¼Ÿé—®é¢˜äºŒï¼Ÿé—®é¢˜ä¸‰ï¼", 3),
            ("å£°æ˜ä¸€ã€‚å£°æ˜äºŒï¼å£°æ˜ä¸‰ï¼Ÿå£°æ˜å››ã€‚", 4),
        ]
        
        for text, expected_count in test_cases:
            sentences = self.analyzer._split_sentences(text)
            assert len(sentences) == expected_count, f"Failed for: {text}"

    def test_fluency_check_comprehensive(self):
        """Test comprehensive fluency checking"""
        test_sentences = [
            "çŸ­ã€‚",  # Very short sentence
            "è¿™æ˜¯ä¸€ä¸ªæ­£å¸¸é•¿åº¦çš„å¥å­ï¼Œåº”è¯¥ä¸ä¼šè§¦å‘ä»»ä½•è­¦å‘Šã€‚",  # Normal sentence
            "è¿™æ˜¯ä¸€ä¸ªæå…¶å†—é•¿çš„å¥å­ï¼Œ" + "å†…å®¹é‡å¤" * 50 + "ï¼Œç”¨æ¥æµ‹è¯•é•¿å¥å­æ£€æµ‹ã€‚",  # Very long sentence
            "é‡å¤æ ‡ç‚¹ç¬¦å·æµ‹è¯•ã€‚ã€‚ã€‚",  # Repeated punctuation
            "æ­£å¸¸å¥å­ç»“æŸã€‚",  # Normal sentence
        ]
        
        issues = self.analyzer._check_fluency(test_sentences)
        
        # Should detect various issues
        issue_types = [issue["type"] for issue in issues]
        assert "short_sentence" in issue_types
        assert "long_sentence" in issue_types
        assert "repeated_punctuation" in issue_types

    def test_readability_calculation_edge_cases(self):
        """Test readability calculation with edge cases"""
        test_cases = [
            ("", 0),  # Empty text
            ("çŸ­å¥ã€‚", lambda x: 0 <= x <= 100),  # Very short text
            ("æ­£å¸¸é•¿åº¦çš„å¥å­ç”¨äºæµ‹è¯•ã€‚", lambda x: 0 <= x <= 100),  # Normal text
            ("æé•¿å¥å­" + "é‡å¤å†…å®¹" * 100 + "ã€‚", lambda x: 0 <= x <= 100),  # Very long sentence
        ]
        
        for text, expected in test_cases:
            sentences = self.analyzer._split_sentences(text) if text else []
            score = self.analyzer._calculate_readability(text, sentences)
            
            if callable(expected):
                assert expected(score), f"Score {score} failed validation for: {text[:50]}..."
            else:
                assert score == expected, f"Expected {expected}, got {score} for: {text}"

    def test_complex_sentence_detection(self):
        """Test complex sentence pattern detection"""
        test_sentences = [
            "è™½ç„¶å¤©æ°”ä¸å¥½ï¼Œä½†æ˜¯æˆ‘ä»¬è¿˜æ˜¯è¦å‡ºé—¨ã€‚",  # although...but
            "ä»–ä¸ä»…èªæ˜è€Œä¸”å‹¤å¥‹ã€‚",  # not only...but also
            "å¦‚æœæ˜å¤©ä¸‹é›¨ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±ä¸å»äº†ã€‚",  # if...then
            "å› ä¸ºæ—¶é—´ç´§æ€¥ï¼Œæ‰€ä»¥æˆ‘ä»¬å¿…é¡»åŠ å¿«é€Ÿåº¦ã€‚",  # because...so
            "è¿™æ˜¯ä¸€ä¸ªç®€å•çš„å¥å­ã€‚",  # Simple sentence
        ]
        
        complex_count = self.analyzer._count_complex_sentences(test_sentences)
        assert complex_count == 4  # First 4 sentences are complex


class TestDataConsistencyValidatorEnhanced:
    """Enhanced tests for DataConsistencyValidator class"""

    def setup_method(self):
        """Setup test fixtures"""
        self.validator = DataConsistencyValidator()

    def test_validate_report_comprehensive(self):
        """Test comprehensive report validation"""
        report_content = """
        æŠ•è¯‰åˆ†ææŠ¥å‘Š
        
        æœ¬æœˆå…±å¤„ç†æŠ•è¯‰1,234ä»¶ï¼Œå®Œæˆç‡è¾¾åˆ°95.5%ã€‚
        å„åœ°åŒºåˆ†å¸ƒå¦‚ä¸‹ï¼š
        - æ˜†æ˜ï¼š456ä»¶ï¼ˆ37.0%ï¼‰
        - å¤§ç†ï¼š321ä»¶ï¼ˆ26.0%ï¼‰
        - ä¸½æ±Ÿï¼š289ä»¶ï¼ˆ23.4%ï¼‰
        - å…¶ä»–ï¼š168ä»¶ï¼ˆ13.6%ï¼‰
        
        æ—¶é—´åˆ†æï¼š
        2024å¹´3æœˆæ•°æ®æ˜¾ç¤ºï¼ŒæŠ•è¯‰é‡æ¯”2024å¹´2æœˆä¸‹é™äº†15.2%ã€‚
        
        å¼‚å¸¸æ•°æ®æµ‹è¯•ï¼š
        - é”™è¯¯ç™¾åˆ†æ¯”ï¼š150%ï¼ˆåº”è¯¥è¢«æ£€æµ‹ä¸ºé”™è¯¯ï¼‰
        - è´Ÿç™¾åˆ†æ¯”ï¼š-5%ï¼ˆåº”è¯¥è¢«æ£€æµ‹ä¸ºé”™è¯¯ï¼‰
        - å¼‚å¸¸å¹´ä»½ï¼š1800å¹´3æœˆï¼ˆåº”è¯¥è¢«æ£€æµ‹ä¸ºå¼‚å¸¸ï¼‰
        """
        
        issues = self.validator.validate_report(report_content)
        
        # Should detect various consistency issues
        issue_types = [issue.issue_type for issue in issues]
        assert QualityIssueType.DATA_INCONSISTENCY in issue_types
        
        # Check for specific issues
        issue_descriptions = [issue.description for issue in issues]
        assert any("150%" in desc for desc in issue_descriptions)
        assert any("-5%" in desc for desc in issue_descriptions)
        assert any("1800å¹´" in desc for desc in issue_descriptions)

    def test_extract_numbers_comprehensive(self):
        """Test comprehensive number extraction"""
        text = """
        æ•°å­—æµ‹è¯•ï¼šæ•´æ•°123ï¼Œå°æ•°45.67ï¼Œåƒåˆ†ä½1,234ï¼Œ
        å¤§æ•°å­—1,234,567.89ï¼Œç™¾åˆ†æ¯”ä¸ç®—æ•°å­—95%ï¼Œ
        è´Ÿæ•°-123.45ï¼Œç§‘å­¦è®¡æ•°æ³•ä¸æ”¯æŒ1.23e5ã€‚
        """
        
        numbers = self.validator._extract_numbers(text)
        
        # Verify extracted numbers
        number_values = [num[1] for num in numbers]
        assert 123.0 in number_values
        assert 45.67 in number_values
        assert 1234.0 in number_values
        assert 1234567.89 in number_values

    def test_extract_percentages_comprehensive(self):
        """Test comprehensive percentage extraction"""
        text = """
        ç™¾åˆ†æ¯”æµ‹è¯•ï¼šæ­£å¸¸ç™¾åˆ†æ¯”95.5%ï¼Œæ•´æ•°ç™¾åˆ†æ¯”80%ï¼Œ
        å°æ•°ç™¾åˆ†æ¯”67.89%ï¼Œå¼‚å¸¸ç™¾åˆ†æ¯”150%ï¼Œè´Ÿç™¾åˆ†æ¯”-10%ã€‚
        """
        
        percentages = self.validator._extract_percentages(text)
        
        # Verify extracted percentages
        percentage_values = [pct[1] for pct in percentages]
        assert 95.5 in percentage_values
        assert 80.0 in percentage_values
        assert 67.89 in percentage_values
        assert 150.0 in percentage_values
        assert -10.0 in percentage_values

    def test_extract_dates_comprehensive(self):
        """Test comprehensive date extraction"""
        text = """
        æ—¥æœŸæµ‹è¯•ï¼š2024å¹´3æœˆï¼Œ2023å¹´12æœˆï¼Œ1æœˆ15æ—¥ï¼Œ
        12æœˆ31æ—¥ï¼Œ2024å¹´2æœˆ29æ—¥ï¼ˆé—°å¹´ï¼‰ï¼Œ13æœˆï¼ˆæ— æ•ˆæœˆä»½ï¼‰ã€‚
        """
        
        dates = self.validator._extract_dates(text)
        
        # Verify extracted dates
        assert len(dates) >= 3
        assert any("2024å¹´3æœˆ" in date for date in dates)
        assert any("2023å¹´12æœˆ" in date for date in dates)

    def test_percentage_consistency_validation(self):
        """Test percentage consistency validation"""
        valid_percentages = [("95.5%", 95.5, "å®Œæˆç‡95.5%")]
        invalid_percentages = [
            ("150%", 150.0, "å¼‚å¸¸ç™¾åˆ†æ¯”150%"),
            ("-10%", -10.0, "è´Ÿç™¾åˆ†æ¯”-10%")
        ]
        
        # Test valid percentages
        valid_issues = self.validator._check_percentage_consistency(valid_percentages)
        assert len(valid_issues) == 0
        
        # Test invalid percentages
        invalid_issues = self.validator._check_percentage_consistency(invalid_percentages)
        assert len(invalid_issues) == 2
        
        # Verify issue details
        for issue in invalid_issues:
            assert issue.issue_type == QualityIssueType.DATA_INCONSISTENCY
            assert issue.severity == QualitySeverity.HIGH

    def test_date_consistency_validation(self):
        """Test date consistency validation"""
        test_dates = [
            "2024å¹´3æœˆ",  # Valid
            "2023å¹´12æœˆ",  # Valid
            "1800å¹´3æœˆ",  # Invalid year (too old)
            "2024å¹´15æœˆ",  # Invalid month
            "2025å¹´2æœˆ",  # Valid (future but reasonable)
        ]
        
        issues = self.validator._check_date_consistency(test_dates)
        
        # Should detect invalid dates
        issue_descriptions = [issue.description for issue in issues]
        assert any("1800å¹´" in desc for desc in issue_descriptions)
        assert any("15æœˆ" in desc for desc in issue_descriptions)

    def test_validate_against_source_data(self):
        """Test validation against source data"""
        report_content = "æ€»æŠ•è¯‰ä»¶æ•°ï¼š1,234ä»¶ï¼Œå®Œæˆç‡ï¼š95.5%"
        source_data = {
            "expected_values": {
                "total_complaints": 1234,
                "completion_rate": "95.5%",
                "missing_value": 999  # This should trigger an issue
            }
        }
        
        issues = self.validator._validate_against_source(report_content, source_data)
        
        # Should detect missing expected value
        assert len(issues) >= 1
        issue_descriptions = [issue.description for issue in issues]
        assert any("999" in desc for desc in issue_descriptions)


class TestReportQualityCheckerEnhanced:
    """Enhanced tests for ReportQualityChecker class"""

    def setup_method(self):
        """Setup test fixtures"""
        self.mock_db = Mock()
        self.checker = ReportQualityChecker(self.mock_db)

    def test_check_report_quality_comprehensive(self):
        """Test comprehensive report quality checking"""
        report_content = """
        # æŠ•è¯‰å¤„ç†åˆ†ææŠ¥å‘Š
        
        ## æ¦‚è¿°
        æœ¬æœˆå…±å¤„ç†æŠ•è¯‰1,234ä»¶ï¼Œå®Œæˆç‡è¾¾åˆ°95.5%ã€‚æ•´ä½“å¤„ç†æ•ˆç‡è¾ƒä¸Šæœˆæå‡äº†12.3%ã€‚
        
        ## è¯¦ç»†åˆ†æ
        å„åœ°åŒºå¤„ç†æƒ…å†µå¦‚ä¸‹ï¼šæ˜†æ˜åœ°åŒºå¤„ç†456ä»¶ï¼Œå¤§ç†åœ°åŒºå¤„ç†321ä»¶ï¼Œä¸½æ±Ÿåœ°åŒºå¤„ç†289ä»¶ã€‚
        ä»æ—¶é—´åˆ†å¸ƒæ¥çœ‹ï¼Œå·¥ä½œæ—¥å¤„ç†é‡æ˜æ˜¾é«˜äºå‘¨æœ«ï¼Œå¹³å‡æ¯æ—¥å¤„ç†é‡ä¸º41ä»¶ã€‚
        
        ## é—®é¢˜è¯†åˆ«
        å‘ç°éƒ¨åˆ†åœ°åŒºå­˜åœ¨å¤„ç†æ—¶é—´è¿‡é•¿çš„é—®é¢˜ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–æµç¨‹ã€‚
        """
        
        # Mock LLM manager
        self.checker.llm_manager.get_available_providers = Mock(return_value=["openai"])
        
        # Mock LLM analysis
        mock_llm_analysis = {
            "overall_assessment": "æŠ¥å‘Šè´¨é‡è‰¯å¥½",
            "fluency_score": 85,
            "logic_score": 88,
            "accuracy_score": 92,
            "completeness_score": 87,
            "suggestions": ["å»ºè®®å¢åŠ æ›´å¤šæ•°æ®æ”¯æ’‘", "å¯ä»¥æ·»åŠ å›¾è¡¨å±•ç¤º"],
            "issues": [
                {
                    "description": "éƒ¨åˆ†å¥å­è¾ƒé•¿ï¼Œå»ºè®®æ‹†åˆ†",
                    "location": "è¯¦ç»†åˆ†æéƒ¨åˆ†",
                    "suggestion": "å°†é•¿å¥æ‹†åˆ†ä¸ºå¤šä¸ªçŸ­å¥"
                }
            ]
        }
        
        with patch.object(self.checker, '_perform_llm_analysis', return_value=mock_llm_analysis):
            result = self.checker.check_report_quality(report_content)
        
        # Verify comprehensive quality check results
        assert isinstance(result, QualityCheckResult)
        assert isinstance(result.metrics, QualityMetrics)
        assert result.metrics.overall_score > 0
        assert result.metrics.word_count > 0
        assert result.processing_time > 0
        assert isinstance(result.timestamp, datetime)

    def test_llm_analysis_error_handling(self):
        """Test LLM analysis error handling"""
        report_content = "æµ‹è¯•æŠ¥å‘Šå†…å®¹"
        
        # Mock LLM manager with no providers
        self.checker.llm_manager.get_available_providers = Mock(return_value=[])
        
        result = self.checker.check_report_quality(report_content, enable_llm_analysis=True)
        
        # Should handle gracefully without LLM
        assert isinstance(result, QualityCheckResult)
        assert result.llm_analysis is None or result.llm_analysis == {}

    def test_optimize_content_functionality(self):
        """Test content optimization functionality"""
        content = "è¿™æ˜¯éœ€è¦ä¼˜åŒ–çš„å†…å®¹ï¼ŒåŒ…å«ä¸€äº›é—®é¢˜ã€‚"
        issues = [
            QualityIssue(
                issue_type=QualityIssueType.LANGUAGE_FLUENCY,
                severity=QualitySeverity.MEDIUM,
                description="å¥å­ç»“æ„å¯ä»¥æ”¹è¿›",
                location="ç¬¬ä¸€æ®µ"
            )
        ]
        
        # Mock LLM manager
        self.checker.llm_manager.get_available_providers = Mock(return_value=["openai"])
        
        # Mock LLM response
        mock_response = Mock()
        mock_response.content = json.dumps({
            "optimized_content": "è¿™æ˜¯ç»è¿‡ä¼˜åŒ–çš„å†…å®¹ï¼Œç»“æ„æ›´åŠ æ¸…æ™°ã€‚",
            "improvements": ["æ”¹è¿›äº†å¥å­ç»“æ„", "æé«˜äº†å¯è¯»æ€§"],
            "confidence": 85
        })
        
        self.checker.llm_manager.call_llm = Mock(return_value=mock_response)
        
        result = self.checker.optimize_content(content, issues)
        
        assert "optimized_content" in result
        assert "improvements" in result
        assert "confidence" in result
        assert result["confidence"] > 0

    def test_quality_metrics_calculation(self):
        """Test quality metrics calculation"""
        language_analysis = {
            "word_count": 150,
            "sentence_count": 8,
            "paragraph_count": 3,
            "avg_sentence_length": 18.75,
            "readability_score": 75,
            "fluency_issues": [],
            "complex_sentences": 2
        }
        
        issues = [
            QualityIssue(
                issue_type=QualityIssueType.DATA_INCONSISTENCY,
                severity=QualitySeverity.HIGH,
                description="æ•°æ®ä¸ä¸€è‡´",
                location="ç¬¬äºŒæ®µ"
            ),
            QualityIssue(
                issue_type=QualityIssueType.LANGUAGE_FLUENCY,
                severity=QualitySeverity.MEDIUM,
                description="è¯­è¨€æµç•…æ€§é—®é¢˜",
                location="ç¬¬ä¸‰æ®µ"
            )
        ]
        
        llm_analysis = {
            "fluency_score": 80,
            "logic_score": 85,
            "accuracy_score": 78,
            "completeness_score": 82
        }
        
        metrics = self.checker._calculate_quality_metrics(
            language_analysis, issues, llm_analysis
        )
        
        assert isinstance(metrics, QualityMetrics)
        assert 0 <= metrics.overall_score <= 100
        assert metrics.word_count == 150
        assert metrics.sentence_count == 8
        assert metrics.high_issues == 1
        assert metrics.medium_issues == 1
        assert metrics.complex_words_ratio >= 0

    def test_generate_suggestions_comprehensive(self):
        """Test comprehensive suggestion generation"""
        issues = [
            QualityIssue(
                issue_type=QualityIssueType.LANGUAGE_FLUENCY,
                severity=QualitySeverity.MEDIUM,
                description="å¥å­è¿‡é•¿",
                location="ç¬¬ä¸€æ®µ",
                suggestion="å»ºè®®æ‹†åˆ†é•¿å¥"
            ),
            QualityIssue(
                issue_type=QualityIssueType.DATA_INCONSISTENCY,
                severity=QualitySeverity.HIGH,
                description="æ•°æ®ä¸ä¸€è‡´",
                location="ç¬¬äºŒæ®µ",
                suggestion="æ£€æŸ¥æ•°æ®å‡†ç¡®æ€§"
            )
        ]
        
        language_analysis = {
            "avg_sentence_length": 60,  # Very long sentences
            "fluency_issues": [
                {"type": "long_sentence"},
                {"type": "repeated_punctuation"},
                {"type": "short_sentence"},
                {"type": "long_sentence"},
                {"type": "repeated_punctuation"},
                {"type": "short_sentence"}  # More than 5 issues
            ]
        }
        
        suggestions = self.checker._generate_suggestions(issues, language_analysis)
        
        assert isinstance(suggestions, list)
        assert len(suggestions) <= 10  # Should be limited
        assert "å»ºè®®æ‹†åˆ†é•¿å¥" in suggestions
        assert "æ£€æŸ¥æ•°æ®å‡†ç¡®æ€§" in suggestions
        assert any("ç¼©çŸ­å¥å­é•¿åº¦" in s for s in suggestions)
        assert any("è¯­è¨€æµç•…æ€§é—®é¢˜" in s for s in suggestions)


class TestIntegrationAndPerformance:
    """Integration and performance tests"""

    def setup_method(self):
        """Setup test fixtures"""
        self.mock_db = Mock()

    def test_end_to_end_report_generation_workflow(self):
        """Test complete end-to-end report generation workflow"""
        # This would be a comprehensive integration test
        # combining all components together
        pass  # Placeholder for complex integration test

    def test_large_report_processing_performance(self):
        """Test performance with large reports"""
        # Create a large report content
        large_content = """
        # å¤§å‹æŠ¥å‘Šæµ‹è¯•
        
        """ + "è¿™æ˜¯æµ‹è¯•æ®µè½å†…å®¹ã€‚" * 1000 + """
        
        ## æ•°æ®åˆ†æ
        """ + "åŒ…å«å¤§é‡æ•°æ®çš„åˆ†æå†…å®¹ã€‚" * 500 + """
        
        ## ç»“è®º
        """ + "è¯¦ç»†çš„ç»“è®ºéƒ¨åˆ†ã€‚" * 200
        
        analyzer = LanguageAnalyzer()
        
        start_time = time.time()
        result = analyzer.analyze_text(large_content)
        processing_time = time.time() - start_time
        
        # Should complete within reasonable time
        assert processing_time < 10.0  # 10 seconds max
        assert result["word_count"] > 10000
        assert result["sentence_count"] > 100

    def test_concurrent_quality_checking(self):
        """Test concurrent quality checking"""
        import concurrent.futures
        
        def check_quality(content_id):
            analyzer = LanguageAnalyzer()
            content = f"æµ‹è¯•å†…å®¹{content_id}ï¼š" + "è¿™æ˜¯æµ‹è¯•å¥å­ã€‚" * 10
            return analyzer.analyze_text(content)
        
        # Process multiple contents concurrently
        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            futures = [executor.submit(check_quality, i) for i in range(20)]
            results = [future.result() for future in concurrent.futures.as_completed(futures)]
        
        assert len(results) == 20
        assert all(result["word_count"] > 0 for result in results)


if __name__ == "__main__":
    pytest.main([__file__, "-v"])