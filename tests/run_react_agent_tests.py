#!/usr/bin/env python3
"""
React Agentå®Œæ•´æµ‹è¯•æ‰§è¡Œè„šæœ¬

åŸºäºæˆ‘ä»¬åˆ†æçš„æ¶æ„è¿è¡Œå®Œæ•´çš„React Agentå’Œå¤šAgentåè°ƒæµ‹è¯•
åŒ…æ‹¬ï¼š
1. React AgentåŸºç¡€åŠŸèƒ½æµ‹è¯•
2. Contextæ§åˆ¶å’Œä¼ é€’æœºåˆ¶æµ‹è¯•  
3. å¤šAgentåè°ƒå·¥ä½œæµæµ‹è¯•
4. ç«¯åˆ°ç«¯ä¸šåŠ¡æµç¨‹æµ‹è¯•
"""

import sys
import os
import subprocess
import asyncio
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
ROOT_DIR = Path(__file__).parent.parent
sys.path.insert(0, str(ROOT_DIR))
sys.path.insert(0, str(ROOT_DIR / "backend"))


class TestRunner:
    """æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self):
        self.test_results = []
        self.start_time = None
        self.end_time = None
    
    def log(self, message: str, level: str = "INFO"):
        """è®°å½•æ—¥å¿—"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        print(f"[{timestamp}] [{level}] {message}")
    
    def run_test_file(self, test_file: str, test_name: str, markers: str = None) -> dict:
        """è¿è¡Œå•ä¸ªæµ‹è¯•æ–‡ä»¶"""
        self.log(f"å¼€å§‹è¿è¡Œ {test_name}")
        
        # æ„å»ºpytestå‘½ä»¤ - ä½¿ç”¨backend venvçš„Python
        python_path = str(ROOT_DIR / "backend" / "venv" / "bin" / "python")
        cmd = [python_path, "-m", "pytest", test_file, "-v", "-s", "--tb=short"]
        
        if markers:
            cmd.extend(["-m", markers])
        
        start_time = datetime.now()
        
        try:
            # è¿è¡Œæµ‹è¯•
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                cwd=ROOT_DIR,
                timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
            )
            
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            # è§£æç»“æœ
            success = result.returncode == 0
            output = result.stdout
            error = result.stderr
            
            # æå–æµ‹è¯•ç»Ÿè®¡
            test_stats = self.parse_pytest_output(output)
            
            test_result = {
                'name': test_name,
                'file': test_file,
                'success': success,
                'duration': duration,
                'stats': test_stats,
                'output': output,
                'error': error
            }
            
            self.test_results.append(test_result)
            
            status = "âœ… æˆåŠŸ" if success else "âŒ å¤±è´¥"
            self.log(f"{test_name} - {status} (è€—æ—¶: {duration:.2f}s)")
            
            if not success:
                self.log(f"é”™è¯¯è¾“å‡º: {error}", "ERROR")
            
            return test_result
            
        except subprocess.TimeoutExpired:
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            test_result = {
                'name': test_name,
                'file': test_file,
                'success': False,
                'duration': duration,
                'stats': {},
                'output': '',
                'error': 'æµ‹è¯•è¶…æ—¶'
            }
            
            self.test_results.append(test_result)
            self.log(f"{test_name} - â° è¶…æ—¶ (è€—æ—¶: {duration:.2f}s)", "ERROR")
            
            return test_result
        
        except Exception as e:
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            test_result = {
                'name': test_name,
                'file': test_file,
                'success': False,
                'duration': duration,
                'stats': {},
                'output': '',
                'error': str(e)
            }
            
            self.test_results.append(test_result)
            self.log(f"{test_name} - ğŸ”¥ å¼‚å¸¸: {e}", "ERROR")
            
            return test_result
    
    def parse_pytest_output(self, output: str) -> dict:
        """è§£æpytestè¾“å‡ºè·å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'passed': 0,
            'failed': 0,
            'skipped': 0,
            'warnings': 0,
            'total': 0
        }
        
        # æŸ¥æ‰¾æµ‹è¯•ç»“æœè¡Œ
        lines = output.split('\n')
        for line in lines:
            if 'passed' in line and ('failed' in line or 'error' in line or 'warnings' in line or 'skipped' in line):
                # è§£æç±»ä¼¼ "5 passed, 2 warnings in 1.23s" çš„è¡Œ
                parts = line.split()
                for i, part in enumerate(parts):
                    if part.isdigit():
                        count = int(part)
                        if i + 1 < len(parts):
                            category = parts[i + 1]
                            if category in stats:
                                stats[category] = count
                            elif category == 'error' or category == 'errors':
                                stats['failed'] = count
                
                break
        
        stats['total'] = sum(v for k, v in stats.items() if k != 'total')
        return stats
    
    async def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        self.log("ğŸš€ å¼€å§‹è¿è¡ŒReact Agentå®Œæ•´æµ‹è¯•å¥—ä»¶")
        self.start_time = datetime.now()
        
        # å®šä¹‰æµ‹è¯•å¥—ä»¶
        test_suite = [
            {
                'file': 'tests/agent/test_react_agent_complete_workflow.py',
                'name': 'React Agentå®Œæ•´å·¥ä½œæµæµ‹è¯•',
                'markers': 'agent'
            },
            {
                'file': 'tests/integration/test_context_agent_coordination.py',
                'name': 'Contextæ§åˆ¶å’ŒAgentåè°ƒæµ‹è¯•',
                'markers': 'integration and agent'
            },
            {
                'file': 'tests/e2e/test_complete_agent_workflow_e2e.py',
                'name': 'ç«¯åˆ°ç«¯ä¸šåŠ¡æµç¨‹æµ‹è¯•',
                'markers': 'e2e and agent and not slow'
            },
            {
                'file': 'tests/e2e/test_complete_agent_workflow_e2e.py',
                'name': 'è´Ÿè½½å‹åŠ›æµ‹è¯•',
                'markers': 'e2e and agent and slow'
            }
        ]
        
        # è¿è¡Œæ¯ä¸ªæµ‹è¯•
        for test_config in test_suite:
            self.run_test_file(
                test_config['file'],
                test_config['name'],
                test_config.get('markers')
            )
        
        self.end_time = datetime.now()
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        self.generate_report()
    
    def generate_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        total_duration = (self.end_time - self.start_time).total_seconds()
        
        self.log("=" * 60)
        self.log("ğŸ“Š React Agentæµ‹è¯•å¥—ä»¶æ‰§è¡ŒæŠ¥å‘Š")
        self.log("=" * 60)
        
        # æ€»ä½“ç»Ÿè®¡
        total_tests = len(self.test_results)
        successful_tests = sum(1 for r in self.test_results if r['success'])
        failed_tests = total_tests - successful_tests
        
        success_rate = (successful_tests / total_tests * 100) if total_tests > 0 else 0
        
        self.log(f"ğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
        self.log(f"   - æµ‹è¯•å¥—ä»¶æ•°é‡: {total_tests}")
        self.log(f"   - æˆåŠŸ: {successful_tests}")
        self.log(f"   - å¤±è´¥: {failed_tests}")
        self.log(f"   - æˆåŠŸç‡: {success_rate:.1f}%")
        self.log(f"   - æ€»è€—æ—¶: {total_duration:.2f}ç§’")
        
        # è¯¦ç»†ç»“æœ
        self.log(f"\nğŸ“‹ è¯¦ç»†ç»“æœ:")
        for result in self.test_results:
            status_icon = "âœ…" if result['success'] else "âŒ"
            self.log(f"   {status_icon} {result['name']} ({result['duration']:.2f}s)")
            
            if result['stats']:
                stats = result['stats']
                if stats['total'] > 0:
                    self.log(f"       æµ‹è¯•ç”¨ä¾‹: {stats['passed']}é€šè¿‡, {stats['failed']}å¤±è´¥, {stats['skipped']}è·³è¿‡")
            
            if not result['success'] and result['error']:
                self.log(f"       é”™è¯¯: {result['error'][:100]}...")
        
        # æ¶æ„éªŒè¯æ€»ç»“
        self.log(f"\nğŸ—ï¸ æ¶æ„éªŒè¯æ€»ç»“:")
        self.log(f"   åŸºäºæˆ‘ä»¬åˆ†æçš„React Agentæ¶æ„:")
        self.log(f"   API â†’ WorkflowOrchestrationAgent â†’ ContextAwareService â†’ AIExecutionEngine â†’ ReactAgent")
        
        if successful_tests >= 3:
            self.log(f"   âœ… æ¶æ„å®Œæ•´æ€§éªŒè¯é€šè¿‡")
            self.log(f"   âœ… Contextä¼ é€’æœºåˆ¶éªŒè¯é€šè¿‡")
            self.log(f"   âœ… å¤šAgentåè°ƒæœºåˆ¶éªŒè¯é€šè¿‡")
        else:
            self.log(f"   âš ï¸ éƒ¨åˆ†æ¶æ„ç»„ä»¶éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
        
        # æ€§èƒ½è¯„ä¼°
        avg_duration = sum(r['duration'] for r in self.test_results) / len(self.test_results)
        if avg_duration < 30:
            self.log(f"   âœ… æ€§èƒ½è¡¨ç°è‰¯å¥½ (å¹³å‡{avg_duration:.1f}s/å¥—ä»¶)")
        else:
            self.log(f"   âš ï¸ æ€§èƒ½éœ€è¦ä¼˜åŒ– (å¹³å‡{avg_duration:.1f}s/å¥—ä»¶)")
        
        # å»ºè®®
        self.log(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        if failed_tests > 0:
            self.log(f"   - æ£€æŸ¥å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹ï¼Œä¿®å¤ç›¸å…³é—®é¢˜")
        if avg_duration > 60:
            self.log(f"   - è€ƒè™‘å¹¶è¡ŒåŒ–æµ‹è¯•æ‰§è¡Œä»¥æå‡æ€§èƒ½")
        if success_rate < 90:
            self.log(f"   - å¢å¼ºé”™è¯¯å¤„ç†å’Œé™çº§æœºåˆ¶")
        else:
            self.log(f"   - å½“å‰æ¶æ„è¿è¡Œè‰¯å¥½ï¼Œå¯è€ƒè™‘æ·»åŠ æ›´å¤šè¾¹ç•Œæƒ…å†µæµ‹è¯•")
        
        self.log("=" * 60)
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        self.save_report_to_file()
    
    def save_report_to_file(self):
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        report_file = ROOT_DIR / "tests" / "react_agent_test_report.json"
        
        import json
        report_data = {
            'test_run_info': {
                'start_time': self.start_time.isoformat(),
                'end_time': self.end_time.isoformat(),
                'total_duration': (self.end_time - self.start_time).total_seconds()
            },
            'summary': {
                'total_test_suites': len(self.test_results),
                'successful_suites': sum(1 for r in self.test_results if r['success']),
                'failed_suites': sum(1 for r in self.test_results if not r['success']),
                'success_rate': (sum(1 for r in self.test_results if r['success']) / len(self.test_results) * 100) if self.test_results else 0
            },
            'detailed_results': [
                {
                    'name': r['name'],
                    'file': r['file'],
                    'success': r['success'],
                    'duration': r['duration'],
                    'stats': r['stats'],
                    'error': r['error'] if not r['success'] else None
                }
                for r in self.test_results
            ]
        }
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        self.log(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")


async def main():
    """ä¸»å‡½æ•°"""
    print("React Agentæ¶æ„å®Œæ•´æµ‹è¯•")
    print("=" * 50)
    print("åŸºäºåˆ†æçš„æ¶æ„è¿›è¡Œå…¨é¢æµ‹è¯•éªŒè¯:")
    print("1. React AgentåŸºç¡€åŠŸèƒ½å’Œæ¨¡å‹é€‰æ‹©")
    print("2. Contextåœ¨å„å±‚çº§é—´çš„ä¼ é€’å’Œç®¡ç†")
    print("3. å¤šAgentåè°ƒå’Œå·¥ä½œæµç¼–æ’")
    print("4. ç«¯åˆ°ç«¯ä¸šåŠ¡æµç¨‹éªŒè¯")
    print("5. æ€§èƒ½å’Œè´Ÿè½½æµ‹è¯•")
    print("=" * 50)
    
    runner = TestRunner()
    await runner.run_all_tests()
    
    return runner.test_results


if __name__ == "__main__":
    # æ£€æŸ¥Pythonç‰ˆæœ¬
    if sys.version_info < (3, 8):
        print("âŒ éœ€è¦Python 3.8æˆ–æ›´é«˜ç‰ˆæœ¬")
        sys.exit(1)
    
    # æ£€æŸ¥ä¾èµ–
    try:
        import pytest
    except ImportError:
        print("âŒ è¯·å®‰è£…pytest: pip install pytest pytest-asyncio")
        sys.exit(1)
    
    # è¿è¡Œæµ‹è¯•
    results = asyncio.run(main())
    
    # æ ¹æ®ç»“æœè®¾ç½®é€€å‡ºç 
    failed_count = sum(1 for r in results if not r['success'])
    sys.exit(failed_count)