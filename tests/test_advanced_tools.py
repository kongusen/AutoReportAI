#!/usr/bin/env python3
"""
æµ‹è¯•é«˜çº§AIå·¥å…·å®ç°
"""

import asyncio
import json
from datetime import datetime

# æµ‹è¯•SQLç”Ÿæˆå™¨
async def test_sql_generator():
    print("=== æµ‹è¯•SQLç”Ÿæˆå™¨ ===")
    
    try:
        from backend.app.services.infrastructure.ai.sql.sql_generator_service import sql_generator_service
        
        # æµ‹è¯•åŸºç¡€æŸ¥è¯¢ç”Ÿæˆ
        requirements = {
            "description": "æŸ¥è¯¢ç”¨æˆ·è¡¨ä¸­ä»Šå¹´æ³¨å†Œçš„ç”¨æˆ·æ€»æ•°",
            "entity": "users",
            "operation": "COUNT",
            "time_range": "this_year",
            "columns": ["*"]
        }
        
        schema_info = {
            "database_type": "mysql",
            "tables": [
                {
                    "name": "users",
                    "columns": [
                        {"name": "id", "type": "int"},
                        {"name": "username", "type": "varchar"},
                        {"name": "email", "type": "varchar"},
                        {"name": "created_at", "type": "datetime"}
                    ]
                }
            ]
        }
        
        result = await sql_generator_service.generate_query(requirements, schema_info)
        
        print(f"âœ… SQLç”ŸæˆæˆåŠŸ")
        print(f"   ç”Ÿæˆçš„SQL: {result['sql'][:100]}...")
        print(f"   æŸ¥è¯¢ç±»å‹: {result['query_type']}")
        print(f"   å¤æ‚åº¦: {result['complexity']}")
        print(f"   é¢„ä¼°æˆæœ¬: {result['estimated_cost']}")
        print(f"   æ€§èƒ½æç¤ºæ•°é‡: {len(result['performance_hints'])}")
        
        return True
        
    except Exception as e:
        print(f"âŒ SQLç”Ÿæˆå™¨æµ‹è¯•å¤±è´¥: {e}")
        return False


# æµ‹è¯•æ•°æ®æºåˆ†æå™¨
async def test_data_source_analyzer():
    print("\n=== æµ‹è¯•æ•°æ®æºåˆ†æå™¨ ===")
    
    try:
        from backend.app.services.infrastructure.ai.analyzer.data_source_analyzer_service import data_source_analyzer_service
        
        # æµ‹è¯•æ•°æ®æºåˆ†æ
        data_source_config = {
            "id": "test_mysql_db",
            "source_type": "mysql", 
            "host": "localhost",
            "port": 3306,
            "database": "test_db",
            "estimated_tables": 15,
            "version": "8.0.25"
        }
        
        result = await data_source_analyzer_service.analyze_data_source(data_source_config)
        
        print(f"âœ… æ•°æ®æºåˆ†ææˆåŠŸ")
        print(f"   æ•´ä½“å¥åº·çŠ¶æ€: {result['overall_health']}")
        print(f"   æ€§èƒ½è¯„åˆ†: {result['performance_score']}")
        print(f"   å“åº”æ—¶é—´: {result['health_check']['response_time']:.3f}s")
        print(f"   å»ºè®®æ•°é‡: {len(result['recommendations'])}")
        print(f"   é—®é¢˜æ•°é‡: {len(result['issues'])}")
        print(f"   æ•°æ®è´¨é‡è¯„åˆ†: {result['data_quality']['overall_score']}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®æºåˆ†æå™¨æµ‹è¯•å¤±è´¥: {e}")
        return False


# æµ‹è¯•Schemaæ£€æŸ¥å™¨
async def test_schema_inspector():
    print("\n=== æµ‹è¯•Schemaæ£€æŸ¥å™¨ ===")
    
    try:
        from backend.app.services.infrastructure.ai.schema.schema_inspector_service import schema_inspector_service
        
        # æµ‹è¯•Schemaåˆ†æ
        schema_info = {
            "schema_name": "ecommerce",
            "database_type": "mysql",
            "tables": [
                {
                    "name": "users",
                    "row_count": 50000,
                    "size_kb": 2048,
                    "columns": [
                        {"name": "id", "type": "int", "is_primary_key": True},
                        {"name": "username", "type": "varchar", "max_length": 50},
                        {"name": "email", "type": "varchar", "max_length": 100},
                        {"name": "created_at", "type": "datetime"}
                    ],
                    "indexes": [
                        {"name": "PRIMARY", "type": "primary", "columns": ["id"]},
                        {"name": "idx_email", "type": "unique", "columns": ["email"]}
                    ]
                },
                {
                    "name": "orders",
                    "row_count": 100000,
                    "size_kb": 5120,
                    "columns": [
                        {"name": "id", "type": "int", "is_primary_key": True},
                        {"name": "user_id", "type": "int", "is_foreign_key": True, "foreign_table": "users"},
                        {"name": "total_amount", "type": "decimal"},
                        {"name": "status", "type": "varchar"},
                        {"name": "created_at", "type": "datetime"}
                    ],
                    "indexes": [
                        {"name": "PRIMARY", "type": "primary", "columns": ["id"]},
                        {"name": "idx_user_id", "type": "index", "columns": ["user_id"]}
                    ]
                }
            ]
        }
        
        result = await schema_inspector_service.inspect_schema(schema_info)
        
        print(f"âœ… Schemaæ£€æŸ¥æˆåŠŸ")
        print(f"   æ•´ä½“å¥åº·è¯„åˆ†: {result['overall_health_score']}")
        print(f"   å¤æ‚åº¦è¯„åˆ†: {result['complexity_score']}")
        print(f"   è¡¨æ•°é‡: {result['table_count']}")
        print(f"   å…³ç³»æ•°é‡: {result['relationship_count']}")
        print(f"   ä¼˜åŒ–æœºä¼š: {len(result['optimization_opportunities'])}")
        print(f"   æ½œåœ¨é—®é¢˜: {len(result['potential_issues'])}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Schemaæ£€æŸ¥å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False


# æµ‹è¯•æ€§èƒ½ä¼˜åŒ–å™¨
async def test_performance_optimizer():
    print("\n=== æµ‹è¯•æ€§èƒ½ä¼˜åŒ–å™¨ ===")
    
    try:
        from backend.app.services.infrastructure.ai.performance.performance_optimizer_service import performance_optimizer_service
        
        # æµ‹è¯•æ€§èƒ½ä¼˜åŒ–
        performance_data = {
            "response_time": 3.5,
            "cpu_usage": 85.0,
            "memory_usage": 75.0,
            "error_rate": 0.08,
            "throughput": 150.0,
            "connection_pool_usage": 0.9,
            "cache_hit_rate": 0.65,
            "bottlenecks": ["database_query", "memory_usage"]
        }
        
        result = await performance_optimizer_service.optimize_performance(performance_data)
        
        print(f"âœ… æ€§èƒ½ä¼˜åŒ–åˆ†ææˆåŠŸ")
        print(f"   æ•´ä½“æ€§èƒ½è¯„åˆ†: {result['overall_score']}")
        print(f"   å½“å‰å“åº”æ—¶é—´: {result['current_performance']['response_time']}s")
        print(f"   ä¼˜åŒ–åå“åº”æ—¶é—´: {result['optimized_performance']['avg_response_time']:.2f}s")
        print(f"   æ£€æµ‹åˆ°ç“¶é¢ˆ: {len(result['bottlenecks'])}")
        print(f"   ä¼˜åŒ–å»ºè®®: {len(result['optimization_recommendations'])}")
        print(f"   å¿«é€Ÿæ”¹è¿›æœºä¼š: {len(result['quick_wins'])}")
        print(f"   æ”¹è¿›æ½œåŠ›: {result['improvement_potential']['overall']:.1%}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ€§èƒ½ä¼˜åŒ–å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False


# æµ‹è¯•æŠ¥å‘Šè´¨é‡æ£€æŸ¥å™¨
async def test_report_quality_checker():
    print("\n=== æµ‹è¯•æŠ¥å‘Šè´¨é‡æ£€æŸ¥å™¨ ===")
    
    try:
        from backend.app.services.infrastructure.ai.quality.report_quality_checker_service import report_quality_checker_service
        
        # æµ‹è¯•æŠ¥å‘Šè´¨é‡æ£€æŸ¥
        report_content = """
        # ç”µå•†å¹³å°æœˆåº¦é”€å”®åˆ†ææŠ¥å‘Š
        
        ## æ‘˜è¦
        æœ¬æœˆç”µå•†å¹³å°æ€»é”€å”®é¢è¾¾åˆ°äº†1,250,000å…ƒï¼Œç›¸æ¯”ä¸Šæœˆå¢é•¿äº†15.8%ã€‚
        è®¢å•æ•°é‡ä¸º8,500å•ï¼Œå¹³å‡è®¢å•ä»·å€¼ä¸º147.06å…ƒã€‚
        
        ## è¯¦ç»†åˆ†æ
        
        ### é”€å”®è¶‹åŠ¿
        æ ¹æ®æ•°æ®æ˜¾ç¤ºï¼Œæœ¬æœˆé”€å”®å‘ˆç°ç¨³å®šä¸Šå‡è¶‹åŠ¿ã€‚å…¶ä¸­ï¼š
        - æ‰‹æœºç±»äº§å“é”€å”®é¢å æ¯”35%ï¼Œè¾¾åˆ°437,500å…ƒ
        - æœè£…ç±»äº§å“é”€å”®é¢å æ¯”28%ï¼Œè¾¾åˆ°350,000å…ƒ
        - å®¶ç”µç±»äº§å“é”€å”®é¢å æ¯”22%ï¼Œè¾¾åˆ°275,000å…ƒ
        
        ### ç”¨æˆ·è¡Œä¸ºåˆ†æ
        é€šè¿‡åˆ†æç”¨æˆ·è´­ä¹°è¡Œä¸ºï¼Œå‘ç°ä»¥ä¸‹ç‰¹ç‚¹ï¼š
        1. ç§»åŠ¨ç«¯è´­ä¹°å æ¯”è¾¾åˆ°68%
        2. æ–°ç”¨æˆ·è½¬åŒ–ç‡ä¸º12.5%
        3. å¤è´­ç‡è¾¾åˆ°35%
        
        ## ç»“è®ºå’Œå»ºè®®
        åŸºäºä»¥ä¸Šåˆ†æï¼Œå»ºè®®ï¼š
        1. ç»§ç»­åŠ å¼ºæ‰‹æœºç±»äº§å“çš„æ¨å¹¿åŠ›åº¦
        2. ä¼˜åŒ–ç§»åŠ¨ç«¯è´­ä¹°ä½“éªŒ
        3. å®æ–½æ–°ç”¨æˆ·æ¿€åŠ±è®¡åˆ’ï¼Œæé«˜è½¬åŒ–ç‡
        4. é€šè¿‡ä¸ªæ€§åŒ–æ¨èæå‡å¤è´­ç‡
        
        æ•°æ®æ¥æºï¼šç”µå•†å¹³å°ä¸šåŠ¡ç³»ç»Ÿ
        """
        
        result = await report_quality_checker_service.check_report_quality(report_content)
        
        print(f"âœ… æŠ¥å‘Šè´¨é‡æ£€æŸ¥æˆåŠŸ")
        print(f"   æ•´ä½“è¯„åˆ†: {result['overall_score']}")
        print(f"   è´¨é‡ç­‰çº§: {result['quality_level']}")
        print(f"   è´¨é‡è¯„çº§: {result['quality_grade']}")
        print(f"   æ˜¯å¦å¯æ¥å—: {result['is_acceptable']}")
        print(f"   æ˜¯å¦éœ€è¦ä¿®è®¢: {result['requires_revision']}")
        print(f"   å‘ç°é—®é¢˜: {len(result['issues'])}")
        print(f"   æ”¹è¿›å»ºè®®: {len(result['suggestions'])}")
        print(f"   è‡ªåŠ¨æ”¹è¿›æœºä¼š: {len(result['auto_improvements'])}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æŠ¥å‘Šè´¨é‡æ£€æŸ¥å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False


# æµ‹è¯•ä¸Šä¸‹æ–‡åˆ†æå™¨
async def test_context_analyzer():
    print("\n=== æµ‹è¯•ä¸Šä¸‹æ–‡åˆ†æå™¨ ===")
    
    try:
        from backend.app.services.infrastructure.ai.context.context_analyzer_service import context_analyzer_service
        
        # æµ‹è¯•ä¸Šä¸‹æ–‡åˆ†æ
        context_data = {
            "user_query": "åˆ†ææœ€è¿‘ä¸€ä¸ªæœˆçš„é”€å”®æ•°æ®ï¼Œç‰¹åˆ«å…³æ³¨æ‰‹æœºäº§å“çš„è¡¨ç°",
            "data_source": "mysql://localhost/ecommerce",
            "template": "monthly_sales_report.html",
            "business_context": "ç”µå•†å¹³å°è¿è¥åˆ†æ",
            "time_range": "last_month",
            "analysis_depth": "deep"
        }
        
        result = context_analyzer_service.analyze_context(context_data)
        
        print(f"âœ… ä¸Šä¸‹æ–‡åˆ†ææˆåŠŸ")
        print(f"   è¯†åˆ«çš„ä¸Šä¸‹æ–‡ç±»å‹: {len(result['metadata']['context_types'])}")
        print(f"   ç”Ÿæˆçš„æ´å¯Ÿ: {len(result['insights'])}")
        print(f"   ç½®ä¿¡åº¦æå‡: {result['confidence_improvement']:.2f}")
        print(f"   åˆ†æå®Œæˆ: {result['analysis_complete']}")
        print(f"   å¢å¼ºåº”ç”¨: {result['metadata']['enhancement_applied']}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ä¸Šä¸‹æ–‡åˆ†æå™¨æµ‹è¯•å¤±è´¥: {e}")
        return False


# æµ‹è¯•å¢å¼ºæ¨ç†
async def test_enhanced_reasoning():
    print("\n=== æµ‹è¯•å¢å¼ºæ¨ç† ===")
    
    try:
        from backend.app.services.infrastructure.ai.reasoning.enhanced_reasoning_service import enhanced_reasoning_service
        
        # æµ‹è¯•å¢å¼ºæ¨ç†
        problem = "åˆ†æç”µå•†å¹³å°é”€å”®æ•°æ®ä¸‹é™çš„å¯èƒ½åŸå› ï¼Œå¹¶æå‡ºè§£å†³æ–¹æ¡ˆ"
        context = {
            "data": "sales_decline_analysis",
            "business_domain": "ecommerce",
            "time_period": "last_quarter",
            "stakeholders": ["marketing_team", "product_team", "management"]
        }
        
        result = await enhanced_reasoning_service.perform_reasoning(problem, context)
        
        print(f"âœ… å¢å¼ºæ¨ç†æˆåŠŸ")
        print(f"   æ¨ç†æ­¥éª¤: {len(result['reasoning_steps'])}")
        print(f"   æ•´ä½“ç½®ä¿¡åº¦: {result['confidence']}")
        print(f"   ç”Ÿæˆå»ºè®®: {len(result['recommendations'])}")
        print(f"   æ¨ç†æ¨¡å¼: {result['metadata']['reasoning_pattern']}")
        print(f"   å¤æ‚åº¦: {result['metadata']['complexity']}")
        print(f"   ä½¿ç”¨æ–¹æ³•: {', '.join(result['metadata']['reasoning_methods'])}")
        
        return True
        
    except Exception as e:
        print(f"âŒ å¢å¼ºæ¨ç†æµ‹è¯•å¤±è´¥: {e}")
        return False


# æµ‹è¯•å·¥å…·ç›‘æ§
async def test_tool_monitoring():
    print("\n=== æµ‹è¯•å·¥å…·ç›‘æ§ ===")
    
    try:
        from backend.app.services.infrastructure.ai.monitoring.tool_monitor_service import tool_monitor_service
        
        # å¯åŠ¨ç›‘æ§
        tool_monitor_service.start_monitoring()
        
        # æ¨¡æ‹Ÿå·¥å…·æ‰§è¡Œç›‘æ§
        async def mock_tool_execution():
            await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿå·¥å…·æ‰§è¡Œ
            return {"result": "success", "data": [1, 2, 3]}
        
        # ç›‘æ§å·¥å…·æ‰§è¡Œ
        result = await tool_monitor_service.monitor_tool_execution(
            "test_tool",
            mock_tool_execution
        )
        
        # è·å–ç³»ç»Ÿæ¦‚è§ˆ
        overview = await tool_monitor_service.get_system_overview()
        
        print(f"âœ… å·¥å…·ç›‘æ§æˆåŠŸ")
        print(f"   ç›‘æ§çŠ¶æ€: {'æ¿€æ´»' if overview['monitoring_active'] else 'æœªæ¿€æ´»'}")
        print(f"   ç›‘æ§çº§åˆ«: {overview['monitoring_level']}")
        print(f"   ç›‘æ§å·¥å…·æ•°: {overview['total_tools_monitored']}")
        print(f"   æ€»æ‰§è¡Œæ¬¡æ•°: {overview['total_executions']}")
        print(f"   æ•´ä½“æˆåŠŸç‡: {overview['overall_success_rate']:.1%}")
        
        # è·å–å·¥å…·æ€§èƒ½æŠ¥å‘Š
        report = await tool_monitor_service.get_tool_performance_report("test_tool")
        if report:
            print(f"   æµ‹è¯•å·¥å…·æ‰§è¡Œæ—¶é—´: {report.avg_execution_time:.3f}s")
            print(f"   æµ‹è¯•å·¥å…·æˆåŠŸç‡: {report.success_rate:.1%}")
        
        return True
        
    except Exception as e:
        print(f"âŒ å·¥å…·ç›‘æ§æµ‹è¯•å¤±è´¥: {e}")
        return False


# ä¸»æµ‹è¯•å‡½æ•°
async def main():
    print("ğŸš€ å¼€å§‹æµ‹è¯•é«˜çº§AIå·¥å…·å®ç°...")
    print(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    test_results = {}
    
    # æŒ‰é¡ºåºæ‰§è¡Œæµ‹è¯•
    tests = [
        ("SQLç”Ÿæˆå™¨", test_sql_generator),
        ("æ•°æ®æºåˆ†æå™¨", test_data_source_analyzer), 
        ("Schemaæ£€æŸ¥å™¨", test_schema_inspector),
        ("æ€§èƒ½ä¼˜åŒ–å™¨", test_performance_optimizer),
        ("æŠ¥å‘Šè´¨é‡æ£€æŸ¥å™¨", test_report_quality_checker),
        ("ä¸Šä¸‹æ–‡åˆ†æå™¨", test_context_analyzer),
        ("å¢å¼ºæ¨ç†", test_enhanced_reasoning),
        ("å·¥å…·ç›‘æ§", test_tool_monitoring)
    ]
    
    for test_name, test_func in tests:
        try:
            success = await test_func()
            test_results[test_name] = success
        except Exception as e:
            print(f"âŒ {test_name} æµ‹è¯•å‡ºç°å¼‚å¸¸: {e}")
            test_results[test_name] = False
    
    # è¾“å‡ºæµ‹è¯•æ€»ç»“
    print("\n" + "="*50)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“")
    print("="*50)
    
    passed = 0
    failed = 0
    
    for test_name, success in test_results.items():
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"{test_name:<20} {status}")
        if success:
            passed += 1
        else:
            failed += 1
    
    total = len(test_results)
    success_rate = (passed / total) * 100 if total > 0 else 0
    
    print("-" * 50)
    print(f"æ€»æµ‹è¯•æ•°: {total}")
    print(f"é€šè¿‡: {passed}")  
    print(f"å¤±è´¥: {failed}")
    print(f"æˆåŠŸç‡: {success_rate:.1f}%")
    
    if success_rate >= 90:
        print("\nğŸ‰ æ‰€æœ‰å·¥å…·å®ç°è´¨é‡è‰¯å¥½ï¼")
    elif success_rate >= 70:
        print("\nâœ¨ å¤§éƒ¨åˆ†å·¥å…·å®ç°æˆåŠŸï¼Œå°‘æ•°éœ€è¦è°ƒæ•´")
    else:
        print("\nâš ï¸  éƒ¨åˆ†å·¥å…·éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
    
    return success_rate >= 70


if __name__ == "__main__":
    asyncio.run(main())