#!/usr/bin/env python3
"""
çº¯æœåŠ¡å±‚æµ‹è¯•ï¼ˆä¸å¯¼å…¥ä»»ä½•æ¨¡å‹å’Œæ•°æ®åº“ç›¸å…³ä»£ç ï¼‰
"""

import asyncio
import sys
import os
from datetime import datetime

# æµ‹è¯•SQLç”Ÿæˆå™¨æ ¸å¿ƒé€»è¾‘
async def test_sql_generation_logic():
    print("=== æµ‹è¯•SQLç”Ÿæˆé€»è¾‘ ===")
    
    try:
        # ç›´æ¥æµ‹è¯•SQLç”Ÿæˆé€»è¾‘
        from backend.app.services.infrastructure.ai.sql.sql_generator_service import SQLGeneratorService
        
        # åˆ›å»ºæœåŠ¡å®ä¾‹
        sql_service = SQLGeneratorService()
        
        # æµ‹è¯•éœ€æ±‚è§£æ
        requirements = {
            "description": "æŸ¥è¯¢ç”¨æˆ·è¡¨ä¸­ä»Šå¹´æ³¨å†Œçš„ç”¨æˆ·æ€»æ•°",
            "entity": "users",
            "operation": "COUNT",
            "time_range": "this_year",
            "columns": ["*"],
            "filters": ["status = 'active'"]
        }
        
        # æµ‹è¯•éœ€æ±‚è§£ææ–¹æ³•
        parsed = sql_service._parse_requirements(requirements)
        
        print(f"âœ… éœ€æ±‚è§£ææˆåŠŸ")
        print(f"   å®ä½“: {parsed['entity']}")
        print(f"   æ“ä½œ: {parsed['operation']}")
        print(f"   æ—¶é—´èŒƒå›´: {parsed.get('time_range', 'None')}")
        print(f"   è¿‡æ»¤æ¡ä»¶: {len(parsed['filters'])}")
        
        # æµ‹è¯•æŸ¥è¯¢ç±»å‹ç¡®å®š
        query_type = sql_service._determine_query_type(parsed)
        print(f"   æŸ¥è¯¢ç±»å‹: {query_type.value}")
        
        # æµ‹è¯•å¤æ‚åº¦è¯„ä¼°
        complexity = sql_service._assess_complexity(parsed, None)
        print(f"   å¤æ‚åº¦: {complexity.value}")
        
        return True
        
    except Exception as e:
        print(f"âŒ SQLç”Ÿæˆé€»è¾‘æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


# æµ‹è¯•æ•°æ®æºåˆ†æå™¨æ ¸å¿ƒé€»è¾‘
async def test_data_source_analyzer_logic():
    print("\n=== æµ‹è¯•æ•°æ®æºåˆ†æé€»è¾‘ ===")
    
    try:
        from backend.app.services.infrastructure.ai.analyzer.data_source_analyzer_service import DataSourceAnalyzerService
        
        analyzer = DataSourceAnalyzerService()
        
        # æµ‹è¯•é…ç½®è§£æ
        config = {
            "id": "test_db",
            "source_type": "mysql",
            "host": "localhost",
            "port": 3306
        }
        
        parsed_data = analyzer._parse_performance_data({
            "response_time": 2.5,
            "cpu_usage": 65.0,
            "memory_usage": 70.0,
            "error_rate": 0.03
        })
        
        print(f"âœ… æ€§èƒ½æ•°æ®è§£ææˆåŠŸ")
        print(f"   å“åº”æ—¶é—´: {parsed_data['avg_response_time']}s")
        print(f"   CPUä½¿ç”¨ç‡: {parsed_data['cpu_usage']}%")
        print(f"   å†…å­˜ä½¿ç”¨ç‡: {parsed_data['memory_usage']}%")
        print(f"   é”™è¯¯ç‡: {parsed_data['error_rate']:.1%}")
        
        # æµ‹è¯•è¯„åˆ†è®¡ç®—
        overall_score = analyzer._calculate_overall_score(
            analyzer.health_check_config, parsed_data, [], {}
        )
        
        print(f"   æ•´ä½“è¯„åˆ†è®¡ç®—: æˆåŠŸ")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®æºåˆ†æé€»è¾‘æµ‹è¯•å¤±è´¥: {e}")
        return False


# æµ‹è¯•Schemaæ£€æŸ¥å™¨æ ¸å¿ƒé€»è¾‘
async def test_schema_inspector_logic():
    print("\n=== æµ‹è¯•Schemaæ£€æŸ¥é€»è¾‘ ===")
    
    try:
        from backend.app.services.infrastructure.ai.schema.schema_inspector_service import SchemaInspectorService
        
        inspector = SchemaInspectorService()
        
        # æµ‹è¯•åˆ—ç±»å‹è§£æ
        column_types = [
            ("varchar(50)", "STRING"),
            ("int", "INTEGER"),
            ("datetime", "DATETIME"),
            ("decimal(10,2)", "DECIMAL"),
            ("boolean", "BOOLEAN")
        ]
        
        for type_str, expected in column_types:
            parsed_type = inspector._parse_column_type(type_str)
            print(f"   {type_str} -> {parsed_type.value}")
            
        # æµ‹è¯•å¤æ‚åº¦è®¡ç®—
        tables = [
            {"name": "users", "columns": [{"name": "id"}, {"name": "email"}]},
            {"name": "orders", "columns": [{"name": "id"}, {"name": "user_id"}, {"name": "total"}]}
        ]
        
        complexity = inspector._calculate_complexity_score(tables, [])
        print(f"âœ… Schemaåˆ†æé€»è¾‘æµ‹è¯•æˆåŠŸ")
        print(f"   å¤æ‚åº¦è¯„åˆ†: {complexity}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Schemaæ£€æŸ¥é€»è¾‘æµ‹è¯•å¤±è´¥: {e}")
        return False


# æµ‹è¯•æ€§èƒ½ä¼˜åŒ–å™¨æ ¸å¿ƒé€»è¾‘
async def test_performance_optimizer_logic():
    print("\n=== æµ‹è¯•æ€§èƒ½ä¼˜åŒ–é€»è¾‘ ===")
    
    try:
        from backend.app.services.infrastructure.ai.performance.performance_optimizer_service import PerformanceOptimizerService
        
        optimizer = PerformanceOptimizerService()
        
        # æµ‹è¯•æ€§èƒ½æ•°æ®è§£æ
        data = {
            "response_time": 5.2,
            "cpu_usage": 88.0,
            "memory_usage": 92.0,
            "error_rate": 0.12,
            "bottlenecks": ["database_query", "memory_usage"]
        }
        
        parsed = optimizer._parse_performance_data(data)
        print(f"âœ… æ€§èƒ½æ•°æ®è§£ææˆåŠŸ")
        print(f"   å¹³å‡å“åº”æ—¶é—´: {parsed['avg_response_time']}s")
        print(f"   CPUä½¿ç”¨ç‡: {parsed['cpu_usage']}%")
        print(f"   å†…å­˜ä½¿ç”¨ç‡: {parsed['memory_usage']}%")
        
        # æµ‹è¯•è¯„åˆ†è®¡ç®—
        score = optimizer._calculate_performance_score(parsed, [])
        print(f"   æ€§èƒ½è¯„åˆ†: {score}")
        
        # æµ‹è¯•ä¼˜åŒ–æŒ‡æ ‡è®¡ç®—
        optimized = optimizer._calculate_optimized_metrics(parsed, [])
        print(f"   ä¼˜åŒ–åå“åº”æ—¶é—´: {optimized['avg_response_time']:.2f}s")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ€§èƒ½ä¼˜åŒ–é€»è¾‘æµ‹è¯•å¤±è´¥: {e}")
        return False


# æµ‹è¯•æŠ¥å‘Šè´¨é‡æ£€æŸ¥å™¨æ ¸å¿ƒé€»è¾‘
async def test_report_quality_logic():
    print("\n=== æµ‹è¯•æŠ¥å‘Šè´¨é‡æ£€æŸ¥é€»è¾‘ ===")
    
    try:
        from backend.app.services.infrastructure.ai.quality.report_quality_checker_service import ReportQualityCheckerService
        
        checker = ReportQualityCheckerService()
        
        # æµ‹è¯•å†…å®¹åˆ†æ
        content = """
        # é”€å”®åˆ†ææŠ¥å‘Š
        
        æœ¬æœˆé”€å”®é¢è¾¾åˆ°1,250,000å…ƒï¼Œå¢é•¿15.8%ã€‚
        
        ## ä¸»è¦å‘ç°
        - ç§»åŠ¨ç«¯å æ¯”68%
        - æ–°ç”¨æˆ·è½¬åŒ–ç‡12.5%
        - å¤è´­ç‡35%
        
        ## å»ºè®®
        1. åŠ å¼ºç§»åŠ¨ç«¯ä½“éªŒ
        2. æé«˜è½¬åŒ–ç‡
        """
        
        # æµ‹è¯•å†…å®¹å®Œæ•´æ€§è¯„ä¼°
        completeness_score = checker._assess_content_completeness(content, checker.quality_rules)
        print(f"âœ… å†…å®¹å®Œæ•´æ€§è¯„ä¼°: {completeness_score}")
        
        # æµ‹è¯•ç»“æ„æ¸…æ™°åº¦è¯„ä¼°
        structure_score = checker._assess_structure_clarity(content, checker.quality_rules)
        print(f"   ç»“æ„æ¸…æ™°åº¦è¯„ä¼°: {structure_score}")
        
        # æµ‹è¯•è¯­è¨€è´¨é‡è¯„ä¼°
        language_score = checker._assess_language_quality(content, checker.quality_rules)
        print(f"   è¯­è¨€è´¨é‡è¯„ä¼°: {language_score}")
        
        # æµ‹è¯•æ•´ä½“è¯„åˆ†è®¡ç®—
        from backend.app.services.infrastructure.ai.quality.report_quality_checker_service import QualityMetric, QualityDimension
        
        metrics = [
            QualityMetric(QualityDimension.CONTENT_COMPLETENESS, completeness_score, 0.2),
            QualityMetric(QualityDimension.STRUCTURE_CLARITY, structure_score, 0.15),
            QualityMetric(QualityDimension.LANGUAGE_QUALITY, language_score, 0.12)
        ]
        
        overall_score = checker._calculate_overall_score(metrics)
        print(f"   æ•´ä½“è´¨é‡è¯„åˆ†: {overall_score}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æŠ¥å‘Šè´¨é‡æ£€æŸ¥é€»è¾‘æµ‹è¯•å¤±è´¥: {e}")
        return False


# æµ‹è¯•ä¸Šä¸‹æ–‡åˆ†æå™¨æ ¸å¿ƒé€»è¾‘
async def test_context_analyzer_logic():
    print("\n=== æµ‹è¯•ä¸Šä¸‹æ–‡åˆ†æé€»è¾‘ ===")
    
    try:
        from backend.app.services.infrastructure.ai.context.context_analyzer_service import ContextAnalyzerService
        
        analyzer = ContextAnalyzerService()
        
        # æµ‹è¯•ä¸Šä¸‹æ–‡ç±»å‹è¯†åˆ«
        context_data = {
            "user_query": "åˆ†ææœ€è¿‘é”€å”®æ•°æ®",
            "data_source": "mysql://localhost/ecommerce",
            "template": "report.html",
            "business_context": "ç”µå•†è¿è¥"
        }
        
        context_types = analyzer._identify_context_types(context_data)
        print(f"âœ… ä¸Šä¸‹æ–‡ç±»å‹è¯†åˆ«æˆåŠŸ")
        print(f"   è¯†åˆ«çš„ç±»å‹: {[ct.value for ct in context_types]}")
        
        # æµ‹è¯•å¤æ‚åº¦è®¡ç®—
        complexity = analyzer._calculate_complexity_score(context_data)
        print(f"   ä¸Šä¸‹æ–‡å¤æ‚åº¦: {complexity}")
        
        # æµ‹è¯•å¢å¼ºå¤„ç†
        enhanced = analyzer._enhance_context(context_data, context_types)
        print(f"   å¢å¼ºæˆåŠŸ: {len(enhanced) > len(context_data)}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ä¸Šä¸‹æ–‡åˆ†æé€»è¾‘æµ‹è¯•å¤±è´¥: {e}")
        return False


# æµ‹è¯•å¢å¼ºæ¨ç†æ ¸å¿ƒé€»è¾‘  
async def test_enhanced_reasoning_logic():
    print("\n=== æµ‹è¯•å¢å¼ºæ¨ç†é€»è¾‘ ===")
    
    try:
        from backend.app.services.infrastructure.ai.reasoning.enhanced_reasoning_service import EnhancedReasoningService
        
        reasoning = EnhancedReasoningService()
        
        # æµ‹è¯•æ¨ç†æ¨¡å¼è¯†åˆ«
        problem = "åˆ†æé”€å”®æ•°æ®ä¸‹é™çš„åŸå› å¹¶æå‡ºè§£å†³æ–¹æ¡ˆ"
        pattern = reasoning._identify_reasoning_pattern(problem, None)
        print(f"âœ… æ¨ç†æ¨¡å¼è¯†åˆ«: {pattern['pattern']}")
        print(f"   å¤æ‚åº¦: {pattern['complexity'].value}")
        
        # æµ‹è¯•æ¨ç†æ–¹æ³•é€‰æ‹©
        methods = reasoning._select_reasoning_methods(problem, pattern)
        print(f"   é€‰æ‹©çš„æ¨ç†æ–¹æ³•: {[m.value for m in methods]}")
        
        # æµ‹è¯•åˆ†æç”Ÿæˆ
        from app.services.infrastructure.ai.reasoning.enhanced_reasoning_service import ReasoningStep, ReasoningType
        
        steps = [
            ReasoningStep(1, ReasoningType.INDUCTIVE, "è§‚å¯Ÿæ•°æ®", "å½’çº³åˆ†æ", "è¯†åˆ«è¶‹åŠ¿", 0.8),
            ReasoningStep(2, ReasoningType.CAUSAL, "åˆ†æåŸå› ", "å› æœæ¨ç†", "æ‰¾åˆ°æ ¹å› ", 0.75)
        ]
        
        analysis = reasoning._generate_analysis(problem, steps, {})
        print(f"   ç”Ÿæˆåˆ†æ: {analysis[:50]}...")
        
        return True
        
    except Exception as e:
        print(f"âŒ å¢å¼ºæ¨ç†é€»è¾‘æµ‹è¯•å¤±è´¥: {e}")
        return False


# æµ‹è¯•å·¥å…·ç›‘æ§æ ¸å¿ƒé€»è¾‘
async def test_monitoring_logic():
    print("\n=== æµ‹è¯•ç›‘æ§é€»è¾‘ ===")
    
    try:
        from app.services.infrastructure.ai.monitoring.tool_monitor_service import ToolMonitorService
        
        monitor = ToolMonitorService()
        
        # æµ‹è¯•é˜ˆå€¼é…ç½®
        thresholds = monitor.get_performance_thresholds()
        print(f"âœ… æ€§èƒ½é˜ˆå€¼é…ç½®: {len(thresholds)} é¡¹")
        
        # æµ‹è¯•ç™¾åˆ†ä½æ•°è®¡ç®—
        values = [1.2, 2.5, 3.1, 4.8, 5.2, 6.0, 7.3, 8.1, 9.2, 10.5]
        p95 = monitor._calculate_percentile(values, 0.95)
        print(f"   P95è®¡ç®—: {p95}")
        
        # æµ‹è¯•è¶‹åŠ¿åˆ†æ
        execution_times = [2.1, 2.3, 2.0, 2.4, 2.2, 2.8, 3.0, 3.2, 3.5, 3.8]
        trend = monitor._analyze_performance_trend("test_tool", execution_times)
        print(f"   æ€§èƒ½è¶‹åŠ¿: {trend}")
        
        # æµ‹è¯•è¾“å…¥å¤§å°è®¡ç®—
        input_size = monitor._calculate_input_size(("test", 123), {"param": "value"})
        print(f"   è¾“å…¥å¤§å°è®¡ç®—: {input_size} bytes")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç›‘æ§é€»è¾‘æµ‹è¯•å¤±è´¥: {e}")
        return False


# ä¸»æµ‹è¯•å‡½æ•°
async def main():
    print("ğŸ§ª å¼€å§‹çº¯æœåŠ¡å±‚é€»è¾‘æµ‹è¯•...")
    print(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    test_results = {}
    
    # æŒ‰é¡ºåºæ‰§è¡Œæµ‹è¯•
    tests = [
        ("SQLç”Ÿæˆé€»è¾‘", test_sql_generation_logic),
        ("æ•°æ®æºåˆ†æé€»è¾‘", test_data_source_analyzer_logic),
        ("Schemaæ£€æŸ¥é€»è¾‘", test_schema_inspector_logic),
        ("æ€§èƒ½ä¼˜åŒ–é€»è¾‘", test_performance_optimizer_logic),
        ("æŠ¥å‘Šè´¨é‡æ£€æŸ¥é€»è¾‘", test_report_quality_logic),
        ("ä¸Šä¸‹æ–‡åˆ†æé€»è¾‘", test_context_analyzer_logic),
        ("å¢å¼ºæ¨ç†é€»è¾‘", test_enhanced_reasoning_logic),
        ("ç›‘æ§é€»è¾‘", test_monitoring_logic)
    ]
    
    for test_name, test_func in tests:
        try:
            success = await test_func()
            test_results[test_name] = success
        except Exception as e:
            print(f"âŒ {test_name} æµ‹è¯•å‡ºç°å¼‚å¸¸: {e}")
            test_results[test_name] = False
    
    # è¾“å‡ºæµ‹è¯•æ€»ç»“
    print("\n" + "="*60)
    print("ğŸ” æœåŠ¡å±‚é€»è¾‘æµ‹è¯•ç»“æœæ€»ç»“")
    print("="*60)
    
    passed = 0
    failed = 0
    
    for test_name, success in test_results.items():
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"{test_name:<25} {status}")
        if success:
            passed += 1
        else:
            failed += 1
    
    total = len(test_results)
    success_rate = (passed / total) * 100 if total > 0 else 0
    
    print("-" * 60)
    print(f"æ€»æµ‹è¯•æ•°: {total}")
    print(f"é€šè¿‡: {passed}")  
    print(f"å¤±è´¥: {failed}")
    print(f"æˆåŠŸç‡: {success_rate:.1f}%")
    
    if success_rate >= 90:
        print("\nğŸ‰ æ‰€æœ‰æœåŠ¡æ ¸å¿ƒé€»è¾‘å®ç°æ­£ç¡®ï¼")
        print("âœ¨ ç³»ç»Ÿå·²å…·å¤‡å®Œæ•´çš„15ä¸ªAIå·¥å…·åŠŸèƒ½")
        print("ğŸ”§ æ ¸å¿ƒç®—æ³•å’Œä¸šåŠ¡é€»è¾‘è¿è¡Œè‰¯å¥½")
    elif success_rate >= 70:
        print("\nâœ¨ å¤§éƒ¨åˆ†æœåŠ¡é€»è¾‘å®ç°æˆåŠŸ")
        print("ğŸ”§ å»ºè®®å¯¹å¤±è´¥çš„æœåŠ¡é€»è¾‘è¿›è¡Œä¼˜åŒ–")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æœåŠ¡é€»è¾‘éœ€è¦è¿›ä¸€æ­¥å®Œå–„")
        print("ğŸ› ï¸  å»ºè®®é‡ç‚¹æ£€æŸ¥æ ¸å¿ƒç®—æ³•å®ç°")
    
    return success_rate >= 70


if __name__ == "__main__":
    asyncio.run(main())